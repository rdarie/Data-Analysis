Timer unit: 1e-07 s

Total time: 9.43935 s
File: C\../../analysis-code/preprocNS5.py
Function: preprocNS5 at line 50

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    50                                           @profile
    51                                           def preprocNS5():
    52                                               # weird scope issue with ns5FileName in particular
    53         1         84.0     84.0      0.0      ns5FileName = allOpts['ns5FileName']
    54         1         87.0     87.0      0.0      arrayName = arguments['arrayName']
    55         1         75.0     75.0      0.0      if arguments['arrayName'] != 'Block':
    56                                                   electrodeMapPath = spikeSortingOpts[arrayName]['electrodeMapPath']
    57                                                   mapExt = electrodeMapPath.split('.')[-1]
    58                                                   if mapExt == 'cmp':
    59                                                       mapDF = prb_meta.cmpToDF(electrodeMapPath)
    60                                                   elif mapExt == 'map':
    61                                                       mapDF = prb_meta.mapToDF(electrodeMapPath)
    62                                                   if 'rawBlockName' in spikeSortingOpts[arrayName]:
    63                                                       ns5FileName = ns5FileName.replace(
    64                                                           'Block', spikeSortingOpts[arrayName]['rawBlockName'])
    65         1        592.0    592.0      0.0      idealDataPath = os.path.join(nspFolder, ns5FileName + '.ns5')
    66         1       4176.0   4176.0      0.0      if not os.path.exists(idealDataPath):
    67                                                   fallBackPath = os.path.join(
    68                                                       nspFolder,
    69                                                       '{}{:0>4}'.format(arrayName, blockIdx) + '.ns5')
    70                                                   print('{} not found;\nFalling back to {}'.format(
    71                                                       idealDataPath, fallBackPath
    72                                                   ))
    73                                                   if os.path.exists(fallBackPath):
    74                                                       shutil.move(
    75                                                           fallBackPath,
    76                                                           idealDataPath)
    77                                                       try:
    78                                                           shutil.move(
    79                                                               fallBackPath.replace('.ns5', '.nev'),
    80                                                               idealDataPath.replace('.ns5', '.nev'))
    81                                                       except Exception:
    82                                                           traceback.print_exc()
    83                                                           print('Ignoring exception...')
    84                                           
    85         1        126.0    126.0      0.0      if arguments['chunkSize'] is not None:
    86                                                   chunkSize = int(arguments['chunkSize'])
    87                                               else:
    88         1         93.0     93.0      0.0          chunkSize = 4000
    89         1         85.0     85.0      0.0      chunkList = None
    90         1         82.0     82.0      0.0      equalChunks = False
    91                                               ###############################################################
    92         1         81.0     81.0      0.0      groupAsigsByBank = True
    93                                               # pdb.set_trace()
    94         1         82.0     82.0      0.0      if groupAsigsByBank:
    95         1         84.0     84.0      0.0          try:
    96         1       1402.0   1402.0      0.0              print('Rewriting list of asigs that will be processed')
    97         1         84.0     84.0      0.0              asigNameListByBank = []
    98                                                       # spikeSortingOpts[arrayName]['asigNameList'] = []
    99         1        134.0    134.0      0.0              for name, group in mapDF.groupby('bank'):
   100                                                           allAsigsInBank = sorted(group['label'].to_list())
   101                                                           theseAsigNames = [
   102                                                               aName
   103                                                               for aName in allAsigsInBank
   104                                                               if aName not in spikeSortingOpts[arrayName]['excludeChans']
   105                                                               ]
   106                                                           asigNameListByBank.append(theseAsigNames)
   107                                                           # spikeSortingOpts[arrayName]['asigNameList'].append(theseAsigNames)
   108                                                           print(theseAsigNames)
   109         1         96.0     96.0      0.0          except Exception:
   110         1         88.0     88.0      0.0              asigNameListByBank = None
   111                                               ###############################################################
   112         1         91.0     91.0      0.0      if arguments['maskMotorEncoder']:
   113                                                   try:
   114                                                       motorEncoderMask = motorEncoderBoundsLookup[int(arguments['blockIdx'])]
   115                                                   except Exception:
   116                                                       traceback.print_exc()
   117                                                       try:
   118                                                           motorEncoderMask = alignTimeBoundsLookup[int(arguments['blockIdx'])]
   119                                                       except Exception:
   120                                                           traceback.print_exc()
   121                                                           motorEncoderMask = None
   122                                               else:
   123         1         82.0     82.0      0.0          motorEncoderMask = None
   124                                               ###############################################################
   125                                               #
   126         1         87.0     87.0      0.0      if arguments['rippleNForm']:
   127                                                   analogInputNames = sorted(
   128                                                       trialFilesFrom['utah']['eventInfo']['inputIDs'].values())
   129                                                   # pdb.set_trace()
   130                                                   ns5.preproc(
   131                                                       fileName=ns5FileName,
   132                                                       rawFolderPath=nspFolder,
   133                                                       outputFolderPath=scratchFolder, mapDF=mapDF,
   134                                                       fillOverflow=False, removeJumps=False, electrodeArrayName=arrayName,
   135                                                       motorEncoderMask=motorEncoderMask,
   136                                                       calcAverageLFP=True, removeMeanAcross=True,
   137                                                       eventInfo=trialFilesFrom['utah']['eventInfo'],
   138                                                       asigNameList=spikeSortingOpts[arrayName]['asigNameList'],
   139                                                       ainpNameList=spikeSortingOpts[arrayName]['ainpNameList'],
   140                                                       spikeSourceType='tdc', writeMode='ow',
   141                                                       chunkSize=chunkSize, equalChunks=equalChunks, chunkList=chunkList,
   142                                                       calcRigEvents=False)
   143                                               #
   144         1         86.0     86.0      0.0      if arguments['forSpikeSorting']:
   145                                                   print('\n\nPreprocNs5, generating spike preview...\n\n')
   146                                                   if asigNameListByBank is not None:
   147                                                       theseAsigNames = asigNameListByBank
   148                                                   else:
   149                                                       theseAsigNames = spikeSortingOpts[arrayName]['asigNameList']
   150                                                   ns5.preproc(
   151                                                       fileName=ns5FileName,
   152                                                       rawFolderPath=nspFolder,
   153                                                       outputFolderPath=scratchFolder, mapDF=mapDF,
   154                                                       fillOverflow=False, removeJumps=False,
   155                                                       calcOutliers=spikeSortingOpts[arrayName]['interpolateOutliers'],
   156                                                       interpolateOutliers=spikeSortingOpts[arrayName]['interpolateOutliers'],
   157                                                       outlierThreshold=spikeSortingOpts[arrayName]['outlierThreshold'],
   158                                                       outlierMaskFilterOpts=outlierMaskFilterOpts,
   159                                                       motorEncoderMask=motorEncoderMask,
   160                                                       calcAverageLFP=True,
   161                                                       eventInfo=trialFilesFrom['utah']['eventInfo'],
   162                                                       asigNameList=theseAsigNames,
   163                                                       ainpNameList=[],
   164                                                       spikeSourceType='',
   165                                                       removeMeanAcross=True,
   166                                                       linearDetrend=True,
   167                                                       nameSuffix='_spike_preview',
   168                                                       LFPFilterOpts=spikeSortingFilterOpts,
   169                                                       # LFPFilterOpts=None,
   170                                                       writeMode='ow',
   171                                                       chunkSize=spikeSortingOpts[arrayName]['previewDuration'],
   172                                                       chunkOffset=spikeSortingOpts[arrayName]['previewOffset'],
   173                                                       equalChunks=False, chunkList=[0],
   174                                                       calcRigEvents=False, outlierRemovalDebugFlag=False)
   175                                               #
   176         1         87.0     87.0      0.0      if arguments['fullSubtractMean']:
   177                                                   print('\n\nPreprocNs5, generating spike extraction data...\n\n')
   178                                                   if asigNameListByBank is not None:
   179                                                       theseAsigNames = asigNameListByBank
   180                                                   else:
   181                                                       theseAsigNames = spikeSortingOpts[arrayName]['asigNameList']
   182                                                   ns5.preproc(
   183                                                       fileName=ns5FileName,
   184                                                       rawFolderPath=nspFolder,
   185                                                       outputFolderPath=scratchFolder, mapDF=mapDF,
   186                                                       fillOverflow=False, removeJumps=False,
   187                                                       calcOutliers=spikeSortingOpts[arrayName]['interpolateOutliers'],
   188                                                       interpolateOutliers=False,
   189                                                       outlierThreshold=spikeSortingOpts[arrayName]['outlierThreshold'],
   190                                                       outlierMaskFilterOpts=outlierMaskFilterOpts,
   191                                                       motorEncoderMask=motorEncoderMask,
   192                                                       calcAverageLFP=True,
   193                                                       removeMeanAcross=True,
   194                                                       linearDetrend=True,
   195                                                       eventInfo=trialFilesFrom['utah']['eventInfo'],
   196                                                       asigNameList=theseAsigNames,
   197                                                       ainpNameList=[],
   198                                                       spikeSourceType='',
   199                                                       nameSuffix='_mean_subtracted',
   200                                                       LFPFilterOpts=spikeSortingFilterOpts,
   201                                                       #
   202                                                       writeMode='ow',
   203                                                       chunkSize=chunkSize, equalChunks=equalChunks, chunkList=chunkList,
   204                                                       calcRigEvents=False)
   205                                               #
   206         1         86.0     86.0      0.0      if arguments['fullSubtractMeanUnfiltered']:
   207                                                   print('\n\nPreprocNs5, generating lfp data...\n\n')
   208                                                   theseAsigNames = [mapDF['label'].iloc[::10].to_list()]
   209                                                   ns5.preproc(
   210                                                       fileName=ns5FileName,
   211                                                       rawFolderPath=nspFolder,
   212                                                       outputFolderPath=scratchFolder, mapDF=mapDF,
   213                                                       fillOverflow=False, removeJumps=False,
   214                                                       interpolateOutliers=False, calcOutliers=True,
   215                                                       outlierThreshold=spikeSortingOpts[arrayName]['outlierThreshold'],
   216                                                       outlierMaskFilterOpts=outlierMaskFilterOpts,
   217                                                       motorEncoderMask=motorEncoderMask,
   218                                                       calcAverageLFP=True,
   219                                                       removeMeanAcross=True,
   220                                                       linearDetrend=False,
   221                                                       eventInfo=trialFilesFrom['utah']['eventInfo'],
   222                                                       asigNameList=spikeSortingOpts[arrayName]['asigNameList'],
   223                                                       ainpNameList=[],
   224                                                       spikeSourceType='',
   225                                                       nameSuffix='',
   226                                                       LFPFilterOpts=None,
   227                                                       writeMode='ow',
   228                                                       chunkSize=chunkSize, equalChunks=equalChunks, chunkList=chunkList,
   229                                                       calcRigEvents=False)
   230                                               #
   231         1         88.0     88.0      0.0      if arguments['fullUnfiltered']:
   232                                                   print('\n\nPreprocNs5, generating lfp data...\n\n')
   233                                                   ns5.preproc(
   234                                                       fileName=ns5FileName,
   235                                                       rawFolderPath=nspFolder,
   236                                                       outputFolderPath=scratchFolder, mapDF=mapDF,
   237                                                       fillOverflow=False, removeJumps=False,
   238                                                       outlierThreshold=spikeSortingOpts[arrayName]['outlierThreshold'],
   239                                                       outlierMaskFilterOpts=outlierMaskFilterOpts,
   240                                                       motorEncoderMask=motorEncoderMask,
   241                                                       calcAverageLFP=False,
   242                                                       removeMeanAcross=False,
   243                                                       linearDetrend=False,
   244                                                       interpolateOutliers=False, calcOutliers=False,
   245                                                       normalizeByImpedance=False,
   246                                                       impedanceFilePath=os.path.join(
   247                                                           remoteBasePath,
   248                                                           '{}_blackrock_impedances.h5'.format(subjectName)),
   249                                                       eventInfo=trialFilesFrom['utah']['eventInfo'],
   250                                                       asigNameList=spikeSortingOpts[arrayName]['asigNameList'],
   251                                                       ainpNameList=[],
   252                                                       spikeSourceType='',
   253                                                       nameSuffix='',
   254                                                       LFPFilterOpts=None,
   255                                                       writeMode='ow',
   256                                                       chunkSize=chunkSize, equalChunks=equalChunks, chunkList=chunkList,
   257                                                       calcRigEvents=False)
   258                                               #
   259         1         82.0     82.0      0.0      if arguments['analogOnly']:
   260                                                   analogInputNames = sorted(
   261                                                       trialFilesFrom['utah']['eventInfo']['inputIDs'].values())
   262                                                   theseAsigNames = [mapDF['label'].iloc[::2].to_list()]
   263                                                   print('\n\nPreprocNs5, generating rig inputs and other analog data...\n\n')
   264                                                   ns5.preproc(
   265                                                       fileName=ns5FileName,
   266                                                       rawFolderPath=nspFolder,
   267                                                       outputFolderPath=scratchFolder, mapDF=mapDF,
   268                                                       fillOverflow=False, removeJumps=False,
   269                                                       interpolateOutliers=False, calcOutliers=False,
   270                                                       calcArtifactTrace=True,
   271                                                       # outlierThreshold=spikeSortingOpts[arrayName]['outlierThreshold'],
   272                                                       # outlierMaskFilterOpts=outlierMaskFilterOpts,
   273                                                       motorEncoderMask=motorEncoderMask,
   274                                                       eventInfo=trialFilesFrom['utah']['eventInfo'],
   275                                                       asigNameList=theseAsigNames,
   276                                                       saveFromAsigNameList=False,
   277                                                       calcAverageLFP=True,
   278                                                       LFPFilterOpts=stimArtifactFilterOpts,
   279                                                       ainpNameList=analogInputNames,
   280                                                       spikeSourceType='',
   281                                                       nameSuffix='_analog_inputs', writeMode='ow',
   282                                                       chunkSize=9999,
   283                                                       calcRigEvents=trialFilesFrom['utah']['calcRigEvents'])
   284                                               #
   285         1         90.0     90.0      0.0      if arguments['fullSubtractMeanWithSpikes']:
   286                                                   spikePath = os.path.join(
   287                                                       scratchFolder, 'tdc_' + ns5FileName + '_mean_subtracted',
   288                                                       'tdc_' + ns5FileName + '_mean_subtracted' + '.nix'
   289                                                       )
   290                                                   ns5.preproc(
   291                                                       fileName=ns5FileName,
   292                                                       rawFolderPath=nspFolder,
   293                                                       outputFolderPath=scratchFolder, mapDF=mapDF,
   294                                                       # swapMaps=None,
   295                                                       fillOverflow=False, removeJumps=False,
   296                                                       motorEncoderMask=motorEncoderMask,
   297                                                       calcAverageLFP=True,
   298                                                       eventInfo=trialFilesFrom['utah']['eventInfo'],
   299                                                       asigNameList=spikeSortingOpts[arrayName]['asigNameList'],
   300                                                       ainpNameList=[],
   301                                                       removeMeanAcross=True,
   302                                                       LFPFilterOpts=None,
   303                                                       outlierMaskFilterOpts=outlierMaskFilterOpts,
   304                                                       nameSuffix='',
   305                                                       spikeSourceType='tdc', spikePath=spikePath,
   306                                                       #
   307                                                       writeMode='ow',
   308                                                       chunkSize=chunkSize, equalChunks=equalChunks, chunkList=chunkList,
   309                                                       calcRigEvents=trialFilesFrom['utah']['calcRigEvents'])
   310                                               ###############################################################################
   311         1        184.0    184.0      0.0      if arguments['ISI'] or arguments['ISIRaw'] or arguments['ISIMinimal']:
   312         1    5133338.0 5133338.0      5.4          mapDF = prb_meta.mapToDF(rippleMapFile[int(arguments['blockIdx'])])
   313                                                   # if 'rippleOriginalMapFile' in expOpts:
   314                                                   #     rippleOriginalMapFile = expOpts['rippleOriginalMapFile']
   315                                                   #     if rippleOriginalMapFile[int(arguments['blockIdx'])] is not None:
   316                                                   #         swapMaps = {
   317                                                   #             'from': prb_meta.mapToDF(rippleOriginalMapFile[int(arguments['blockIdx'])]),
   318                                                   #             'to': mapDF
   319                                                   #         }
   320                                                   #     else:
   321                                                   #         swapMaps = None
   322                                                   # else:
   323                                                   #     swapMaps = None
   324         1         44.0     44.0      0.0      if arguments['ISI']:
   325                                                   ns5.preproc(
   326                                                       fileName=ns5FileName,
   327                                                       rawFolderPath=nspFolder,
   328                                                       outputFolderPath=scratchFolder, mapDF=mapDF,
   329                                                       # swapMaps=swapMaps,
   330                                                       fillOverflow=False, removeJumps=False,
   331                                                       motorEncoderMask=motorEncoderMask,
   332                                                       eventInfo=trialFilesFrom['utah']['eventInfo'],
   333                                                       spikeSourceType='nev', writeMode='ow',
   334                                                       chunkSize=chunkSize, equalChunks=equalChunks,
   335                                                       chunkList=chunkList,
   336                                                       calcRigEvents=trialFilesFrom['utah']['calcRigEvents'],
   337                                                       normalizeByImpedance=False, removeMeanAcross=False,
   338                                                       asigNameList=asigNameList, ainpNameList=ainpNameList,
   339                                                       # LFPFilterOpts=LFPFilterOpts,
   340                                                       LFPFilterOpts=None,
   341                                                       calcAverageLFP=True)
   342                                                   if arguments['transferISIStimLog']:
   343                                                       try:
   344                                                           jsonSrcPath = os.path.join(nspFolder, ns5FileName + '_autoStimLog.json')
   345                                                           jsonDestPath = trialBasePath.replace('.nix', '_autoStimLog.json')
   346                                                           shutil.copyfile(jsonSrcPath, jsonDestPath)
   347                                                       except Exception:
   348                                                           traceback.print_exc()
   349         1         30.0     30.0      0.0      if arguments['ISIMinimal']:
   350         1         34.0     34.0      0.0          ns5.preproc(
   351         1         24.0     24.0      0.0              fileName=ns5FileName,
   352         1         26.0     26.0      0.0              rawFolderPath=nspFolder,
   353         1         25.0     25.0      0.0              outputFolderPath=scratchFolder,
   354         1         24.0     24.0      0.0              mapDF=mapDF,
   355                                                       #swapMaps=swapMaps,
   356         1         24.0     24.0      0.0              fillOverflow=False, removeJumps=False,
   357         1         24.0     24.0      0.0              motorEncoderMask=motorEncoderMask,
   358         1         35.0     35.0      0.0              eventInfo=trialFilesFrom['utah']['eventInfo'],
   359         1         24.0     24.0      0.0              spikeSourceType='nev', writeMode='ow',
   360         1         24.0     24.0      0.0              chunkSize=chunkSize, equalChunks=equalChunks,
   361         1         24.0     24.0      0.0              chunkList=chunkList,
   362         1         25.0     25.0      0.0              calcRigEvents=trialFilesFrom['utah']['calcRigEvents'],
   363         1         24.0     24.0      0.0              normalizeByImpedance=False, removeMeanAcross=False,
   364         1         26.0     26.0      0.0              asigNameList=[], ainpNameList=ainpNameList,
   365                                                       # LFPFilterOpts=LFPFilterOpts,
   366         1   88995422.0 88995422.0     94.3              LFPFilterOpts=None, calcAverageLFP=False)
   367         1         54.0     54.0      0.0          if arguments['transferISIStimLog']:
   368         1         37.0     37.0      0.0              try:
   369         1        302.0    302.0      0.0                  jsonSrcPath = os.path.join(nspFolder, ns5FileName + '_autoStimLog.json')
   370         1         54.0     54.0      0.0                  jsonDestPath = trialBasePath.replace('.nix', '_autoStimLog.json')
   371         1     255261.0 255261.0      0.3                  shutil.copyfile(jsonSrcPath, jsonDestPath)
   372                                                       except Exception:
   373                                                           traceback.print_exc()
   374                                               ##################################################################################
   375         1        134.0    134.0      0.0      if arguments['ISIRaw']:
   376                                                   ns5.preproc(
   377                                                       fileName=ns5FileName,
   378                                                       rawFolderPath=nspFolder,
   379                                                       outputFolderPath=scratchFolder, mapDF=None,
   380                                                       fillOverflow=False, removeJumps=False,
   381                                                       motorEncoderMask=motorEncoderMask,
   382                                                       eventInfo=trialFilesFrom['utah']['eventInfo'],
   383                                                       spikeSourceType='nev', writeMode='ow',
   384                                                       chunkSize=chunkSize, equalChunks=equalChunks,
   385                                                       chunkList=chunkList,
   386                                                       calcRigEvents=trialFilesFrom['utah']['calcRigEvents'],
   387                                                       normalizeByImpedance=False, removeMeanAcross=False,
   388                                                       asigNameList=None, ainpNameList=None, nameSuffix='_raw',
   389                                                       LFPFilterOpts=LFPFilterOpts, calcAverageLFP=True)
   390         1         72.0     72.0      0.0      return

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: analogSignalsToDataFrame at line 43

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    43                                           @profile
    44                                           def analogSignalsToDataFrame(
    45                                                   analogsignals, idxT='t', useChanNames=False):
    46                                               asigList = []
    47                                               for asig in analogsignals:
    48                                                   if asig.shape[1] == 1:
    49                                                       if useChanNames:
    50                                                           colNames = [str(asig.channel_index.name)]
    51                                                       else:
    52                                                           colNames = [str(asig.name)]
    53                                                   else:
    54                                                       colNames = [
    55                                                           asig.name +
    56                                                           '_{}'.format(i) for i in
    57                                                           asig.channel_index.channel_ids
    58                                                           ]
    59                                                   asigList.append(
    60                                                       pd.DataFrame(
    61                                                           asig.magnitude, columns=colNames,
    62                                                           index=range(asig.shape[0])))
    63                                               asigList.append(
    64                                                   pd.DataFrame(
    65                                                       asig.times.magnitude, columns=[idxT],
    66                                                       index=range(asig.shape[0])))
    67                                               return pd.concat(asigList, axis=1)

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: listChanNames at line 69

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    69                                           @profile
    70                                           def listChanNames(
    71                                                   dataBlock, chanQuery,
    72                                                   objType=AnalogSignalProxy, condition=None):
    73                                               allChanList = [
    74                                                   i.name
    75                                                   for i in dataBlock.filter(objects=objType)]
    76                                               if condition == 'hasAsigs':
    77                                                   allChanList = [
    78                                                       i
    79                                                       for i in allChanList
    80                                                       if len(dataBlock.filter(objects=objType, name=i)[0].analogsignals)
    81                                                   ]
    82                                               chansToTrigger = pd.DataFrame(
    83                                                   np.unique(allChanList),
    84                                                   columns=['chanName'])
    85                                               if chanQuery is not None:
    86                                                   chansToTrigger = chansToTrigger.query(
    87                                                       chanQuery, engine='python')['chanName'].to_list()
    88                                               else:
    89                                                   chansToTrigger = chansToTrigger['chanName'].to_list()
    90                                               return chansToTrigger

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: spikeDictToSpikeTrains at line 92

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    92                                           @profile
    93                                           def spikeDictToSpikeTrains(
    94                                                   spikes, block=None, seg=None,
    95                                                   probeName='insTD', t_stop=None,
    96                                                   waveformUnits=pq.uV,
    97                                                   sampling_rate=3e4 * pq.Hz):
    98                                           
    99                                               if block is None:
   100                                                   assert seg is None
   101                                                   block = Block()
   102                                                   seg = Segment(name=probeName + ' segment')
   103                                                   block.segments.append(seg)
   104                                           
   105                                               if t_stop is None:
   106                                                   t_stop = hf.getLastSpikeTime(spikes) + 1
   107                                           
   108                                               for idx, chanName in enumerate(spikes['ChannelID']):
   109                                                   #  unique units on this channel
   110                                                   unitsOnThisChan = pd.unique(spikes['Classification'][idx])
   111                                                   nixChanName = probeName + '{}'.format(chanName)
   112                                                   chanIdx = ChannelIndex(
   113                                                       name=nixChanName,
   114                                                       index=np.asarray([idx]),
   115                                                       channel_names=np.asarray([nixChanName]))
   116                                                   block.channel_indexes.append(chanIdx)
   117                                                   
   118                                                   for unitIdx, unitName in enumerate(unitsOnThisChan):
   119                                                       unitMask = spikes['Classification'][idx] == unitName
   120                                                       # this unit's spike timestamps
   121                                                       theseTimes = spikes['TimeStamps'][idx][unitMask]
   122                                                       # this unit's waveforms
   123                                                       if len(spikes['Waveforms'][idx].shape) == 3:
   124                                                           theseWaveforms = spikes['Waveforms'][idx][unitMask, :, :]
   125                                                           theseWaveforms = np.swapaxes(theseWaveforms, 1, 2)
   126                                                       elif len(spikes['Waveforms'][idx].shape) == 2:
   127                                                           theseWaveforms = (
   128                                                               spikes['Waveforms'][idx][unitMask, np.newaxis, :])
   129                                                       else:
   130                                                           raise(Exception('spikes[Waveforms] has bad shape'))
   131                                           
   132                                                       unitName = '{}#{}'.format(nixChanName, unitIdx)
   133                                                       unit = Unit(name=unitName)
   134                                                       unit.channel_index = chanIdx
   135                                                       chanIdx.units.append(unit)
   136                                           
   137                                                       train = SpikeTrain(
   138                                                           times=theseTimes, t_stop=t_stop, units='sec',
   139                                                           name=unitName, sampling_rate=sampling_rate,
   140                                                           waveforms=theseWaveforms*waveformUnits,
   141                                                           left_sweep=0, dtype=np.float32)
   142                                                       unit.spiketrains.append(train)
   143                                                       seg.spiketrains.append(train)
   144                                           
   145                                                       unit.create_relationship()
   146                                                   chanIdx.create_relationship()
   147                                               seg.create_relationship()
   148                                               block.create_relationship()
   149                                               return block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: spikeTrainsToSpikeDict at line 151

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   151                                           @profile
   152                                           def spikeTrainsToSpikeDict(
   153                                                   spiketrains):
   154                                               nCh = len(spiketrains)
   155                                               spikes = {
   156                                                   'ChannelID': [i for i in range(nCh)],
   157                                                   'Classification': [np.asarray([]) for i in range(nCh)],
   158                                                   'NEUEVWAV_HeaderIndices': [None for i in range(nCh)],
   159                                                   'TimeStamps': [np.asarray([]) for i in range(nCh)],
   160                                                   'Units': 'uV',
   161                                                   'Waveforms': [np.asarray([]) for i in range(nCh)],
   162                                                   'basic_headers': {'TimeStampResolution': 3e4},
   163                                                   'extended_headers': []
   164                                                   }
   165                                               for idx, st in enumerate(spiketrains):
   166                                                   spikes['ChannelID'][idx] = st.name
   167                                                   if len(spikes['TimeStamps'][idx]):
   168                                                       spikes['TimeStamps'][idx] = np.stack((
   169                                                           spikes['TimeStamps'][idx],
   170                                                           st.times.magnitude), axis=-1)
   171                                                   else:
   172                                                       spikes['TimeStamps'][idx] = st.times.magnitude
   173                                                   
   174                                                   theseWaveforms = np.swapaxes(
   175                                                       st.waveforms, 1, 2)
   176                                                   theseWaveforms = np.atleast_2d(np.squeeze(
   177                                                       theseWaveforms))
   178                                                       
   179                                                   if len(spikes['Waveforms'][idx]):
   180                                                       spikes['Waveforms'][idx] = np.stack((
   181                                                           spikes['Waveforms'][idx],
   182                                                           theseWaveforms.magnitude), axis=-1)
   183                                                   else:
   184                                                       spikes['Waveforms'][idx] = theseWaveforms.magnitude
   185                                                   
   186                                                   classVals = st.times.magnitude ** 0 * idx
   187                                                   if len(spikes['Classification'][idx]):
   188                                                       spikes['Classification'][idx] = np.stack((
   189                                                           spikes['Classification'][idx],
   190                                                           classVals), axis=-1)
   191                                                   else:
   192                                                       spikes['Classification'][idx] = classVals
   193                                               return spikes

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: channelIndexesToSpikeDict at line 195

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   195                                           @profile
   196                                           def channelIndexesToSpikeDict(
   197                                                   channel_indexes):
   198                                               nCh = len(channel_indexes)
   199                                               spikes = {
   200                                                   'ChannelID': [i for i in range(nCh)],
   201                                                   'Classification': [np.asarray([]) for i in range(nCh)],
   202                                                   'NEUEVWAV_HeaderIndices': [None for i in range(nCh)],
   203                                                   'TimeStamps': [np.asarray([]) for i in range(nCh)],
   204                                                   'Units': 'uV',
   205                                                   'Waveforms': [np.asarray([]) for i in range(nCh)],
   206                                                   'basic_headers': {'TimeStampResolution': 3e4},
   207                                                   'extended_headers': []
   208                                                   }
   209                                               #  allocate fields for annotations
   210                                               for dummyCh in channel_indexes:
   211                                                   if len(dummyCh.units):
   212                                                       dummyUnit = dummyCh.units[0]
   213                                                       if len(dummyUnit.spiketrains):
   214                                                           if len(dummyUnit.spiketrains[0].times):
   215                                                               break
   216                                               dummySt = [
   217                                                   st
   218                                                   for st in dummyUnit.spiketrains
   219                                                   if len(st.times)][0]
   220                                               #  allocate fields for array annotations (per spike)
   221                                               if dummySt.array_annotations:
   222                                                   for key in dummySt.array_annotations.keys():
   223                                                       spikes.update({key: [np.asarray([]) for i in range(nCh)]})
   224                                                   
   225                                               maxUnitIdx = 0
   226                                               for idx, chIdx in enumerate(channel_indexes):
   227                                                   spikes['ChannelID'][idx] = chIdx.name
   228                                                   for unitIdx, thisUnit in enumerate(chIdx.units):
   229                                                       for stIdx, st in enumerate(thisUnit.spiketrains):
   230                                                           if not len(st.times):
   231                                                               continue
   232                                                           #  print(
   233                                                           #      'unit {} has {} spiketrains'.format(
   234                                                           #          thisUnit.name,
   235                                                           #          len(thisUnit.spiketrains)))
   236                                                           if len(spikes['TimeStamps'][idx]):
   237                                                               spikes['TimeStamps'][idx] = np.concatenate((
   238                                                                   spikes['TimeStamps'][idx],
   239                                                                   st.times.magnitude), axis=0)
   240                                                           else:
   241                                                               spikes['TimeStamps'][idx] = st.times.magnitude
   242                                                           #  reshape waveforms to comply with BRM convention
   243                                                           theseWaveforms = np.swapaxes(
   244                                                               st.waveforms, 1, 2)
   245                                                           theseWaveforms = np.atleast_2d(np.squeeze(
   246                                                               theseWaveforms))
   247                                                           #  append waveforms
   248                                                           if len(spikes['Waveforms'][idx]):
   249                                                               try:
   250                                                                   spikes['Waveforms'][idx] = np.concatenate((
   251                                                                       spikes['Waveforms'][idx],
   252                                                                       theseWaveforms.magnitude), axis=0)
   253                                                               except Exception:
   254                                                                   traceback.print_exc()
   255                                                           else:
   256                                                               spikes['Waveforms'][idx] = theseWaveforms.magnitude
   257                                                           #  give each unit a global index
   258                                                           classVals = st.times.magnitude ** 0 * maxUnitIdx
   259                                                           st.array_annotations.update({'Classification': classVals})
   260                                                           #  expand array_annotations into spikes dict
   261                                                           for key, value in st.array_annotations.items():
   262                                                               if len(spikes[key][idx]):
   263                                                                   spikes[key][idx] = np.concatenate((
   264                                                                       spikes[key][idx],
   265                                                                       value), axis=0)
   266                                                               else:
   267                                                                   spikes[key][idx] = value
   268                                                           for key, value in st.annotations.items():
   269                                                               if key not in spikes['basic_headers']:
   270                                                                   spikes['basic_headers'].update({key: {}})
   271                                                               try:
   272                                                                   spikes['basic_headers'][key].update({maxUnitIdx: value})
   273                                                               except Exception:
   274                                                                   pass
   275                                                           maxUnitIdx += 1
   276                                               return spikes

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: unitSpikeTrainArrayAnnToDF at line 278

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   278                                           @profile
   279                                           def unitSpikeTrainArrayAnnToDF(
   280                                                   spikeTrainContainer):
   281                                               #  list contains different segments
   282                                               if isinstance(spikeTrainContainer, ChannelIndex):
   283                                                   assert len(spikeTrainContainer.units) == 0
   284                                                   spiketrains = spikeTrainContainer.units[0].spiketrains
   285                                               elif isinstance(spikeTrainContainer, Unit):
   286                                                   spiketrains = spikeTrainContainer.spiketrains
   287                                               elif isinstance(spikeTrainContainer, list):
   288                                                   spiketrains = spikeTrainContainer
   289                                               fullAnnotationsDict = {}
   290                                               for segIdx, st in enumerate(spiketrains):
   291                                                   theseAnnDF = pd.DataFrame(st.array_annotations)
   292                                                   theseAnnDF['t'] = st.times.magnitude
   293                                                   fullAnnotationsDict.update({segIdx: theseAnnDF})
   294                                               annotationsDF = pd.concat(
   295                                                   fullAnnotationsDict, names=['segment', 'index'], sort=True)
   296                                               return annotationsDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getSpikeDFMetadata at line 298

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   298                                           @profile
   299                                           def getSpikeDFMetadata(spikeDF, metaDataCols):
   300                                               spikeDF.reset_index(inplace=True)
   301                                               metaDataCols = np.atleast_1d(metaDataCols)
   302                                               spikeDF.index.name = 'metaDataIdx'
   303                                               metaDataDF = spikeDF.loc[:, metaDataCols].copy()
   304                                               newSpikeDF = spikeDF.drop(columns=metaDataCols).reset_index()
   305                                               return newSpikeDF, metaDataDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: transposeSpikeDF at line 307

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   307                                           @profile
   308                                           def transposeSpikeDF(
   309                                                   spikeDF, transposeToColumns,
   310                                                   fastTranspose=False):
   311                                               newColumnNames = np.atleast_1d(transposeToColumns).tolist()
   312                                               originalColumnNames = np.atleast_1d(spikeDF.columns.names)
   313                                               metaDataCols = np.setdiff1d(spikeDF.index.names, newColumnNames).tolist()
   314                                               if fastTranspose:
   315                                                   #  fast but memory inefficient
   316                                                   return spikeDF.stack().unstack(transposeToColumns)
   317                                               else:
   318                                                   raise(Warning('Caution! transposeSpikeDF might not be working, needs testing RD 06252019'))
   319                                                   #  stash annotations, transpose, recover annotations
   320                                                   newSpikeDF, metaDataDF = getSpikeDFMetadata(spikeDF, metaDataCols)
   321                                                   del spikeDF
   322                                                   gc.collect()
   323                                                   #
   324                                                   newSpikeDF = newSpikeDF.stack().unstack(newColumnNames)
   325                                                   newSpikeDF.reset_index(inplace=True)
   326                                                   #  set the index
   327                                                   newIdxLabels = np.concatenate(
   328                                                       [originalColumnNames, metaDataCols]).tolist()
   329                                                   newSpikeDF.loc[:, metaDataCols] = (
   330                                                       metaDataDF
   331                                                       .loc[newSpikeDF['metaDataIdx'].to_list(), metaDataCols]
   332                                                       .to_numpy())
   333                                                   newSpikeDF = (
   334                                                       newSpikeDF
   335                                                       .drop(columns=['metaDataIdx'])
   336                                                       .set_index(newIdxLabels))
   337                                                   return newSpikeDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: concatenateBlocks at line 339

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   339                                           @profile
   340                                           def concatenateBlocks(
   341                                                   asigBlocks, spikeBlocks, eventBlocks, chunkingMetadata,
   342                                                   samplingRate, chanQuery, lazy, trackMemory, verbose
   343                                                   ):
   344                                               # Scan ahead through all files and ensure that
   345                                               # spikeTrains and units are present across all assembled files
   346                                               channelIndexCache = {}
   347                                               unitCache = {}
   348                                               asigCache = []
   349                                               asigAnnCache = {}
   350                                               spiketrainCache = {}
   351                                               eventCache = {}
   352                                               # get list of channels and units
   353                                               for idx, (chunkIdxStr, chunkMeta) in enumerate(chunkingMetadata.items()):
   354                                                   gc.collect()
   355                                                   chunkIdx = int(chunkIdxStr)
   356                                                   asigBlock = asigBlocks[chunkIdx]
   357                                                   asigSeg = asigBlock.segments[0]
   358                                                   spikeBlock = spikeBlocks[chunkIdx]
   359                                                   eventBlock = eventBlocks[chunkIdx]
   360                                                   eventSeg = eventBlock.segments[0]
   361                                                   for chIdx in asigBlock.filter(objects=ChannelIndex):
   362                                                       chAlreadyThere = (chIdx.name in channelIndexCache.keys())
   363                                                       if not chAlreadyThere:
   364                                                           newChIdx = copy(chIdx)
   365                                                           newChIdx.analogsignals = []
   366                                                           newChIdx.units = []
   367                                                           channelIndexCache[chIdx.name] = newChIdx
   368                                                   for unit in (spikeBlock.filter(objects=Unit)):
   369                                                       if lazy:
   370                                                           theseSpiketrains = []
   371                                                           for stP in unit.spiketrains:
   372                                                               st = loadStProxy(stP)
   373                                                               if len(st.times) > 0:
   374                                                                   theseSpiketrains.append(st)
   375                                                       else:
   376                                                           theseSpiketrains = [
   377                                                               st
   378                                                               for st in unit.spiketrains
   379                                                               if len(st.times)
   380                                                               ]
   381                                                       for st in theseSpiketrains:
   382                                                           st = loadObjArrayAnn(st)
   383                                                           if len(st.times):
   384                                                               st.magnitude[:] = st.times.magnitude + spikeBlock.annotations['chunkTStart']
   385                                                               st.t_start = min(0 * pq.s, st.times[0] * 0.999)
   386                                                               st.t_stop = max(
   387                                                                   st.t_stop + spikeBlock.annotations['chunkTStart'] * pq.s,
   388                                                                   st.times[-1] * 1.001)
   389                                                           else:
   390                                                               st.t_start += spikeBlock.annotations['chunkTStart'] * pq.s
   391                                                               st.t_stop += spikeBlock.annotations['chunkTStart'] * pq.s
   392                                                       uAlreadyThere = (unit.name in unitCache.keys())
   393                                                       if not uAlreadyThere:
   394                                                           newUnit = copy(unit)
   395                                                           newUnit.spiketrains = []
   396                                                           newUnit.annotations['parentChanName'] = unit.channel_index.name
   397                                                           unitCache[unit.name] = newUnit
   398                                                           spiketrainCache[unit.name] = theseSpiketrains
   399                                                       else:
   400                                                           spiketrainCache[unit.name] = spiketrainCache[unit.name] + theseSpiketrains
   401                                                   #
   402                                                   if lazy:
   403                                                       evList = [
   404                                                           evP.load()
   405                                                           for evP in eventSeg.events]
   406                                                   else:
   407                                                       evList = eventSeg.events
   408                                                   for event in evList:
   409                                                       event.magnitude[:] = event.magnitude + eventBlock.annotations['chunkTStart']
   410                                                       if event.name in eventCache.keys():
   411                                                           eventCache[event.name].append(event)
   412                                                       else:
   413                                                           eventCache[event.name] = [event]
   414                                                   # take the requested analog signal channels
   415                                                   if lazy:
   416                                                       tdChanNames = listChanNames(
   417                                                           asigBlock, chanQuery, objType=AnalogSignalProxy)
   418                                                       #############
   419                                                       # tdChanNames = ['seg0_utah1', 'seg0_utah10']
   420                                                       ##############
   421                                                       asigList = []
   422                                                       for asigP in asigSeg.analogsignals:
   423                                                           if asigP.name in tdChanNames:
   424                                                               asig = asigP.load()
   425                                                               asig.channel_index = asigP.channel_index
   426                                                               asigList.append(asig)
   427                                                               if trackMemory:
   428                                                                   print('loading {} from proxy object. memory usage: {:.1f} MB'.format(
   429                                                                       asigP.name, prf.memory_usage_psutil()))
   430                                                   else:
   431                                                       tdChanNames = listChanNames(
   432                                                           asigBlock, chanQuery, objType=AnalogSignal)
   433                                                       asigList = [
   434                                                           asig
   435                                                           for asig in asigSeg.analogsignals
   436                                                           if asig.name in tdChanNames
   437                                                           ]
   438                                                   for asig in asigList:
   439                                                       if asig.size > 0:
   440                                                           dummyAsig = asig
   441                                                   if idx == 0:
   442                                                       outputBlock = Block(
   443                                                           name=asigBlock.name,
   444                                                           file_origin=asigBlock.file_origin,
   445                                                           file_datetime=asigBlock.file_datetime,
   446                                                           rec_datetime=asigBlock.rec_datetime,
   447                                                           **asigBlock.annotations
   448                                                       )
   449                                                       newSeg = Segment(
   450                                                           index=0, name=asigSeg.name,
   451                                                           description=asigSeg.description,
   452                                                           file_origin=asigSeg.file_origin,
   453                                                           file_datetime=asigSeg.file_datetime,
   454                                                           rec_datetime=asigSeg.rec_datetime,
   455                                                           **asigSeg.annotations
   456                                                       )
   457                                                       outputBlock.segments = [newSeg]
   458                                                       for asig in asigList:
   459                                                           asigAnnCache[asig.name] = asig.annotations
   460                                                           asigAnnCache[asig.name]['parentChanName'] = asig.channel_index.name
   461                                                       asigUnits = dummyAsig.units
   462                                                   tdDF = analogSignalsToDataFrame(asigList)
   463                                                   del asigList  # asigs saved to dataframe, no longer needed
   464                                                   tdDF.loc[:, 't'] += asigBlock.annotations['chunkTStart']
   465                                                   tdDF.set_index('t', inplace=True)
   466                                                   if samplingRate != dummyAsig.sampling_rate:
   467                                                       lowPassOpts = {
   468                                                           'low': {
   469                                                               'Wn': float(samplingRate / 2),
   470                                                               'N': 4,
   471                                                               'btype': 'low',
   472                                                               'ftype': 'bessel'
   473                                                           }
   474                                                       }
   475                                                       newT = pd.Series(
   476                                                           np.arange(
   477                                                               dummyAsig.t_start + asigBlock.annotations['chunkTStart'] * pq.s,
   478                                                               dummyAsig.t_stop + asigBlock.annotations['chunkTStart'] * pq.s,
   479                                                               1/samplingRate))
   480                                                       if samplingRate < dummyAsig.sampling_rate:
   481                                                           filterCoeffs = hf.makeFilterCoeffsSOS(
   482                                                               lowPassOpts, float(dummyAsig.sampling_rate))
   483                                                           if trackMemory:
   484                                                               print('Filtering analog data before downsampling. memory usage: {:.1f} MB'.format(
   485                                                                   prf.memory_usage_psutil()))
   486                                                           '''
   487                                                           ### check that axis=0 is the correct option
   488                                                           dummyDF = tdDF.iloc[:, :4].copy()
   489                                                           filteredAsigs0 = signal.sosfiltfilt( filterCoeffs, dummyDF.to_numpy(), axis=0)
   490                                                           filteredAsigs1 = signal.sosfiltfilt( filterCoeffs, dummyDF.to_numpy(), axis=1)
   491                                                           ###
   492                                                           '''
   493                                                           filteredAsigs = signal.sosfiltfilt(
   494                                                               filterCoeffs, tdDF.to_numpy(),
   495                                                               axis=0)
   496                                                           tdDF = pd.DataFrame(
   497                                                               filteredAsigs,
   498                                                               index=tdDF.index,
   499                                                               columns=tdDF.columns)
   500                                                           if trackMemory:
   501                                                               print('Just finished analog data filtering before downsampling. memory usage: {:.1f} MB'.format(
   502                                                                   prf.memory_usage_psutil()))
   503                                                       tdInterp = hf.interpolateDF(
   504                                                           tdDF, newT,
   505                                                           kind='linear', fill_value='extrapolate',
   506                                                           verbose=verbose)
   507                                                       # free up memory used by full resolution asigs
   508                                                       del tdDF
   509                                                   else:
   510                                                       tdInterp = tdDF
   511                                                   #
   512                                                   asigCache.append(tdInterp)
   513                                                   #
   514                                                   print('Finished chunk {}'.format(chunkIdxStr))
   515                                               allTdDF = pd.concat(asigCache)
   516                                               # TODO: check for nans, if, for example a signal is partially missing
   517                                               allTdDF.fillna(method='bfill', inplace=True)
   518                                               allTdDF.fillna(method='ffill', inplace=True)
   519                                               for asigName in allTdDF.columns:
   520                                                   newAsig = AnalogSignal(
   521                                                       allTdDF[asigName].to_numpy() * asigUnits,
   522                                                       name=asigName,
   523                                                       sampling_rate=samplingRate,
   524                                                       dtype=np.float32,
   525                                                       **asigAnnCache[asigName])
   526                                                   chIdxName = asigAnnCache[asigName]['parentChanName']
   527                                                   chIdx = channelIndexCache[chIdxName]
   528                                                   # cross-assign ownership to containers
   529                                                   chIdx.analogsignals.append(newAsig)
   530                                                   newSeg.analogsignals.append(newAsig)
   531                                                   newAsig.channel_index = chIdx
   532                                                   newAsig.segment = newSeg
   533                                               #
   534                                               for uName, unit in unitCache.items():
   535                                                   # concatenate spike times, waveforms, etc.
   536                                                   if len(spiketrainCache[unit.name]):
   537                                                       consolidatedTimes = np.concatenate([
   538                                                               st.times.magnitude
   539                                                               for st in spiketrainCache[unit.name]
   540                                                           ])
   541                                                       # TODO:   decide whether to include this step
   542                                                       #         which snaps the spike times to the nearest
   543                                                       #         *sampled* data point
   544                                                       #
   545                                                       # consolidatedTimes, timesIndex = hf.closestSeries(
   546                                                       #     takeFrom=pd.Series(consolidatedTimes),
   547                                                       #     compareTo=pd.Series(allTdDF.index))
   548                                                       #
   549                                                       # find an example spiketrain with array_annotations
   550                                                       for st in spiketrainCache[unit.name]:
   551                                                           if len(st.times):
   552                                                               dummySt = st
   553                                                               break
   554                                                       consolidatedAnn = {
   555                                                           key: np.array([])
   556                                                           for key, value in dummySt.array_annotations.items()
   557                                                           }
   558                                                       for key, value in consolidatedAnn.items():
   559                                                           consolidatedAnn[key] = np.concatenate([
   560                                                               st.annotations[key]
   561                                                               for st in spiketrainCache[unit.name]
   562                                                           ])
   563                                                       consolidatedWaveforms = np.concatenate([
   564                                                           st.waveforms
   565                                                           for st in spiketrainCache[unit.name]
   566                                                           ])
   567                                                       spikeTStop = max([
   568                                                           st.t_stop
   569                                                           for st in spiketrainCache[unit.name]
   570                                                           ])
   571                                                       spikeTStart = max([
   572                                                           st.t_start
   573                                                           for st in spiketrainCache[unit.name]
   574                                                           ])
   575                                                       spikeAnnotations = {
   576                                                           key: value
   577                                                           for key, value in dummySt.annotations.items()
   578                                                           if key not in dummySt.annotations['arrayAnnNames']
   579                                                       }
   580                                                       newSt = SpikeTrain(
   581                                                           name=dummySt.name,
   582                                                           times=consolidatedTimes, units='sec', t_stop=spikeTStop,
   583                                                           waveforms=consolidatedWaveforms * dummySt.waveforms.units,
   584                                                           left_sweep=dummySt.left_sweep,
   585                                                           sampling_rate=dummySt.sampling_rate,
   586                                                           t_start=spikeTStart, **spikeAnnotations,
   587                                                           array_annotations=consolidatedAnn)
   588                                                       # cross-assign ownership to containers
   589                                                       unit.spiketrains.append(newSt)
   590                                                       newSt.unit = unit
   591                                                       newSeg.spiketrains.append(newSt)
   592                                                       newSt.segment = newSeg
   593                                                       # link chIdxes and Units
   594                                                       if unit.annotations['parentChanName'] in channelIndexCache:
   595                                                           chIdx = channelIndexCache[unit.annotations['parentChanName']]
   596                                                           if unit not in chIdx.units:
   597                                                               chIdx.units.append(unit)
   598                                                               unit.channel_index = chIdx
   599                                                       else:
   600                                                           newChIdx = ChannelIndex(
   601                                                               name=unit.annotations['parentChanName'], index=0)
   602                                                           channelIndexCache[unit.annotations['parentChanName']] = newChIdx
   603                                                           if unit not in newChIdx.units:
   604                                                               newChIdx.units.append(unit)
   605                                                               unit.channel_index = newChIdx
   606                                               #
   607                                               for evName, eventList in eventCache.items():
   608                                                   consolidatedTimes = np.concatenate([
   609                                                       ev.times.magnitude
   610                                                       for ev in eventList
   611                                                       ])
   612                                                   consolidatedLabels = np.concatenate([
   613                                                       ev.labels
   614                                                       for ev in eventList
   615                                                       ])
   616                                                   newEvent = Event(
   617                                                       name=evName,
   618                                                       times=consolidatedTimes * pq.s,
   619                                                       labels=consolidatedLabels
   620                                                       )
   621                                                   # if len(newEvent):
   622                                                   newEvent.segment = newSeg
   623                                                   newSeg.events.append(newEvent)
   624                                               for chIdxName, chIdx in channelIndexCache.items():
   625                                                   if len(chIdx.analogsignals) or len(chIdx.units):
   626                                                       outputBlock.channel_indexes.append(chIdx)
   627                                                       chIdx.block = outputBlock
   628                                               #
   629                                               outputBlock = purgeNixAnn(outputBlock)
   630                                               createRelationship = False
   631                                               if createRelationship:
   632                                                   outputBlock.create_relationship()
   633                                               return outputBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: concatenateEventsContainer at line 660

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   660                                           @profile
   661                                           def concatenateEventsContainer(eventContainer, linkParents=True):
   662                                               if isinstance(eventContainer, dict):
   663                                                   listOfEvents = list(eventContainer.values())
   664                                               else:
   665                                                   listOfEvents = eventContainer
   666                                               nonEmptyEvents = [ev for ev in listOfEvents if len(ev.times)]
   667                                               if not len(nonEmptyEvents) > 0:
   668                                                   return listOfEvents[0]
   669                                               masterEvent = listOfEvents[0]
   670                                               for evIdx, ev in enumerate(listOfEvents[1:]):
   671                                                   try:
   672                                                       masterEvent = masterEvent.merge(ev)
   673                                                   except Exception:
   674                                                       traceback.print_exc()
   675                                                       pdb.set_trace()
   676                                               if masterEvent.array_annotations is not None:
   677                                                   arrayAnnNames = list(masterEvent.array_annotations.keys())
   678                                                   masterEvent.annotations.update(masterEvent.array_annotations)
   679                                                   masterEvent.annotations['arrayAnnNames'] = arrayAnnNames
   680                                               if linkParents:
   681                                                   masterEvent.segment = listOfEvents[0].segment
   682                                                   if isinstance(masterEvent, SpikeTrain):
   683                                                       masterEvent.unit = listOfEvents[0].unit
   684                                               return masterEvent

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: unitSpikeTrainWaveformsToDF at line 743

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   743                                           @profile
   744                                           def unitSpikeTrainWaveformsToDF(
   745                                                   spikeTrainContainer,
   746                                                   dataQuery=None,
   747                                                   transposeToColumns='bin', fastTranspose=True,
   748                                                   lags=None, decimate=1, rollingWindow=None,
   749                                                   getMetaData=True, verbose=False,
   750                                                   whichSegments=None, windowSize=None, procFun=None):
   751                                               #  list contains different segments from *one* unit
   752                                               if isinstance(spikeTrainContainer, ChannelIndex):
   753                                                   assert len(spikeTrainContainer.units) == 0
   754                                                   spiketrains = spikeTrainContainer.units[0].spiketrains
   755                                               elif isinstance(spikeTrainContainer, Unit):
   756                                                   spiketrains = spikeTrainContainer.spiketrains
   757                                               else:
   758                                                   raise(Exception('not a valid container'))
   759                                               # TODO check if really need to assert uniqueness?
   760                                               uniqueSpiketrains = []
   761                                               for st in spiketrains:
   762                                                   if not np.any([st is i for i in uniqueSpiketrains]):
   763                                                       uniqueSpiketrains.append(st)
   764                                               #  subsampling options
   765                                               decimate = int(decimate)
   766                                               if whichSegments is not None:
   767                                                   uniqueSpiketrains = [
   768                                                       uniqueSpiketrains[i]
   769                                                       for i in whichSegments
   770                                                   ]
   771                                               #
   772                                               waveformsList = []
   773                                               #
   774                                               for segIdx, stIn in enumerate(uniqueSpiketrains):
   775                                                   if verbose:
   776                                                       print('extracting spiketrain from {}'.format(stIn.segment))
   777                                                   #  make sure is not a proxyObj
   778                                                   if isinstance(stIn, SpikeTrainProxy):
   779                                                       st = loadStProxy(stIn)
   780                                                       if (getMetaData) or (dataQuery is not None):
   781                                                           # if there's a query, get metadata temporarily to resolve it
   782                                                           st = loadObjArrayAnn(st)
   783                                                   else:
   784                                                       st = stIn
   785                                                   #  extract bins spaced by decimate argument
   786                                                   if not st.times.any():
   787                                                       continue
   788                                                   if verbose:
   789                                                       print('extracting wf from {}'.format(stIn.segment))
   790                                                   wf = np.asarray(
   791                                                       np.squeeze(st.waveforms),
   792                                                       dtype='float32')
   793                                                   if wf.ndim == 3:
   794                                                       print('Waveforms from more than one channel!')
   795                                                       if wf.shape[1] > 0:
   796                                                           wf = wf[:, 0, :]
   797                                                   wfDF = pd.DataFrame(wf)
   798                                                   samplingRate = st.sampling_rate
   799                                                   bins = (
   800                                                       np.asarray(wfDF.columns) / samplingRate -
   801                                                       st.left_sweep)
   802                                                   wfDF.columns = np.around(bins.magnitude, decimals=6)
   803                                                   if windowSize is not None:
   804                                                       winMask = (
   805                                                           (wfDF.columns >= windowSize[0]) &
   806                                                           (wfDF.columns <= windowSize[1]))
   807                                                       wfDF = wfDF.loc[:, winMask]
   808                                                   if procFun is not None:
   809                                                       wfDF = procFun(wfDF, st)
   810                                                   idxLabels = ['segment', 'originalIndex', 't']
   811                                                   wfDF.loc[:, 't'] = np.asarray(st.times.magnitude)
   812                                                   if (getMetaData) or (dataQuery is not None):
   813                                                       # if there's a query, get metadata temporarily to resolve it
   814                                                       annDict = {}
   815                                                       for k, values in st.array_annotations.items():
   816                                                           if isinstance(getMetaData, Iterable):
   817                                                               # if selecting metadata fields, check that
   818                                                               # the key is in the provided list
   819                                                               if k not in getMetaData:
   820                                                                   continue
   821                                                           if isinstance(values[0], str):
   822                                                               v = np.asarray(values, dtype='str')
   823                                                           else:
   824                                                               v = np.asarray(values)
   825                                                           annDict.update({k: v})
   826                                                       skipAnnNames = (
   827                                                           st.annotations['arrayAnnNames'] +
   828                                                           [
   829                                                               'arrayAnnNames', 'arrayAnnDTypes',
   830                                                               'nix_name', 'neo_name', 'id',
   831                                                               'cell_label', 'cluster_label', 'max_on_channel', 'binWidth']
   832                                                           )
   833                                                       annDF = pd.DataFrame(annDict)
   834                                                       for k, value in st.annotations.items():
   835                                                           if isinstance(getMetaData, Iterable):
   836                                                               # if selecting metadata fields, check that
   837                                                               # the key is in the provided list
   838                                                               if k not in getMetaData:
   839                                                                   continue
   840                                                           if k not in skipAnnNames:
   841                                                               annDF.loc[:, k] = value
   842                                                       #
   843                                                       if isinstance(getMetaData, Iterable):
   844                                                           doNotFillList = idxLabels + ['feature', 'bin']
   845                                                           fieldsNeedFiller = [
   846                                                               mdn
   847                                                               for mdn in getMetaData
   848                                                               if (mdn not in doNotFillList) and (mdn not in annDF.columns)]
   849                                                           for mdName in fieldsNeedFiller:
   850                                                               annDF.loc[:, mdName] = 'NA'
   851                                                       annColumns = annDF.columns.to_list()
   852                                                       if getMetaData:
   853                                                           for annNm in annColumns:
   854                                                               if annNm not in idxLabels:
   855                                                                   idxLabels.append(annNm)
   856                                                           # idxLabels += annColumns
   857                                                       spikeDF = annDF.join(wfDF)
   858                                                   else:
   859                                                       spikeDF = wfDF
   860                                                       del wfDF, st
   861                                                   spikeDF.loc[:, 'segment'] = segIdx
   862                                                   spikeDF.loc[:, 'originalIndex'] = spikeDF.index
   863                                                   spikeDF.columns.name = 'bin'
   864                                                   #
   865                                                   if dataQuery is not None:
   866                                                       spikeDF.query(dataQuery, inplace=True)
   867                                                       if not getMetaData:
   868                                                           spikeDF.drop(columns=annColumns, inplace=True)
   869                                                   waveformsList.append(spikeDF)
   870                                               #
   871                                               zeroLagWaveformsDF = pd.concat(waveformsList, axis='index')
   872                                               if verbose:
   873                                                   prf.print_memory_usage('before transposing waveforms')
   874                                               # TODO implement lags and rolling window addition here
   875                                               metaDF = zeroLagWaveformsDF.loc[:, idxLabels].copy()
   876                                               zeroLagWaveformsDF.drop(columns=idxLabels, inplace=True)
   877                                               if lags is None:
   878                                                   lags = [0]
   879                                               laggedWaveformsDict = {
   880                                                   (spikeTrainContainer.name, k): None for k in lags}
   881                                               for lag in lags:
   882                                                   if isinstance(lag, int):
   883                                                       shiftedWaveform = zeroLagWaveformsDF.shift(
   884                                                           lag, axis='columns')
   885                                                       if rollingWindow is not None:
   886                                                           halfRollingWin = int(np.ceil(rollingWindow/2))
   887                                                           seekIdx = slice(
   888                                                               halfRollingWin, -halfRollingWin+1, decimate)
   889                                                           # seekIdx = slice(None, None, decimate)
   890                                                           #shiftedWaveform = (
   891                                                           #    shiftedWaveform
   892                                                           #    .rolling(
   893                                                           #        window=rollingWindow, win_type='gaussian',
   894                                                           #        axis='columns', center=True)
   895                                                           #    .mean(std=halfRollingWin))
   896                                                           shiftedWaveform = (
   897                                                               shiftedWaveform
   898                                                               .rolling(
   899                                                                   window=rollingWindow, 
   900                                                                   axis='columns', center=True)
   901                                                               .mean())
   902                                                       else:
   903                                                           halfRollingWin = 0
   904                                                           seekIdx = slice(None, None, decimate)
   905                                                           if False:
   906                                                               oldShiftedWaveform = zeroLagWaveformsDF.shift(
   907                                                                   lag, axis='columns')
   908                                                               plt.plot(oldShiftedWaveform.iloc[0, :])
   909                                                               plt.plot(shiftedWaveform.iloc[0, :])
   910                                                               plt.show()
   911                                                       laggedWaveformsDict[
   912                                                           (spikeTrainContainer.name, lag)] = (
   913                                                               shiftedWaveform.iloc[:, seekIdx].copy())
   914                                                   if isinstance(lag, tuple):
   915                                                       halfRollingWin = int(np.ceil(lag[1]/2))
   916                                                       seekIdx = slice(
   917                                                           halfRollingWin, -halfRollingWin+1, decimate)
   918                                                       # seekIdx = slice(None, None, decimate)
   919                                                       shiftedWaveform = (
   920                                                           zeroLagWaveformsDF
   921                                                           .shift(lag[0], axis='columns')
   922                                                           .rolling(
   923                                                               window=lag[1], win_type='gaussian',
   924                                                               axis='columns', center=True)
   925                                                           .mean(std=halfRollingWin))
   926                                                       laggedWaveformsDict[
   927                                                           (spikeTrainContainer.name, lag)] = (
   928                                                               shiftedWaveform.iloc[:, seekIdx].copy())
   929                                               #
   930                                               if transposeToColumns == 'feature':
   931                                                   # stack the bin, name the feature column
   932                                                   # 
   933                                                   for idx, (key, value) in enumerate(laggedWaveformsDict.items()):
   934                                                       if idx == 0:
   935                                                           stackedIndexDF = pd.concat(
   936                                                               [metaDF, value], axis='columns')
   937                                                           stackedIndexDF.set_index(idxLabels, inplace=True)
   938                                                           # don't drop nans for now - might need to keep track of them
   939                                                           # if we need to equalize to another array later
   940                                                           newIndex = stackedIndexDF.stack(dropna=False).index
   941                                                           idxLabels.append('bin')
   942                                                       laggedWaveformsDict[key] = value.stack(dropna=False).to_frame(name=key).reset_index(drop=True)
   943                                                   waveformsDF = pd.concat(
   944                                                       laggedWaveformsDict.values(),
   945                                                       axis='columns')
   946                                                   waveformsDF.columns.names = ['feature', 'lag']
   947                                                   waveformsDF.index = newIndex
   948                                                   waveformsDF.columns.name = 'feature'
   949                                               elif transposeToColumns == 'bin':
   950                                                   # add the feature column
   951                                                   waveformsDF = pd.concat(
   952                                                       laggedWaveformsDict,
   953                                                       names=['feature', 'lag', 'originalDummy']).reset_index()
   954                                                   waveformsDF = pd.concat(
   955                                                       [
   956                                                           metaDF.reset_index(drop=True),
   957                                                           waveformsDF.drop(columns='originalDummy')],
   958                                                       axis='columns')
   959                                                   idxLabels += ['feature', 'lag']
   960                                                   waveformsDF.columns.name = 'bin'
   961                                                   waveformsDF.set_index(idxLabels, inplace=True)
   962                                               #
   963                                               if transposeToColumns != waveformsDF.columns.name:
   964                                                   waveformsDF = transposeSpikeDF(
   965                                                       waveformsDF, transposeToColumns,
   966                                                       fastTranspose=fastTranspose)
   967                                               return waveformsDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: concatenateUnitSpikeTrainWaveformsDF at line 969

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   969                                           @profile
   970                                           def concatenateUnitSpikeTrainWaveformsDF(
   971                                                   units, dataQuery=None,
   972                                                   transposeToColumns='bin', concatOn='index',
   973                                                   fastTranspose=True, getMetaData=True, verbose=False,
   974                                                   addLags=None, decimate=1, rollingWindow=None,
   975                                                   metaDataToCategories=False, windowSize=None,
   976                                                   whichSegments=None, procFun=None):
   977                                               allUnits = []
   978                                               for thisUnit in units:
   979                                                   hasAnySpikes = []
   980                                                   for stIn in thisUnit.spiketrains:
   981                                                       if isinstance(stIn, SpikeTrainProxy):
   982                                                           st = stIn.load(
   983                                                               magnitude_mode='rescaled',
   984                                                               load_waveforms=False)
   985                                                       else:
   986                                                           st = stIn
   987                                                       hasAnySpikes.append(st.times.any())
   988                                                   if np.any(hasAnySpikes):
   989                                                       allUnits.append(thisUnit)
   990                                               waveformsList = []
   991                                               for idx, thisUnit in enumerate(allUnits):
   992                                                   if verbose:
   993                                                       print('concatenating unitDF {}'.format(thisUnit.name))
   994                                                   lags = None
   995                                                   if addLags is not None:
   996                                                       if thisUnit.name in addLags:
   997                                                           lags = addLags[thisUnit.name]
   998                                                   unitWaveforms = unitSpikeTrainWaveformsToDF(
   999                                                       thisUnit, dataQuery=dataQuery,
  1000                                                       transposeToColumns=transposeToColumns,
  1001                                                       fastTranspose=fastTranspose, getMetaData=getMetaData,
  1002                                                       lags=lags, decimate=decimate, rollingWindow=rollingWindow,
  1003                                                       verbose=verbose, windowSize=windowSize,
  1004                                                       whichSegments=whichSegments, procFun=procFun)
  1005                                                   if idx == 0:
  1006                                                       idxLabels = unitWaveforms.index.names
  1007                                                   if (concatOn == 'columns') and (idx > 0):
  1008                                                       # other than first time, we already have the metadata
  1009                                                       unitWaveforms.reset_index(drop=True, inplace=True)
  1010                                                   else:
  1011                                                       # first time, or if concatenating indices,
  1012                                                       # keep the the metadata
  1013                                                       unitWaveforms.reset_index(inplace=True)
  1014                                                       if metaDataToCategories:
  1015                                                           # convert metadata to categoricals to free memory
  1016                                                           #
  1017                                                           unitWaveforms[idxLabels] = (
  1018                                                               unitWaveforms[idxLabels]
  1019                                                               .astype('category')
  1020                                                               )
  1021                                                   waveformsList.append(unitWaveforms)
  1022                                                   del unitWaveforms
  1023                                                   if verbose:
  1024                                                       print('memory usage: {:.1f} MB'.format(prf.memory_usage_psutil()))
  1025                                               if verbose:
  1026                                                   print(
  1027                                                       'about to join all, memory usage: {:.1f} MB'
  1028                                                       .format(prf.memory_usage_psutil()))
  1029                                               #  if concatenating indexes, reset the index of the result
  1030                                               #  ignoreIndex = (concatOn == 'index')
  1031                                               allWaveforms = pd.concat(
  1032                                                   waveformsList, axis=concatOn,
  1033                                                   # ignore_index=ignoreIndex
  1034                                                   )
  1035                                               del waveformsList
  1036                                               if verbose:
  1037                                                   print(
  1038                                                       'finished concatenating, memory usage: {:.1f} MB'
  1039                                                       .format(prf.memory_usage_psutil()))
  1040                                               try:
  1041                                                   allWaveforms.set_index(idxLabels, inplace=True)
  1042                                                   allWaveforms.sort_index(
  1043                                                       level=['segment', 'originalIndex', 't'],
  1044                                                       axis='index', inplace=True, kind='mergesort')
  1045                                                   allWaveforms.sort_index(
  1046                                                       axis='columns', inplace=True, kind='mergesort')
  1047                                               except Exception:
  1048                                                   pdb.set_trace()
  1049                                               return allWaveforms

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: alignedAsigsToDF at line 1051

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1051                                           @profile
  1052                                           def alignedAsigsToDF(
  1053                                                   dataBlock, unitNames=None,
  1054                                                   unitQuery=None, dataQuery=None,
  1055                                                   collapseSizes=False, verbose=False,
  1056                                                   duplicateControlsByProgram=False,
  1057                                                   amplitudeColumn='amplitude',
  1058                                                   programColumn='program',
  1059                                                   electrodeColumn='electrode',
  1060                                                   transposeToColumns='bin', concatOn='index', fastTranspose=True,
  1061                                                   addLags=None, decimate=1, rollingWindow=None,
  1062                                                   whichSegments=None, windowSize=None,
  1063                                                   getMetaData=True, metaDataToCategories=True,
  1064                                                   outlierTrials=None, invertOutlierMask=False,
  1065                                                   makeControlProgram=False, removeFuzzyName=False, procFun=None):
  1066                                               #  channels to trigger
  1067                                               if unitNames is None:
  1068                                                   unitNames = listChanNames(dataBlock, unitQuery, objType=Unit)
  1069                                               allUnits = []
  1070                                               for uName in unitNames:
  1071                                                   allUnits += dataBlock.filter(objects=Unit, name=uName)
  1072                                               allWaveforms = concatenateUnitSpikeTrainWaveformsDF(
  1073                                                   allUnits, dataQuery=dataQuery,
  1074                                                   transposeToColumns=transposeToColumns, concatOn=concatOn,
  1075                                                   fastTranspose=fastTranspose,
  1076                                                   addLags=addLags, decimate=decimate, rollingWindow=rollingWindow,
  1077                                                   verbose=verbose, whichSegments=whichSegments,
  1078                                                   windowSize=windowSize, procFun=procFun,
  1079                                                   getMetaData=getMetaData, metaDataToCategories=metaDataToCategories)
  1080                                               #
  1081                                               manipulateIndex = np.any(
  1082                                                   [
  1083                                                       collapseSizes, duplicateControlsByProgram,
  1084                                                       makeControlProgram, removeFuzzyName
  1085                                                       ])
  1086                                               if outlierTrials is not None:
  1087                                                   def rejectionLookup(entry):
  1088                                                       key = []
  1089                                                       for subKey in outlierTrials.index.names:
  1090                                                           keyIdx = allWaveforms.index.names.index(subKey)
  1091                                                           key.append(entry[keyIdx])
  1092                                                       # print(key)
  1093                                                       # outlierTrials.iloc[1, :]
  1094                                                       # allWaveforms.iloc[1, :]
  1095                                                       return outlierTrials[tuple(key)]
  1096                                                   #
  1097                                                   outlierMask = np.asarray(
  1098                                                       allWaveforms.index.map(rejectionLookup),
  1099                                                       dtype=np.bool)
  1100                                                   if invertOutlierMask:
  1101                                                       outlierMask = ~outlierMask
  1102                                                   allWaveforms = allWaveforms.loc[~outlierMask, :]
  1103                                               if manipulateIndex and getMetaData:
  1104                                                   idxLabels = allWaveforms.index.names
  1105                                                   allWaveforms.reset_index(inplace=True)
  1106                                                   # 
  1107                                                   if collapseSizes:
  1108                                                       try:
  1109                                                           allWaveforms.loc[allWaveforms['pedalSizeCat'] == 'XL', 'pedalSizeCat'] = 'L'
  1110                                                           allWaveforms.loc[allWaveforms['pedalSizeCat'] == 'XS', 'pedalSizeCat'] = 'S'
  1111                                                       except Exception:
  1112                                                           traceback.print_exc()
  1113                                                   if makeControlProgram:
  1114                                                       try:
  1115                                                           allWaveforms.loc[allWaveforms[amplitudeColumn] == 0, programColumn] = 999
  1116                                                           allWaveforms.loc[allWaveforms[amplitudeColumn] == 0, electrodeColumn] = 'control'
  1117                                                       except Exception:
  1118                                                           traceback.print_exc()
  1119                                                   if duplicateControlsByProgram:
  1120                                                       #
  1121                                                       noStimWaveforms = (
  1122                                                           allWaveforms
  1123                                                           .loc[allWaveforms[amplitudeColumn] == 0, :]
  1124                                                           )
  1125                                                       stimWaveforms = (
  1126                                                           allWaveforms
  1127                                                           .loc[allWaveforms[amplitudeColumn] != 0, :]
  1128                                                           .copy()
  1129                                                           )
  1130                                                       uniqProgs = stimWaveforms[programColumn].unique()
  1131                                                       progElecLookup = {}
  1132                                                       #pdb.set_trace()
  1133                                                       for progIdx in uniqProgs:
  1134                                                           theseStimDF = stimWaveforms.loc[
  1135                                                               stimWaveforms[programColumn] == progIdx,
  1136                                                               electrodeColumn]
  1137                                                           elecIdx = theseStimDF.iloc[0]
  1138                                                           progElecLookup.update({progIdx: elecIdx})
  1139                                                       #
  1140                                                       if makeControlProgram:
  1141                                                           uniqProgs = np.append(uniqProgs, 999)
  1142                                                           progElecLookup.update({999: 'control'})
  1143                                                       #
  1144                                                       for progIdx in uniqProgs:
  1145                                                           dummyWaveforms = noStimWaveforms.copy()
  1146                                                           dummyWaveforms.loc[:, programColumn] = progIdx
  1147                                                           dummyWaveforms.loc[:, electrodeColumn] = progElecLookup[progIdx]
  1148                                                           stimWaveforms = pd.concat([stimWaveforms, dummyWaveforms])
  1149                                                       stimWaveforms.reset_index(drop=True, inplace=True)
  1150                                                       allWaveforms = stimWaveforms
  1151                                                   #
  1152                                                   if removeFuzzyName:
  1153                                                       fuzzyNamesBase = [
  1154                                                           i.replace('Fuzzy', '')
  1155                                                           for i in idxLabels
  1156                                                           if 'Fuzzy' in i]
  1157                                                       colRenamer = {n + 'Fuzzy': n for n in fuzzyNamesBase}
  1158                                                       fuzzyNamesBasePresent = [
  1159                                                           i
  1160                                                           for i in fuzzyNamesBase
  1161                                                           if i in allWaveforms.columns]
  1162                                                       allWaveforms.drop(columns=fuzzyNamesBasePresent, inplace=True)
  1163                                                       allWaveforms.rename(columns=colRenamer, inplace=True)
  1164                                                       idxLabels = np.unique(
  1165                                                           [i.replace('Fuzzy', '') for i in idxLabels])
  1166                                                   #
  1167                                                   allWaveforms.set_index(
  1168                                                       list(idxLabels),
  1169                                                       inplace=True)
  1170                                                   if isinstance(allWaveforms.columns, pd.MultiIndex):
  1171                                                       allWaveforms.columns = allWaveforms.columns.remove_unused_levels()
  1172                                               #
  1173                                               if transposeToColumns == 'feature':
  1174                                                   zipNames = zip(pd.unique(allWaveforms.columns.get_level_values('feature')).tolist(), unitNames)
  1175                                                   try:
  1176                                                       assert np.all([i == j for i, j in zipNames]), 'columns out of requested order!'
  1177                                                   except Exception:
  1178                                                       traceback.print_exc()
  1179                                                       allWaveforms.reindex(columns=unitNames)
  1180                                               if isinstance(allWaveforms.columns, pd.MultiIndex):
  1181                                                   allWaveforms.columns = allWaveforms.columns.remove_unused_levels()
  1182                                               allWaveforms.sort_index(
  1183                                                   axis='columns', inplace=True, kind='mergesort')
  1184                                               return allWaveforms

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getAsigsAlignedToEvents at line 1186

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1186                                           @profile
  1187                                           def getAsigsAlignedToEvents(
  1188                                                   eventBlock=None, signalBlock=None,
  1189                                                   chansToTrigger=None, chanQuery=None,
  1190                                                   eventName=None, windowSize=None,
  1191                                                   minNReps=None,
  1192                                                   appendToExisting=False,
  1193                                                   checkReferences=True, verbose=False,
  1194                                                   fileName=None, folderPath=None, chunkSize=None
  1195                                                   ):
  1196                                               #  get signals from same block as events?
  1197                                               if signalBlock is None:
  1198                                                   signalBlock = eventBlock
  1199                                               #  channels to trigger
  1200                                               if chansToTrigger is None:
  1201                                                   chansToTrigger = listChanNames(
  1202                                                       signalBlock, chanQuery, objType=ChannelIndex, condition='hasAsigs')
  1203                                               #  allocate block for spiketrains
  1204                                               masterBlock = Block()
  1205                                               try:
  1206                                                   masterBlock.name = signalBlock.annotations['neo_name']
  1207                                                   masterBlock.annotate(nix_name=signalBlock.annotations['neo_name'])
  1208                                               except Exception:
  1209                                                   masterBlock.name = signalBlock.name
  1210                                                   masterBlock.annotate(neo_name=signalBlock.name)
  1211                                                   masterBlock.annotate(nix_name=signalBlock.name)
  1212                                               #  make channels and units for triggered time series
  1213                                               for chanName in chansToTrigger:
  1214                                                   chanIdx = ChannelIndex(name=chanName + '#0', index=[0])
  1215                                                   chanIdx.annotate(nix_name=chanIdx.name)
  1216                                                   thisUnit = Unit(name=chanIdx.name)
  1217                                                   thisUnit.annotate(nix_name=chanIdx.name)
  1218                                                   chanIdx.units.append(thisUnit)
  1219                                                   thisUnit.channel_index = chanIdx
  1220                                                   masterBlock.channel_indexes.append(chanIdx)
  1221                                                   sigChanIdxList = signalBlock.filter(
  1222                                                       objects=ChannelIndex, name=chanName)
  1223                                                   if len(sigChanIdxList):
  1224                                                       sigChanIdx = sigChanIdxList[0]
  1225                                                       if sigChanIdx.coordinates is not None:
  1226                                                           coordUnits = sigChanIdx.coordinates[0][0].units
  1227                                                           chanIdx.coordinates = np.asarray(sigChanIdx.coordinates) * coordUnits
  1228                                                           thisUnit.annotations['parentChanXCoords'] = float(chanIdx.coordinates[:, 0].magnitude)
  1229                                                           thisUnit.annotations['parentChanYCoords'] = float(chanIdx.coordinates[:, 1].magnitude)
  1230                                                           thisUnit.annotations['parentChanCoordinateUnits'] = '{}'.format(coordUnits)
  1231                                               #
  1232                                               totalNSegs = 0
  1233                                               #  print([evSeg.events[3].name for evSeg in eventBlock.segments])
  1234                                               allAlignEventsList = []
  1235                                               for segIdx, eventSeg in enumerate(eventBlock.segments):
  1236                                                   thisEventName = 'seg{}_{}'.format(segIdx, eventName)
  1237                                                   try:
  1238                                                       assert len(eventSeg.filter(name=thisEventName)) == 1
  1239                                                   except Exception:
  1240                                                       traceback.print_exc()
  1241                                                   allEvIn = eventSeg.filter(name=thisEventName)[0]
  1242                                                   if isinstance(allEvIn, EventProxy):
  1243                                                       allAlignEvents = loadObjArrayAnn(allEvIn.load())
  1244                                                   elif isinstance(allEvIn, Event):
  1245                                                       allAlignEvents = allEvIn
  1246                                                   else:
  1247                                                       raise(Exception(
  1248                                                           '{} must be an Event or EventProxy!'
  1249                                                           .format(eventName)))
  1250                                                   allAlignEventsList.append(allAlignEvents)
  1251                                               allAlignEventsDF = unitSpikeTrainArrayAnnToDF(allAlignEventsList)
  1252                                               #
  1253                                               breakDownData = (
  1254                                                   allAlignEventsDF
  1255                                                   .groupby(minNReps['categories'])
  1256                                                   .agg('count')
  1257                                                   .iloc[:, 0]
  1258                                                   )
  1259                                               try:
  1260                                                   breakDownData[breakDownData > minNReps['n']].to_csv(
  1261                                                       os.path.join(
  1262                                                           folderPath, 'numRepetitionsEachCondition.csv'
  1263                                                       ), header=True
  1264                                                   )
  1265                                               except Exception:
  1266                                                   traceback.print_exc()
  1267                                               allAlignEventsDF.loc[:, 'keepMask'] = False
  1268                                               for name, group in allAlignEventsDF.groupby(minNReps['categories']):
  1269                                                   allAlignEventsDF.loc[group.index, 'keepMask'] = (
  1270                                                       breakDownData[name] > minNReps['n'])
  1271                                               for segIdx, group in allAlignEventsDF.groupby('segment'):
  1272                                                   allAlignEventsList[segIdx].array_annotations['keepMask'] = group['keepMask'].to_numpy()
  1273                                               #
  1274                                               for segIdx, eventSeg in enumerate(eventBlock.segments):
  1275                                                   if verbose:
  1276                                                       print(
  1277                                                           'getAsigsAlignedToEvents on segment {} of {}'
  1278                                                           .format(segIdx + 1, len(eventBlock.segments)))
  1279                                                   allAlignEvents = allAlignEventsList[segIdx]
  1280                                                   if chunkSize is None:
  1281                                                       alignEventGroups = [allAlignEvents]
  1282                                                   else:
  1283                                                       nChunks = max(
  1284                                                           int(np.floor(allAlignEvents.shape[0] / chunkSize)),
  1285                                                           1)
  1286                                                       alignEventGroups = []
  1287                                                       for i in range(nChunks):
  1288                                                           if not (i == (nChunks - 1)):
  1289                                                               # not last one
  1290                                                               alignEventGroups.append(
  1291                                                                   allAlignEvents[i * chunkSize: (i + 1) * chunkSize])
  1292                                                           else:
  1293                                                               alignEventGroups.append(
  1294                                                                   allAlignEvents[i * chunkSize:])
  1295                                                   signalSeg = signalBlock.segments[segIdx]
  1296                                                   for subSegIdx, alignEvents in enumerate(alignEventGroups):
  1297                                                       # seg to contain triggered time series
  1298                                                       if verbose:
  1299                                                           print(
  1300                                                               'getAsigsAlignedToEvents on subSegment {} of {}'
  1301                                                               .format(subSegIdx + 1, len(alignEventGroups)))
  1302                                                       if not alignEvents.shape[0] > 0:
  1303                                                           continue
  1304                                                       newSeg = Segment(name='seg{}_'.format(int(totalNSegs)))
  1305                                                       newSeg.annotate(nix_name=newSeg.name)
  1306                                                       masterBlock.segments.append(newSeg)
  1307                                                       for chanName in chansToTrigger:
  1308                                                           asigName = 'seg{}_{}'.format(segIdx, chanName)
  1309                                                           if verbose:
  1310                                                               print(
  1311                                                                   'getAsigsAlignedToEvents on channel {}'
  1312                                                                   .format(chanName))
  1313                                                           assert len(signalSeg.filter(name=asigName)) == 1
  1314                                                           asig = signalSeg.filter(name=asigName)[0]
  1315                                                           nominalWinLen = int(
  1316                                                               (windowSize[1] - windowSize[0]) *
  1317                                                               asig.sampling_rate - 1)
  1318                                                           validMask = (
  1319                                                               ((
  1320                                                                   alignEvents + windowSize[1] +
  1321                                                                   asig.sampling_rate ** (-1)) < asig.t_stop) &
  1322                                                               ((
  1323                                                                   alignEvents + windowSize[0] -
  1324                                                                   asig.sampling_rate ** (-1)) > asig.t_start)
  1325                                                               )
  1326                                                           thisKeepMask = alignEvents.array_annotations['keepMask']
  1327                                                           fullMask = (validMask & thisKeepMask)
  1328                                                           alignEvents = alignEvents[fullMask]
  1329                                                           # array_annotations get sliced with the event, but regular anns do not
  1330                                                           for annName in alignEvents.annotations['arrayAnnNames']:
  1331                                                               alignEvents.annotations[annName] = (
  1332                                                                   alignEvents.array_annotations[annName])
  1333                                                           if isinstance(asig, AnalogSignalProxy):
  1334                                                               if checkReferences:
  1335                                                                   da = (
  1336                                                                       asig
  1337                                                                       ._rawio
  1338                                                                       .da_list['blocks'][0]['segments'][segIdx]['data'])
  1339                                                                   print('segIdx {}, asig.name {}'.format(
  1340                                                                       segIdx, asig.name))
  1341                                                                   print('asig._global_channel_indexes = {}'.format(
  1342                                                                       asig._global_channel_indexes))
  1343                                                                   print('asig references {}'.format(
  1344                                                                       da[asig._global_channel_indexes[0]]))
  1345                                                                   try:
  1346                                                                       assert (
  1347                                                                           asig.name
  1348                                                                           in da[asig._global_channel_indexes[0]].name)
  1349                                                                   except Exception:
  1350                                                                       traceback.print_exc()
  1351                                                               rawWaveforms = [
  1352                                                                   asig.load(
  1353                                                                       time_slice=(t + windowSize[0], t + windowSize[1]))
  1354                                                                   for t in alignEvents]
  1355                                                               if any([rW.shape[0] < nominalWinLen for rW in rawWaveforms]):
  1356                                                                   rawWaveforms = [
  1357                                                                       asig.load(
  1358                                                                           time_slice=(t + windowSize[0], t + windowSize[1] + asig.sampling_period))
  1359                                                                       for t in alignEvents]
  1360                                                           elif isinstance(asig, AnalogSignal):
  1361                                                               rawWaveforms = []
  1362                                                               for t in alignEvents:
  1363                                                                   asigMask = (asig.times > t + windowSize[0]) & (asig.times < t + windowSize[1])
  1364                                                                   rawWaveforms.append(asig[asigMask[:, np.newaxis]])
  1365                                                           else:
  1366                                                               raise(Exception('{} must be an AnalogSignal or AnalogSignalProxy!'.format(asigName)))
  1367                                                           #
  1368                                                           samplingRate = asig.sampling_rate
  1369                                                           waveformUnits = rawWaveforms[0].units
  1370                                                           #  fix length if roundoff error
  1371                                                           #  minLen = min([rW.shape[0] for rW in rawWaveforms])
  1372                                                           rawWaveforms = [rW[:nominalWinLen] for rW in rawWaveforms]
  1373                                                           #
  1374                                                           spikeWaveforms = (
  1375                                                               np.hstack([rW.magnitude for rW in rawWaveforms])
  1376                                                               .transpose()[:, np.newaxis, :] * waveformUnits
  1377                                                               )
  1378                                                           #
  1379                                                           thisUnit = masterBlock.filter(
  1380                                                               objects=Unit, name=chanName + '#0')[0]
  1381                                                           skipEventAnnNames = (
  1382                                                               ['nix_name', 'neo_name']
  1383                                                               )
  1384                                                           stAnn = {
  1385                                                               k: v
  1386                                                               for k, v in alignEvents.annotations.items()
  1387                                                               if k not in skipEventAnnNames
  1388                                                               }
  1389                                                           skipAsigAnnNames = (
  1390                                                               ['channel_id', 'nix_name', 'neo_name']
  1391                                                               )
  1392                                                           stAnn.update({
  1393                                                               k: v
  1394                                                               for k, v in asig.annotations.items()
  1395                                                               if k not in skipAsigAnnNames
  1396                                                           })
  1397                                                           st = SpikeTrain(
  1398                                                               name='seg{}_{}'.format(int(totalNSegs), thisUnit.name),
  1399                                                               times=alignEvents.times,
  1400                                                               waveforms=spikeWaveforms,
  1401                                                               t_start=asig.t_start, t_stop=asig.t_stop,
  1402                                                               left_sweep=windowSize[0] * (-1),
  1403                                                               sampling_rate=samplingRate,
  1404                                                               **stAnn
  1405                                                               )
  1406                                                           st.annotate(nix_name=st.name)
  1407                                                           st.annotations['unitAnnotations'] = json.dumps(
  1408                                                               thisUnit.annotations.copy())
  1409                                                           thisUnit.spiketrains.append(st)
  1410                                                           newSeg.spiketrains.append(st)
  1411                                                           st.unit = thisUnit
  1412                                                       totalNSegs += 1
  1413                                               try:
  1414                                                   eventBlock.filter(
  1415                                                       objects=EventProxy)[0]._rawio.file.close()
  1416                                               except Exception:
  1417                                                   traceback.print_exc()
  1418                                               if signalBlock is not eventBlock:
  1419                                                   try:
  1420                                                       signalBlock.filter(
  1421                                                           objects=AnalogSignalProxy)[0]._rawio.file.close()
  1422                                                   except Exception:
  1423                                                       traceback.print_exc()
  1424                                               triggeredPath = os.path.join(
  1425                                                   folderPath, fileName + '.nix')
  1426                                               if not os.path.exists(triggeredPath):
  1427                                                   appendToExisting = False
  1428                                           
  1429                                               if appendToExisting:
  1430                                                   allSegs = list(range(len(masterBlock.segments)))
  1431                                                   addBlockToNIX(
  1432                                                       masterBlock, neoSegIdx=allSegs,
  1433                                                       writeSpikes=True,
  1434                                                       fileName=fileName,
  1435                                                       folderPath=folderPath,
  1436                                                       purgeNixNames=False,
  1437                                                       nixBlockIdx=0, nixSegIdx=allSegs)
  1438                                               else:
  1439                                                   if os.path.exists(triggeredPath):
  1440                                                       os.remove(triggeredPath)
  1441                                                   masterBlock = purgeNixAnn(masterBlock)
  1442                                                   writer = NixIO(filename=triggeredPath)
  1443                                                   writer.write_block(masterBlock, use_obj_names=True)
  1444                                                   writer.close()
  1445                                               return masterBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: alignedAsigDFtoSpikeTrain at line 1447

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1447                                           @profile
  1448                                           def alignedAsigDFtoSpikeTrain(
  1449                                                   allWaveforms, dataBlock=None, matchSamplingRate=True):
  1450                                               masterBlock = Block()
  1451                                               masterBlock.name = dataBlock.annotations['neo_name']
  1452                                               masterBlock.annotate(nix_name=dataBlock.annotations['neo_name'])
  1453                                               for segIdx, group in allWaveforms.groupby('segment'):
  1454                                                   print('Saving trajectoriess for segment {}'.format(segIdx))
  1455                                                   dataSeg = dataBlock.segments[segIdx]
  1456                                                   exSt = dataSeg.spiketrains[0]
  1457                                                   if isinstance(exSt, SpikeTrainProxy):
  1458                                                       print(
  1459                                                           'alignedAsigDFtoSpikeTrain basing seg {} on {}'
  1460                                                           .format(segIdx, exSt.name))
  1461                                                       stProxy = exSt
  1462                                                       exSt = loadStProxy(stProxy)
  1463                                                       exSt = loadObjArrayAnn(exSt)
  1464                                                   print('exSt.left_sweep is {}'.format(exSt.left_sweep))
  1465                                                   wfBins = ((np.arange(exSt.waveforms.shape[2]) / (exSt.sampling_rate)) - exSt.left_sweep).magnitude
  1466                                                   # seg to contain triggered time series
  1467                                                   newSeg = Segment(name=dataSeg.annotations['neo_name'])
  1468                                                   newSeg.annotate(nix_name=dataSeg.annotations['neo_name'])
  1469                                                   masterBlock.segments.append(newSeg)
  1470                                                   #
  1471                                                   if group.columns.name == 'bin':
  1472                                                       grouper = group.groupby('feature')
  1473                                                       colsAre = 'bin'
  1474                                                   elif group.columns.name == 'feature':
  1475                                                       grouper = group.iteritems()
  1476                                                       colsAre = 'feature'
  1477                                                   for featName, featGroup in grouper:
  1478                                                       print('Saving {}...'.format(featName))
  1479                                                       if featName[-2:] == '#0':
  1480                                                           cleanFeatName = featName
  1481                                                       else:
  1482                                                           cleanFeatName = featName + '#0'
  1483                                                       if segIdx == 0:
  1484                                                           #  allocate units
  1485                                                           chanIdx = ChannelIndex(
  1486                                                               name=cleanFeatName, index=[0])
  1487                                                           chanIdx.annotate(nix_name=chanIdx.name)
  1488                                                           thisUnit = Unit(name=chanIdx.name)
  1489                                                           thisUnit.annotate(nix_name=chanIdx.name)
  1490                                                           chanIdx.units.append(thisUnit)
  1491                                                           thisUnit.channel_index = chanIdx
  1492                                                           masterBlock.channel_indexes.append(chanIdx)
  1493                                                       else:
  1494                                                           thisUnit = masterBlock.filter(
  1495                                                               objects=Unit, name=cleanFeatName)[0]
  1496                                                       if colsAre == 'bin':
  1497                                                           spikeWaveformsDF = featGroup
  1498                                                       elif colsAre == 'feature':
  1499                                                           if isinstance(featGroup, pd.Series):
  1500                                                               featGroup = featGroup.to_frame(name=featName)
  1501                                                               featGroup.columns.name = 'feature'
  1502                                                           spikeWaveformsDF = transposeSpikeDF(
  1503                                                               featGroup,
  1504                                                               'bin', fastTranspose=True)
  1505                                                       if matchSamplingRate:
  1506                                                           if len(spikeWaveformsDF.columns) != len(wfBins):
  1507                                                               wfDF = spikeWaveformsDF.reset_index(drop=True).T
  1508                                                               wfDF = hf.interpolateDF(wfDF, wfBins)
  1509                                                               spikeWaveformsDF = wfDF.T.set_index(spikeWaveformsDF.index)
  1510                                                       spikeWaveforms = spikeWaveformsDF.to_numpy()[:, np.newaxis, :]
  1511                                                       arrAnnDF = spikeWaveformsDF.index.to_frame()
  1512                                                       spikeTimes = arrAnnDF['t']
  1513                                                       arrAnnDF.drop(columns='t', inplace=True)
  1514                                                       arrAnn = {}
  1515                                                       colsToKeep = arrAnnDF.columns.drop(['originalIndex', 'feature', 'segment', 'lag'])
  1516                                                       for cName in colsToKeep:
  1517                                                           values = arrAnnDF[cName].to_numpy()
  1518                                                           if isinstance(values[0], str):
  1519                                                               values = values.astype('U')
  1520                                                           arrAnn.update({str(cName): values.flatten()})
  1521                                                       arrayAnnNames = {
  1522                                                           'arrayAnnNames': list(arrAnn.keys())}
  1523                                                       st = SpikeTrain(
  1524                                                           name='seg{}_{}'.format(int(segIdx), thisUnit.name),
  1525                                                           times=spikeTimes.to_numpy() * exSt.units,
  1526                                                           waveforms=spikeWaveforms * pq.dimensionless,
  1527                                                           t_start=exSt.t_start, t_stop=exSt.t_stop,
  1528                                                           left_sweep=exSt.left_sweep,
  1529                                                           sampling_rate=exSt.sampling_rate,
  1530                                                           **arrAnn, **arrayAnnNames
  1531                                                           )
  1532                                                       st.annotate(nix_name=st.name)
  1533                                                       thisUnit.spiketrains.append(st)
  1534                                                       newSeg.spiketrains.append(st)
  1535                                                       st.unit = thisUnit
  1536                                               return masterBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: dataFrameToAnalogSignals at line 1538

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1538                                           @profile
  1539                                           def dataFrameToAnalogSignals(
  1540                                                   df,
  1541                                                   block=None, seg=None,
  1542                                                   idxT='NSPTime',
  1543                                                   probeName='insTD', samplingRate=500*pq.Hz,
  1544                                                   timeUnits=pq.s, measureUnits=pq.mV,
  1545                                                   dataCol=['channel_0', 'channel_1'],
  1546                                                   useColNames=False, forceColNames=None,
  1547                                                   namePrefix='', nameSuffix='', verbose=False):
  1548                                               if block is None:
  1549                                                   assert seg is None
  1550                                                   block = Block(name=probeName)
  1551                                                   seg = Segment(name='seg0_' + probeName)
  1552                                                   block.segments.append(seg)
  1553                                               if verbose:
  1554                                                   print('in dataFrameToAnalogSignals...')
  1555                                               for idx, colName in enumerate(dataCol):
  1556                                                   if verbose:
  1557                                                       print('    {}'.format(colName))
  1558                                                   if forceColNames is not None:
  1559                                                       chanName = forceColNames[idx]
  1560                                                   elif useColNames:
  1561                                                       chanName = namePrefix + colName + nameSuffix
  1562                                                   else:
  1563                                                       chanName = namePrefix + (probeName.lower() + '{}'.format(idx)) + nameSuffix
  1564                                                   #
  1565                                                   chanIdx = ChannelIndex(
  1566                                                       name=chanName,
  1567                                                       # index=None,
  1568                                                       index=np.asarray([idx]),
  1569                                                       # channel_names=np.asarray([chanName])
  1570                                                       )
  1571                                                   block.channel_indexes.append(chanIdx)
  1572                                                   asig = AnalogSignal(
  1573                                                       df[colName].to_numpy() * measureUnits,
  1574                                                       name='seg0_' + chanName,
  1575                                                       sampling_rate=samplingRate,
  1576                                                       dtype=np.float32,
  1577                                                       # **ann
  1578                                                       )
  1579                                                   if idxT is not None:
  1580                                                       asig.t_start = df[idxT].iloc[0] * timeUnits
  1581                                                   else:
  1582                                                       asig.t_start = df.index[0] * timeUnits
  1583                                                   asig.channel_index = chanIdx
  1584                                                   # assign ownership to containers
  1585                                                   chanIdx.analogsignals.append(asig)
  1586                                                   seg.analogsignals.append(asig)
  1587                                                   chanIdx.create_relationship()
  1588                                               # assign parent to children
  1589                                               block.create_relationship()
  1590                                               seg.create_relationship()
  1591                                               return block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: eventDataFrameToEvents at line 1593

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1593                                           @profile
  1594                                           def eventDataFrameToEvents(
  1595                                                   eventDF, idxT=None,
  1596                                                   annCol=None,
  1597                                                   eventName='', tUnits=pq.s,
  1598                                                   makeList=True
  1599                                                   ):
  1600                                               if makeList:
  1601                                                   eventList = []
  1602                                                   for colName in annCol:
  1603                                                       originalDType = type(eventDF[colName].to_numpy()[0]).__name__
  1604                                                       event = Event(
  1605                                                           name=eventName + colName,
  1606                                                           times=eventDF[idxT].to_numpy() * tUnits,
  1607                                                           labels=eventDF[colName].astype(originalDType).to_numpy()
  1608                                                           )
  1609                                                       event.annotate(originalDType=originalDType)
  1610                                                       eventList.append(event)
  1611                                                   return eventList
  1612                                               else:
  1613                                                   if annCol is None:
  1614                                                       annCol = eventDF.drop(columns=idxT).columns
  1615                                                   event = Event(
  1616                                                       name=eventName,
  1617                                                       times=eventDF[idxT].to_numpy() * tUnits,
  1618                                                       labels=np.asarray(eventDF.index)
  1619                                                       )
  1620                                                   event.annotations.update(
  1621                                                       {
  1622                                                           'arrayAnnNames': [],
  1623                                                           'arrayAnnDTypes': []
  1624                                                           })
  1625                                                   for colName in annCol:
  1626                                                       originalDType = type(eventDF[colName].to_numpy()[0]).__name__
  1627                                                       arrayAnn = eventDF[colName].astype(originalDType).to_numpy()
  1628                                                       event.array_annotations.update(
  1629                                                           {colName: arrayAnn})
  1630                                                       event.annotations['arrayAnnNames'].append(colName)
  1631                                                       event.annotations['arrayAnnDTypes'].append(originalDType)
  1632                                                       event.annotations.update(
  1633                                                           {colName: arrayAnn})
  1634                                                   return event

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: eventsToDataFrame at line 1636

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1636                                           @profile
  1637                                           def eventsToDataFrame(
  1638                                                   events, idxT='t', names=None
  1639                                                   ):
  1640                                               eventDict = {}
  1641                                               calculatedT = False
  1642                                               for event in events:
  1643                                                   if names is not None:
  1644                                                       if event.name not in names:
  1645                                                           continue
  1646                                                   if len(event.times):
  1647                                                       if not calculatedT:
  1648                                                           t = pd.Series(event.times.magnitude)
  1649                                                           calculatedT = True
  1650                                                       try:
  1651                                                           values = event.array_annotations['labels']
  1652                                                       except Exception:
  1653                                                           values = event.labels
  1654                                                       if isinstance(values[0], bytes):
  1655                                                           #  event came from hdf, need to recover dtype
  1656                                                           if 'originalDType' in event.annotations:
  1657                                                               dtypeStr = event.annotations['originalDType'].split(';')[-1]
  1658                                                               if 'np.' not in dtypeStr:
  1659                                                                   dtypeStr = 'np.' + dtypeStr
  1660                                                               originalDType = eval(dtypeStr)
  1661                                                               values = np.asarray(values, dtype=originalDType)
  1662                                                           else:
  1663                                                               values = np.asarray(values, dtype=np.str)
  1664                                                       #  print(values.dtype)
  1665                                                       eventDict.update({
  1666                                                           event.name: pd.Series(values)})
  1667                                               eventDict.update({idxT: t})
  1668                                               return pd.concat(eventDict, axis=1)

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadSpikeMats at line 1670

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1670                                           @profile
  1671                                           def loadSpikeMats(
  1672                                                   dataPath, rasterOpts,
  1673                                                   alignTimes=None, chans=None, loadAll=False,
  1674                                                   absoluteBins=False, transposeSpikeMat=False,
  1675                                                   checkReferences=False,
  1676                                                   aggregateFun=None):
  1677                                           
  1678                                               reader = nixio_fr.NixIO(filename=dataPath)
  1679                                               chanNames = reader.header['signal_channels']['name']
  1680                                               
  1681                                               if chans is not None:
  1682                                                   sigMask = np.isin(chanNames, chans)
  1683                                                   chanNames = chanNames[sigMask]
  1684                                                   
  1685                                               chanIdx = reader.channel_name_to_index(chanNames)
  1686                                               
  1687                                               if not loadAll:
  1688                                                   assert alignTimes is not None
  1689                                                   spikeMats = {i: None for i in alignTimes.index}
  1690                                                   validTrials = pd.Series(True, index=alignTimes.index)
  1691                                               else:
  1692                                                   spikeMats = {
  1693                                                       i: None for i in range(reader.segment_count(block_index=0))}
  1694                                                   validTrials = None
  1695                                               
  1696                                               for segIdx in range(reader.segment_count(block_index=0)):
  1697                                                   if checkReferences:
  1698                                                       for i, cIdx in enumerate(chanIdx):
  1699                                                           da = reader.da_list['blocks'][0]['segments'][segIdx]['data'][cIdx]
  1700                                                           print('name {}, da.name {}'.format(chanNames[i], da.name))
  1701                                                           try:
  1702                                                               assert chanNames[i] in da.name, 'reference problem!!'
  1703                                                           except Exception:
  1704                                                               traceback.print_exc()
  1705                                                   tStart = reader.get_signal_t_start(
  1706                                                       block_index=0, seg_index=segIdx)
  1707                                                   fs = reader.get_signal_sampling_rate(
  1708                                                       channel_indexes=chanIdx
  1709                                                       )
  1710                                                   sigSize = reader.get_signal_size(
  1711                                                       block_index=0, seg_index=segIdx
  1712                                                       )
  1713                                                   tStop = sigSize / fs + tStart
  1714                                                   #  convert to indices early to avoid floating point problems
  1715                                                   
  1716                                                   intervalIdx = int(round(rasterOpts['binInterval'] * fs))
  1717                                                   #  halfIntervalIdx = int(round(intervalIdx / 2))
  1718                                                   
  1719                                                   widthIdx = int(round(rasterOpts['binWidth'] * fs))
  1720                                                   halfWidthIdx = int(round(widthIdx / 2))
  1721                                                   
  1722                                                   if rasterOpts['smoothKernelWidth'] is not None:
  1723                                                       kernWidthIdx = int(round(rasterOpts['smoothKernelWidth'] * fs))
  1724                                                   
  1725                                                   theBins = None
  1726                                           
  1727                                                   if not loadAll:
  1728                                                       winStartIdx = int(round(rasterOpts['windowSize'][0] * fs))
  1729                                                       winStopIdx = int(round(rasterOpts['windowSize'][1] * fs))
  1730                                                       timeMask = (alignTimes > tStart) & (alignTimes < tStop)
  1731                                                       maskedTimes = alignTimes[timeMask]
  1732                                                   else:
  1733                                                       #  irrelevant, will load all
  1734                                                       maskedTimes = pd.Series(np.nan)
  1735                                           
  1736                                                   for idx, tOnset in maskedTimes.iteritems():
  1737                                                       if not loadAll:
  1738                                                           idxOnset = int(round((tOnset - tStart) * fs))
  1739                                                           #  can't not be ints
  1740                                                           iStart = idxOnset + winStartIdx - int(3 * halfWidthIdx)
  1741                                                           iStop = idxOnset + winStopIdx + int(3 * halfWidthIdx)
  1742                                                       else:
  1743                                                           winStartIdx = 0
  1744                                                           iStart = 0
  1745                                                           iStop = sigSize
  1746                                           
  1747                                                       if iStart < 0:
  1748                                                           #  near the first edge
  1749                                                           validTrials[idx] = False
  1750                                                       elif (sigSize < iStop):
  1751                                                           #  near the ending edge
  1752                                                           validTrials[idx] = False
  1753                                                       else:
  1754                                                           #  valid slices
  1755                                                           try:
  1756                                                               rawSpikeMat = pd.DataFrame(
  1757                                                                   reader.get_analogsignal_chunk(
  1758                                                                       block_index=0, seg_index=segIdx,
  1759                                                                       i_start=iStart, i_stop=iStop,
  1760                                                                       channel_names=chanNames))
  1761                                                           except Exception:
  1762                                                               traceback.print_exc()
  1763                                                               #
  1764                                                           if aggregateFun is None:
  1765                                                               procSpikeMat = rawSpikeMat.rolling(
  1766                                                                   window=3 * widthIdx, center=True,
  1767                                                                   win_type='gaussian'
  1768                                                                   ).mean(std=halfWidthIdx)
  1769                                                           else:
  1770                                                               procSpikeMat = rawSpikeMat.rolling(
  1771                                                                   window=widthIdx, center=True
  1772                                                                   ).apply(
  1773                                                                       aggregateFun,
  1774                                                                       raw=True,
  1775                                                                       kwargs={'fs': fs, 'nSamp': widthIdx})
  1776                                                           #
  1777                                                           if rasterOpts['smoothKernelWidth'] is not None:
  1778                                                               procSpikeMat = (
  1779                                                                   procSpikeMat
  1780                                                                   .rolling(
  1781                                                                       window=3 * kernWidthIdx, center=True,
  1782                                                                       win_type='gaussian')
  1783                                                                   .mean(std=kernWidthIdx/2)
  1784                                                                   .dropna().iloc[::intervalIdx, :]
  1785                                                               )
  1786                                                           else:
  1787                                                               procSpikeMat = (
  1788                                                                   procSpikeMat
  1789                                                                   .dropna().iloc[::intervalIdx, :]
  1790                                                               )
  1791                                           
  1792                                                           procSpikeMat.columns = chanNames
  1793                                                           procSpikeMat.columns.name = 'unit'
  1794                                                           if theBins is None:
  1795                                                               theBins = np.asarray(
  1796                                                                   procSpikeMat.index + winStartIdx) / fs
  1797                                                           if absoluteBins:
  1798                                                               procSpikeMat.index = theBins + idxOnset / fs
  1799                                                           else:
  1800                                                               procSpikeMat.index = theBins
  1801                                                           procSpikeMat.index.name = 'bin'
  1802                                                           if loadAll:
  1803                                                               smIdx = segIdx
  1804                                                           else:
  1805                                                               smIdx = idx
  1806                                                               
  1807                                                           spikeMats[smIdx] = procSpikeMat
  1808                                                           if transposeSpikeMat:
  1809                                                               spikeMats[smIdx] = spikeMats[smIdx].transpose()
  1810                                                       #  plt.imshow(rawSpikeMat.to_numpy(), aspect='equal'); plt.show()
  1811                                               return spikeMats, validTrials

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: synchronizeINStoNSP at line 1813

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1813                                           @profile
  1814                                           def synchronizeINStoNSP(
  1815                                                   tapTimestampsNSP=None, tapTimestampsINS=None,
  1816                                                   precalculatedFun=None,
  1817                                                   NSPTimeRanges=(None, None),
  1818                                                   td=None, accel=None, insBlock=None, trialSegment=None, degree=1,
  1819                                                   trimSpiketrains=False
  1820                                                   ):
  1821                                               print('Trial Segment {}'.format(trialSegment))
  1822                                               if precalculatedFun is None:
  1823                                                   assert ((tapTimestampsNSP is not None) & (tapTimestampsINS is not None))
  1824                                                   # sanity check that the intervals match
  1825                                                   insDiff = tapTimestampsINS.diff().dropna().values
  1826                                                   nspDiff = tapTimestampsNSP.diff().dropna().values
  1827                                                   print('On the INS, the diff() between taps was\n{}'.format(insDiff))
  1828                                                   print('On the NSP, the diff() between taps was\n{}'.format(nspDiff))
  1829                                                   print('This amounts to a msec difference of\n{}'.format(
  1830                                                       (insDiff - nspDiff) * 1e3))
  1831                                                   if (insDiff - nspDiff > 20e-3).any():
  1832                                                       raise(Exception('Tap trains too different!'))
  1833                                                   #
  1834                                                   if degree > 0:
  1835                                                       synchPolyCoeffsINStoNSP = np.polyfit(
  1836                                                           x=tapTimestampsINS.values, y=tapTimestampsNSP.values,
  1837                                                           deg=degree)
  1838                                                   else:
  1839                                                       timeOffset = tapTimestampsNSP.values - tapTimestampsINS.values
  1840                                                       synchPolyCoeffsINStoNSP = np.array([1, np.mean(timeOffset)])
  1841                                                   timeInterpFunINStoNSP = np.poly1d(synchPolyCoeffsINStoNSP)
  1842                                               else:
  1843                                                   timeInterpFunINStoNSP = precalculatedFun
  1844                                               if td is not None:
  1845                                                   td.loc[:, 'NSPTime'] = pd.Series(
  1846                                                       timeInterpFunINStoNSP(td['t']), index=td['t'].index)
  1847                                                   td.loc[:, 'NSPTime'] = timeInterpFunINStoNSP(td['t'].to_numpy())
  1848                                               if accel is not None:
  1849                                                   accel.loc[:, 'NSPTime'] = pd.Series(
  1850                                                       timeInterpFunINStoNSP(accel['t']), index=accel['t'].index)
  1851                                               if insBlock is not None:
  1852                                                   # allUnits = [st.unit for st in insBlock.segments[0].spiketrains]
  1853                                                   # [un.name for un in insBlock.filter(objects=Unit)]
  1854                                                   for unit in insBlock.filter(objects=Unit):
  1855                                                       tStart = NSPTimeRanges[0]
  1856                                                       tStop = NSPTimeRanges[1]
  1857                                                       uniqueSt = []
  1858                                                       for st in unit.spiketrains:
  1859                                                           if st not in uniqueSt:
  1860                                                               uniqueSt.append(st)
  1861                                                           else:
  1862                                                               continue
  1863                                                           print('Synchronizing {}'.format(st.name))
  1864                                                           if len(st.times):
  1865                                                               segMaskSt = np.array(
  1866                                                                   st.array_annotations['trialSegment'],
  1867                                                                   dtype=np.int) == trialSegment
  1868                                                               st.magnitude[segMaskSt] = (
  1869                                                                   timeInterpFunINStoNSP(st.times[segMaskSt].magnitude))
  1870                                                               if trimSpiketrains:
  1871                                                                   print('Trimming spiketrain')
  1872                                                                   #  kludgey fix for weirdness concerning t_start
  1873                                                                   st.t_start = min(tStart, st.times[0] * 0.999)
  1874                                                                   st.t_stop = min(tStop, st.times[-1] * 1.001)
  1875                                                                   validMask = st < st.t_stop
  1876                                                                   if ~validMask.all():
  1877                                                                       print('Deleted some spikes')
  1878                                                                       st = st[validMask]
  1879                                                                       # delete invalid spikes
  1880                                                                       if 'arrayAnnNames' in st.annotations.keys():
  1881                                                                           for key in st.annotations['arrayAnnNames']:
  1882                                                                               try:
  1883                                                                                   # st.annotations[key] = np.array(st.array_annotations[key])
  1884                                                                                   st.annotations[key] = np.delete(st.annotations[key], ~validMask)
  1885                                                                               except Exception:
  1886                                                                                   traceback.print_exc()
  1887                                                                                   pdb.set_trace()
  1888                                                           else:
  1889                                                               if trimSpiketrains:
  1890                                                                   st.t_start = tStart
  1891                                                                   st.t_stop = tStop
  1892                                                   #
  1893                                                   allEvents = [
  1894                                                       ev
  1895                                                       for ev in insBlock.filter(objects=Event)
  1896                                                       if ('ins' in ev.name) and ('concatenate' not in ev.name)]
  1897                                                   concatEvents = [
  1898                                                       ev
  1899                                                       for ev in insBlock.filter(objects=Event)
  1900                                                       if ('ins' in ev.name) and ('concatenate' in ev.name)]
  1901                                                   eventsDF = eventsToDataFrame(allEvents, idxT='t')
  1902                                                   newNames = {i: childBaseName(i, 'seg') for i in eventsDF.columns}
  1903                                                   eventsDF.rename(columns=newNames, inplace=True)
  1904                                                   segMask = hf.getStimSerialTrialSegMask(eventsDF, trialSegment)
  1905                                                   evTStart = eventsDF.loc[segMask, 't'].min() * pq.s
  1906                                                   evTStop = eventsDF.loc[segMask, 't'].max() * pq.s
  1907                                                   # print('allEvents[0].shape = {}'.format(allEvents[0].shape))
  1908                                                   # print('allEvents[0].magnitude[segMask][0] = {}'.format(allEvents[0].magnitude[segMask][0]))
  1909                                                   for event in (allEvents + concatEvents):
  1910                                                       if trimSpiketrains:
  1911                                                           thisSegMask = (event.times >= evTStart) & (event.times <= evTStop)
  1912                                                       else:
  1913                                                           thisSegMask = (event.times >= evTStart) & (event.times < evTStop)
  1914                                                       event.magnitude[thisSegMask] = (
  1915                                                           timeInterpFunINStoNSP(event.times[thisSegMask].magnitude))
  1916                                                   # print('allEvents[0].magnitude[segMask][0] = {}'.format(allEvents[0].magnitude[segMask][0]))
  1917                                                   # if len(concatEvents) > trialSegment:
  1918                                                   #     concatEvents[trialSegment].magnitude[:] = timeInterpFunINStoNSP(
  1919                                                   #         concatEvents[trialSegment].times[:].magnitude)
  1920                                               return td, accel, insBlock, timeInterpFunINStoNSP

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: findSegsIncluding at line 1922

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1922                                           @profile
  1923                                           def findSegsIncluding(
  1924                                                   block, timeSlice=None):
  1925                                               segBoundsList = []
  1926                                               for segIdx, seg in enumerate(block.segments):
  1927                                                   segBoundsList.append(pd.DataFrame({
  1928                                                       't_start': seg.t_start,
  1929                                                       't_stop': seg.t_stop
  1930                                                       }, index=[segIdx]))
  1931                                           
  1932                                               segBounds = pd.concat(segBoundsList)
  1933                                               if timeSlice[0] is not None:
  1934                                                   segMask = (segBounds['t_start'] * pq.s >= timeSlice[0]) & (
  1935                                                       segBounds['t_stop'] * pq.s <= timeSlice[1])
  1936                                                   requestedSegs = segBounds.loc[segMask, :]
  1937                                               else:
  1938                                                   timeSlice = (None, None)
  1939                                                   requestedSegs = segBounds
  1940                                               return segBounds, requestedSegs

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: findSegsIncluded at line 1942

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1942                                           @profile
  1943                                           def findSegsIncluded(
  1944                                                   block, timeSlice=None):
  1945                                               segBoundsList = []
  1946                                               for segIdx, seg in enumerate(block.segments):
  1947                                                   segBoundsList.append(pd.DataFrame({
  1948                                                       't_start': seg.t_start,
  1949                                                       't_stop': seg.t_stop
  1950                                                       }, index=[segIdx]))
  1951                                           
  1952                                               segBounds = pd.concat(segBoundsList)
  1953                                               if timeSlice[0] is not None:
  1954                                                   segMask = (segBounds['t_start'] * pq.s <= timeSlice[0]) | (
  1955                                                       segBounds['t_stop'] * pq.s >= timeSlice[1])
  1956                                                   requestedSegs = segBounds.loc[segMask, :]
  1957                                               else:
  1958                                                   timeSlice = (None, None)
  1959                                                   requestedSegs = segBounds
  1960                                               return segBounds, requestedSegs

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getElecLookupTable at line 1962

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1962                                           @profile
  1963                                           def getElecLookupTable(
  1964                                                   block, elecIds=None):
  1965                                               lookupTableList = []
  1966                                               for metaIdx, chanIdx in enumerate(block.channel_indexes):
  1967                                                   if chanIdx.analogsignals:
  1968                                                       #  print(chanIdx.name)
  1969                                                       lookupTableList.append(pd.DataFrame({
  1970                                                           'channelNames': np.asarray(chanIdx.channel_names, dtype=np.str),
  1971                                                           'index': chanIdx.index,
  1972                                                           'metaIndex': metaIdx * chanIdx.index**0,
  1973                                                           'localIndex': (
  1974                                                               list(range(chanIdx.analogsignals[0].shape[1])))
  1975                                                           }))
  1976                                               lookupTable = pd.concat(lookupTableList, ignore_index=True)
  1977                                           
  1978                                               if elecIds is None:
  1979                                                   requestedIndices = lookupTable
  1980                                               else:
  1981                                                   if isinstance(elecIds[0], str):
  1982                                                       idxMask = lookupTable['channelNames'].isin(elecIds)
  1983                                                       requestedIndices = lookupTable.loc[idxMask, :]
  1984                                               return lookupTable, requestedIndices

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getNIXData at line 1986

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1986                                           @profile
  1987                                           def getNIXData(
  1988                                                   fileName=None,
  1989                                                   folderPath=None,
  1990                                                   reader=None, blockIdx=0,
  1991                                                   elecIds=None, startTime_s=None,
  1992                                                   dataLength_s=None, downsample=1,
  1993                                                   signal_group_mode='group-by-same-units',
  1994                                                   closeReader=False):
  1995                                               #  Open file and extract headers
  1996                                               if reader is None:
  1997                                                   assert (fileName is not None) and (folderPath is not None)
  1998                                                   filePath = os.path.join(folderPath, fileName) + '.nix'
  1999                                                   reader = nixio_fr.NixIO(filename=filePath)
  2000                                           
  2001                                               block = reader.read_block(
  2002                                                   block_index=blockIdx, lazy=True,
  2003                                                   signal_group_mode=signal_group_mode)
  2004                                           
  2005                                               for segIdx, seg in enumerate(block.segments):
  2006                                                   seg.events = [i.load() for i in seg.events]
  2007                                                   seg.epochs = [i.load() for i in seg.epochs]
  2008                                           
  2009                                               # find elecIds
  2010                                               lookupTable, requestedIndices = getElecLookupTable(
  2011                                                   block, elecIds=elecIds)
  2012                                           
  2013                                               # find segments that contain the requested times
  2014                                               if dataLength_s is not None:
  2015                                                   assert startTime_s is not None
  2016                                                   timeSlice = (
  2017                                                       startTime_s * pq.s,
  2018                                                       (startTime_s + dataLength_s) * pq.s)
  2019                                               else:
  2020                                                   timeSlice = (None, None)
  2021                                               segBounds, requestedSegs = findSegsIncluding(block, timeSlice)
  2022                                               #
  2023                                               data = pd.DataFrame(columns=elecIds + ['t'])
  2024                                               for segIdx in requestedSegs.index:
  2025                                                   seg = block.segments[segIdx]
  2026                                                   if dataLength_s is not None:
  2027                                                       timeSlice = (
  2028                                                           max(timeSlice[0], seg.t_start),
  2029                                                           min(timeSlice[1], seg.t_stop)
  2030                                                           )
  2031                                                   else:
  2032                                                       timeSlice = (seg.t_start, seg.t_stop)
  2033                                                   segData = pd.DataFrame()
  2034                                                   for metaIdx in pd.unique(requestedIndices['metaIndex']):
  2035                                                       metaIdxMatch = requestedIndices['metaIndex'] == metaIdx
  2036                                                       theseRequestedIndices = requestedIndices.loc[
  2037                                                           metaIdxMatch, :]
  2038                                                       theseElecIds = theseRequestedIndices['channelNames']
  2039                                                       asig = seg.analogsignals[metaIdx]
  2040                                                       thisTimeSlice = (
  2041                                                           max(timeSlice[0], asig.t_start),
  2042                                                           min(timeSlice[1], asig.t_stop)
  2043                                                           )
  2044                                                       reqData = asig.load(
  2045                                                           time_slice=thisTimeSlice,
  2046                                                           channel_indexes=theseRequestedIndices['localIndex'].to_numpy())
  2047                                                       segData = pd.concat((
  2048                                                               segData,
  2049                                                               pd.DataFrame(
  2050                                                                   reqData.magnitude, columns=theseElecIds.to_numpy())),
  2051                                                           axis=1)
  2052                                                   segT = reqData.times
  2053                                                   segData['t'] = segT
  2054                                                   data = pd.concat(
  2055                                                       (data, segData),
  2056                                                       axis=0, ignore_index=True)
  2057                                               channelData = {
  2058                                                   'data': data,
  2059                                                   't': data['t']
  2060                                                   }
  2061                                               if closeReader:
  2062                                                   reader.file.close()
  2063                                                   block = None
  2064                                                   # closing the reader breaks its connection to the block
  2065                                               return channelData, block

Total time: 0.0004085 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: childBaseName at line 2067

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2067                                           @profile
  2068                                           def childBaseName(
  2069                                                   childName, searchTerm):
  2070       342       1831.0      5.4     44.8      if searchTerm in childName:
  2071                                                   baseName = '_'.join(childName.split('_')[1:])
  2072                                               else:
  2073       342       1237.0      3.6     30.3          baseName = childName
  2074       342       1017.0      3.0     24.9      return baseName

Total time: 0.747349 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: readBlockFixNames at line 2076

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2076                                           @profile
  2077                                           def readBlockFixNames(
  2078                                                   rawioReader,
  2079                                                   block_index=0, signal_group_mode='split-all',
  2080                                                   lazy=True, mapDF=None, reduceChannelIndexes=False,
  2081                                                   loadList=None, purgeNixNames=False
  2082                                                   ):
  2083         2         53.0     26.5      0.0      headerSignalChan = pd.DataFrame(
  2084         2      95001.0  47500.5      1.3          rawioReader.header['signal_channels']).set_index('id')
  2085         2         51.0     25.5      0.0      headerUnitChan = pd.DataFrame(
  2086         2      82079.0  41039.5      1.1          rawioReader.header['unit_channels']).set_index('id')
  2087         2         57.0     28.5      0.0      dataBlock = rawioReader.read_block(
  2088         2         26.0     13.0      0.0          block_index=block_index, lazy=lazy,
  2089         2     995882.0 497941.0     13.3          signal_group_mode=signal_group_mode)
  2090         2         63.0     31.5      0.0      if dataBlock.name is None:
  2091                                                   if 'neo_name' in dataBlock.annotations:
  2092                                                       dataBlock.name = dataBlock.annotations['neo_name']
  2093                                               #  on first segment, rename the chan_indexes and units
  2094         2         35.0     17.5      0.0      seg0 = dataBlock.segments[0]
  2095                                               asigLikeList = (
  2096         2       1656.0    828.0      0.0          seg0.filter(objects=AnalogSignalProxy) +
  2097         2       1576.0    788.0      0.0          seg0.filter(objects=AnalogSignal))
  2098         2         31.0     15.5      0.0      if mapDF is not None:
  2099         2        835.0    417.5      0.0          if headerSignalChan.size > 0:
  2100         2         30.0     15.0      0.0              asigNameChanger = {}
  2101       130       5439.0     41.8      0.1              for nevID in mapDF['nevID']:
  2102       128       6014.0     47.0      0.1                  if int(nevID) in headerSignalChan.index:
  2103                                                               labelFromMap = (
  2104       128       2273.0     17.8      0.0                          mapDF
  2105       128    1049668.0   8200.5     14.0                          .loc[mapDF['nevID'] == nevID, 'label']
  2106       128      38209.0    298.5      0.5                          .iloc[0])
  2107                                                               asigNameChanger[
  2108       128      37679.0    294.4      0.5                          headerSignalChan.loc[int(nevID), 'name']] = labelFromMap
  2109                                                   else:
  2110                                                       asigOrigNames = np.unique(
  2111                                                           [i.split('#')[0] for i in headerUnitChan['name']])
  2112                                                       asigNameChanger = {}
  2113                                                       for origName in asigOrigNames:
  2114                                                           # ripple specific
  2115                                                           formattedName = origName.replace('.', '_').replace(' raw', '')
  2116                                                           if mapDF['label'].str.contains(formattedName).any():
  2117                                                               asigNameChanger[origName] = formattedName
  2118                                               else:
  2119                                                   asigNameChanger = dict()
  2120       142       2535.0     17.9      0.0      for asig in asigLikeList:
  2121       140      11428.0     81.6      0.2          asigBaseName = childBaseName(asig.name, 'seg')
  2122                                                   asig.name = (
  2123                                                       asigNameChanger[asigBaseName]
  2124       140       3108.0     22.2      0.0              if asigBaseName in asigNameChanger
  2125        12        168.0     14.0      0.0              else asigBaseName)
  2126       140       2249.0     16.1      0.0          if mapDF is not None:
  2127       140     756640.0   5404.6     10.1              if (mapDF['label'] == asig.name).any():
  2128       128    1078646.0   8426.9     14.4                  asig.annotations['xCoords'] = float(mapDF.loc[mapDF['label'] == asig.name, 'xcoords'].iloc[0])
  2129       128    1065740.0   8326.1     14.3                  asig.annotations['yCoords'] = float(mapDF.loc[mapDF['label'] == asig.name, 'ycoords'].iloc[0])
  2130       128    1071556.0   8371.5     14.3                  asig.annotations['zCoords'] = float(mapDF.loc[mapDF['label'] == asig.name, 'zcoords'].iloc[0])
  2131       140       4211.0     30.1      0.1          if 'Channel group ' in asig.channel_index.name:
  2132                                                       newChanName = (
  2133                                                           asigNameChanger[asigBaseName]
  2134       140       2976.0     21.3      0.0                  if asigBaseName in asigNameChanger
  2135        12        161.0     13.4      0.0                  else asigBaseName)
  2136       140       2533.0     18.1      0.0              asig.channel_index.name = newChanName
  2137       140       2769.0     19.8      0.0              if 'neo_name' in asig.channel_index.annotations:
  2138                                                           asig.channel_index.annotations['neo_name'] = newChanName
  2139       140       2293.0     16.4      0.0              if 'nix_name' in asig.channel_index.annotations:
  2140                                                           asig.channel_index.annotations['nix_name'] = newChanName
  2141       140       2283.0     16.3      0.0              if mapDF is not None:
  2142       140       2138.0     15.3      0.0                  try:
  2143       140       3768.0     26.9      0.1                      asig.channel_index.coordinates = np.asarray([
  2144       140       8388.0     59.9      0.1                          asig.annotations['xCoords'], asig.annotations['yCoords'], asig.annotations['zCoords']
  2145       128      41490.0    324.1      0.6                      ])[np.newaxis, :] * pq.um
  2146        12        186.0     15.5      0.0                  except Exception:
  2147        12        180.0     15.0      0.0                      pass
  2148                                               spikeTrainLikeList = (
  2149         2       2152.0   1076.0      0.0          seg0.filter(objects=SpikeTrainProxy) +
  2150         2       1547.0    773.5      0.0          seg0.filter(objects=SpikeTrain))
  2151                                               # add channels for channelIndex that has no asigs but has spikes
  2152         2         29.0     14.5      0.0      nExtraChans = 0
  2153        24        526.0     21.9      0.0      for stp in spikeTrainLikeList:
  2154        22       1996.0     90.7      0.0          stpBaseName = childBaseName(stp.name, 'seg')
  2155        22       6591.0    299.6      0.1          nameParser = re.search(r'ch(\d*)#(\d*)', stpBaseName)
  2156        22        414.0     18.8      0.0          if nameParser is not None:
  2157                                                       # first time at this unit, rename it
  2158        22        758.0     34.5      0.0              chanId = int(nameParser.group(1))
  2159        22        537.0     24.4      0.0              unitId = int(nameParser.group(2))
  2160        22        409.0     18.6      0.0              if chanId >= 5121:
  2161        22        384.0     17.5      0.0                  isRippleStimChan = True
  2162        22        438.0     19.9      0.0                  chanId = chanId - 5120
  2163                                                       else:
  2164                                                           isRippleStimChan = False
  2165                                                       ####################
  2166                                                       # asigBaseName = headerSignalChan.loc[chanId, 'name']
  2167                                                       # if mapDF is not None:
  2168                                                       #     if asigBaseName in asigNameChanger:
  2169                                                       #         chanIdLabel = (
  2170                                                       #             asigNameChanger[asigBaseName]
  2171                                                       #             if asigBaseName in asigNameChanger
  2172                                                       #             else asigBaseName)
  2173                                                       #     else:
  2174                                                       #         chanIdLabel = asigBaseName
  2175                                                       # else:
  2176                                                       #     chanIdLabel = asigBaseName
  2177                                                       ###################
  2178                                                       # if swapMaps is not None:
  2179                                                       #     nameCandidates = (swapMaps['to'].loc[swapMaps['to']['nevID'] == chanId, 'label']).to_list()
  2180                                                       # elif mapDF is not None:
  2181                                                       #     nameCandidates = (mapDF.loc[mapDF['nevID'] == chanId, 'label']).to_list()
  2182                                                       # else:
  2183                                                       #     nameCandidates = []
  2184                                                       ##############################
  2185        22        391.0     17.8      0.0              if mapDF is not None:
  2186                                                           nameCandidates = (
  2187        22        576.0     26.2      0.0                      mapDF
  2188        22     218797.0   9945.3      2.9                      .loc[mapDF['nevID'] == chanId, 'label']
  2189                                                               .to_list())
  2190                                                       else:
  2191                                                           nameCandidates = []
  2192        22        611.0     27.8      0.0              if len(nameCandidates) == 1:
  2193        22        403.0     18.3      0.0                  chanIdLabel = nameCandidates[0]
  2194                                                       elif chanId in headerSignalChan:
  2195                                                           chanIdLabel = headerSignalChan.loc[chanId, 'name']
  2196                                                       else:
  2197                                                           chanIdLabel = 'ch{}'.format(chanId)
  2198                                                       #
  2199        22        368.0     16.7      0.0              if isRippleStimChan:
  2200        22        690.0     31.4      0.0                  stp.name = '{}_stim#{}'.format(chanIdLabel, unitId)
  2201                                                       else:
  2202                                                           stp.name = '{}#{}'.format(chanIdLabel, unitId)
  2203        22        542.0     24.6      0.0              stp.unit.name = stp.name
  2204                                                   ########################################
  2205                                                   # sanitize ripple names ####
  2206        22        624.0     28.4      0.0          stp.name = stp.name.replace('.', '_').replace(' raw', '')
  2207        22        538.0     24.5      0.0          stp.unit.name = stp.unit.name.replace('.', '_').replace(' raw', '')
  2208                                                   ###########################################
  2209        22        502.0     22.8      0.0          if 'ChannelIndex for ' in stp.unit.channel_index.name:
  2210        22        498.0     22.6      0.0              newChanName = stp.name.replace('_stim#0', '')
  2211                                                       # remove unit #
  2212        22       3396.0    154.4      0.0              newChanName = re.sub(r'#\d', '', newChanName)
  2213        22        473.0     21.5      0.0              stp.unit.channel_index.name = newChanName
  2214                                                       # units and analogsignals have different channel_indexes when loaded by nix
  2215                                                       # add them to each other's parent list
  2216        22        463.0     21.0      0.0              allMatchingChIdx = dataBlock.filter(
  2217        22     747534.0  33978.8     10.0                  objects=ChannelIndex, name=newChanName)
  2218        22        623.0     28.3      0.0              if (len(allMatchingChIdx) > 1) and reduceChannelIndexes:
  2219        22        434.0     19.7      0.0                  assert len(allMatchingChIdx) == 2
  2220                                                           targetChIdx = [
  2221        22        441.0     20.0      0.0                      ch
  2222        22        926.0     42.1      0.0                      for ch in allMatchingChIdx
  2223        22        445.0     20.2      0.0                      if ch is not stp.unit.channel_index][0]
  2224        22        436.0     19.8      0.0                  oldChIdx = stp.unit.channel_index
  2225        22        526.0     23.9      0.0                  targetChIdx.units.append(stp.unit)
  2226        22        454.0     20.6      0.0                  stp.unit.channel_index = targetChIdx
  2227        22        510.0     23.2      0.0                  oldChIdx.units.remove(stp.unit)
  2228        22        545.0     24.8      0.0                  if not (len(oldChIdx.units) or len(oldChIdx.analogsignals)):
  2229        22        763.0     34.7      0.0                      dataBlock.channel_indexes.remove(oldChIdx)
  2230        22        399.0     18.1      0.0                  del oldChIdx
  2231        22      14350.0    652.3      0.2                  targetChIdx.create_relationship()
  2232                                                       elif reduceChannelIndexes:
  2233                                                           if newChanName not in headerSignalChan['name']:
  2234                                                               stp.unit.channel_index.index = np.asarray(
  2235                                                                   [headerSignalChan['name'].size + nExtraChans])
  2236                                                               stp.unit.channel_index.channel_ids = np.asarray(
  2237                                                                   [headerSignalChan['name'].size + nExtraChans])
  2238                                                               stp.unit.channel_index.channel_names = np.asarray(
  2239                                                                   [newChanName])
  2240                                                               nExtraChans += 1
  2241                                                           if 'neo_name' not in allMatchingChIdx[0].annotations:
  2242                                                               allMatchingChIdx[0].annotations['neo_name'] = allMatchingChIdx[0].name
  2243                                                           if 'nix_name' not in allMatchingChIdx[0].annotations:
  2244                                                               allMatchingChIdx[0].annotations['nix_name'] = allMatchingChIdx[0].name
  2245        22        775.0     35.2      0.0          stp.unit.channel_index.name = stp.unit.channel_index.name.replace('.', '_').replace(' raw', '')
  2246                                               #  rename the children
  2247                                               typesNeedRenaming = [
  2248         2         33.0     16.5      0.0          SpikeTrainProxy, AnalogSignalProxy, EventProxy,
  2249         2         36.0     18.0      0.0          SpikeTrain, AnalogSignal, Event]
  2250         4         85.0     21.2      0.0      for segIdx, seg in enumerate(dataBlock.segments):
  2251         2         33.0     16.5      0.0          if seg.name is None:
  2252                                                       seg.name = 'seg{}_'.format(segIdx)
  2253                                                   else:
  2254         2         57.0     28.5      0.0              if 'seg{}_'.format(segIdx) not in seg.name:
  2255                                                           seg.name = (
  2256         2         30.0     15.0      0.0                      'seg{}_{}'
  2257                                                               .format(
  2258         2         29.0     14.5      0.0                          segIdx,
  2259         2        159.0     79.5      0.0                          childBaseName(seg.name, 'seg')))
  2260        14        216.0     15.4      0.0          for objType in typesNeedRenaming:
  2261       190      12109.0     63.7      0.2              for child in seg.filter(objects=objType):
  2262       178       3394.0     19.1      0.0                  if 'seg{}_'.format(segIdx) not in child.name:
  2263                                                               child.name = (
  2264       178       2638.0     14.8      0.0                          'seg{}_{}'
  2265                                                                   .format(
  2266       178       9409.0     52.9      0.1                              segIdx, childBaseName(child.name, 'seg')))
  2267                                                           #  todo: decide if below is needed
  2268                                                           #  elif 'seg' in child.name:
  2269                                                           #      childBaseName = '_'.join(child.name.split('_')[1:])
  2270                                                           #      child.name = 'seg{}_{}'.format(segIdx, childBaseName)
  2271                                               # [i.name for i in dataBlock.filter(objects=Unit)]
  2272                                               # [i.name for i in dataBlock.filter(objects=ChannelIndex)]
  2273                                               # [i.name for i in dataBlock.filter(objects=SpikeTrain)]
  2274                                               # [i.name for i in dataBlock.filter(objects=SpikeTrainProxy)]
  2275         2         30.0     15.0      0.0      if lazy:
  2276        24      48873.0   2036.4      0.7          for stP in dataBlock.filter(objects=SpikeTrainProxy):
  2277        22        355.0     16.1      0.0              if 'unitAnnotations' in stP.annotations:
  2278                                                           unAnnStr = stP.annotations['unitAnnotations']
  2279                                                           stP.unit.annotations.update(json.loads(unAnnStr))
  2280         2         31.0     15.5      0.0      if (loadList is not None) and lazy:
  2281                                                   if 'asigs' in loadList:
  2282                                                       loadAsigList(
  2283                                                           dataBlock, listOfAsigProxyNames=loadList['asigs'],
  2284                                                           replaceInParents=True)
  2285                                                   if 'events' in loadList:
  2286                                                       loadEventList(
  2287                                                           dataBlock,
  2288                                                           listOfEventNames=loadList['events'],
  2289                                                           replaceInParents=True)
  2290                                                   if 'spiketrains' in loadList:
  2291                                                       loadSpikeTrainList(
  2292                                                           dataBlock,
  2293                                                           listOfSpikeTrainNames=loadList['spiketrains'],
  2294                                                           replaceInParents=True)
  2295         2         31.0     15.5      0.0      if purgeNixNames:
  2296                                                   dataBlock = purgeNixAnn(dataBlock)
  2297         2         26.0     13.0      0.0      return dataBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadSpikeTrainList at line 2299

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2299                                           @profile
  2300                                           def loadSpikeTrainList(
  2301                                                   dataBlock, listOfSpikeTrainNames=None,
  2302                                                   replaceInParents=True):
  2303                                               listOfSpikeTrains = []
  2304                                               if listOfSpikeTrainNames is None:
  2305                                                   listOfSpikeTrainNames = [
  2306                                                       stp.name
  2307                                                       for stp in dataBlock.filter(objects=SpikeTrainProxy)]
  2308                                               for stP in dataBlock.filter(objects=SpikeTrainProxy):
  2309                                                   if stP.name in listOfSpikeTrainNames:
  2310                                                       st = loadObjArrayAnn(stP.load())
  2311                                                       listOfSpikeTrains.append(st)
  2312                                                       if replaceInParents:
  2313                                                           seg = stP.segment
  2314                                                           segStNames = [s.name for s in seg.spiketrains]
  2315                                                           idxInSeg = segStNames.index(stP.name)
  2316                                                           seg.spiketrains[idxInSeg] = st
  2317                                                           #
  2318                                                           unit = stP.unit
  2319                                                           unitStNames = [s.name for s in unit.spiketrains]
  2320                                                           st.unit = unit
  2321                                                           idxInUnit = unitStNames.index(stP.name)
  2322                                                           unit.spiketrains[idxInUnit] = st
  2323                                               return listOfSpikeTrains

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadEventList at line 2325

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2325                                           @profile
  2326                                           def loadEventList(
  2327                                                   dataBlock,
  2328                                                   listOfEventNames=None, replaceInParents=True):
  2329                                               listOfEvents = []
  2330                                               if listOfEventNames is None:
  2331                                                   listOfEventNames = [
  2332                                                       evp.name
  2333                                                       for evp in dataBlock.filter(objects=EventProxy)]
  2334                                               for evP in dataBlock.filter(objects=EventProxy):
  2335                                                   if evP.name in listOfEventNames:
  2336                                                       ev = loadObjArrayAnn(evP.load())
  2337                                                       listOfEvents.append(ev)
  2338                                                       if replaceInParents:
  2339                                                           seg = evP.segment
  2340                                                           segEvNames = [e.name for e in seg.events]
  2341                                                           idxInSeg = segEvNames.index(evP.name)
  2342                                                           seg.events[idxInSeg] = ev
  2343                                               return listOfEvents

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadAsigList at line 2345

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2345                                           @profile
  2346                                           def loadAsigList(
  2347                                                   dataBlock, listOfAsigProxyNames=None, replaceInParents=True):
  2348                                               listOfAsigs = []
  2349                                               if listOfAsigProxyNames is None:
  2350                                                   listOfAsigProxyNames = [
  2351                                                       asigp.name
  2352                                                       for asigp in dataBlock.filter(objects=AnalogSignalProxy)]
  2353                                               for asigP in dataBlock.filter(objects=AnalogSignalProxy):
  2354                                                   if asigP.name in listOfAsigProxyNames:
  2355                                                       asig = asigP.load()
  2356                                                       asig.annotations = asigP.annotations.copy()
  2357                                                       listOfAsigs.append(asig)
  2358                                                       #
  2359                                                       if replaceInParents:
  2360                                                           seg = asigP.segment
  2361                                                           segAsigNames = [ag.name for ag in seg.analogsignals]
  2362                                                           asig.segment = seg
  2363                                                           idxInSeg = segAsigNames.index(asigP.name)
  2364                                                           seg.analogsignals[idxInSeg] = asig
  2365                                                           #
  2366                                                           chIdx = asigP.channel_index
  2367                                                           chIdxAsigNames = [ag.name for ag in chIdx.analogsignals]
  2368                                                           asig.channel_index = chIdx
  2369                                                           idxInChIdx = chIdxAsigNames.index(asigP.name)
  2370                                                           chIdx.analogsignals[idxInChIdx] = asig
  2371                                               return listOfAsigs

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: addBlockToNIX at line 2373

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2373                                           @profile
  2374                                           def addBlockToNIX(
  2375                                                   newBlock, neoSegIdx=[0],
  2376                                                   writeAsigs=True, writeSpikes=True, writeEvents=True,
  2377                                                   asigNameList=None,
  2378                                                   purgeNixNames=False,
  2379                                                   fileName=None,
  2380                                                   folderPath=None,
  2381                                                   nixBlockIdx=0, nixSegIdx=[0],
  2382                                                   ):
  2383                                               #  base file name
  2384                                               trialBasePath = os.path.join(folderPath, fileName)
  2385                                               if writeAsigs:
  2386                                                   # peek at file to ensure compatibility
  2387                                                   reader = nixio_fr.NixIO(filename=trialBasePath + '.nix')
  2388                                                   tempBlock = reader.read_block(
  2389                                                       block_index=nixBlockIdx,
  2390                                                       lazy=True, signal_group_mode='split-all')
  2391                                                   checkCompatible = {i: False for i in nixSegIdx}
  2392                                                   forceShape = {i: None for i in nixSegIdx}
  2393                                                   forceType = {i: None for i in nixSegIdx}
  2394                                                   forceFS = {i: None for i in nixSegIdx}
  2395                                                   for nixIdx in nixSegIdx:
  2396                                                       tempAsigList = tempBlock.segments[nixIdx].filter(
  2397                                                           objects=AnalogSignalProxy)
  2398                                                       if len(tempAsigList) > 0:
  2399                                                           tempAsig = tempAsigList[0]
  2400                                                           checkCompatible[nixIdx] = True
  2401                                                           forceType[nixIdx] = tempAsig.dtype
  2402                                                           forceShape[nixIdx] = tempAsig.shape[0]  # ? docs say shape[1], but that's confusing
  2403                                                           forceFS[nixIdx] = tempAsig.sampling_rate
  2404                                                   reader.file.close()
  2405                                               #  if newBlock was loaded from a nix file, strip the old nix_names away:
  2406                                               #  todo: replace with function from this module
  2407                                               if purgeNixNames:
  2408                                                   newBlock = purgeNixAnn(newBlock)
  2409                                               #
  2410                                               writer = NixIO(filename=trialBasePath + '.nix')
  2411                                               nixblock = writer.nix_file.blocks[nixBlockIdx]
  2412                                               nixblockName = nixblock.name
  2413                                               if 'nix_name' in newBlock.annotations.keys():
  2414                                                   try:
  2415                                                       assert newBlock.annotations['nix_name'] == nixblockName
  2416                                                   except Exception:
  2417                                                       newBlock.annotations['nix_name'] = nixblockName
  2418                                               else:
  2419                                                   newBlock.annotate(nix_name=nixblockName)
  2420                                               #
  2421                                               for idx, segIdx in enumerate(neoSegIdx):
  2422                                                   nixIdx = nixSegIdx[idx]
  2423                                                   newSeg = newBlock.segments[segIdx]
  2424                                                   nixgroup = nixblock.groups[nixIdx]
  2425                                                   nixSegName = nixgroup.name
  2426                                                   if 'nix_name' in newSeg.annotations.keys():
  2427                                                       try:
  2428                                                           assert newSeg.annotations['nix_name'] == nixSegName
  2429                                                       except Exception:
  2430                                                           newSeg.annotations['nix_name'] = nixSegName
  2431                                                   else:
  2432                                                       newSeg.annotate(nix_name=nixSegName)
  2433                                                   #
  2434                                                   if writeEvents:
  2435                                                       eventList = newSeg.events
  2436                                                       eventOrder = np.argsort([i.name for i in eventList])
  2437                                                       for event in [eventList[i] for i in eventOrder]:
  2438                                                           event = writer._write_event(event, nixblock, nixgroup)
  2439                                                   #
  2440                                                   if writeAsigs:
  2441                                                       asigList = newSeg.filter(objects=AnalogSignal)
  2442                                                       asigOrder = np.argsort([i.name for i in asigList])
  2443                                                       for asig in [asigList[i] for i in asigOrder]:
  2444                                                           if checkCompatible[nixIdx]:
  2445                                                               assert asig.dtype == forceType[nixIdx]
  2446                                                               assert asig.sampling_rate == forceFS[nixIdx]
  2447                                                               #  print('asig.shape[0] = {}'.format(asig.shape[0]))
  2448                                                               #  print('forceShape[nixIdx] = {}'.format(forceShape[nixIdx]))
  2449                                                               assert asig.shape[0] == forceShape[nixIdx]
  2450                                                           asig = writer._write_analogsignal(asig, nixblock, nixgroup)
  2451                                                       #  for isig in newSeg.filter(objects=IrregularlySampledSignal):
  2452                                                       #      isig = writer._write_irregularlysampledsignal(
  2453                                                       #          isig, nixblock, nixgroup)
  2454                                                   #
  2455                                                   if writeSpikes:
  2456                                                       stList = newSeg.filter(objects=SpikeTrain)
  2457                                                       stOrder = np.argsort([i.name for i in stList])
  2458                                                       for st in [stList[i] for i in stOrder]:
  2459                                                           st = writer._write_spiketrain(st, nixblock, nixgroup)
  2460                                               #
  2461                                               for chanIdx in newBlock.filter(objects=ChannelIndex):
  2462                                                   chanIdx = writer._write_channelindex(chanIdx, nixblock)
  2463                                                   #  auto descends into units inside of _write_channelindex
  2464                                               writer._create_source_links(newBlock, nixblock)
  2465                                               writer.close()
  2466                                               print('Done adding block to Nix.')
  2467                                               return newBlock

Total time: 0.0329319 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadStProxy at line 2469

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2469                                           @profile
  2470                                           def loadStProxy(stProxy):
  2471        11         89.0      8.1      0.0      try:
  2472        11         89.0      8.1      0.0          st = stProxy.load(
  2473        11         49.0      4.5      0.0              magnitude_mode='rescaled',
  2474        11     328868.0  29897.1     99.9              load_waveforms=True)
  2475                                               except Exception:
  2476                                                   st = stProxy.load(
  2477                                                       magnitude_mode='rescaled',
  2478                                                       load_waveforms=False)
  2479                                                   st.waveforms = np.asarray([]).reshape((0, 0, 0))*pq.mV
  2480        11        224.0     20.4      0.1      return st

Total time: 8.89918 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: preproc at line 2482

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2482                                           @profile
  2483                                           def preproc(
  2484                                                   fileName='Trial001',
  2485                                                   rawFolderPath='./',
  2486                                                   outputFolderPath='./', mapDF=None,
  2487                                                   # swapMaps=None,
  2488                                                   electrodeArrayName='utah',
  2489                                                   fillOverflow=True, removeJumps=True,
  2490                                                   removeMeanAcross=False,
  2491                                                   linearDetrend=False,
  2492                                                   interpolateOutliers=False, calcOutliers=False,
  2493                                                   outlierMaskFilterOpts=None,
  2494                                                   outlierThreshold=1,
  2495                                                   calcArtifactTrace=False,
  2496                                                   motorEncoderMask=None,
  2497                                                   calcAverageLFP=False,
  2498                                                   eventInfo=None,
  2499                                                   spikeSourceType='', spikePath=None,
  2500                                                   chunkSize=1800, equalChunks=True, chunkList=None, chunkOffset=0,
  2501                                                   writeMode='rw',
  2502                                                   signal_group_mode='split-all', trialInfo=None,
  2503                                                   asigNameList=None, ainpNameList=None, nameSuffix='',
  2504                                                   saveFromAsigNameList=True,
  2505                                                   calcRigEvents=True, normalizeByImpedance=False,
  2506                                                   LFPFilterOpts=None, encoderCountPerDegree=180e2,
  2507                                                   outlierRemovalDebugFlag=False, impedanceFilePath=None
  2508                                                   ):
  2509                                               #  base file name
  2510         1        202.0    202.0      0.0      rawBasePath = os.path.join(rawFolderPath, fileName)
  2511         1         15.0     15.0      0.0      outputFilePath = os.path.join(
  2512         1         13.0     13.0      0.0          outputFolderPath,
  2513         1        122.0    122.0      0.0          fileName + nameSuffix + '.nix')
  2514         1      70194.0  70194.0      0.1      if os.path.exists(outputFilePath):
  2515         1      30653.0  30653.0      0.0          os.remove(outputFilePath)
  2516                                               #  instantiate reader, get metadata
  2517         1       1475.0   1475.0      0.0      print('Loading\n{}\n'.format(rawBasePath))
  2518         1         65.0     65.0      0.0      reader = BlackrockIO(
  2519         1     815382.0 815382.0      0.9          filename=rawBasePath, nsx_to_load=5)
  2520         1     607061.0 607061.0      0.7      reader.parse_header()
  2521                                               # metadata = reader.header
  2522                                               #  absolute section index
  2523         1         29.0     29.0      0.0      dummyBlock = readBlockFixNames(
  2524         1         15.0     15.0      0.0          reader,
  2525         1         14.0     14.0      0.0          block_index=0, lazy=True,
  2526         1         15.0     15.0      0.0          signal_group_mode=signal_group_mode,
  2527         1    3840486.0 3840486.0      4.3          mapDF=mapDF, reduceChannelIndexes=True,
  2528                                                   # swapMaps=swapMaps
  2529                                                   )
  2530         1         24.0     24.0      0.0      segLen = dummyBlock.segments[0].analogsignals[0].shape[0] / (
  2531         1        475.0    475.0      0.0          dummyBlock.segments[0].analogsignals[0].sampling_rate)
  2532         1        221.0    221.0      0.0      nChunks = math.ceil(segLen / chunkSize)
  2533                                               #
  2534         1         16.0     16.0      0.0      if equalChunks:
  2535                                                   actualChunkSize = (segLen / nChunks).magnitude
  2536                                               else:
  2537         1         15.0     15.0      0.0          actualChunkSize = chunkSize
  2538         1         15.0     15.0      0.0      if chunkList is None:
  2539         1         23.0     23.0      0.0          chunkList = range(nChunks)
  2540         1         15.0     15.0      0.0      chunkingMetadata = {}
  2541         2         63.0     31.5      0.0      for chunkIdx in chunkList:
  2542         1        571.0    571.0      0.0          print('preproc on chunk {}'.format(chunkIdx))
  2543                                                   #  instantiate spike reader if requested
  2544         1         18.0     18.0      0.0          if spikeSourceType == 'tdc':
  2545                                                       if spikePath is None:
  2546                                                           spikePath = os.path.join(
  2547                                                               outputFolderPath, 'tdc_' + fileName,
  2548                                                               'tdc_' + fileName + '.nix')
  2549                                                       print('loading {}'.format(spikePath))
  2550                                                       spikeReader = nixio_fr.NixIO(filename=spikePath)
  2551                                                   else:
  2552         1         14.0     14.0      0.0              spikeReader = None
  2553                                                   #  absolute section index
  2554         1         33.0     33.0      0.0          block = readBlockFixNames(
  2555         1         32.0     32.0      0.0              reader,
  2556         1         32.0     32.0      0.0              block_index=0, lazy=True,
  2557         1         42.0     42.0      0.0              signal_group_mode=signal_group_mode,
  2558         1    3742396.0 3742396.0      4.2              mapDF=mapDF, reduceChannelIndexes=True,
  2559                                                       # swapMaps=swapMaps
  2560                                                       )
  2561         1         14.0     14.0      0.0          if spikeReader is not None:
  2562                                                       spikeBlock = readBlockFixNames(
  2563                                                           spikeReader, block_index=0, lazy=True,
  2564                                                           signal_group_mode=signal_group_mode,
  2565                                                           mapDF=mapDF, reduceChannelIndexes=True,
  2566                                                           # swapMaps=swapMaps
  2567                                                           )
  2568                                                       spikeBlock = purgeNixAnn(spikeBlock)
  2569                                                   else:
  2570         1         14.0     14.0      0.0              spikeBlock = None
  2571                                                   #
  2572                                                   #  instantiate writer
  2573         1         15.0     15.0      0.0          if (nChunks == 1) or (len(chunkList) == 1):
  2574         1         12.0     12.0      0.0              partNameSuffix = ""
  2575         1         13.0     13.0      0.0              thisChunkOutFilePath = outputFilePath
  2576                                                   else:
  2577                                                       partNameSuffix = '_pt{:0>3}'.format(chunkIdx)
  2578                                                       thisChunkOutFilePath = (
  2579                                                           outputFilePath
  2580                                                           .replace('.nix', partNameSuffix + '.nix'))
  2581                                                   #
  2582         1        902.0    902.0      0.0          if os.path.exists(thisChunkOutFilePath):
  2583                                                       os.remove(thisChunkOutFilePath)
  2584         1         16.0     16.0      0.0          writer = NixIO(
  2585         1      66217.0  66217.0      0.1              filename=thisChunkOutFilePath, mode=writeMode)
  2586         1         25.0     25.0      0.0          chunkTStart = chunkIdx * actualChunkSize + chunkOffset
  2587         1         17.0     17.0      0.0          chunkTStop = (chunkIdx + 1) * actualChunkSize + chunkOffset
  2588                                                   chunkingMetadata[chunkIdx] = {
  2589         1         14.0     14.0      0.0              'filename': thisChunkOutFilePath,
  2590         1         14.0     14.0      0.0              'partNameSuffix': partNameSuffix,
  2591         1         14.0     14.0      0.0              'chunkTStart': chunkTStart,
  2592         1         21.0     21.0      0.0              'chunkTStop': chunkTStop}
  2593         1        140.0    140.0      0.0          block.annotate(chunkTStart=chunkTStart)
  2594         1         58.0     58.0      0.0          block.annotate(chunkTStop=chunkTStop)
  2595         1         15.0     15.0      0.0          block.annotate(
  2596                                                       recDatetimeStr=(
  2597         1         22.0     22.0      0.0                  block
  2598                                                           .rec_datetime
  2599         1        184.0    184.0      0.0                  .replace(tzinfo=timezone.utc)
  2600                                                           .isoformat())
  2601                                                       )
  2602                                                   #
  2603         1         20.0     20.0      0.0          preprocBlockToNix(
  2604         1         15.0     15.0      0.0              block, writer,
  2605         1         14.0     14.0      0.0              chunkTStart=chunkTStart,
  2606         1         14.0     14.0      0.0              chunkTStop=chunkTStop,
  2607         1         14.0     14.0      0.0              fillOverflow=fillOverflow,
  2608         1         14.0     14.0      0.0              removeJumps=removeJumps,
  2609         1         14.0     14.0      0.0              interpolateOutliers=interpolateOutliers,
  2610         1         13.0     13.0      0.0              calcOutliers=calcOutliers,
  2611         1         14.0     14.0      0.0              outlierThreshold=outlierThreshold,
  2612         1         13.0     13.0      0.0              outlierMaskFilterOpts=outlierMaskFilterOpts,
  2613         1         14.0     14.0      0.0              calcArtifactTrace=calcArtifactTrace,
  2614         1         14.0     14.0      0.0              linearDetrend=linearDetrend,
  2615         1         14.0     14.0      0.0              motorEncoderMask=motorEncoderMask,
  2616         1         14.0     14.0      0.0              electrodeArrayName=electrodeArrayName,
  2617         1         13.0     13.0      0.0              calcAverageLFP=calcAverageLFP,
  2618         1         14.0     14.0      0.0              eventInfo=eventInfo,
  2619         1         15.0     15.0      0.0              asigNameList=asigNameList, ainpNameList=ainpNameList,
  2620         1         13.0     13.0      0.0              saveFromAsigNameList=saveFromAsigNameList,
  2621         1         15.0     15.0      0.0              spikeSourceType=spikeSourceType,
  2622         1         14.0     14.0      0.0              spikeBlock=spikeBlock,
  2623         1         13.0     13.0      0.0              calcRigEvents=calcRigEvents,
  2624         1         14.0     14.0      0.0              normalizeByImpedance=normalizeByImpedance,
  2625         1         14.0     14.0      0.0              removeMeanAcross=removeMeanAcross,
  2626         1         14.0     14.0      0.0              LFPFilterOpts=LFPFilterOpts,
  2627         1         13.0     13.0      0.0              encoderCountPerDegree=encoderCountPerDegree,
  2628         1         14.0     14.0      0.0              outlierRemovalDebugFlag=outlierRemovalDebugFlag,
  2629         1   78968087.0 78968087.0     88.7              impedanceFilePath=impedanceFilePath,
  2630                                                       )
  2631                                                   #### diagnostics
  2632         1         37.0     37.0      0.0          diagnosticFolder = os.path.join(
  2633         1         15.0     15.0      0.0              outputFolderPath,
  2634         1        199.0    199.0      0.0              'preprocDiagnostics',
  2635                                                       # fileName + nameSuffix + partNameSuffix
  2636                                                       )
  2637         1        880.0    880.0      0.0          if not os.path.exists(diagnosticFolder):
  2638                                                       os.mkdir(diagnosticFolder)
  2639         1         17.0     17.0      0.0          asigDiagnostics = {}
  2640         1         15.0     15.0      0.0          outlierDiagnostics = {}
  2641         1         15.0     15.0      0.0          diagnosticText = ''
  2642         2       6840.0   3420.0      0.0          for asig in block.filter(objects=AnalogSignal):
  2643         1         17.0     17.0      0.0              annNames = ['mean_removal_r2', 'mean_removal_group']
  2644         3         44.0     14.7      0.0              for annName in annNames:
  2645         2         36.0     18.0      0.0                  if annName in asig.annotations:
  2646                                                               if asig.name not in asigDiagnostics:
  2647                                                                   asigDiagnostics[asig.name] = {}
  2648                                                               asigDiagnostics[asig.name].update({
  2649                                                                   annName: asig.annotations[annName]})
  2650                                                       annNames = [
  2651         1         14.0     14.0      0.0                  'outlierProportion', 'nDim',
  2652         1         16.0     16.0      0.0                  'noveltyThreshold', 'outlierThreshold'
  2653                                                           ]
  2654         5         75.0     15.0      0.0              for annName in annNames:
  2655         4         61.0     15.2      0.0                  if annName in asig.annotations:
  2656                                                               if asig.name not in outlierDiagnostics:
  2657                                                                   outlierDiagnostics[asig.name] = {}
  2658                                                               outlierDiagnostics[asig.name].update({
  2659                                                                   annName: '{}'.format(asig.annotations[annName])
  2660                                                               })
  2661         1         15.0     15.0      0.0          if removeMeanAcross:
  2662                                                       asigDiagnosticsDF = pd.DataFrame(asigDiagnostics).T
  2663                                                       asigDiagnosticsDF.sort_values(by='mean_removal_r2', inplace=True)
  2664                                                       diagnosticText += '<h2>LFP Diagnostics</h2>\n'
  2665                                                       diagnosticText += asigDiagnosticsDF.to_html()
  2666                                                       fig, ax = plt.subplots()
  2667                                                       sns.distplot(asigDiagnosticsDF['mean_removal_r2'], ax=ax)
  2668                                                       ax.set_ylabel('Count of analog signals')
  2669                                                       ax.set_xlabel('R^2 of regressing mean against signal')
  2670                                                       fig.savefig(os.path.join(
  2671                                                               diagnosticFolder,
  2672                                                               fileName + nameSuffix + partNameSuffix + '_meanRemovalR2.png'
  2673                                                           ))
  2674         1         15.0     15.0      0.0          if interpolateOutliers:
  2675                                                       outlierDiagnosticsDF = pd.DataFrame(outlierDiagnostics).T
  2676                                                       diagnosticText += '<h2>Outlier Diagnostics</h2>\n'
  2677                                                       diagnosticText += outlierDiagnosticsDF.to_html()
  2678         1         18.0     18.0      0.0          diagnosticTextPath = os.path.join(
  2679         1         15.0     15.0      0.0              diagnosticFolder,
  2680         1        173.0    173.0      0.0              fileName + nameSuffix + partNameSuffix + '_asigDiagnostics.html'
  2681                                                       )
  2682         1       1700.0   1700.0      0.0          with open(diagnosticTextPath, 'w') as _f:
  2683         1        276.0    276.0      0.0              _f.write(diagnosticText)
  2684         1     829177.0 829177.0      0.9          writer.close()
  2685         1         43.0     43.0      0.0      chunkingInfoPath = os.path.join(
  2686         1         22.0     22.0      0.0          outputFolderPath,
  2687         1         24.0     24.0      0.0          fileName + nameSuffix +
  2688         1        338.0    338.0      0.0          '_chunkingInfo.json'
  2689                                                   )
  2690         1        892.0    892.0      0.0      if os.path.exists(chunkingInfoPath):
  2691                                                   os.remove(chunkingInfoPath)
  2692         1       1958.0   1958.0      0.0      with open(chunkingInfoPath, 'w') as f:
  2693         1       2851.0   2851.0      0.0          json.dump(chunkingMetadata, f)
  2694         1         40.0     40.0      0.0      return

Total time: 7.88069 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: preprocBlockToNix at line 2696

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2696                                           @profile
  2697                                           def preprocBlockToNix(
  2698                                                   block, writer,
  2699                                                   chunkTStart=None,
  2700                                                   chunkTStop=None,
  2701                                                   eventInfo=None,
  2702                                                   fillOverflow=False, calcAverageLFP=False,
  2703                                                   interpolateOutliers=False, calcOutliers=False,
  2704                                                   calcArtifactTrace=False,
  2705                                                   outlierMaskFilterOpts=None,
  2706                                                   useMeanToCenter=False,   # mean center? median center?
  2707                                                   linearDetrend=False,
  2708                                                   zScoreEachTrace=False,
  2709                                                   outlierThreshold=1,
  2710                                                   motorEncoderMask=None,
  2711                                                   electrodeArrayName='utah',
  2712                                                   removeJumps=False, trackMemory=True,
  2713                                                   asigNameList=None, ainpNameList=None,
  2714                                                   saveFromAsigNameList=True,
  2715                                                   spikeSourceType='', spikeBlock=None,
  2716                                                   calcRigEvents=True,
  2717                                                   normalizeByImpedance=True,
  2718                                                   impedanceFilePath=None,
  2719                                                   removeMeanAcross=False,
  2720                                                   LFPFilterOpts=None, encoderCountPerDegree=180e2,
  2721                                                   outlierRemovalDebugFlag=False,
  2722                                                   ):
  2723                                               #  prune out nev spike placeholders
  2724                                               #  (will get added back on a chunk by chunk basis,
  2725                                               #  if not pruning units)
  2726         1         52.0     52.0      0.0      if spikeSourceType == 'nev':
  2727         1         48.0     48.0      0.0          pruneOutUnits = False
  2728                                               else:
  2729                                                   pruneOutUnits = True
  2730                                               #
  2731        71       3302.0     46.5      0.0      for chanIdx in block.channel_indexes:
  2732        70       3420.0     48.9      0.0          if chanIdx.units:
  2733        22       1033.0     47.0      0.0              for unit in chanIdx.units:
  2734        11        525.0     47.7      0.0                  if unit.spiketrains:
  2735        11        526.0     47.8      0.0                      unit.spiketrains = []
  2736        11        508.0     46.2      0.0              if pruneOutUnits:
  2737                                                           chanIdx.units = []
  2738                                               #
  2739         1         48.0     48.0      0.0      if spikeBlock is not None:
  2740                                                   for chanIdx in spikeBlock.channel_indexes:
  2741                                                       if chanIdx.units:
  2742                                                           for unit in chanIdx.units:
  2743                                                               if unit.spiketrains:
  2744                                                                   unit.spiketrains = []
  2745                                               #  precalculate new segment
  2746         1         51.0     51.0      0.0      seg = block.segments[0]
  2747                                               #  remove chanIndexes assigned to units; makes more sense to
  2748                                               #  only use chanIdx for asigs and spikes on that asig
  2749                                               #  block.channel_indexes = (
  2750                                               #      [chanIdx for chanIdx in block.channel_indexes if (
  2751                                               #          chanIdx.analogsignals)])
  2752         1         48.0     48.0      0.0      if calcAverageLFP:
  2753                                                   lastIndex = len(block.channel_indexes)
  2754                                                   lastID = block.channel_indexes[-1].channel_ids[0] + 1
  2755                                                   if asigNameList is None:
  2756                                                       asigNameList = [
  2757                                                           [
  2758                                                               childBaseName(a.name, 'seg')
  2759                                                               for a in seg.analogsignals
  2760                                                               if not (('ainp' in a.name) or ('analog' in a.name))]
  2761                                                           ]
  2762                                                   nMeanChans = len(asigNameList)
  2763                                                   #
  2764                                                   meanChIdxList = []
  2765                                                   for meanChIdx in range(nMeanChans):
  2766                                                       tempChIdx = ChannelIndex(
  2767                                                           index=[lastIndex + meanChIdx],
  2768                                                           channel_names=['{}_rawAverage_{}'.format(electrodeArrayName, meanChIdx)],
  2769                                                           channel_ids=[lastID + meanChIdx],
  2770                                                           name='{}_rawAverage_{}'.format(electrodeArrayName, meanChIdx),
  2771                                                           file_origin=block.channel_indexes[-1].file_origin
  2772                                                           )
  2773                                                       tempChIdx.merge_annotations(block.channel_indexes[-1])
  2774                                                       block.channel_indexes.append(tempChIdx)
  2775                                                       meanChIdxList.append(tempChIdx)
  2776                                                       lastIndex += 1
  2777                                                       lastID += 1
  2778                                                   lastIndex = len(block.channel_indexes)
  2779                                                   lastID = block.channel_indexes[-1].channel_ids[0] + 1
  2780                                                   # if calcArtifactTrace:
  2781                                                   if True:
  2782                                                       artChIdxList = []
  2783                                                       for artChIdx in range(nMeanChans):
  2784                                                           tempChIdx = ChannelIndex(
  2785                                                               index=[lastIndex + artChIdx],
  2786                                                               channel_names=['{}_artifact_{}'.format(electrodeArrayName, artChIdx)],
  2787                                                               channel_ids=[lastID + artChIdx],
  2788                                                               name='{}_artifact_{}'.format(electrodeArrayName, artChIdx),
  2789                                                               file_origin=block.channel_indexes[-1].file_origin
  2790                                                               )
  2791                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2792                                                           block.channel_indexes.append(tempChIdx)
  2793                                                           artChIdxList.append(tempChIdx)
  2794                                                           lastIndex += 1
  2795                                                           lastID += 1
  2796                                                   # if calcOutliers:
  2797                                                   if True:
  2798                                                       devChIdxList = []
  2799                                                       for devChIdx in range(nMeanChans):
  2800                                                           tempChIdx = ChannelIndex(
  2801                                                               index=[lastIndex + devChIdx],
  2802                                                               channel_names=['{}_deviation_{}'.format(electrodeArrayName, devChIdx)],
  2803                                                               channel_ids=[lastID + devChIdx],
  2804                                                               name='{}_deviation_{}'.format(electrodeArrayName, devChIdx),
  2805                                                               file_origin=block.channel_indexes[-1].file_origin
  2806                                                               )
  2807                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2808                                                           block.channel_indexes.append(tempChIdx)
  2809                                                           devChIdxList.append(tempChIdx)
  2810                                                           lastIndex += 1
  2811                                                           lastID += 1
  2812                                                       smDevChIdxList = []
  2813                                                       for devChIdx in range(nMeanChans):
  2814                                                           tempChIdx = ChannelIndex(
  2815                                                               index=[lastIndex + devChIdx],
  2816                                                               channel_names=['{}_smoothed_deviation_{}'.format(electrodeArrayName, devChIdx)],
  2817                                                               channel_ids=[lastID + devChIdx],
  2818                                                               name='{}_smoothed_deviation_{}'.format(electrodeArrayName, devChIdx),
  2819                                                               file_origin=block.channel_indexes[-1].file_origin
  2820                                                               )
  2821                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2822                                                           block.channel_indexes.append(tempChIdx)
  2823                                                           smDevChIdxList.append(tempChIdx)
  2824                                                           lastIndex += 1
  2825                                                           lastID += 1
  2826                                                       outMaskChIdxList = []
  2827                                                       for outMaskChIdx in range(nMeanChans):
  2828                                                           tempChIdx = ChannelIndex(
  2829                                                               index=[lastIndex + outMaskChIdx],
  2830                                                               channel_names=['{}_outlierMask_{}'.format(
  2831                                                                   electrodeArrayName, outMaskChIdx)],
  2832                                                               channel_ids=[lastID + outMaskChIdx],
  2833                                                               name='{}_outlierMask_{}'.format(
  2834                                                                   electrodeArrayName, outMaskChIdx),
  2835                                                               file_origin=block.channel_indexes[-1].file_origin
  2836                                                               )
  2837                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2838                                                           block.channel_indexes.append(tempChIdx)
  2839                                                           outMaskChIdxList.append(tempChIdx)
  2840                                                           lastIndex += 1
  2841                                                           lastID += 1
  2842                                               #  delete asig and irsig proxies from channel index list
  2843        71       3449.0     48.6      0.0      for metaIdx, chanIdx in enumerate(block.channel_indexes):
  2844        70       3422.0     48.9      0.0          if chanIdx.analogsignals:
  2845        70       3479.0     49.7      0.0              chanIdx.analogsignals = []
  2846        70       3419.0     48.8      0.0          if chanIdx.irregularlysampledsignals:
  2847                                                       chanIdx.irregularlysampledsignals = []
  2848         1         49.0     49.0      0.0      newSeg = Segment(
  2849         1         50.0     50.0      0.0              index=0, name=seg.name,
  2850         1         50.0     50.0      0.0              description=seg.description,
  2851         1         50.0     50.0      0.0              file_origin=seg.file_origin,
  2852         1         49.0     49.0      0.0              file_datetime=seg.file_datetime,
  2853         1         52.0     52.0      0.0              rec_datetime=seg.rec_datetime,
  2854         1        517.0    517.0      0.0              **seg.annotations
  2855                                                   )
  2856         1         51.0     51.0      0.0      block.segments = [newSeg]
  2857         1     386743.0 386743.0      0.5      block, nixblock = writer.write_block_meta(block)
  2858                                               # descend into Segments
  2859         1        111.0    111.0      0.0      if impedanceFilePath is not None:
  2860                                                   try:
  2861                                                       impedances = prb_meta.getLatestImpedance(
  2862                                                           block=block, impedanceFilePath=impedanceFilePath)
  2863                                                       averageImpedance = impedances['impedance'].median()
  2864                                                   except Exception:
  2865                                                       traceback.print_exc()
  2866                                               # for segIdx, seg in enumerate(oldSegList):
  2867         1         98.0     98.0      0.0      if spikeBlock is not None:
  2868                                                   spikeSeg = spikeBlock.segments[0]
  2869                                               else:
  2870         1         95.0     95.0      0.0          spikeSeg = seg
  2871                                               #
  2872         1         96.0     96.0      0.0      if trackMemory:
  2873         1        107.0    107.0      0.0          print('memory usage: {:.1f} MB'.format(
  2874         1       2272.0   2272.0      0.0              prf.memory_usage_psutil()))
  2875         1     179474.0 179474.0      0.2      newSeg, nixgroup = writer._write_segment_meta(newSeg, nixblock)
  2876                                               #  trim down list of analog signals if necessary
  2877         1         88.0     88.0      0.0      asigNameListSeg = []
  2878         1         83.0     83.0      0.0      if (removeMeanAcross or calcAverageLFP):
  2879                                                   meanGroups = {}
  2880         1         92.0     92.0      0.0      for subListIdx, subList in enumerate(asigNameList):
  2881                                                   subListSeg = [
  2882                                                       'seg{}_{}'.format(0, a)
  2883                                                       for a in subList]
  2884                                                   asigNameListSeg += subListSeg
  2885                                                   if (removeMeanAcross or calcAverageLFP):
  2886                                                       meanGroups[subListIdx] = subListSeg
  2887         1         81.0     81.0      0.0      aSigList = []
  2888                                               # [asig.name for asig in seg.analogsignals]
  2889        71       5373.0     75.7      0.0      for a in seg.analogsignals:
  2890                                                   # if np.any([n in a.name for n in asigNameListSeg]):
  2891        70       5556.0     79.4      0.0          if a.name in asigNameListSeg:
  2892                                                       aSigList.append(a)
  2893         1         72.0     72.0      0.0      if ainpNameList is not None:
  2894                                                   ainpNameListSeg = [
  2895         1         73.0     73.0      0.0              'seg{}_{}'.format(0, a)
  2896         1        113.0    113.0      0.0              for a in ainpNameList]
  2897         1         72.0     72.0      0.0          ainpList = []
  2898        71       4642.0     65.4      0.0          for a in seg.analogsignals:
  2899        70      12779.0    182.6      0.0              if np.any([n == a.name for n in ainpNameListSeg]):
  2900         1         75.0     75.0      0.0                  ainpList.append(a)
  2901                                               else:
  2902                                                   ainpList = [
  2903                                                       a
  2904                                                       for a in seg.analogsignals
  2905                                                       if (('ainp' in a.name) or ('analog' in a.name))]
  2906                                                   ainpNameListSeg = [a.name for a in aSigList]
  2907         1         78.0     78.0      0.0      nAsigs = len(aSigList)
  2908         1         76.0     76.0      0.0      if LFPFilterOpts is not None:
  2909                                                   def filterFun(sig, filterCoeffs=None):
  2910                                                       # sig[:] = signal.sosfiltfilt(
  2911                                                       sig[:] = signal.sosfilt(
  2912                                                           filterCoeffs, sig.magnitude.flatten())[:, np.newaxis] * sig.units
  2913                                                       return sig
  2914                                                   filterCoeffs = hf.makeFilterCoeffsSOS(
  2915                                                       LFPFilterOpts, float(seg.analogsignals[0].sampling_rate))
  2916                                                   if False:
  2917                                                       fig, ax1, ax2 = hf.plotFilterResponse(
  2918                                                           filterCoeffs,
  2919                                                           float(seg.analogsignals[0].sampling_rate))
  2920                                                       fig2, ax3, ax4 = hf.plotFilterImpulseResponse(
  2921                                                           LFPFilterOpts,
  2922                                                           float(seg.analogsignals[0].sampling_rate))
  2923                                                       plt.show()
  2924                                               # first pass through asigs, if removing mean across channels
  2925         1         78.0     78.0      0.0      if (removeMeanAcross or calcAverageLFP):
  2926                                                   for aSigIdx, aSigProxy in enumerate(seg.analogsignals):
  2927                                                       if aSigIdx == 0:
  2928                                                           # check bounds
  2929                                                           tStart = max(chunkTStart * pq.s, aSigProxy.t_start)
  2930                                                           tStop = min(chunkTStop * pq.s, aSigProxy.t_stop)
  2931                                                       loadThisOne = (aSigProxy in aSigList)
  2932                                                       if loadThisOne:
  2933                                                           if trackMemory:
  2934                                                               print(
  2935                                                                   'Extracting asig for mean, memory usage: {:.1f} MB'.format(
  2936                                                                       prf.memory_usage_psutil()))
  2937                                                           chanIdx = aSigProxy.channel_index
  2938                                                           asig = aSigProxy.load(
  2939                                                               time_slice=(tStart, tStop),
  2940                                                               magnitude_mode='rescaled')
  2941                                                           if 'tempLFPStore' not in locals():
  2942                                                               tempLFPStore = pd.DataFrame(
  2943                                                                   np.zeros(
  2944                                                                       (asig.shape[0], nAsigs),
  2945                                                                       dtype=np.float32),
  2946                                                                   columns=asigNameListSeg)
  2947                                                           if 'dummyAsig' not in locals():
  2948                                                               dummyAsig = asig.copy()
  2949                                                           #  perform requested preproc operations
  2950                                                           #  if LFPFilterOpts is not None:
  2951                                                           #      asig[:] = filterFun(
  2952                                                           #          asig, filterCoeffs=filterCoeffs)
  2953                                                           if normalizeByImpedance:
  2954                                                               elNmMatchMsk = impedances['elec'] == chanIdx.name
  2955                                                               '''
  2956                                                               asig.magnitude[:] = (
  2957                                                                   (asig.magnitude - np.median(asig.magnitude)) /
  2958                                                                   np.min(
  2959                                                                       impedances.loc[elNmMatchMsk, 'impedance']
  2960                                                                       ))
  2961                                                               '''
  2962                                                               asig.magnitude[:] = (
  2963                                                                   (asig.magnitude) * averageImpedance /
  2964                                                                   np.min(
  2965                                                                       impedances.loc[elNmMatchMsk, 'impedance']
  2966                                                                       ))
  2967                                                           # if fillOverflow:
  2968                                                           #     # fill in overflow:
  2969                                                           #     '''
  2970                                                           #     timeSection['data'], overflowMask = hf.fillInOverflow(
  2971                                                           #         timeSection['data'], fillMethod = 'average')
  2972                                                           #     badData.update({'overflow': overflowMask})
  2973                                                           #     '''
  2974                                                           #     pass
  2975                                                           # if removeJumps:
  2976                                                           #     # find unusual jumps in derivative or amplitude
  2977                                                           #     '''
  2978                                                           #     timeSection['data'], newBadData = hf.fillInJumps(timeSection['data'],
  2979                                                           #     timeSection['samp_per_s'], smoothing_ms = 0.5, nStdDiff = 50,
  2980                                                           #     nStdAmp = 100)
  2981                                                           #     badData.update(newBadData)
  2982                                                           #     '''
  2983                                                           #     pass
  2984                                                           tempLFPStore.loc[:, aSigProxy.name] = asig.magnitude.flatten()
  2985                                                           del asig
  2986                                                           gc.collect()
  2987                                                   # end of first pass
  2988                                                   if (removeMeanAcross or calcAverageLFP):
  2989                                                       centerLFP = np.zeros(
  2990                                                           (tempLFPStore.shape[0], len(asigNameList)),
  2991                                                           dtype=np.float32)
  2992                                                       spreadLFP = np.zeros(
  2993                                                           (tempLFPStore.shape[0], len(asigNameList)),
  2994                                                           dtype=np.float32)
  2995                                                       # if calcOutliers:
  2996                                                       if True:
  2997                                                           if outlierMaskFilterOpts is not None:
  2998                                                               filterCoeffsOutlierMask = hf.makeFilterCoeffsSOS(
  2999                                                                   outlierMaskFilterOpts, float(dummyAsig.sampling_rate))
  3000                                                           lfpDeviation = np.zeros(
  3001                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3002                                                               dtype=np.float32)
  3003                                                           smoothedDeviation = np.zeros(
  3004                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3005                                                               dtype=np.float32)
  3006                                                           outlierMask = np.zeros(
  3007                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3008                                                               dtype=np.bool)
  3009                                                           outlierMetadata = {}
  3010                                                       # if calcArtifactTrace:
  3011                                                       if True:
  3012                                                           artifactSignal = np.zeros(
  3013                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3014                                                               dtype=np.float32)
  3015                                                       ###############
  3016                                                       # tempLFPStore.iloc[:, 0] = np.nan  # for debugging axes
  3017                                                       #############
  3018                                                       plotDevFilterDebug = False
  3019                                                       if plotDevFilterDebug:
  3020                                                           try:
  3021                                                               devFiltDebugMask = (dummyAsig.times > 90 * pq.s) & (dummyAsig.times < 92 * pq.s)
  3022                                                           except Exception:
  3023                                                               pdb.set_trace()
  3024                                                           plotColIdx = 1
  3025                                                           ddfFig, ddfAx = plt.subplots(len(asigNameList), 1)
  3026                                                           ddfFig2, ddfAx2 = plt.subplots()
  3027                                                           ddfFig3, ddfAx3 = plt.subplots(
  3028                                                               1, len(asigNameList),
  3029                                                               sharey=True)
  3030                                                           if len(asigNameList) == 1:
  3031                                                               ddfAx = np.asarray([ddfAx])
  3032                                                               ddfAx3 = np.asarray([ddfAx3])
  3033                                                       for subListIdx, subList in enumerate(asigNameList):
  3034                                                           columnsForThisGroup = meanGroups[subListIdx]
  3035                                                           if trackMemory:
  3036                                                               print(
  3037                                                                   'asig group {}: calculating mean, memory usage: {:.1f} MB'.format(
  3038                                                                       subListIdx, prf.memory_usage_psutil()))
  3039                                                               print('this group contains\n{}'.format(columnsForThisGroup))
  3040                                                           if plotDevFilterDebug:
  3041                                                               ddfAx3[subListIdx].plot(
  3042                                                                   dummyAsig.times[devFiltDebugMask],
  3043                                                                   tempLFPStore.loc[:, columnsForThisGroup].iloc[devFiltDebugMask, plotColIdx],
  3044                                                                   label='original ch'
  3045                                                                   )
  3046                                                           if fillOverflow:
  3047                                                               print('Filling overflow...')
  3048                                                               # fill in overflow:
  3049                                                               tempLFPStore.loc[:, columnsForThisGroup], pltHandles = hf.fillInOverflow2(
  3050                                                                   tempLFPStore.loc[:, columnsForThisGroup].to_numpy(),
  3051                                                                   overFlowFillType='average',
  3052                                                                   overFlowThreshold=8000,
  3053                                                                   debuggingPlots=plotDevFilterDebug
  3054                                                                   )
  3055                                                               if plotDevFilterDebug:
  3056                                                                   pltHandles['ax'].set_title('ch grp {}'.format(subListIdx))
  3057                                                                   ddfAx3[subListIdx].plot(
  3058                                                                       dummyAsig.times[devFiltDebugMask],
  3059                                                                       tempLFPStore.loc[:, columnsForThisGroup].iloc[devFiltDebugMask, plotColIdx],
  3060                                                                       label='filled ch'
  3061                                                                       )
  3062                                                           # zscore of each trace
  3063                                                           if zScoreEachTrace:
  3064                                                               print('About to calculate zscore of each trace (along columns) for prelim outlier detection')
  3065                                                               columnZScore = pd.DataFrame(
  3066                                                                   stats.zscore(
  3067                                                                       tempLFPStore.loc[:, columnsForThisGroup],
  3068                                                                       axis=1),
  3069                                                                   index=tempLFPStore.index,
  3070                                                                   columns=columnsForThisGroup
  3071                                                                   )
  3072                                                               excludeFromMeanMask = columnZScore.abs() > 6
  3073                                                               if useMeanToCenter:
  3074                                                                   centerLFP[:, subListIdx] = (
  3075                                                                       tempLFPStore
  3076                                                                       .loc[:, columnsForThisGroup]
  3077                                                                       .mask(excludeFromMeanMask)
  3078                                                                       .mean(axis=1).to_numpy()
  3079                                                                       )
  3080                                                               else:
  3081                                                                   centerLFP[:, subListIdx] = (
  3082                                                                       tempLFPStore
  3083                                                                       .loc[:, columnsForThisGroup]
  3084                                                                       .mask(excludeFromMeanMask)
  3085                                                                       .median(axis=1).to_numpy()
  3086                                                                       )
  3087                                                           else:
  3088                                                               if useMeanToCenter:
  3089                                                                   centerLFP[:, subListIdx] = (
  3090                                                                       tempLFPStore
  3091                                                                       .loc[:, columnsForThisGroup]
  3092                                                                       .mean(axis=1).to_numpy()
  3093                                                                       )
  3094                                                               else:
  3095                                                                   centerLFP[:, subListIdx] = (
  3096                                                                       tempLFPStore
  3097                                                                       .loc[:, columnsForThisGroup]
  3098                                                                       .median(axis=1).to_numpy()
  3099                                                                       )
  3100                                                           if calcArtifactTrace:
  3101                                                               if LFPFilterOpts is not None:
  3102                                                                   print('applying LFPFilterOpts to cached asigs for artifact ID')
  3103                                                                   # tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfilt(
  3104                                                                   tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfiltfilt(
  3105                                                                       filterCoeffs, tempLFPStore.loc[:, columnsForThisGroup],
  3106                                                                       axis=0)
  3107                                                                   if useMeanToCenter:
  3108                                                                       tempCenter = (
  3109                                                                           tempLFPStore
  3110                                                                           .loc[:, columnsForThisGroup]
  3111                                                                           .mean(axis=1).diff().fillna(0)
  3112                                                                           )
  3113                                                                   else:
  3114                                                                       tempCenter = (
  3115                                                                           tempLFPStore
  3116                                                                           .loc[:, columnsForThisGroup]
  3117                                                                           .median(axis=1).diff().fillna(0)
  3118                                                                           )
  3119                                                               artifactSignal[:, subListIdx] = np.abs(stats.zscore(tempCenter.to_numpy()))
  3120                                                           if calcOutliers:
  3121                                                               if plotDevFilterDebug:
  3122                                                                   ddfAx[subListIdx].plot(
  3123                                                                       dummyAsig.times[devFiltDebugMask],
  3124                                                                       centerLFP[devFiltDebugMask, subListIdx],
  3125                                                                       label='mean of ch group'
  3126                                                                       )
  3127                                                               # filter the traces, if needed
  3128                                                               if LFPFilterOpts is not None:
  3129                                                                   print('applying LFPFilterOpts to cached asigs before outlier detection')
  3130                                                                   # tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfiltfilt(
  3131                                                                   tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfilt(
  3132                                                                       filterCoeffs, tempLFPStore.loc[:, columnsForThisGroup],
  3133                                                                       axis=0)
  3134                                                                   if plotDevFilterDebug:
  3135                                                                       ddfAx3[subListIdx].plot(
  3136                                                                           dummyAsig.times[devFiltDebugMask],
  3137                                                                           tempLFPStore.loc[:, columnsForThisGroup].iloc[devFiltDebugMask, plotColIdx],
  3138                                                                           label='filtered ch'
  3139                                                                           )
  3140                                                               ##################################
  3141                                                               print('Whitening cached traces before outlier detection')
  3142                                                               whitenByPCA = True
  3143                                                               if whitenByPCA:
  3144                                                                   projector = PCA(
  3145                                                                       n_components=None, whiten=True)
  3146                                                                   pcs = projector.fit_transform(
  3147                                                                       tempLFPStore.loc[:, columnsForThisGroup])
  3148                                                                   explVarMask = (
  3149                                                                       np.cumsum(projector.explained_variance_ratio_) < 1 - 1e-2)
  3150                                                                   explVarMask[0] = True  # (keep at least 1)
  3151                                                                   pcs = pcs[:, explVarMask]
  3152                                                                   nDim = pcs.shape[1]
  3153                                                                   lfpDeviation[:, subListIdx] = (pcs ** 2).sum(axis=1)
  3154                                                               else:  # whiten by mahalanobis distance
  3155                                                                   est = EmpiricalCovariance()
  3156                                                                   est.fit(tempLFPStore.loc[:, columnsForThisGroup].to_numpy())
  3157                                                                   lfpDeviation[:, subListIdx] = est.mahalanobis(
  3158                                                                       tempLFPStore.loc[:, columnsForThisGroup].to_numpy())
  3159                                                                   nDim = tempLFPStore.loc[:, columnsForThisGroup].shape[1]
  3160                                                               #
  3161                                                               transformedDeviation = stats.norm.isf(stats.chi2.sf(lfpDeviation[:, subListIdx], nDim))
  3162                                                               infMask = np.isinf(transformedDeviation)
  3163                                                               if infMask.any():
  3164                                                                   transformedDeviation[infMask] = transformedDeviation[~infMask].max()
  3165                                                               debugProbaTrans = False
  3166                                                               if debugProbaTrans:
  3167                                                                   fig, ax = plt.subplots()
  3168                                                                   tAx = ax.twinx()
  3169                                                                   plotMask = (dummyAsig.times >= 60 * pq.s) & (dummyAsig.times < 95 * pq.s)
  3170                                                                   ax.plot(dummyAsig.times[plotMask], transformedDeviation[plotMask], c='b', label='transformed deviation')
  3171                                                                   tAx.plot(dummyAsig.times[plotMask], lfpDeviation[plotMask, subListIdx], c='r', label='original deviation')
  3172                                                                   ax.legend(loc='upper left')
  3173                                                                   tAx.legend(loc='upper right')
  3174                                                                   plt.show()
  3175                                                               lfpDeviation[:, subListIdx] = transformedDeviation
  3176                                                               noveltyThreshold = stats.norm.interval(outlierThreshold)[1]
  3177                                                               # chi2Bounds = stats.chi2.interval(outlierThreshold, nDim)
  3178                                                               # lfpDeviation[:, subListIdx] = lfpDeviation[:, subListIdx] / chi2Bounds[1]
  3179                                                               # print('nDim = {}, chi2Lim = {}'.format(nDim, chi2Bounds))
  3180                                                               # noveltyThreshold = 1
  3181                                                               #
  3182                                                               outlierMetadata[subListIdx] = {
  3183                                                                   'nDim': nDim,
  3184                                                                   'noveltyThreshold': noveltyThreshold,
  3185                                                                   'outlierThreshold': outlierThreshold
  3186                                                                   }
  3187                                                               # smoothedDeviation = signal.sosfilt(
  3188                                                               print('Smoothing deviation')
  3189                                                               tempSmDev = signal.sosfiltfilt(
  3190                                                                   filterCoeffsOutlierMask, lfpDeviation[:, subListIdx])
  3191                                                               smoothedDeviation[:, subListIdx] = tempSmDev
  3192                                                               if plotDevFilterDebug:
  3193                                                                   ddfAx[subListIdx].plot(
  3194                                                                       dummyAsig.times[devFiltDebugMask],
  3195                                                                       lfpDeviation[devFiltDebugMask, subListIdx],
  3196                                                                       label='original deviation (ch grp {})'.format(subListIdx))
  3197                                                                   ddfAx[subListIdx].plot(
  3198                                                                       dummyAsig.times[devFiltDebugMask],
  3199                                                                       smoothedDeviation[devFiltDebugMask, subListIdx],
  3200                                                                       label='filtered deviation (ch grp {})'.format(subListIdx))
  3201                                                               ##
  3202                                                               print('Calculating outlier mask')
  3203                                                               outlierMask[:, subListIdx] = (
  3204                                                                   smoothedDeviation[:, subListIdx] > noveltyThreshold)
  3205                                                               if plotDevFilterDebug:
  3206                                                                   ddfAx[subListIdx].axhline(noveltyThreshold, c='r')
  3207                                                       if plotDevFilterDebug and calcOutliers:
  3208                                                           for subListIdx, subList in enumerate(asigNameList):
  3209                                                               ddfAx[subListIdx].legend(loc='upper right')
  3210                                                               ddfAx[subListIdx].set_title('Deviation')
  3211                                                               ddfAx3[subListIdx].legend(loc='upper right')
  3212                                                               ddfAx3[subListIdx].set_title('Example channel')
  3213                                                               ddfAx2.plot(
  3214                                                                   dummyAsig.times[devFiltDebugMask],
  3215                                                                   smoothedDeviation[devFiltDebugMask, subListIdx],
  3216                                                                   label='ch grp {}'.format(subListIdx))
  3217                                                               ddfAx2.set_title('Smoothed Deviation')
  3218                                                           ddfAx2.legend(loc='upper right')
  3219                                                           plt.show()
  3220                                                       #############
  3221                                                       del tempLFPStore
  3222                                                       gc.collect()
  3223         1         79.0     79.0      0.0      if (removeMeanAcross or calcAverageLFP):
  3224                                                   for mIdx, meanChIdx in enumerate(meanChIdxList):
  3225                                                       meanAsig = AnalogSignal(
  3226                                                           centerLFP[:, mIdx],
  3227                                                           units=dummyAsig.units,
  3228                                                           sampling_rate=dummyAsig.sampling_rate,
  3229                                                           # name='seg{}_{}'.format(idx, meanChIdx.name)
  3230                                                           name='seg{}_{}'.format(0, meanChIdx.name),
  3231                                                           t_start=tStart
  3232                                                       )
  3233                                                       # assign ownership to containers
  3234                                                       meanChIdx.analogsignals.append(meanAsig)
  3235                                                       newSeg.analogsignals.append(meanAsig)
  3236                                                       # assign parent to children
  3237                                                       meanChIdx.create_relationship()
  3238                                                       newSeg.create_relationship()
  3239                                                       # write out to file
  3240                                                       if LFPFilterOpts is not None:
  3241                                                           meanAsig[:] = filterFun(
  3242                                                               meanAsig, filterCoeffs=filterCoeffs)
  3243                                                       meanAsig = writer._write_analogsignal(
  3244                                                           meanAsig, nixblock, nixgroup)
  3245                                                   # if calcArtifactTrace:
  3246                                                   if True:
  3247                                                       for mIdx, artChIdx in enumerate(artChIdxList):
  3248                                                           artAsig = AnalogSignal(
  3249                                                               artifactSignal[:, mIdx],
  3250                                                               units=dummyAsig.units,
  3251                                                               sampling_rate=dummyAsig.sampling_rate,
  3252                                                               # name='seg{}_{}'.format(idx, devChIdx.name)
  3253                                                               name='seg{}_{}'.format(0, artChIdx.name),
  3254                                                               t_start=tStart
  3255                                                               )
  3256                                                           # assign ownership to containers
  3257                                                           artChIdx.analogsignals.append(artAsig)
  3258                                                           newSeg.analogsignals.append(artAsig)
  3259                                                           # assign parent to children
  3260                                                           artChIdx.create_relationship()
  3261                                                           newSeg.create_relationship()
  3262                                                           # write out to file
  3263                                                           artAsig = writer._write_analogsignal(
  3264                                                               artAsig, nixblock, nixgroup)
  3265                                                           #########################################################
  3266                                                   # if calcOutliers:
  3267                                                   if True:
  3268                                                       for mIdx, devChIdx in enumerate(devChIdxList):
  3269                                                           devAsig = AnalogSignal(
  3270                                                               lfpDeviation[:, mIdx],
  3271                                                               units=dummyAsig.units,
  3272                                                               sampling_rate=dummyAsig.sampling_rate,
  3273                                                               # name='seg{}_{}'.format(idx, devChIdx.name)
  3274                                                               name='seg{}_{}'.format(0, devChIdx.name),
  3275                                                               t_start=tStart
  3276                                                               )
  3277                                                           # assign ownership to containers
  3278                                                           devChIdx.analogsignals.append(devAsig)
  3279                                                           newSeg.analogsignals.append(devAsig)
  3280                                                           # assign parent to children
  3281                                                           devChIdx.create_relationship()
  3282                                                           newSeg.create_relationship()
  3283                                                           # write out to file
  3284                                                           devAsig = writer._write_analogsignal(
  3285                                                               devAsig, nixblock, nixgroup)
  3286                                                           #########################################################
  3287                                                       for mIdx, smDevChIdx in enumerate(smDevChIdxList):
  3288                                                           smDevAsig = AnalogSignal(
  3289                                                               smoothedDeviation[:, mIdx],
  3290                                                               units=dummyAsig.units,
  3291                                                               sampling_rate=dummyAsig.sampling_rate,
  3292                                                               # name='seg{}_{}'.format(idx, devChIdx.name)
  3293                                                               name='seg{}_{}'.format(0, smDevChIdx.name),
  3294                                                               t_start=tStart
  3295                                                               )
  3296                                                           # assign ownership to containers
  3297                                                           smDevChIdx.analogsignals.append(smDevAsig)
  3298                                                           newSeg.analogsignals.append(smDevAsig)
  3299                                                           # assign parent to children
  3300                                                           smDevChIdx.create_relationship()
  3301                                                           newSeg.create_relationship()
  3302                                                           # write out to file
  3303                                                           smDevAsig = writer._write_analogsignal(
  3304                                                               smDevAsig, nixblock, nixgroup)
  3305                                                           #########################################################
  3306                                                       for mIdx, outMaskChIdx in enumerate(outMaskChIdxList):
  3307                                                           outMaskAsig = AnalogSignal(
  3308                                                               outlierMask[:, mIdx],
  3309                                                               units=dummyAsig.units,
  3310                                                               sampling_rate=dummyAsig.sampling_rate,
  3311                                                               # name='seg{}_{}'.format(idx, outMaskChIdx.name)
  3312                                                               name='seg{}_{}'.format(0, outMaskChIdx.name),
  3313                                                               t_start=tStart, dtype=np.float32
  3314                                                               )
  3315                                                           outMaskAsig.annotations['outlierProportion'] = np.mean(outlierMask[:, mIdx])
  3316                                                           if calcOutliers:
  3317                                                               outMaskAsig.annotations.update(outlierMetadata[mIdx])
  3318                                                           # assign ownership to containers
  3319                                                           outMaskChIdx.analogsignals.append(outMaskAsig)
  3320                                                           newSeg.analogsignals.append(outMaskAsig)
  3321                                                           # assign parent to children
  3322                                                           outMaskChIdx.create_relationship()
  3323                                                           newSeg.create_relationship()
  3324                                                           # write out to file
  3325                                                           outMaskAsig = writer._write_analogsignal(
  3326                                                               outMaskAsig, nixblock, nixgroup)
  3327                                                   #
  3328                                                   w0 = 60
  3329                                                   bandQ = 20
  3330                                                   bw = w0/bandQ
  3331                                                   noiseSos = signal.iirfilter(
  3332                                                       N=8, Wn=[w0 - bw/2, w0 + bw/2],
  3333                                                       btype='band', ftype='butter',
  3334                                                       analog=False, fs=float(dummyAsig.sampling_rate),
  3335                                                       output='sos')
  3336                                                   # signal.hilbert does not have an option to zero pad
  3337                                                   nextLen = fftpack.helper.next_fast_len(dummyAsig.shape[0])
  3338                                                   deficit = int(nextLen - dummyAsig.shape[0])
  3339                                                   lDef = int(np.floor(deficit / 2))
  3340                                                   rDef = int(np.ceil(deficit / 2)) + 1
  3341                                                   temp = np.pad(
  3342                                                       dummyAsig.magnitude.flatten(),
  3343                                                       (lDef, rDef), mode='constant')
  3344                                                   # lineNoise = signal.sosfiltfilt(
  3345                                                   lineNoise = signal.sosfilt(
  3346                                                       noiseSos, temp, axis=0)
  3347                                                   lineNoiseH = signal.hilbert(lineNoise)
  3348                                                   lineNoise = lineNoise[lDef:-rDef]
  3349                                                   lineNoiseH = lineNoiseH[lDef:-rDef]
  3350                                                   lineNoisePhase = np.angle(lineNoiseH)
  3351                                                   lineNoisePhaseDF = pd.DataFrame(
  3352                                                       lineNoisePhase,
  3353                                                       index=dummyAsig.times,
  3354                                                       columns=['phase']
  3355                                                       )
  3356                                                   plotHilbert = False
  3357                                                   if plotHilbert:
  3358                                                       lineNoiseFreq = (
  3359                                                           np.diff(np.unwrap(lineNoisePhase)) /
  3360                                                           (2.0*np.pi) * float(dummyAsig.sampling_rate))
  3361                                                       lineNoiseEnvelope = np.abs(lineNoiseH)
  3362                                                       i1 = 300000; i2 = 330000
  3363                                                       fig, ax = plt.subplots(2, 1, sharex=True)
  3364                                                       ax[0].plot(dummyAsig.times[devFiltDebugMask], dummyAsig.magnitude[devFiltDebugMask, :])
  3365                                                       ax[0].plot(dummyAsig.times[devFiltDebugMask], lineNoise[devFiltDebugMask])
  3366                                                       ax[0].plot(dummyAsig.times[devFiltDebugMask], lineNoiseEnvelope[devFiltDebugMask])
  3367                                                       axFr = ax[1].twinx()
  3368                                                       ax[1].plot(
  3369                                                           dummyAsig.times[devFiltDebugMask], lineNoisePhase[devFiltDebugMask],
  3370                                                           c='r', label='phase')
  3371                                                       ax[1].legend()
  3372                                                       axFr.plot(
  3373                                                           dummyAsig.times[devFiltDebugMask], lineNoiseFreq[devFiltDebugMask],
  3374                                                           label='freq')
  3375                                                       axFr.set_ylim([59, 61])
  3376                                                       axFr.legend()
  3377                                                       plt.show()
  3378                                               # second pass through asigs, to save
  3379        71       5232.0     73.7      0.0      for aSigIdx, aSigProxy in enumerate(seg.analogsignals):
  3380        70       5460.0     78.0      0.0          if aSigIdx == 0:
  3381                                                       # check bounds
  3382         1       1643.0   1643.0      0.0              tStart = max(chunkTStart * pq.s, aSigProxy.t_start)
  3383         1       5018.0   5018.0      0.0              tStop = min(chunkTStop * pq.s, aSigProxy.t_stop)
  3384                                                   loadThisOne = (
  3385        70       5182.0     74.0      0.0              (saveFromAsigNameList and (aSigProxy in aSigList)) or
  3386        70       5150.0     73.6      0.0              (aSigProxy in ainpList)
  3387                                                       )
  3388        70       5218.0     74.5      0.0          if loadThisOne:
  3389         1         74.0     74.0      0.0              if trackMemory:
  3390         1         75.0     75.0      0.0                  print('writing asig {} ({}) memory usage: {:.1f} MB'.format(
  3391         1       1320.0   1320.0      0.0                      aSigIdx, aSigProxy.name, prf.memory_usage_psutil()))
  3392         1         55.0     55.0      0.0              chanIdx = aSigProxy.channel_index
  3393         1         54.0     54.0      0.0              asig = aSigProxy.load(
  3394         1         53.0     53.0      0.0                  time_slice=(tStart, tStop),
  3395         1   55413157.0 55413157.0     70.3                  magnitude_mode='rescaled')
  3396                                                       #  link AnalogSignal and ID providing channel_index
  3397         1         77.0     77.0      0.0              asig.channel_index = chanIdx
  3398                                                       #  perform requested preproc operations
  3399         1        260.0    260.0      0.0              if 'impedances' in locals():
  3400                                                           elNmMatchMsk = impedances['elec'] == chanIdx.name
  3401                                                           if elNmMatchMsk.any():
  3402                                                               originalImpedance = np.min(
  3403                                                                   impedances.loc[elNmMatchMsk, 'impedance']
  3404                                                                   )
  3405                                                               asig.annotations['originalImpedance'] = originalImpedance
  3406                                                               if normalizeByImpedance and (aSigProxy not in ainpList):
  3407                                                                   '''
  3408                                                                   asig.magnitude[:] = (
  3409                                                                       (asig.magnitude - np.median(asig.magnitude)) /
  3410                                                                       np.min(
  3411                                                                           impedances.loc[elNmMatchMsk, 'impedance']
  3412                                                                           )
  3413                                                                       )
  3414                                                                   '''
  3415                                                                   print('Normalizing {} by {} kOhms'.format(asig.name, originalImpedance))
  3416                                                                   asig.magnitude[:] = (
  3417                                                                       (asig.magnitude * averageImpedance) / originalImpedance
  3418                                                                       )
  3419         1         54.0     54.0      0.0              if fillOverflow:
  3420                                                           # fill in overflow:
  3421                                                           asig.magnitude[:], _ = hf.fillInOverflow2(
  3422                                                               asig.magnitude[:],
  3423                                                               overFlowFillType='average',
  3424                                                               overFlowThreshold=8000,
  3425                                                               debuggingPlots=False
  3426                                                               )
  3427         1         52.0     52.0      0.0              if removeJumps:
  3428                                                           # find unusual jumps in derivative or amplitude
  3429                                                           '''
  3430                                                           timeSection['data'], newBadData = hf.fillInJumps(timeSection['data'],
  3431                                                           timeSection['samp_per_s'], smoothing_ms = 0.5, nStdDiff = 50,
  3432                                                           nStdAmp = 100)
  3433                                                           badData.update(newBadData)
  3434                                                           '''
  3435                                                           pass
  3436         1         52.0     52.0      0.0              if calcAverageLFP and (aSigProxy not in ainpList):
  3437                                                           for k, cols in meanGroups.items():
  3438                                                               if asig.name in cols:
  3439                                                                   whichColumnToSubtract = k
  3440                                                           noiseModel = np.polyfit(
  3441                                                               centerLFP[:, whichColumnToSubtract],
  3442                                                               asig.magnitude.flatten(), 1, full=True)
  3443                                                           rSq = 1 - noiseModel[1][0] / np.sum(asig.magnitude.flatten() ** 2)
  3444                                                           asig.annotations['mean_removal_r2'] = rSq
  3445                                                           asig.annotations['mean_removal_group'] = whichColumnToSubtract
  3446                                                           if linearDetrend:
  3447                                                               noiseTerm = np.polyval(
  3448                                                                   noiseModel[0],
  3449                                                                   centerLFP[:, whichColumnToSubtract])
  3450                                                           else:
  3451                                                               noiseTerm = centerLFP[:, whichColumnToSubtract]
  3452                                                           ###
  3453                                                           plotMeanSubtraction = False
  3454                                                           if plotMeanSubtraction:
  3455                                                               i1 = 300000; i2 = 330000
  3456                                                               fig, ax = plt.subplots(1, 1)
  3457                                                               ax.plot(asig.times[devFiltDebugMask], asig.magnitude[devFiltDebugMask, :], label='channel')
  3458                                                               ax.plot(asig.times[devFiltDebugMask], centerLFP[devFiltDebugMask, whichColumnToSubtract], label='mean')
  3459                                                               ax.plot(asig.times[devFiltDebugMask], noiseTerm[devFiltDebugMask], label='adjusted mean')
  3460                                                               ax.legend()
  3461                                                               plt.show()
  3462                                                           ###
  3463                                                           if removeMeanAcross:
  3464                                                               asig.magnitude[:] = np.atleast_2d(
  3465                                                                   asig.magnitude.flatten() - noiseTerm).transpose()
  3466                                                               # asig.magnitude[:] = (
  3467                                                               #     asig.magnitude - np.median(asig.magnitude))
  3468         1         54.0     54.0      0.0              if (LFPFilterOpts is not None) and (aSigProxy not in ainpList):
  3469                                                           asig.magnitude[:] = filterFun(asig, filterCoeffs=filterCoeffs)
  3470         1         53.0     53.0      0.0              if (interpolateOutliers) and (aSigProxy not in ainpList) and (not outlierRemovalDebugFlag):
  3471                                                           for k, cols in meanGroups.items():
  3472                                                               if asig.name in cols:
  3473                                                                   whichColumnToSubtract = k
  3474                                                           tempSer = pd.Series(asig.magnitude.flatten())
  3475                                                           tempSer.loc[outlierMask[:, whichColumnToSubtract]] = np.nan
  3476                                                           tempSer = (
  3477                                                               tempSer
  3478                                                               .interpolate(method='linear', limit_area='inside')
  3479                                                               .fillna(method='ffill')
  3480                                                               .fillna(method='bfill')
  3481                                                               )
  3482                                                           asig.magnitude[:, 0] = tempSer.to_numpy()
  3483                                                       # pdb.set_trace()
  3484         1         58.0     58.0      0.0              if (aSigProxy in aSigList) or (aSigProxy in ainpList):
  3485                                                           # assign ownership to containers
  3486         1         78.0     78.0      0.0                  chanIdx.analogsignals.append(asig)
  3487         1         62.0     62.0      0.0                  newSeg.analogsignals.append(asig)
  3488                                                           # assign parent to children
  3489         1        625.0    625.0      0.0                  chanIdx.create_relationship()
  3490         1       1826.0   1826.0      0.0                  newSeg.create_relationship()
  3491                                                           # write out to file
  3492         1         68.0     68.0      0.0                  asig = writer._write_analogsignal(
  3493         1    1170952.0 1170952.0      1.5                      asig, nixblock, nixgroup)
  3494         1         62.0     62.0      0.0              del asig
  3495         1     550303.0 550303.0      0.7              gc.collect()
  3496         1         59.0     59.0      0.0      for irSigIdx, irSigProxy in enumerate(
  3497         1         72.0     72.0      0.0              seg.irregularlysampledsignals):
  3498                                                   chanIdx = irSigProxy.channel_index
  3499                                                   #
  3500                                                   isig = irSigProxy.load(
  3501                                                       time_slice=(tStart, tStop),
  3502                                                       magnitude_mode='rescaled')
  3503                                                   #  link irregularlysampledSignal
  3504                                                   #  and ID providing channel_index
  3505                                                   isig.channel_index = chanIdx
  3506                                                   # assign ownership to containers
  3507                                                   chanIdx.irregularlysampledsignals.append(isig)
  3508                                                   newSeg.irregularlysampledsignals.append(isig)
  3509                                                   # assign parent to children
  3510                                                   chanIdx.create_relationship()
  3511                                                   newSeg.create_relationship()
  3512                                                   # write out to file
  3513                                                   isig = writer._write_irregularlysampledsignal(
  3514                                                       isig, nixblock, nixgroup)
  3515                                                   del isig
  3516                                                   gc.collect()
  3517                                               #
  3518         1         64.0     64.0      0.0      if len(spikeSourceType):
  3519        12        842.0     70.2      0.0          for stIdx, stProxy in enumerate(spikeSeg.spiketrains):
  3520        11        664.0     60.4      0.0              if trackMemory:
  3521        11        739.0     67.2      0.0                  print('writing spiketrains mem usage: {}'.format(
  3522        11      17672.0   1606.5      0.0                      prf.memory_usage_psutil()))
  3523        11        810.0     73.6      0.0              unit = stProxy.unit
  3524        11     332792.0  30253.8      0.4              st = loadStProxy(stProxy)
  3525                                                       #  have to manually slice tStop and tStart because
  3526                                                       #  array annotations are not saved natively in the nix file
  3527                                                       #  (we're getting them as plain annotations)
  3528        11        846.0     76.9      0.0              timeMask = np.asarray(
  3529        11      20827.0   1893.4      0.0                  (st.times >= tStart) & (st.times < tStop),
  3530        11       1229.0    111.7      0.0                  dtype=np.bool)
  3531        11        744.0     67.6      0.0              try:
  3532        11        813.0     73.9      0.0                  if 'arrayAnnNames' in st.annotations:
  3533                                                               for key in st.annotations['arrayAnnNames']:
  3534                                                                   st.annotations[key] = np.asarray(
  3535                                                                       st.annotations[key])[timeMask]
  3536        11      16843.0   1531.2      0.0                  st = st[timeMask]
  3537        11        856.0     77.8      0.0                  st.t_start = tStart
  3538        11        745.0     67.7      0.0                  st.t_stop = tStop
  3539                                                       except Exception:
  3540                                                           traceback.print_exc()
  3541                                                       #  tdc may or may not have the same channel ids, but
  3542                                                       #  it will have consistent channel names
  3543        11        803.0     73.0      0.0              nameParser = re.search(
  3544        11       7743.0    703.9      0.0                  r'([a-zA-Z0-9]*)#(\d*)', unit.name)
  3545        11        857.0     77.9      0.0              chanLabel = nameParser.group(1)
  3546        11        854.0     77.6      0.0              unitId = nameParser.group(2)
  3547                                                       #
  3548        11       1008.0     91.6      0.0              chIdxName = unit.name.replace('_stim', '').split('#')[0]
  3549        11     304942.0  27722.0      0.4              chanIdx = block.filter(objects=ChannelIndex, name=chIdxName)[0]
  3550                                                       # [i.name for i in block.filter(objects=ChannelIndex)]
  3551                                                       # [i.name for i in spikeBlock.filter(objects=Unit)]
  3552                                                       #  print(unit.name)
  3553        11        779.0     70.8      0.0              if not (unit in chanIdx.units):
  3554                                                           # first time at this unit, add to its chanIdx
  3555                                                           unit.channel_index = chanIdx
  3556                                                           chanIdx.units.append(unit)
  3557                                                       #  except Exception:
  3558                                                       #      traceback.print_exc()
  3559        11        933.0     84.8      0.0              st.name = 'seg{}_{}'.format(0, unit.name)
  3560                                                       # st.name = 'seg{}_{}'.format(idx, unit.name)
  3561                                                       #  link SpikeTrain and ID providing unit
  3562        11        653.0     59.4      0.0              if calcAverageLFP:
  3563                                                           if 'arrayAnnNames' in st.annotations:
  3564                                                               st.annotations['arrayAnnNames'] = list(st.annotations['arrayAnnNames'])
  3565                                                           else:
  3566                                                               st.annotations['arrayAnnNames'] = []
  3567                                                           st.annotations['arrayAnnNames'].append('phase60hz')
  3568                                                           phase60hz = hf.interpolateDF(
  3569                                                               lineNoisePhaseDF,
  3570                                                               newX=st.times, columns=['phase']).to_numpy().flatten()
  3571                                                           st.annotations.update({'phase60hz': phase60hz})
  3572                                                           plotPhaseDist = False
  3573                                                           if plotPhaseDist:
  3574                                                               sns.distplot(phase60hz)
  3575                                                               plt.show()
  3576        11        657.0     59.7      0.0              st.unit = unit
  3577                                                       # assign ownership to containers
  3578        11        689.0     62.6      0.0              unit.spiketrains.append(st)
  3579        11        705.0     64.1      0.0              newSeg.spiketrains.append(st)
  3580                                                       # assign parent to children
  3581        11       3897.0    354.3      0.0              unit.create_relationship()
  3582        11       4792.0    435.6      0.0              newSeg.create_relationship()
  3583                                                       # write out to file
  3584        11    4985620.0 453238.2      6.3              st = writer._write_spiketrain(st, nixblock, nixgroup)
  3585        11        867.0     78.8      0.0              del st
  3586                                               #  process proprio trial related events
  3587         1         55.0     55.0      0.0      if calcRigEvents:
  3588                                                   print('Processing rig events...')
  3589                                                   analogData = []
  3590                                                   for key, value in eventInfo['inputIDs'].items():
  3591                                                       searchName = 'seg{}_'.format(0) + value
  3592                                                       ainpAsig = seg.filter(
  3593                                                           objects=AnalogSignalProxy,
  3594                                                           name=searchName)[0]
  3595                                                       ainpData = ainpAsig.load(
  3596                                                           time_slice=(tStart, tStop),
  3597                                                           magnitude_mode='rescaled')
  3598                                                       analogData.append(
  3599                                                           pd.DataFrame(ainpData.magnitude, columns=[key]))
  3600                                                       del ainpData
  3601                                                       gc.collect()
  3602                                                   motorData = pd.concat(analogData, axis=1)
  3603                                                   del analogData
  3604                                                   gc.collect()
  3605                                                   if motorEncoderMask is not None:
  3606                                                       ainpData = ainpAsig.load(
  3607                                                           time_slice=(tStart, tStop),
  3608                                                           magnitude_mode='rescaled')
  3609                                                       ainpTime = ainpData.times.magnitude
  3610                                                       meTimeMask = np.zeros_like(ainpTime, dtype=np.bool)
  3611                                                       for meTimeBounds in motorEncoderMask:
  3612                                                           meTimeMask = (
  3613                                                               meTimeMask |
  3614                                                               (
  3615                                                                   (ainpTime > meTimeBounds[0]) &
  3616                                                                   (ainpTime < meTimeBounds[1])
  3617                                                                   )
  3618                                                               )
  3619                                                       columnsToOverride = ['A-', 'A+', 'B-', 'B+', 'Z-', 'Z+']
  3620                                                       for colName in columnsToOverride:
  3621                                                           motorData.loc[~meTimeMask, colName] = motorData.loc[:, colName].quantile(q=0.05)
  3622                                                       del ainpData, ainpTime
  3623                                                       gc.collect()
  3624                                                   motorData = mea.processMotorData(
  3625                                                       motorData, ainpAsig.sampling_rate.magnitude,
  3626                                                       encoderCountPerDegree=encoderCountPerDegree
  3627                                                       )
  3628                                                   keepCols = [
  3629                                                       'position', 'velocity', 'velocityCat',
  3630                                                       'rightBut_int', 'leftBut_int',
  3631                                                       'rightLED_int', 'leftLED_int', 'simiTrigs_int']
  3632                                                   for colName in keepCols:
  3633                                                       if trackMemory:
  3634                                                           print('writing motorData memory usage: {:.1f} MB'.format(
  3635                                                               prf.memory_usage_psutil()))
  3636                                                       chanIdx = ChannelIndex(
  3637                                                           name=colName,
  3638                                                           index=np.asarray([0]),
  3639                                                           channel_names=np.asarray([0]))
  3640                                                       block.channel_indexes.append(chanIdx)
  3641                                                       motorAsig = AnalogSignal(
  3642                                                           motorData[colName].to_numpy() * pq.mV,
  3643                                                           name=colName,
  3644                                                           sampling_rate=ainpAsig.sampling_rate,
  3645                                                           dtype=np.float32)
  3646                                                       motorAsig.t_start = ainpAsig.t_start
  3647                                                       motorAsig.channel_index = chanIdx
  3648                                                       # assign ownership to containers
  3649                                                       chanIdx.analogsignals.append(motorAsig)
  3650                                                       newSeg.analogsignals.append(motorAsig)
  3651                                                       chanIdx.create_relationship()
  3652                                                       newSeg.create_relationship()
  3653                                                       # write out to file
  3654                                                       motorAsig = writer._write_analogsignal(
  3655                                                           motorAsig, nixblock, nixgroup)
  3656                                                       del motorAsig
  3657                                                       gc.collect()
  3658                                                   _, trialEvents = mea.getTrials(
  3659                                                       motorData, ainpAsig.sampling_rate.magnitude,
  3660                                                       float(tStart.magnitude), trialType=None)
  3661                                                   trialEvents.fillna(0)
  3662                                                   trialEvents.rename(
  3663                                                       columns={
  3664                                                           'Label': 'rig_property',
  3665                                                           'Details': 'rig_value'},
  3666                                                       inplace=True)
  3667                                                   del motorData
  3668                                                   gc.collect()
  3669                                                   eventList = eventDataFrameToEvents(
  3670                                                       trialEvents,
  3671                                                       idxT='Time',
  3672                                                       annCol=['rig_property', 'rig_value'])
  3673                                                   for event in eventList:
  3674                                                       if trackMemory:
  3675                                                           print(
  3676                                                               'writing motor events memory usage: {:.1f} MB'
  3677                                                               .format(prf.memory_usage_psutil()))
  3678                                                       event.segment = newSeg
  3679                                                       newSeg.events.append(event)
  3680                                                       newSeg.create_relationship()
  3681                                                       # write out to file
  3682                                                       event = writer._write_event(event, nixblock, nixgroup)
  3683                                                       del event
  3684                                                       gc.collect()
  3685                                                   del trialEvents, eventList
  3686                                               #
  3687         9        903.0    100.3      0.0      for eventProxy in seg.events:
  3688         8        517.0     64.6      0.0          event = eventProxy.load(
  3689         8      66957.0   8369.6      0.1              time_slice=(tStart, tStop))
  3690         8        513.0     64.1      0.0          event.t_start = tStart
  3691         8        471.0     58.9      0.0          event.t_stop = tStop
  3692         8        445.0     55.6      0.0          event.segment = newSeg
  3693         8        545.0     68.1      0.0          newSeg.events.append(event)
  3694         8       5065.0    633.1      0.0          newSeg.create_relationship()
  3695                                                   # write out to file
  3696         8    1107699.0 138462.4      1.4          event = writer._write_event(event, nixblock, nixgroup)
  3697         8        532.0     66.5      0.0          del event
  3698         8    4217818.0 527227.2      5.4          gc.collect()
  3699                                               #
  3700         1        212.0    212.0      0.0      for epochProxy in seg.epochs:
  3701                                                   epoch = epochProxy.load(
  3702                                                       time_slice=(tStart, tStop))
  3703                                                   epoch.t_start = tStart
  3704                                                   epoch.t_stop = tStop
  3705                                                   epoch.segment = newSeg
  3706                                                   newSeg.events.append(epoch)
  3707                                                   newSeg.create_relationship()
  3708                                                   # write out to file
  3709                                                   epoch = writer._write_epoch(epoch, nixblock, nixgroup)
  3710                                                   del epoch
  3711                                                   gc.collect()
  3712                                               #
  3713         1        178.0    178.0      0.0      chanIdxDiscardNames = []
  3714                                               # descend into ChannelIndexes
  3715        71       5627.0     79.3      0.0      for chanIdx in block.channel_indexes:
  3716        70       5825.0     83.2      0.0          if chanIdx.analogsignals or chanIdx.units:
  3717        12    6687539.0 557294.9      8.5              chanIdx = writer._write_channelindex(chanIdx, nixblock)
  3718                                                   else:
  3719        58       5045.0     87.0      0.0              chanIdxDiscardNames.append(chanIdx.name)
  3720                                               block.channel_indexes = [
  3721         1         60.0     60.0      0.0          i
  3722         1        721.0    721.0      0.0          for i in block.channel_indexes
  3723                                                   if i.name not in chanIdxDiscardNames
  3724                                                   ]
  3725         1    3192810.0 3192810.0      4.1      writer._create_source_links(block, nixblock)
  3726         1         58.0     58.0      0.0      return

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: purgeNixAnn at line 3728

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3728                                           @profile
  3729                                           def purgeNixAnn(
  3730                                                   block, annNames=['nix_name', 'neo_name']):
  3731                                               for annName in annNames:
  3732                                                   block.annotations.pop(annName, None)
  3733                                               for child in block.children_recur:
  3734                                                   if child.annotations:
  3735                                                       child.annotations = {
  3736                                                           k: v
  3737                                                           for k, v in child.annotations.items()
  3738                                                           if k not in annNames}
  3739                                               for child in block.data_children_recur:
  3740                                                   if child.annotations:
  3741                                                       child.annotations = {
  3742                                                           k: v
  3743                                                           for k, v in child.annotations.items()
  3744                                                           if k not in annNames}
  3745                                               return block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadContainerArrayAnn at line 3747

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3747                                           @profile
  3748                                           def loadContainerArrayAnn(
  3749                                                   container=None, trainList=None
  3750                                                   ):
  3751                                               assert (container is not None) or (trainList is not None)
  3752                                               #
  3753                                               spikesAndEvents = []
  3754                                               returnObj = []
  3755                                               if container is not None:
  3756                                                   #  need the line below! (RD: don't remember why, consider removing)
  3757                                                   container.create_relationship()
  3758                                                   #
  3759                                                   spikesAndEvents += (
  3760                                                       container.filter(objects=SpikeTrain) +
  3761                                                       container.filter(objects=Event)
  3762                                                       )
  3763                                                   returnObj.append(container)
  3764                                               if trainList is not None:
  3765                                                   spikesAndEvents += trainList
  3766                                                   returnObj.append(trainList)
  3767                                               #
  3768                                               if len(returnObj) == 1:
  3769                                                   returnObj = returnObj[0]
  3770                                               else:
  3771                                                   returnObj = tuple(returnObj)
  3772                                               #
  3773                                               for st in spikesAndEvents:
  3774                                                   st = loadObjArrayAnn(st)
  3775                                               return returnObj

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadObjArrayAnn at line 3777

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3777                                           @profile
  3778                                           def loadObjArrayAnn(st):
  3779                                               if 'arrayAnnNames' in st.annotations.keys():
  3780                                                   if isinstance(st.annotations['arrayAnnNames'], str):
  3781                                                       st.annotations['arrayAnnNames'] = [st.annotations['arrayAnnNames']]
  3782                                                   elif isinstance(st.annotations['arrayAnnNames'], tuple):
  3783                                                       st.annotations['arrayAnnNames'] = [i for i in st.annotations['arrayAnnNames']]
  3784                                                   #
  3785                                                   for key in st.annotations['arrayAnnNames']:
  3786                                                       #  fromRaw, the ann come back as tuple, need to recast
  3787                                                       try:
  3788                                                           if len(st.times) == 1:
  3789                                                               st.annotations[key] = np.atleast_1d(st.annotations[key]).flatten()
  3790                                                           st.array_annotations.update(
  3791                                                               {key: np.asarray(st.annotations[key])})
  3792                                                           st.annotations[key] = np.asarray(st.annotations[key])
  3793                                                       except Exception:
  3794                                                           print('Error with {}'.format(st.name))
  3795                                                           traceback.print_exc()
  3796                                                           pdb.set_trace()
  3797                                               if hasattr(st, 'waveforms'):
  3798                                                   if st.waveforms is None:
  3799                                                       st.waveforms = np.asarray([]).reshape((0, 0, 0)) * pq.mV
  3800                                                   elif not len(st.waveforms):
  3801                                                       st.waveforms = np.asarray([]).reshape((0, 0, 0)) * pq.mV
  3802                                               return st

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadWithArrayAnn at line 3804

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3804                                           @profile
  3805                                           def loadWithArrayAnn(
  3806                                                   dataPath, fromRaw=False,
  3807                                                   mapDF=None, reduceChannelIndexes=False):
  3808                                               if fromRaw:
  3809                                                   reader = nixio_fr.NixIO(filename=dataPath)
  3810                                                   block = readBlockFixNames(
  3811                                                       reader, lazy=False,
  3812                                                       mapDF=mapDF,
  3813                                                       reduceChannelIndexes=reduceChannelIndexes)
  3814                                               else:
  3815                                                   reader = NixIO(filename=dataPath)
  3816                                                   block = reader.read_block()
  3817                                                   # [un.name for un in block.filter(objects=Unit)]
  3818                                                   # [len(un.spiketrains) for un in block.filter(objects=Unit)]
  3819                                               
  3820                                               block = loadContainerArrayAnn(container=block)
  3821                                               
  3822                                               if fromRaw:
  3823                                                   reader.file.close()
  3824                                               else:
  3825                                                   reader.close()
  3826                                               return block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: blockFromPath at line 3828

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3828                                           @profile
  3829                                           def blockFromPath(
  3830                                                   dataPath, lazy=False, mapDF=None,
  3831                                                   reduceChannelIndexes=False, loadList=None,
  3832                                                   purgeNixNames=False, chunkingInfoPath=None):
  3833                                               chunkingMetadata = None
  3834                                               if chunkingInfoPath is not None:
  3835                                                   if os.path.exists(chunkingInfoPath):
  3836                                                       with open(chunkingInfoPath, 'r') as f:
  3837                                                           chunkingMetadata = json.load(f)
  3838                                               if chunkingMetadata is None:
  3839                                                   chunkingMetadata = {
  3840                                                       '0': {
  3841                                                           'filename': dataPath,
  3842                                                           'partNameSuffix': '',
  3843                                                           'chunkTStart': 0,
  3844                                                           'chunkTStop': 'NaN'
  3845                                                       }}
  3846                                               for idx, (chunkIdxStr, chunkMeta) in enumerate(chunkingMetadata.items()):   
  3847                                                   thisDataPath = chunkMeta['filename']
  3848                                                   assert os.path.exists(thisDataPath)
  3849                                                   if idx == 0:
  3850                                                       if lazy:
  3851                                                           dataReader = nixio_fr.NixIO(
  3852                                                               filename=thisDataPath)
  3853                                                           dataBlock = readBlockFixNames(
  3854                                                               dataReader, lazy=lazy, mapDF=mapDF,
  3855                                                               reduceChannelIndexes=reduceChannelIndexes,
  3856                                                               purgeNixNames=purgeNixNames, loadList=loadList)
  3857                                                       else:
  3858                                                           dataReader = None
  3859                                                           dataBlock = loadWithArrayAnn(thisDataPath)
  3860                                                   else:
  3861                                                       if lazy:
  3862                                                           dataReader2 = nixio_fr.NixIO(
  3863                                                               filename=thisDataPath)
  3864                                                           dataBlock2 = readBlockFixNames(
  3865                                                               dataReader2, lazy=lazy, mapDF=mapDF,
  3866                                                               reduceChannelIndexes=reduceChannelIndexes, loadList=loadList)
  3867                                                       else:
  3868                                                           dataReader2 = None
  3869                                                           dataBlock2 = loadWithArrayAnn(thisDataPath)
  3870                                                       maxSegIdx = len(dataBlock.segments)
  3871                                                       typesNeedRenaming = [
  3872                                                           SpikeTrainProxy, AnalogSignalProxy, EventProxy,
  3873                                                           SpikeTrain, AnalogSignal, Event]
  3874                                                       for segIdx, seg in enumerate(dataBlock2.segments):
  3875                                                           if seg.name is None:
  3876                                                               seg.name = 'seg{}_'.format(maxSegIdx + segIdx)
  3877                                                           else:
  3878                                                               if 'seg{}_'.format(maxSegIdx + segIdx) not in seg.name:
  3879                                                                   seg.name = (
  3880                                                                       'seg{}_{}'
  3881                                                                       .format(
  3882                                                                           maxSegIdx + segIdx,
  3883                                                                           childBaseName(seg.name, 'seg')))
  3884                                                           for objType in typesNeedRenaming:
  3885                                                               for child in seg.filter(objects=objType):
  3886                                                                   if 'seg{}_'.format(maxSegIdx + segIdx) not in child.name:
  3887                                                                       child.name = (
  3888                                                                           'seg{}_{}'
  3889                                                                           .format(
  3890                                                                               maxSegIdx + segIdx, childBaseName(child.name, 'seg')))
  3891                                                       dataBlock.merge(dataBlock2)
  3892                                               return dataReader, dataBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: calcBinarizedArray at line 3894

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3894                                           @profile
  3895                                           def calcBinarizedArray(
  3896                                                   dataBlock, samplingRate,
  3897                                                   binnedSpikePath=None,
  3898                                                   saveToFile=True, matchT=None):
  3899                                               #
  3900                                               spikeMatBlock = Block(name=dataBlock.name + '_binarized')
  3901                                               spikeMatBlock.merge_annotations(dataBlock)
  3902                                               #
  3903                                               allSpikeTrains = [
  3904                                                   i for i in dataBlock.filter(objects=SpikeTrain)]
  3905                                               #
  3906                                               for st in allSpikeTrains:
  3907                                                   chanList = spikeMatBlock.filter(
  3908                                                       objects=ChannelIndex, name=st.unit.name)
  3909                                                   if not len(chanList):
  3910                                                       chanIdx = ChannelIndex(name=st.unit.name, index=np.asarray([0]))
  3911                                                       #  print(chanIdx.name)
  3912                                                       spikeMatBlock.channel_indexes.append(chanIdx)
  3913                                                       thisUnit = Unit(name=st.unit.name)
  3914                                                       chanIdx.units.append(thisUnit)
  3915                                                       thisUnit.channel_index = chanIdx
  3916                                               #
  3917                                               for segIdx, seg in enumerate(dataBlock.segments):
  3918                                                   newSeg = Segment(name='seg{}_{}'.format(segIdx, spikeMatBlock.name))
  3919                                                   newSeg.merge_annotations(seg)
  3920                                                   spikeMatBlock.segments.append(newSeg)
  3921                                                   #  tStart = dataBlock.segments[0].t_start
  3922                                                   #  tStop = dataBlock.segments[0].t_stop
  3923                                                   tStart = seg.t_start
  3924                                                   tStop = seg.t_stop
  3925                                                   # make dummy binary spike train, in case ths chan didn't fire
  3926                                                   segSpikeTrains = [
  3927                                                       i for i in seg.filter(objects=SpikeTrain) if '#' in i.name]
  3928                                                   dummyBin = binarize(
  3929                                                       segSpikeTrains[0],
  3930                                                       sampling_rate=samplingRate,
  3931                                                       t_start=tStart,
  3932                                                       t_stop=tStop + samplingRate ** -1) * 0
  3933                                                   for chanIdx in spikeMatBlock.channel_indexes:
  3934                                                       #  print(chanIdx.name)
  3935                                                       stList = seg.filter(
  3936                                                           objects=SpikeTrain,
  3937                                                           name='seg{}_{}'.format(segIdx, chanIdx.name)
  3938                                                           )
  3939                                                       if len(stList):
  3940                                                           st = stList[0]
  3941                                                           print('binarizing {}'.format(st.name))
  3942                                                           stBin = binarize(
  3943                                                               st,
  3944                                                               sampling_rate=samplingRate,
  3945                                                               t_start=tStart,
  3946                                                               t_stop=tStop + samplingRate ** -1)
  3947                                                           spikeMatBlock.segments[segIdx].spiketrains.append(st)
  3948                                                           #  to do: link st to spikematblock's chidx and units
  3949                                                           assert len(chanIdx.filter(objects=Unit)) == 1
  3950                                                           thisUnit = chanIdx.filter(objects=Unit)[0]
  3951                                                           thisUnit.spiketrains.append(st)
  3952                                                           st.unit = thisUnit
  3953                                                           st.segment = spikeMatBlock.segments[segIdx]
  3954                                                       else:
  3955                                                           print('{} has no spikes'.format(st.name))
  3956                                                           stBin = dummyBin
  3957                                                       skipStAnnNames = [
  3958                                                           'nix_name', 'neo_name', 'arrayAnnNames']
  3959                                                       if 'arrayAnnNames' in st.annotations:
  3960                                                           skipStAnnNames += list(st.annotations['arrayAnnNames'])
  3961                                                       asigAnn = {
  3962                                                           k: v
  3963                                                           for k, v in st.annotations.items()
  3964                                                           if k not in skipStAnnNames
  3965                                                           }
  3966                                                       asig = AnalogSignal(
  3967                                                           stBin * samplingRate,
  3968                                                           name='seg{}_{}_raster'.format(segIdx, st.unit.name),
  3969                                                           sampling_rate=samplingRate,
  3970                                                           dtype=np.int,
  3971                                                           **asigAnn)
  3972                                                       if matchT is not None:
  3973                                                           asig = asig[:matchT.shape[0], :]
  3974                                                       asig.t_start = tStart
  3975                                                       asig.annotate(binWidth=1 / samplingRate.magnitude)
  3976                                                       chanIdx.analogsignals.append(asig)
  3977                                                       asig.channel_index = chanIdx
  3978                                                       spikeMatBlock.segments[segIdx].analogsignals.append(asig)
  3979                                               #
  3980                                               for chanIdx in spikeMatBlock.channel_indexes:
  3981                                                   chanIdx.name = chanIdx.name + '_raster'
  3982                                               #
  3983                                               spikeMatBlock.create_relationship()
  3984                                               spikeMatBlock = purgeNixAnn(spikeMatBlock)
  3985                                               if saveToFile:
  3986                                                   if os.path.exists(binnedSpikePath):
  3987                                                       os.remove(binnedSpikePath)
  3988                                                   writer = NixIO(filename=binnedSpikePath)
  3989                                                   writer.write_block(spikeMatBlock, use_obj_names=True)
  3990                                                   writer.close()
  3991                                               return spikeMatBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: calcFR at line 3993

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3993                                           @profile
  3994                                           def calcFR(
  3995                                                   binnedPath, dataPath,
  3996                                                   suffix='fr', aggregateFun=None,
  3997                                                   chanNames=None, rasterOpts=None, verbose=False
  3998                                                   ):
  3999                                               print('Loading rasters...')
  4000                                               masterSpikeMats, _ = loadSpikeMats(
  4001                                                   binnedPath, rasterOpts,
  4002                                                   aggregateFun=aggregateFun,
  4003                                                   chans=chanNames,
  4004                                                   loadAll=True, checkReferences=False)
  4005                                               print('Loading data file...')
  4006                                               dataReader = nixio_fr.NixIO(
  4007                                                   filename=dataPath)
  4008                                               dataBlock = dataReader.read_block(
  4009                                                   block_index=0, lazy=True,
  4010                                                   signal_group_mode='split-all')
  4011                                               masterBlock = Block()
  4012                                               masterBlock.name = dataBlock.annotations['neo_name']
  4013                                               #
  4014                                               for segIdx, segSpikeMat in masterSpikeMats.items():
  4015                                                   print('Calculating FR for segment {}'.format(segIdx))
  4016                                                   spikeMatDF = segSpikeMat.reset_index().rename(
  4017                                                       columns={'bin': 't'})
  4018                                           
  4019                                                   dataSeg = dataBlock.segments[segIdx]
  4020                                                   dummyAsig = dataSeg.filter(
  4021                                                       objects=AnalogSignalProxy)[0].load(channel_indexes=[0])
  4022                                                   samplingRate = dummyAsig.sampling_rate
  4023                                                   newT = dummyAsig.times.magnitude
  4024                                                   spikeMatDF['t'] = spikeMatDF['t'] + newT[0]
  4025                                           
  4026                                                   segSpikeMatInterp = hf.interpolateDF(
  4027                                                       spikeMatDF, pd.Series(newT),
  4028                                                       kind='linear', fill_value=(0, 0),
  4029                                                       x='t')
  4030                                                   spikeMatBlockInterp = dataFrameToAnalogSignals(
  4031                                                       segSpikeMatInterp,
  4032                                                       idxT='t', useColNames=True,
  4033                                                       dataCol=segSpikeMatInterp.drop(columns='t').columns,
  4034                                                       samplingRate=samplingRate)
  4035                                                   spikeMatBlockInterp.name = dataBlock.annotations['neo_name']
  4036                                                   spikeMatBlockInterp.annotate(
  4037                                                       nix_name=dataBlock.annotations['neo_name'])
  4038                                                   spikeMatBlockInterp.segments[0].name = dataSeg.annotations['neo_name']
  4039                                                   spikeMatBlockInterp.segments[0].annotate(
  4040                                                       nix_name=dataSeg.annotations['neo_name'])
  4041                                                   asigList = spikeMatBlockInterp.filter(objects=AnalogSignal)
  4042                                                   for asig in asigList:
  4043                                                       asig.annotate(binWidth=rasterOpts['binWidth'])
  4044                                                       if '_raster' in asig.name:
  4045                                                           asig.name = asig.name.replace('_raster', '_' + suffix)
  4046                                                       asig.name = 'seg{}_{}'.format(segIdx, childBaseName(asig.name, 'seg'))
  4047                                                       asig.annotate(nix_name=asig.name)
  4048                                                   chanIdxList = spikeMatBlockInterp.filter(objects=ChannelIndex)
  4049                                                   for chanIdx in chanIdxList:
  4050                                                       if '_raster' in chanIdx.name:
  4051                                                           chanIdx.name = chanIdx.name.replace('_raster', '_' + suffix)
  4052                                                       chanIdx.annotate(nix_name=chanIdx.name)
  4053                                           
  4054                                                   # masterBlock.merge(spikeMatBlockInterp)
  4055                                                   frBlockPath = dataPath.replace('_analyze.nix', '_fr.nix')
  4056                                                   writer = NixIO(filename=frBlockPath)
  4057                                                   writer.write_block(spikeMatBlockInterp, use_obj_names=True)
  4058                                                   writer.close()
  4059                                               #
  4060                                               dataReader.file.close()
  4061                                               return masterBlock

Timer unit: 1e-07 s

Total time: 142.915 s
File: C\../../analysis-code/preprocDelsysCSV.py
Function: preprocDelsysWrapper at line 65

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    65                                           @profile
    66                                           def preprocDelsysWrapper():
    67         1         69.0     69.0      0.0      headerDataList = []
    68         1       1277.0   1277.0      0.0      print('Loading header from {} ...'.format(delsysPath))
    69         1     118024.0 118024.0      0.0      with open(delsysPath, 'r') as f:
    70         1         35.0     35.0      0.0          expr = r'Label: ([\S\s]+) Sampling frequency: ([\S\s]+) Number of points: ([\S\s]+) start: ([\S\s]+) Unit: ([\S\s]+) Domain Unit: ([\S\s]+)\n'
    71         1         28.0     28.0      0.0          delimIdx = 0
    72        59       1795.0     30.4      0.0          for line in f:
    73        59      31293.0    530.4      0.0              matches = re.search(expr, line)
    74        59       1127.0     19.1      0.0              if matches:
    75        56       1056.0     18.9      0.0                  headerDataList.append({
    76        56       1444.0     25.8      0.0                      'label': str(matches.groups()[0]),
    77        56       1438.0     25.7      0.0                      'fs': float(matches.groups()[1]),
    78        56       1364.0     24.4      0.0                      'nSamp': int(matches.groups()[2]),
    79        56       1382.0     24.7      0.0                      'start': float(matches.groups()[3]),
    80        56       1297.0     23.2      0.0                      'units': str(matches.groups()[4]),
    81        56       1433.0     25.6      0.0                      'domainUnits': str(matches.groups()[5])
    82                                                           })
    83         3         57.0     19.0      0.0              elif line == ' \n':
    84         1        646.0    646.0      0.0                  break
    85        58       1121.0     19.3      0.0              delimIdx += 1
    86         1      19616.0  19616.0      0.0      headerData = pd.DataFrame(headerDataList)
    87         1       3750.0   3750.0      0.0      samplingRate = np.round(headerData['fs'].max())
    88                                               #
    89         1        653.0    653.0      0.0      print('Loading raw data from {} ...'.format(delsysPath))
    90         1  519939520.0 519939520.0     36.4      rawData = pd.read_csv(delsysPath, skiprows=delimIdx, low_memory=False)
    91                                               # for idx, cName in enumerate(rawData.columns): print('{}: {}'.format(idx, cName))
    92         1       1524.0   1524.0      0.0      domainCols = [cName for cName in rawData.columns if 'X[' in cName]
    93         1        920.0    920.0      0.0      featureCols = [cName for cName in rawData.columns if 'X[' not in cName]
    94         1         47.0     47.0      0.0      collatedDataList = []
    95         1        943.0    943.0      0.0      print('Assembling list of vectors...')
    96        57     112225.0   1968.9      0.0      for idx, (dom, feat) in enumerate(tqdm(iter(zip(domainCols, featureCols)))):
    97        56      90637.0   1618.5      0.0          newFeat = rawData[feat].to_numpy()
    98        56    2154827.0  38479.1      0.2          keepDataMask = rawData[feat].notna()
    99        56  454757697.0 8120673.2     31.8          newIndex = rawData[dom].interpolate(method='linear')[keepDataMask]
   100        56   43541922.0 777534.3      3.0          duplIndex = newIndex.duplicated()
   101        56       3354.0     59.9      0.0          thisFeat = pd.DataFrame(
   102        56    7392187.0 132003.3      0.5              newFeat[keepDataMask][~duplIndex],
   103        56    4068111.0  72644.8      0.3              index=newIndex[~duplIndex],
   104        56     935692.0  16708.8      0.1              columns=[feat])
   105        56       2444.0     43.6      0.0          if idx == 0:
   106         1        206.0    206.0      0.0              runningT = [thisFeat.index[0], thisFeat.index[-1]]
   107                                                   else:
   108        55       9298.0    169.1      0.0              runningT[0] = min(runningT[0], thisFeat.index[0])
   109        55       4240.0     77.1      0.0              runningT[-1] = max(runningT[-1], thisFeat.index[-1])
   110        56       1940.0     34.6      0.0          collatedDataList.append(thisFeat)
   111         1      36662.0  36662.0      0.0      resampledT = np.arange(runningT[0], runningT[-1], samplingRate ** (-1))
   112                                               # 
   113         1         41.0     41.0      0.0      featureNames = pd.concat([
   114         1         25.0     25.0      0.0          df.columns.to_series()
   115         1     503764.0 503764.0      0.0          for df in collatedDataList])
   116         1         55.0     55.0      0.0      if arguments['chanQuery'] is not None:
   117         1         43.0     43.0      0.0          if arguments['chanQuery'] in namedQueries['chan']:
   118         1         30.0     30.0      0.0              chanQuery = namedQueries['chan'][arguments['chanQuery']]
   119                                                   else:
   120                                                       chanQuery = arguments['chanQuery']
   121         1         47.0     47.0      0.0          chanQuery = chanQuery.replace('chanName', 'featureNames').replace('Emg', 'EMG')
   122                                                   # pdb.set_trace()
   123         1      13753.0  13753.0      0.0          featureNames = featureNames[eval(chanQuery)]
   124                                                   collatedDataList = [
   125         1         38.0     38.0      0.0              df
   126         1     662315.0 662315.0      0.0              for df in collatedDataList
   127                                                       if featureNames.str.contains(df.columns[0]).any()]
   128         1        538.0    538.0      0.0      print('interpolating...')
   129        57     458681.0   8047.0      0.0      for idx, thisFeat in enumerate(tqdm(collatedDataList)):
   130                                                   # tempT = np.unique(np.concatenate([resampledT, thisFeat.index.to_numpy()]))
   131                                                   # pdb.set_trace()
   132        56      11177.0    199.6      0.0          thisColName = thisFeat.columns[0]
   133        56      24597.0    439.2      0.0          print('    {}'.format(thisColName))
   134                                                   # Delsys pads zeros where the signal dropped, interpolate those here
   135                                                   zeroAndStaysZero = (
   136        56     792284.0  14147.9      0.1              (thisFeat[thisColName] == 0) &
   137        56    4050643.0  72332.9      0.3              (thisFeat[thisColName].diff() == 0))
   138                                                   zeroAndWasZero = (
   139        56     632630.0  11297.0      0.0              (thisFeat[thisColName] == 0) &
   140        56    3900677.0  69654.9      0.3              (thisFeat[thisColName].diff(periods=-1) == 0))
   141        56    1025417.0  18311.0      0.1          badMask = zeroAndStaysZero | zeroAndWasZero
   142        56    4331521.0  77348.6      0.3          thisFeat.loc[badMask, thisColName] = np.nan
   143                                                   # pdb.set_trace()
   144        56   13544700.0 241869.6      0.9          thisFeat = thisFeat.interpolate(method='linear', axis=0)
   145        56    4099209.0  73200.2      0.3          thisFeat = thisFeat.fillna(method='bfill').fillna(method='ffill')
   146        56       3197.0     57.1      0.0          outputFeat = hf.interpolateDF(
   147        56       1409.0     25.2      0.0              thisFeat, resampledT,
   148        56       1357.0     24.2      0.0              kind='linear', fill_value=(0, 0),
   149        56  269594574.0 4814188.8     18.9              x=None, columns=None, verbose=arguments['verbose'])
   150        56       4303.0     76.8      0.0          if ('Acc' in thisColName) and arguments['notchAccChans']:
   151                                                       # acc channels present an unusual 75 Hz oscillation
   152                                                       # that is removed here
   153                                                       if 'filterCoeffs' not in locals():
   154                                                           filterOpts = {
   155                                                               'bandstop': {
   156                                                                   'Wn': 75,
   157                                                                   'Q': 5,
   158                                                                   'nHarmonics': 1,
   159                                                                   'N': 4,
   160                                                                   'btype': 'bandstop',
   161                                                                   'ftype': 'bessel'
   162                                                               }
   163                                                           }
   164                                                           filterCoeffs = hf.makeFilterCoeffsSOS(
   165                                                               filterOpts.copy(), samplingRate)
   166                                                       print('        notch filtering at {} Hz (Q = {})'.format(
   167                                                           filterOpts['bandstop']['Wn'], filterOpts['bandstop']['Q']))
   168                                                       filteredFeat = signal.sosfiltfilt(
   169                                                           filterCoeffs, outputFeat[thisColName].to_numpy())
   170                                                       '''
   171                                                       # debug filtering
   172                                                       if arguments['plotting']:
   173                                                           fig, ax = plt.subplots()
   174                                                           ax.plot(resampledT, filteredFeat, label='filtered')
   175                                                           ax.plot(resampledT, outputFeat, label='original')
   176                                                           ax.legend()
   177                                                           plt.show()
   178                                                       '''
   179                                                       outputFeat.loc[:, thisColName] = filteredFeat
   180        56       2021.0     36.1      0.0          collatedDataList[idx] = outputFeat
   181                                                   '''
   182                                                   collatedDataList[idx] = (
   183                                                       thisFeat.reindex(tempT)
   184                                                       .interpolate(method='linear')
   185                                                       .fillna(method='ffill').fillna(method='bfill'))
   186                                                   absentInNew = ~collatedDataList[idx].index.isin(resampledT)
   187                                                   collatedDataList[idx].drop(
   188                                                       index=collatedDataList[idx].index[absentInNew],
   189                                                       inplace=True)
   190        56       1475.0     26.3      0.0          '''
   191         1        792.0    792.0      0.0      print('Concatenating...')
   192         1    9428233.0 9428233.0      0.7      collatedData = pd.concat(collatedDataList, axis=1)
   193                                               collatedData.columns = [
   194         1         92.0     92.0      0.0          re.sub('[\s+]', '', re.sub(r'[^a-zA-Z]', ' ', colName).title())
   195         1      41053.0  41053.0      0.0          for colName in collatedData.columns
   196                                                   ]
   197         1         91.0     91.0      0.0      collatedData.rename(
   198                                                   columns={
   199         1         62.0     62.0      0.0              'TrignoAnalogInputAdapterAnalogA': 'AnalogInputAdapterAnalog',
   200         1         67.0     67.0      0.0              'AnalogInputAdapterAnalogA': 'AnalogInputAdapterAnalog'},
   201         1   10754299.0 10754299.0      0.8          inplace=True)
   202                                               '''
   203                                               collatedData.rename(
   204                                                   columns={
   205                                                       'AnalogInputAdapterAnalogA': 'AnalogInputAdapterAnalog'},
   206                                                   inplace=True)
   207                                               '''
   208         1    2796653.0 2796653.0      0.2      collatedData.fillna(method='bfill', inplace=True)
   209         1         46.0     46.0      0.0      collatedData.index.name = 't'
   210         1      56889.0  56889.0      0.0      collatedData.reset_index(inplace=True)
   211                                               '''
   212                                               if arguments['plotting']:
   213                                                   fig, ax = plt.subplots()
   214                                                   pNames = [
   215                                                       'AnalogInputAdapterAnalog',
   216                                                       'RVastusLateralisEmg',
   217                                                       'RSemitendinosusEmg', 'RPeroneusLongusEmg']
   218                                                   for cName in pNames:
   219                                                       plt.plot(
   220                                                           collatedData['t'],
   221                                                           collatedData[cName] / collatedData[cName].abs().max(),
   222                                                           '.-')
   223                                                   plt.show()
   224                                               '''
   225         1         67.0     67.0      0.0      dataBlock = ns5.dataFrameToAnalogSignals(
   226         1         36.0     36.0      0.0          collatedData,
   227         1         35.0     35.0      0.0          idxT='t', useColNames=True, probeName='',
   228         1   10952533.0 10952533.0      0.8          dataCol=collatedData.drop(columns='t').columns,
   229         1    6883298.0 6883298.0      0.5          samplingRate=samplingRate * pq.Hz, verbose=arguments['verbose'])
   230         1         53.0     53.0      0.0      dataBlock.name = 'delsys'
   231         1         75.0     75.0      0.0      outPathName = os.path.join(
   232         1        434.0    434.0      0.0          scratchFolder, ns5FileName + '_delsys.nix')
   233         1       1772.0   1772.0      0.0      if os.path.exists(outPathName):
   234         1      73213.0  73213.0      0.0          os.remove(outPathName)
   235         1     103148.0 103148.0      0.0      writer = NixIO(filename=outPathName)
   236         1   50443997.0 50443997.0      3.5      writer.write_block(dataBlock, use_obj_names=True)
   237         1     707249.0 707249.0      0.0      writer.close()
   238         1         33.0     33.0      0.0      return

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: analogSignalsToDataFrame at line 43

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    43                                           @profile
    44                                           def analogSignalsToDataFrame(
    45                                                   analogsignals, idxT='t', useChanNames=False):
    46                                               asigList = []
    47                                               for asig in analogsignals:
    48                                                   if asig.shape[1] == 1:
    49                                                       if useChanNames:
    50                                                           colNames = [str(asig.channel_index.name)]
    51                                                       else:
    52                                                           colNames = [str(asig.name)]
    53                                                   else:
    54                                                       colNames = [
    55                                                           asig.name +
    56                                                           '_{}'.format(i) for i in
    57                                                           asig.channel_index.channel_ids
    58                                                           ]
    59                                                   asigList.append(
    60                                                       pd.DataFrame(
    61                                                           asig.magnitude, columns=colNames,
    62                                                           index=range(asig.shape[0])))
    63                                               asigList.append(
    64                                                   pd.DataFrame(
    65                                                       asig.times.magnitude, columns=[idxT],
    66                                                       index=range(asig.shape[0])))
    67                                               return pd.concat(asigList, axis=1)

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: listChanNames at line 69

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    69                                           @profile
    70                                           def listChanNames(
    71                                                   dataBlock, chanQuery,
    72                                                   objType=AnalogSignalProxy, condition=None):
    73                                               allChanList = [
    74                                                   i.name
    75                                                   for i in dataBlock.filter(objects=objType)]
    76                                               if condition == 'hasAsigs':
    77                                                   allChanList = [
    78                                                       i
    79                                                       for i in allChanList
    80                                                       if len(dataBlock.filter(objects=objType, name=i)[0].analogsignals)
    81                                                   ]
    82                                               chansToTrigger = pd.DataFrame(
    83                                                   np.unique(allChanList),
    84                                                   columns=['chanName'])
    85                                               if chanQuery is not None:
    86                                                   chansToTrigger = chansToTrigger.query(
    87                                                       chanQuery, engine='python')['chanName'].to_list()
    88                                               else:
    89                                                   chansToTrigger = chansToTrigger['chanName'].to_list()
    90                                               return chansToTrigger

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: spikeDictToSpikeTrains at line 92

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    92                                           @profile
    93                                           def spikeDictToSpikeTrains(
    94                                                   spikes, block=None, seg=None,
    95                                                   probeName='insTD', t_stop=None,
    96                                                   waveformUnits=pq.uV,
    97                                                   sampling_rate=3e4 * pq.Hz):
    98                                           
    99                                               if block is None:
   100                                                   assert seg is None
   101                                                   block = Block()
   102                                                   seg = Segment(name=probeName + ' segment')
   103                                                   block.segments.append(seg)
   104                                           
   105                                               if t_stop is None:
   106                                                   t_stop = hf.getLastSpikeTime(spikes) + 1
   107                                           
   108                                               for idx, chanName in enumerate(spikes['ChannelID']):
   109                                                   #  unique units on this channel
   110                                                   unitsOnThisChan = pd.unique(spikes['Classification'][idx])
   111                                                   nixChanName = probeName + '{}'.format(chanName)
   112                                                   chanIdx = ChannelIndex(
   113                                                       name=nixChanName,
   114                                                       index=np.asarray([idx]),
   115                                                       channel_names=np.asarray([nixChanName]))
   116                                                   block.channel_indexes.append(chanIdx)
   117                                                   
   118                                                   for unitIdx, unitName in enumerate(unitsOnThisChan):
   119                                                       unitMask = spikes['Classification'][idx] == unitName
   120                                                       # this unit's spike timestamps
   121                                                       theseTimes = spikes['TimeStamps'][idx][unitMask]
   122                                                       # this unit's waveforms
   123                                                       if len(spikes['Waveforms'][idx].shape) == 3:
   124                                                           theseWaveforms = spikes['Waveforms'][idx][unitMask, :, :]
   125                                                           theseWaveforms = np.swapaxes(theseWaveforms, 1, 2)
   126                                                       elif len(spikes['Waveforms'][idx].shape) == 2:
   127                                                           theseWaveforms = (
   128                                                               spikes['Waveforms'][idx][unitMask, np.newaxis, :])
   129                                                       else:
   130                                                           raise(Exception('spikes[Waveforms] has bad shape'))
   131                                           
   132                                                       unitName = '{}#{}'.format(nixChanName, unitIdx)
   133                                                       unit = Unit(name=unitName)
   134                                                       unit.channel_index = chanIdx
   135                                                       chanIdx.units.append(unit)
   136                                           
   137                                                       train = SpikeTrain(
   138                                                           times=theseTimes, t_stop=t_stop, units='sec',
   139                                                           name=unitName, sampling_rate=sampling_rate,
   140                                                           waveforms=theseWaveforms*waveformUnits,
   141                                                           left_sweep=0, dtype=np.float32)
   142                                                       unit.spiketrains.append(train)
   143                                                       seg.spiketrains.append(train)
   144                                           
   145                                                       unit.create_relationship()
   146                                                   chanIdx.create_relationship()
   147                                               seg.create_relationship()
   148                                               block.create_relationship()
   149                                               return block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: spikeTrainsToSpikeDict at line 151

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   151                                           @profile
   152                                           def spikeTrainsToSpikeDict(
   153                                                   spiketrains):
   154                                               nCh = len(spiketrains)
   155                                               spikes = {
   156                                                   'ChannelID': [i for i in range(nCh)],
   157                                                   'Classification': [np.asarray([]) for i in range(nCh)],
   158                                                   'NEUEVWAV_HeaderIndices': [None for i in range(nCh)],
   159                                                   'TimeStamps': [np.asarray([]) for i in range(nCh)],
   160                                                   'Units': 'uV',
   161                                                   'Waveforms': [np.asarray([]) for i in range(nCh)],
   162                                                   'basic_headers': {'TimeStampResolution': 3e4},
   163                                                   'extended_headers': []
   164                                                   }
   165                                               for idx, st in enumerate(spiketrains):
   166                                                   spikes['ChannelID'][idx] = st.name
   167                                                   if len(spikes['TimeStamps'][idx]):
   168                                                       spikes['TimeStamps'][idx] = np.stack((
   169                                                           spikes['TimeStamps'][idx],
   170                                                           st.times.magnitude), axis=-1)
   171                                                   else:
   172                                                       spikes['TimeStamps'][idx] = st.times.magnitude
   173                                                   
   174                                                   theseWaveforms = np.swapaxes(
   175                                                       st.waveforms, 1, 2)
   176                                                   theseWaveforms = np.atleast_2d(np.squeeze(
   177                                                       theseWaveforms))
   178                                                       
   179                                                   if len(spikes['Waveforms'][idx]):
   180                                                       spikes['Waveforms'][idx] = np.stack((
   181                                                           spikes['Waveforms'][idx],
   182                                                           theseWaveforms.magnitude), axis=-1)
   183                                                   else:
   184                                                       spikes['Waveforms'][idx] = theseWaveforms.magnitude
   185                                                   
   186                                                   classVals = st.times.magnitude ** 0 * idx
   187                                                   if len(spikes['Classification'][idx]):
   188                                                       spikes['Classification'][idx] = np.stack((
   189                                                           spikes['Classification'][idx],
   190                                                           classVals), axis=-1)
   191                                                   else:
   192                                                       spikes['Classification'][idx] = classVals
   193                                               return spikes

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: channelIndexesToSpikeDict at line 195

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   195                                           @profile
   196                                           def channelIndexesToSpikeDict(
   197                                                   channel_indexes):
   198                                               nCh = len(channel_indexes)
   199                                               spikes = {
   200                                                   'ChannelID': [i for i in range(nCh)],
   201                                                   'Classification': [np.asarray([]) for i in range(nCh)],
   202                                                   'NEUEVWAV_HeaderIndices': [None for i in range(nCh)],
   203                                                   'TimeStamps': [np.asarray([]) for i in range(nCh)],
   204                                                   'Units': 'uV',
   205                                                   'Waveforms': [np.asarray([]) for i in range(nCh)],
   206                                                   'basic_headers': {'TimeStampResolution': 3e4},
   207                                                   'extended_headers': []
   208                                                   }
   209                                               #  allocate fields for annotations
   210                                               for dummyCh in channel_indexes:
   211                                                   if len(dummyCh.units):
   212                                                       dummyUnit = dummyCh.units[0]
   213                                                       if len(dummyUnit.spiketrains):
   214                                                           if len(dummyUnit.spiketrains[0].times):
   215                                                               break
   216                                               dummySt = [
   217                                                   st
   218                                                   for st in dummyUnit.spiketrains
   219                                                   if len(st.times)][0]
   220                                               #  allocate fields for array annotations (per spike)
   221                                               if dummySt.array_annotations:
   222                                                   for key in dummySt.array_annotations.keys():
   223                                                       spikes.update({key: [np.asarray([]) for i in range(nCh)]})
   224                                                   
   225                                               maxUnitIdx = 0
   226                                               for idx, chIdx in enumerate(channel_indexes):
   227                                                   spikes['ChannelID'][idx] = chIdx.name
   228                                                   for unitIdx, thisUnit in enumerate(chIdx.units):
   229                                                       for stIdx, st in enumerate(thisUnit.spiketrains):
   230                                                           if not len(st.times):
   231                                                               continue
   232                                                           #  print(
   233                                                           #      'unit {} has {} spiketrains'.format(
   234                                                           #          thisUnit.name,
   235                                                           #          len(thisUnit.spiketrains)))
   236                                                           if len(spikes['TimeStamps'][idx]):
   237                                                               spikes['TimeStamps'][idx] = np.concatenate((
   238                                                                   spikes['TimeStamps'][idx],
   239                                                                   st.times.magnitude), axis=0)
   240                                                           else:
   241                                                               spikes['TimeStamps'][idx] = st.times.magnitude
   242                                                           #  reshape waveforms to comply with BRM convention
   243                                                           theseWaveforms = np.swapaxes(
   244                                                               st.waveforms, 1, 2)
   245                                                           theseWaveforms = np.atleast_2d(np.squeeze(
   246                                                               theseWaveforms))
   247                                                           #  append waveforms
   248                                                           if len(spikes['Waveforms'][idx]):
   249                                                               try:
   250                                                                   spikes['Waveforms'][idx] = np.concatenate((
   251                                                                       spikes['Waveforms'][idx],
   252                                                                       theseWaveforms.magnitude), axis=0)
   253                                                               except Exception:
   254                                                                   traceback.print_exc()
   255                                                           else:
   256                                                               spikes['Waveforms'][idx] = theseWaveforms.magnitude
   257                                                           #  give each unit a global index
   258                                                           classVals = st.times.magnitude ** 0 * maxUnitIdx
   259                                                           st.array_annotations.update({'Classification': classVals})
   260                                                           #  expand array_annotations into spikes dict
   261                                                           for key, value in st.array_annotations.items():
   262                                                               if len(spikes[key][idx]):
   263                                                                   spikes[key][idx] = np.concatenate((
   264                                                                       spikes[key][idx],
   265                                                                       value), axis=0)
   266                                                               else:
   267                                                                   spikes[key][idx] = value
   268                                                           for key, value in st.annotations.items():
   269                                                               if key not in spikes['basic_headers']:
   270                                                                   spikes['basic_headers'].update({key: {}})
   271                                                               try:
   272                                                                   spikes['basic_headers'][key].update({maxUnitIdx: value})
   273                                                               except Exception:
   274                                                                   pass
   275                                                           maxUnitIdx += 1
   276                                               return spikes

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: unitSpikeTrainArrayAnnToDF at line 278

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   278                                           @profile
   279                                           def unitSpikeTrainArrayAnnToDF(
   280                                                   spikeTrainContainer):
   281                                               #  list contains different segments
   282                                               if isinstance(spikeTrainContainer, ChannelIndex):
   283                                                   assert len(spikeTrainContainer.units) == 0
   284                                                   spiketrains = spikeTrainContainer.units[0].spiketrains
   285                                               elif isinstance(spikeTrainContainer, Unit):
   286                                                   spiketrains = spikeTrainContainer.spiketrains
   287                                               elif isinstance(spikeTrainContainer, list):
   288                                                   spiketrains = spikeTrainContainer
   289                                               fullAnnotationsDict = {}
   290                                               for segIdx, st in enumerate(spiketrains):
   291                                                   theseAnnDF = pd.DataFrame(st.array_annotations)
   292                                                   theseAnnDF['t'] = st.times.magnitude
   293                                                   fullAnnotationsDict.update({segIdx: theseAnnDF})
   294                                               annotationsDF = pd.concat(
   295                                                   fullAnnotationsDict, names=['segment', 'index'], sort=True)
   296                                               return annotationsDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getSpikeDFMetadata at line 298

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   298                                           @profile
   299                                           def getSpikeDFMetadata(spikeDF, metaDataCols):
   300                                               spikeDF.reset_index(inplace=True)
   301                                               metaDataCols = np.atleast_1d(metaDataCols)
   302                                               spikeDF.index.name = 'metaDataIdx'
   303                                               metaDataDF = spikeDF.loc[:, metaDataCols].copy()
   304                                               newSpikeDF = spikeDF.drop(columns=metaDataCols).reset_index()
   305                                               return newSpikeDF, metaDataDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: transposeSpikeDF at line 307

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   307                                           @profile
   308                                           def transposeSpikeDF(
   309                                                   spikeDF, transposeToColumns,
   310                                                   fastTranspose=False):
   311                                               newColumnNames = np.atleast_1d(transposeToColumns).tolist()
   312                                               originalColumnNames = np.atleast_1d(spikeDF.columns.names)
   313                                               metaDataCols = np.setdiff1d(spikeDF.index.names, newColumnNames).tolist()
   314                                               if fastTranspose:
   315                                                   #  fast but memory inefficient
   316                                                   return spikeDF.stack().unstack(transposeToColumns)
   317                                               else:
   318                                                   raise(Warning('Caution! transposeSpikeDF might not be working, needs testing RD 06252019'))
   319                                                   #  stash annotations, transpose, recover annotations
   320                                                   newSpikeDF, metaDataDF = getSpikeDFMetadata(spikeDF, metaDataCols)
   321                                                   del spikeDF
   322                                                   gc.collect()
   323                                                   #
   324                                                   newSpikeDF = newSpikeDF.stack().unstack(newColumnNames)
   325                                                   newSpikeDF.reset_index(inplace=True)
   326                                                   #  set the index
   327                                                   newIdxLabels = np.concatenate(
   328                                                       [originalColumnNames, metaDataCols]).tolist()
   329                                                   newSpikeDF.loc[:, metaDataCols] = (
   330                                                       metaDataDF
   331                                                       .loc[newSpikeDF['metaDataIdx'].to_list(), metaDataCols]
   332                                                       .to_numpy())
   333                                                   newSpikeDF = (
   334                                                       newSpikeDF
   335                                                       .drop(columns=['metaDataIdx'])
   336                                                       .set_index(newIdxLabels))
   337                                                   return newSpikeDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: concatenateBlocks at line 339

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   339                                           @profile
   340                                           def concatenateBlocks(
   341                                                   asigBlocks, spikeBlocks, eventBlocks, chunkingMetadata,
   342                                                   samplingRate, chanQuery, lazy, trackMemory, verbose
   343                                                   ):
   344                                               # Scan ahead through all files and ensure that
   345                                               # spikeTrains and units are present across all assembled files
   346                                               channelIndexCache = {}
   347                                               unitCache = {}
   348                                               asigCache = []
   349                                               asigAnnCache = {}
   350                                               spiketrainCache = {}
   351                                               eventCache = {}
   352                                               # get list of channels and units
   353                                               for idx, (chunkIdxStr, chunkMeta) in enumerate(chunkingMetadata.items()):
   354                                                   gc.collect()
   355                                                   chunkIdx = int(chunkIdxStr)
   356                                                   asigBlock = asigBlocks[chunkIdx]
   357                                                   asigSeg = asigBlock.segments[0]
   358                                                   spikeBlock = spikeBlocks[chunkIdx]
   359                                                   eventBlock = eventBlocks[chunkIdx]
   360                                                   eventSeg = eventBlock.segments[0]
   361                                                   for chIdx in asigBlock.filter(objects=ChannelIndex):
   362                                                       chAlreadyThere = (chIdx.name in channelIndexCache.keys())
   363                                                       if not chAlreadyThere:
   364                                                           newChIdx = copy(chIdx)
   365                                                           newChIdx.analogsignals = []
   366                                                           newChIdx.units = []
   367                                                           channelIndexCache[chIdx.name] = newChIdx
   368                                                   for unit in (spikeBlock.filter(objects=Unit)):
   369                                                       if lazy:
   370                                                           theseSpiketrains = []
   371                                                           for stP in unit.spiketrains:
   372                                                               st = loadStProxy(stP)
   373                                                               if len(st.times) > 0:
   374                                                                   theseSpiketrains.append(st)
   375                                                       else:
   376                                                           theseSpiketrains = [
   377                                                               st
   378                                                               for st in unit.spiketrains
   379                                                               if len(st.times)
   380                                                               ]
   381                                                       for st in theseSpiketrains:
   382                                                           st = loadObjArrayAnn(st)
   383                                                           if len(st.times):
   384                                                               st.magnitude[:] = st.times.magnitude + spikeBlock.annotations['chunkTStart']
   385                                                               st.t_start = min(0 * pq.s, st.times[0] * 0.999)
   386                                                               st.t_stop = max(
   387                                                                   st.t_stop + spikeBlock.annotations['chunkTStart'] * pq.s,
   388                                                                   st.times[-1] * 1.001)
   389                                                           else:
   390                                                               st.t_start += spikeBlock.annotations['chunkTStart'] * pq.s
   391                                                               st.t_stop += spikeBlock.annotations['chunkTStart'] * pq.s
   392                                                       uAlreadyThere = (unit.name in unitCache.keys())
   393                                                       if not uAlreadyThere:
   394                                                           newUnit = copy(unit)
   395                                                           newUnit.spiketrains = []
   396                                                           newUnit.annotations['parentChanName'] = unit.channel_index.name
   397                                                           unitCache[unit.name] = newUnit
   398                                                           spiketrainCache[unit.name] = theseSpiketrains
   399                                                       else:
   400                                                           spiketrainCache[unit.name] = spiketrainCache[unit.name] + theseSpiketrains
   401                                                   #
   402                                                   if lazy:
   403                                                       evList = [
   404                                                           evP.load()
   405                                                           for evP in eventSeg.events]
   406                                                   else:
   407                                                       evList = eventSeg.events
   408                                                   for event in evList:
   409                                                       event.magnitude[:] = event.magnitude + eventBlock.annotations['chunkTStart']
   410                                                       if event.name in eventCache.keys():
   411                                                           eventCache[event.name].append(event)
   412                                                       else:
   413                                                           eventCache[event.name] = [event]
   414                                                   # take the requested analog signal channels
   415                                                   if lazy:
   416                                                       tdChanNames = listChanNames(
   417                                                           asigBlock, chanQuery, objType=AnalogSignalProxy)
   418                                                       #############
   419                                                       # tdChanNames = ['seg0_utah1', 'seg0_utah10']
   420                                                       ##############
   421                                                       asigList = []
   422                                                       for asigP in asigSeg.analogsignals:
   423                                                           if asigP.name in tdChanNames:
   424                                                               asig = asigP.load()
   425                                                               asig.channel_index = asigP.channel_index
   426                                                               asigList.append(asig)
   427                                                               if trackMemory:
   428                                                                   print('loading {} from proxy object. memory usage: {:.1f} MB'.format(
   429                                                                       asigP.name, prf.memory_usage_psutil()))
   430                                                   else:
   431                                                       tdChanNames = listChanNames(
   432                                                           asigBlock, chanQuery, objType=AnalogSignal)
   433                                                       asigList = [
   434                                                           asig
   435                                                           for asig in asigSeg.analogsignals
   436                                                           if asig.name in tdChanNames
   437                                                           ]
   438                                                   for asig in asigList:
   439                                                       if asig.size > 0:
   440                                                           dummyAsig = asig
   441                                                   if idx == 0:
   442                                                       outputBlock = Block(
   443                                                           name=asigBlock.name,
   444                                                           file_origin=asigBlock.file_origin,
   445                                                           file_datetime=asigBlock.file_datetime,
   446                                                           rec_datetime=asigBlock.rec_datetime,
   447                                                           **asigBlock.annotations
   448                                                       )
   449                                                       newSeg = Segment(
   450                                                           index=0, name=asigSeg.name,
   451                                                           description=asigSeg.description,
   452                                                           file_origin=asigSeg.file_origin,
   453                                                           file_datetime=asigSeg.file_datetime,
   454                                                           rec_datetime=asigSeg.rec_datetime,
   455                                                           **asigSeg.annotations
   456                                                       )
   457                                                       outputBlock.segments = [newSeg]
   458                                                       for asig in asigList:
   459                                                           asigAnnCache[asig.name] = asig.annotations
   460                                                           asigAnnCache[asig.name]['parentChanName'] = asig.channel_index.name
   461                                                       asigUnits = dummyAsig.units
   462                                                   tdDF = analogSignalsToDataFrame(asigList)
   463                                                   del asigList  # asigs saved to dataframe, no longer needed
   464                                                   tdDF.loc[:, 't'] += asigBlock.annotations['chunkTStart']
   465                                                   tdDF.set_index('t', inplace=True)
   466                                                   if samplingRate != dummyAsig.sampling_rate:
   467                                                       lowPassOpts = {
   468                                                           'low': {
   469                                                               'Wn': float(samplingRate / 2),
   470                                                               'N': 4,
   471                                                               'btype': 'low',
   472                                                               'ftype': 'bessel'
   473                                                           }
   474                                                       }
   475                                                       newT = pd.Series(
   476                                                           np.arange(
   477                                                               dummyAsig.t_start + asigBlock.annotations['chunkTStart'] * pq.s,
   478                                                               dummyAsig.t_stop + asigBlock.annotations['chunkTStart'] * pq.s,
   479                                                               1/samplingRate))
   480                                                       if samplingRate < dummyAsig.sampling_rate:
   481                                                           filterCoeffs = hf.makeFilterCoeffsSOS(
   482                                                               lowPassOpts, float(dummyAsig.sampling_rate))
   483                                                           if trackMemory:
   484                                                               print('Filtering analog data before downsampling. memory usage: {:.1f} MB'.format(
   485                                                                   prf.memory_usage_psutil()))
   486                                                           '''
   487                                                           ### check that axis=0 is the correct option
   488                                                           dummyDF = tdDF.iloc[:, :4].copy()
   489                                                           filteredAsigs0 = signal.sosfiltfilt( filterCoeffs, dummyDF.to_numpy(), axis=0)
   490                                                           filteredAsigs1 = signal.sosfiltfilt( filterCoeffs, dummyDF.to_numpy(), axis=1)
   491                                                           ###
   492                                                           '''
   493                                                           filteredAsigs = signal.sosfiltfilt(
   494                                                               filterCoeffs, tdDF.to_numpy(),
   495                                                               axis=0)
   496                                                           tdDF = pd.DataFrame(
   497                                                               filteredAsigs,
   498                                                               index=tdDF.index,
   499                                                               columns=tdDF.columns)
   500                                                           if trackMemory:
   501                                                               print('Just finished analog data filtering before downsampling. memory usage: {:.1f} MB'.format(
   502                                                                   prf.memory_usage_psutil()))
   503                                                       tdInterp = hf.interpolateDF(
   504                                                           tdDF, newT,
   505                                                           kind='linear', fill_value='extrapolate',
   506                                                           verbose=verbose)
   507                                                       # free up memory used by full resolution asigs
   508                                                       del tdDF
   509                                                   else:
   510                                                       tdInterp = tdDF
   511                                                   #
   512                                                   asigCache.append(tdInterp)
   513                                                   #
   514                                                   print('Finished chunk {}'.format(chunkIdxStr))
   515                                               allTdDF = pd.concat(asigCache)
   516                                               # TODO: check for nans, if, for example a signal is partially missing
   517                                               allTdDF.fillna(method='bfill', inplace=True)
   518                                               allTdDF.fillna(method='ffill', inplace=True)
   519                                               for asigName in allTdDF.columns:
   520                                                   newAsig = AnalogSignal(
   521                                                       allTdDF[asigName].to_numpy() * asigUnits,
   522                                                       name=asigName,
   523                                                       sampling_rate=samplingRate,
   524                                                       dtype=np.float32,
   525                                                       **asigAnnCache[asigName])
   526                                                   chIdxName = asigAnnCache[asigName]['parentChanName']
   527                                                   chIdx = channelIndexCache[chIdxName]
   528                                                   # cross-assign ownership to containers
   529                                                   chIdx.analogsignals.append(newAsig)
   530                                                   newSeg.analogsignals.append(newAsig)
   531                                                   newAsig.channel_index = chIdx
   532                                                   newAsig.segment = newSeg
   533                                               #
   534                                               for uName, unit in unitCache.items():
   535                                                   # concatenate spike times, waveforms, etc.
   536                                                   if len(spiketrainCache[unit.name]):
   537                                                       consolidatedTimes = np.concatenate([
   538                                                               st.times.magnitude
   539                                                               for st in spiketrainCache[unit.name]
   540                                                           ])
   541                                                       # TODO:   decide whether to include this step
   542                                                       #         which snaps the spike times to the nearest
   543                                                       #         *sampled* data point
   544                                                       #
   545                                                       # consolidatedTimes, timesIndex = hf.closestSeries(
   546                                                       #     takeFrom=pd.Series(consolidatedTimes),
   547                                                       #     compareTo=pd.Series(allTdDF.index))
   548                                                       #
   549                                                       # find an example spiketrain with array_annotations
   550                                                       for st in spiketrainCache[unit.name]:
   551                                                           if len(st.times):
   552                                                               dummySt = st
   553                                                               break
   554                                                       consolidatedAnn = {
   555                                                           key: np.array([])
   556                                                           for key, value in dummySt.array_annotations.items()
   557                                                           }
   558                                                       for key, value in consolidatedAnn.items():
   559                                                           consolidatedAnn[key] = np.concatenate([
   560                                                               st.annotations[key]
   561                                                               for st in spiketrainCache[unit.name]
   562                                                           ])
   563                                                       consolidatedWaveforms = np.concatenate([
   564                                                           st.waveforms
   565                                                           for st in spiketrainCache[unit.name]
   566                                                           ])
   567                                                       spikeTStop = max([
   568                                                           st.t_stop
   569                                                           for st in spiketrainCache[unit.name]
   570                                                           ])
   571                                                       spikeTStart = max([
   572                                                           st.t_start
   573                                                           for st in spiketrainCache[unit.name]
   574                                                           ])
   575                                                       spikeAnnotations = {
   576                                                           key: value
   577                                                           for key, value in dummySt.annotations.items()
   578                                                           if key not in dummySt.annotations['arrayAnnNames']
   579                                                       }
   580                                                       newSt = SpikeTrain(
   581                                                           name=dummySt.name,
   582                                                           times=consolidatedTimes, units='sec', t_stop=spikeTStop,
   583                                                           waveforms=consolidatedWaveforms * dummySt.waveforms.units,
   584                                                           left_sweep=dummySt.left_sweep,
   585                                                           sampling_rate=dummySt.sampling_rate,
   586                                                           t_start=spikeTStart, **spikeAnnotations,
   587                                                           array_annotations=consolidatedAnn)
   588                                                       # cross-assign ownership to containers
   589                                                       unit.spiketrains.append(newSt)
   590                                                       newSt.unit = unit
   591                                                       newSeg.spiketrains.append(newSt)
   592                                                       newSt.segment = newSeg
   593                                                       # link chIdxes and Units
   594                                                       if unit.annotations['parentChanName'] in channelIndexCache:
   595                                                           chIdx = channelIndexCache[unit.annotations['parentChanName']]
   596                                                           if unit not in chIdx.units:
   597                                                               chIdx.units.append(unit)
   598                                                               unit.channel_index = chIdx
   599                                                       else:
   600                                                           newChIdx = ChannelIndex(
   601                                                               name=unit.annotations['parentChanName'], index=0)
   602                                                           channelIndexCache[unit.annotations['parentChanName']] = newChIdx
   603                                                           if unit not in newChIdx.units:
   604                                                               newChIdx.units.append(unit)
   605                                                               unit.channel_index = newChIdx
   606                                               #
   607                                               for evName, eventList in eventCache.items():
   608                                                   consolidatedTimes = np.concatenate([
   609                                                       ev.times.magnitude
   610                                                       for ev in eventList
   611                                                       ])
   612                                                   consolidatedLabels = np.concatenate([
   613                                                       ev.labels
   614                                                       for ev in eventList
   615                                                       ])
   616                                                   newEvent = Event(
   617                                                       name=evName,
   618                                                       times=consolidatedTimes * pq.s,
   619                                                       labels=consolidatedLabels
   620                                                       )
   621                                                   # if len(newEvent):
   622                                                   newEvent.segment = newSeg
   623                                                   newSeg.events.append(newEvent)
   624                                               for chIdxName, chIdx in channelIndexCache.items():
   625                                                   if len(chIdx.analogsignals) or len(chIdx.units):
   626                                                       outputBlock.channel_indexes.append(chIdx)
   627                                                       chIdx.block = outputBlock
   628                                               #
   629                                               outputBlock = purgeNixAnn(outputBlock)
   630                                               createRelationship = False
   631                                               if createRelationship:
   632                                                   outputBlock.create_relationship()
   633                                               return outputBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: concatenateEventsContainer at line 660

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   660                                           @profile
   661                                           def concatenateEventsContainer(eventContainer, linkParents=True):
   662                                               if isinstance(eventContainer, dict):
   663                                                   listOfEvents = list(eventContainer.values())
   664                                               else:
   665                                                   listOfEvents = eventContainer
   666                                               nonEmptyEvents = [ev for ev in listOfEvents if len(ev.times)]
   667                                               if not len(nonEmptyEvents) > 0:
   668                                                   return listOfEvents[0]
   669                                               masterEvent = listOfEvents[0]
   670                                               for evIdx, ev in enumerate(listOfEvents[1:]):
   671                                                   try:
   672                                                       masterEvent = masterEvent.merge(ev)
   673                                                   except Exception:
   674                                                       traceback.print_exc()
   675                                                       pdb.set_trace()
   676                                               if masterEvent.array_annotations is not None:
   677                                                   arrayAnnNames = list(masterEvent.array_annotations.keys())
   678                                                   masterEvent.annotations.update(masterEvent.array_annotations)
   679                                                   masterEvent.annotations['arrayAnnNames'] = arrayAnnNames
   680                                               if linkParents:
   681                                                   masterEvent.segment = listOfEvents[0].segment
   682                                                   if isinstance(masterEvent, SpikeTrain):
   683                                                       masterEvent.unit = listOfEvents[0].unit
   684                                               return masterEvent

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: unitSpikeTrainWaveformsToDF at line 743

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   743                                           @profile
   744                                           def unitSpikeTrainWaveformsToDF(
   745                                                   spikeTrainContainer,
   746                                                   dataQuery=None,
   747                                                   transposeToColumns='bin', fastTranspose=True,
   748                                                   lags=None, decimate=1, rollingWindow=None,
   749                                                   getMetaData=True, verbose=False,
   750                                                   whichSegments=None, windowSize=None, procFun=None):
   751                                               #  list contains different segments from *one* unit
   752                                               if isinstance(spikeTrainContainer, ChannelIndex):
   753                                                   assert len(spikeTrainContainer.units) == 0
   754                                                   spiketrains = spikeTrainContainer.units[0].spiketrains
   755                                               elif isinstance(spikeTrainContainer, Unit):
   756                                                   spiketrains = spikeTrainContainer.spiketrains
   757                                               else:
   758                                                   raise(Exception('not a valid container'))
   759                                               # TODO check if really need to assert uniqueness?
   760                                               uniqueSpiketrains = []
   761                                               for st in spiketrains:
   762                                                   if not np.any([st is i for i in uniqueSpiketrains]):
   763                                                       uniqueSpiketrains.append(st)
   764                                               #  subsampling options
   765                                               decimate = int(decimate)
   766                                               if whichSegments is not None:
   767                                                   uniqueSpiketrains = [
   768                                                       uniqueSpiketrains[i]
   769                                                       for i in whichSegments
   770                                                   ]
   771                                               #
   772                                               waveformsList = []
   773                                               #
   774                                               for segIdx, stIn in enumerate(uniqueSpiketrains):
   775                                                   if verbose:
   776                                                       print('extracting spiketrain from {}'.format(stIn.segment))
   777                                                   #  make sure is not a proxyObj
   778                                                   if isinstance(stIn, SpikeTrainProxy):
   779                                                       st = loadStProxy(stIn)
   780                                                       if (getMetaData) or (dataQuery is not None):
   781                                                           # if there's a query, get metadata temporarily to resolve it
   782                                                           st = loadObjArrayAnn(st)
   783                                                   else:
   784                                                       st = stIn
   785                                                   #  extract bins spaced by decimate argument
   786                                                   if not st.times.any():
   787                                                       continue
   788                                                   if verbose:
   789                                                       print('extracting wf from {}'.format(stIn.segment))
   790                                                   wf = np.asarray(
   791                                                       np.squeeze(st.waveforms),
   792                                                       dtype='float32')
   793                                                   if wf.ndim == 3:
   794                                                       print('Waveforms from more than one channel!')
   795                                                       if wf.shape[1] > 0:
   796                                                           wf = wf[:, 0, :]
   797                                                   wfDF = pd.DataFrame(wf)
   798                                                   samplingRate = st.sampling_rate
   799                                                   bins = (
   800                                                       np.asarray(wfDF.columns) / samplingRate -
   801                                                       st.left_sweep)
   802                                                   wfDF.columns = np.around(bins.magnitude, decimals=6)
   803                                                   if windowSize is not None:
   804                                                       winMask = (
   805                                                           (wfDF.columns >= windowSize[0]) &
   806                                                           (wfDF.columns <= windowSize[1]))
   807                                                       wfDF = wfDF.loc[:, winMask]
   808                                                   if procFun is not None:
   809                                                       wfDF = procFun(wfDF, st)
   810                                                   idxLabels = ['segment', 'originalIndex', 't']
   811                                                   wfDF.loc[:, 't'] = np.asarray(st.times.magnitude)
   812                                                   if (getMetaData) or (dataQuery is not None):
   813                                                       # if there's a query, get metadata temporarily to resolve it
   814                                                       annDict = {}
   815                                                       for k, values in st.array_annotations.items():
   816                                                           if isinstance(getMetaData, Iterable):
   817                                                               # if selecting metadata fields, check that
   818                                                               # the key is in the provided list
   819                                                               if k not in getMetaData:
   820                                                                   continue
   821                                                           if isinstance(values[0], str):
   822                                                               v = np.asarray(values, dtype='str')
   823                                                           else:
   824                                                               v = np.asarray(values)
   825                                                           annDict.update({k: v})
   826                                                       skipAnnNames = (
   827                                                           st.annotations['arrayAnnNames'] +
   828                                                           [
   829                                                               'arrayAnnNames', 'arrayAnnDTypes',
   830                                                               'nix_name', 'neo_name', 'id',
   831                                                               'cell_label', 'cluster_label', 'max_on_channel', 'binWidth']
   832                                                           )
   833                                                       annDF = pd.DataFrame(annDict)
   834                                                       for k, value in st.annotations.items():
   835                                                           if isinstance(getMetaData, Iterable):
   836                                                               # if selecting metadata fields, check that
   837                                                               # the key is in the provided list
   838                                                               if k not in getMetaData:
   839                                                                   continue
   840                                                           if k not in skipAnnNames:
   841                                                               annDF.loc[:, k] = value
   842                                                       #
   843                                                       if isinstance(getMetaData, Iterable):
   844                                                           doNotFillList = idxLabels + ['feature', 'bin']
   845                                                           fieldsNeedFiller = [
   846                                                               mdn
   847                                                               for mdn in getMetaData
   848                                                               if (mdn not in doNotFillList) and (mdn not in annDF.columns)]
   849                                                           for mdName in fieldsNeedFiller:
   850                                                               annDF.loc[:, mdName] = 'NA'
   851                                                       annColumns = annDF.columns.to_list()
   852                                                       if getMetaData:
   853                                                           for annNm in annColumns:
   854                                                               if annNm not in idxLabels:
   855                                                                   idxLabels.append(annNm)
   856                                                           # idxLabels += annColumns
   857                                                       spikeDF = annDF.join(wfDF)
   858                                                   else:
   859                                                       spikeDF = wfDF
   860                                                       del wfDF, st
   861                                                   spikeDF.loc[:, 'segment'] = segIdx
   862                                                   spikeDF.loc[:, 'originalIndex'] = spikeDF.index
   863                                                   spikeDF.columns.name = 'bin'
   864                                                   #
   865                                                   if dataQuery is not None:
   866                                                       spikeDF.query(dataQuery, inplace=True)
   867                                                       if not getMetaData:
   868                                                           spikeDF.drop(columns=annColumns, inplace=True)
   869                                                   waveformsList.append(spikeDF)
   870                                               #
   871                                               zeroLagWaveformsDF = pd.concat(waveformsList, axis='index')
   872                                               if verbose:
   873                                                   prf.print_memory_usage('before transposing waveforms')
   874                                               # TODO implement lags and rolling window addition here
   875                                               metaDF = zeroLagWaveformsDF.loc[:, idxLabels].copy()
   876                                               zeroLagWaveformsDF.drop(columns=idxLabels, inplace=True)
   877                                               if lags is None:
   878                                                   lags = [0]
   879                                               laggedWaveformsDict = {
   880                                                   (spikeTrainContainer.name, k): None for k in lags}
   881                                               for lag in lags:
   882                                                   if isinstance(lag, int):
   883                                                       shiftedWaveform = zeroLagWaveformsDF.shift(
   884                                                           lag, axis='columns')
   885                                                       if rollingWindow is not None:
   886                                                           halfRollingWin = int(np.ceil(rollingWindow/2))
   887                                                           seekIdx = slice(
   888                                                               halfRollingWin, -halfRollingWin+1, decimate)
   889                                                           # seekIdx = slice(None, None, decimate)
   890                                                           #shiftedWaveform = (
   891                                                           #    shiftedWaveform
   892                                                           #    .rolling(
   893                                                           #        window=rollingWindow, win_type='gaussian',
   894                                                           #        axis='columns', center=True)
   895                                                           #    .mean(std=halfRollingWin))
   896                                                           shiftedWaveform = (
   897                                                               shiftedWaveform
   898                                                               .rolling(
   899                                                                   window=rollingWindow, 
   900                                                                   axis='columns', center=True)
   901                                                               .mean())
   902                                                       else:
   903                                                           halfRollingWin = 0
   904                                                           seekIdx = slice(None, None, decimate)
   905                                                           if False:
   906                                                               oldShiftedWaveform = zeroLagWaveformsDF.shift(
   907                                                                   lag, axis='columns')
   908                                                               plt.plot(oldShiftedWaveform.iloc[0, :])
   909                                                               plt.plot(shiftedWaveform.iloc[0, :])
   910                                                               plt.show()
   911                                                       laggedWaveformsDict[
   912                                                           (spikeTrainContainer.name, lag)] = (
   913                                                               shiftedWaveform.iloc[:, seekIdx].copy())
   914                                                   if isinstance(lag, tuple):
   915                                                       halfRollingWin = int(np.ceil(lag[1]/2))
   916                                                       seekIdx = slice(
   917                                                           halfRollingWin, -halfRollingWin+1, decimate)
   918                                                       # seekIdx = slice(None, None, decimate)
   919                                                       shiftedWaveform = (
   920                                                           zeroLagWaveformsDF
   921                                                           .shift(lag[0], axis='columns')
   922                                                           .rolling(
   923                                                               window=lag[1], win_type='gaussian',
   924                                                               axis='columns', center=True)
   925                                                           .mean(std=halfRollingWin))
   926                                                       laggedWaveformsDict[
   927                                                           (spikeTrainContainer.name, lag)] = (
   928                                                               shiftedWaveform.iloc[:, seekIdx].copy())
   929                                               #
   930                                               if transposeToColumns == 'feature':
   931                                                   # stack the bin, name the feature column
   932                                                   # 
   933                                                   for idx, (key, value) in enumerate(laggedWaveformsDict.items()):
   934                                                       if idx == 0:
   935                                                           stackedIndexDF = pd.concat(
   936                                                               [metaDF, value], axis='columns')
   937                                                           stackedIndexDF.set_index(idxLabels, inplace=True)
   938                                                           # don't drop nans for now - might need to keep track of them
   939                                                           # if we need to equalize to another array later
   940                                                           newIndex = stackedIndexDF.stack(dropna=False).index
   941                                                           idxLabels.append('bin')
   942                                                       laggedWaveformsDict[key] = value.stack(dropna=False).to_frame(name=key).reset_index(drop=True)
   943                                                   waveformsDF = pd.concat(
   944                                                       laggedWaveformsDict.values(),
   945                                                       axis='columns')
   946                                                   waveformsDF.columns.names = ['feature', 'lag']
   947                                                   waveformsDF.index = newIndex
   948                                                   waveformsDF.columns.name = 'feature'
   949                                               elif transposeToColumns == 'bin':
   950                                                   # add the feature column
   951                                                   waveformsDF = pd.concat(
   952                                                       laggedWaveformsDict,
   953                                                       names=['feature', 'lag', 'originalDummy']).reset_index()
   954                                                   waveformsDF = pd.concat(
   955                                                       [
   956                                                           metaDF.reset_index(drop=True),
   957                                                           waveformsDF.drop(columns='originalDummy')],
   958                                                       axis='columns')
   959                                                   idxLabels += ['feature', 'lag']
   960                                                   waveformsDF.columns.name = 'bin'
   961                                                   waveformsDF.set_index(idxLabels, inplace=True)
   962                                               #
   963                                               if transposeToColumns != waveformsDF.columns.name:
   964                                                   waveformsDF = transposeSpikeDF(
   965                                                       waveformsDF, transposeToColumns,
   966                                                       fastTranspose=fastTranspose)
   967                                               return waveformsDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: concatenateUnitSpikeTrainWaveformsDF at line 969

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   969                                           @profile
   970                                           def concatenateUnitSpikeTrainWaveformsDF(
   971                                                   units, dataQuery=None,
   972                                                   transposeToColumns='bin', concatOn='index',
   973                                                   fastTranspose=True, getMetaData=True, verbose=False,
   974                                                   addLags=None, decimate=1, rollingWindow=None,
   975                                                   metaDataToCategories=False, windowSize=None,
   976                                                   whichSegments=None, procFun=None):
   977                                               allUnits = []
   978                                               for thisUnit in units:
   979                                                   hasAnySpikes = []
   980                                                   for stIn in thisUnit.spiketrains:
   981                                                       if isinstance(stIn, SpikeTrainProxy):
   982                                                           st = stIn.load(
   983                                                               magnitude_mode='rescaled',
   984                                                               load_waveforms=False)
   985                                                       else:
   986                                                           st = stIn
   987                                                       hasAnySpikes.append(st.times.any())
   988                                                   if np.any(hasAnySpikes):
   989                                                       allUnits.append(thisUnit)
   990                                               waveformsList = []
   991                                               for idx, thisUnit in enumerate(allUnits):
   992                                                   if verbose:
   993                                                       print('concatenating unitDF {}'.format(thisUnit.name))
   994                                                   lags = None
   995                                                   if addLags is not None:
   996                                                       if thisUnit.name in addLags:
   997                                                           lags = addLags[thisUnit.name]
   998                                                   unitWaveforms = unitSpikeTrainWaveformsToDF(
   999                                                       thisUnit, dataQuery=dataQuery,
  1000                                                       transposeToColumns=transposeToColumns,
  1001                                                       fastTranspose=fastTranspose, getMetaData=getMetaData,
  1002                                                       lags=lags, decimate=decimate, rollingWindow=rollingWindow,
  1003                                                       verbose=verbose, windowSize=windowSize,
  1004                                                       whichSegments=whichSegments, procFun=procFun)
  1005                                                   if idx == 0:
  1006                                                       idxLabels = unitWaveforms.index.names
  1007                                                   if (concatOn == 'columns') and (idx > 0):
  1008                                                       # other than first time, we already have the metadata
  1009                                                       unitWaveforms.reset_index(drop=True, inplace=True)
  1010                                                   else:
  1011                                                       # first time, or if concatenating indices,
  1012                                                       # keep the the metadata
  1013                                                       unitWaveforms.reset_index(inplace=True)
  1014                                                       if metaDataToCategories:
  1015                                                           # convert metadata to categoricals to free memory
  1016                                                           #
  1017                                                           unitWaveforms[idxLabels] = (
  1018                                                               unitWaveforms[idxLabels]
  1019                                                               .astype('category')
  1020                                                               )
  1021                                                   waveformsList.append(unitWaveforms)
  1022                                                   del unitWaveforms
  1023                                                   if verbose:
  1024                                                       print('memory usage: {:.1f} MB'.format(prf.memory_usage_psutil()))
  1025                                               if verbose:
  1026                                                   print(
  1027                                                       'about to join all, memory usage: {:.1f} MB'
  1028                                                       .format(prf.memory_usage_psutil()))
  1029                                               #  if concatenating indexes, reset the index of the result
  1030                                               #  ignoreIndex = (concatOn == 'index')
  1031                                               allWaveforms = pd.concat(
  1032                                                   waveformsList, axis=concatOn,
  1033                                                   # ignore_index=ignoreIndex
  1034                                                   )
  1035                                               del waveformsList
  1036                                               if verbose:
  1037                                                   print(
  1038                                                       'finished concatenating, memory usage: {:.1f} MB'
  1039                                                       .format(prf.memory_usage_psutil()))
  1040                                               try:
  1041                                                   allWaveforms.set_index(idxLabels, inplace=True)
  1042                                                   allWaveforms.sort_index(
  1043                                                       level=['segment', 'originalIndex', 't'],
  1044                                                       axis='index', inplace=True, kind='mergesort')
  1045                                                   allWaveforms.sort_index(
  1046                                                       axis='columns', inplace=True, kind='mergesort')
  1047                                               except Exception:
  1048                                                   pdb.set_trace()
  1049                                               return allWaveforms

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: alignedAsigsToDF at line 1051

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1051                                           @profile
  1052                                           def alignedAsigsToDF(
  1053                                                   dataBlock, unitNames=None,
  1054                                                   unitQuery=None, dataQuery=None,
  1055                                                   collapseSizes=False, verbose=False,
  1056                                                   duplicateControlsByProgram=False,
  1057                                                   amplitudeColumn='amplitude',
  1058                                                   programColumn='program',
  1059                                                   electrodeColumn='electrode',
  1060                                                   transposeToColumns='bin', concatOn='index', fastTranspose=True,
  1061                                                   addLags=None, decimate=1, rollingWindow=None,
  1062                                                   whichSegments=None, windowSize=None,
  1063                                                   getMetaData=True, metaDataToCategories=True,
  1064                                                   outlierTrials=None, invertOutlierMask=False,
  1065                                                   makeControlProgram=False, removeFuzzyName=False, procFun=None):
  1066                                               #  channels to trigger
  1067                                               if unitNames is None:
  1068                                                   unitNames = listChanNames(dataBlock, unitQuery, objType=Unit)
  1069                                               allUnits = []
  1070                                               for uName in unitNames:
  1071                                                   allUnits += dataBlock.filter(objects=Unit, name=uName)
  1072                                               allWaveforms = concatenateUnitSpikeTrainWaveformsDF(
  1073                                                   allUnits, dataQuery=dataQuery,
  1074                                                   transposeToColumns=transposeToColumns, concatOn=concatOn,
  1075                                                   fastTranspose=fastTranspose,
  1076                                                   addLags=addLags, decimate=decimate, rollingWindow=rollingWindow,
  1077                                                   verbose=verbose, whichSegments=whichSegments,
  1078                                                   windowSize=windowSize, procFun=procFun,
  1079                                                   getMetaData=getMetaData, metaDataToCategories=metaDataToCategories)
  1080                                               #
  1081                                               manipulateIndex = np.any(
  1082                                                   [
  1083                                                       collapseSizes, duplicateControlsByProgram,
  1084                                                       makeControlProgram, removeFuzzyName
  1085                                                       ])
  1086                                               if outlierTrials is not None:
  1087                                                   def rejectionLookup(entry):
  1088                                                       key = []
  1089                                                       for subKey in outlierTrials.index.names:
  1090                                                           keyIdx = allWaveforms.index.names.index(subKey)
  1091                                                           key.append(entry[keyIdx])
  1092                                                       # print(key)
  1093                                                       # outlierTrials.iloc[1, :]
  1094                                                       # allWaveforms.iloc[1, :]
  1095                                                       return outlierTrials[tuple(key)]
  1096                                                   #
  1097                                                   outlierMask = np.asarray(
  1098                                                       allWaveforms.index.map(rejectionLookup),
  1099                                                       dtype=np.bool)
  1100                                                   if invertOutlierMask:
  1101                                                       outlierMask = ~outlierMask
  1102                                                   allWaveforms = allWaveforms.loc[~outlierMask, :]
  1103                                               if manipulateIndex and getMetaData:
  1104                                                   idxLabels = allWaveforms.index.names
  1105                                                   allWaveforms.reset_index(inplace=True)
  1106                                                   # 
  1107                                                   if collapseSizes:
  1108                                                       try:
  1109                                                           allWaveforms.loc[allWaveforms['pedalSizeCat'] == 'XL', 'pedalSizeCat'] = 'L'
  1110                                                           allWaveforms.loc[allWaveforms['pedalSizeCat'] == 'XS', 'pedalSizeCat'] = 'S'
  1111                                                       except Exception:
  1112                                                           traceback.print_exc()
  1113                                                   if makeControlProgram:
  1114                                                       try:
  1115                                                           allWaveforms.loc[allWaveforms[amplitudeColumn] == 0, programColumn] = 999
  1116                                                           allWaveforms.loc[allWaveforms[amplitudeColumn] == 0, electrodeColumn] = 'control'
  1117                                                       except Exception:
  1118                                                           traceback.print_exc()
  1119                                                   if duplicateControlsByProgram:
  1120                                                       #
  1121                                                       noStimWaveforms = (
  1122                                                           allWaveforms
  1123                                                           .loc[allWaveforms[amplitudeColumn] == 0, :]
  1124                                                           )
  1125                                                       stimWaveforms = (
  1126                                                           allWaveforms
  1127                                                           .loc[allWaveforms[amplitudeColumn] != 0, :]
  1128                                                           .copy()
  1129                                                           )
  1130                                                       uniqProgs = stimWaveforms[programColumn].unique()
  1131                                                       progElecLookup = {}
  1132                                                       #pdb.set_trace()
  1133                                                       for progIdx in uniqProgs:
  1134                                                           theseStimDF = stimWaveforms.loc[
  1135                                                               stimWaveforms[programColumn] == progIdx,
  1136                                                               electrodeColumn]
  1137                                                           elecIdx = theseStimDF.iloc[0]
  1138                                                           progElecLookup.update({progIdx: elecIdx})
  1139                                                       #
  1140                                                       if makeControlProgram:
  1141                                                           uniqProgs = np.append(uniqProgs, 999)
  1142                                                           progElecLookup.update({999: 'control'})
  1143                                                       #
  1144                                                       for progIdx in uniqProgs:
  1145                                                           dummyWaveforms = noStimWaveforms.copy()
  1146                                                           dummyWaveforms.loc[:, programColumn] = progIdx
  1147                                                           dummyWaveforms.loc[:, electrodeColumn] = progElecLookup[progIdx]
  1148                                                           stimWaveforms = pd.concat([stimWaveforms, dummyWaveforms])
  1149                                                       stimWaveforms.reset_index(drop=True, inplace=True)
  1150                                                       allWaveforms = stimWaveforms
  1151                                                   #
  1152                                                   if removeFuzzyName:
  1153                                                       fuzzyNamesBase = [
  1154                                                           i.replace('Fuzzy', '')
  1155                                                           for i in idxLabels
  1156                                                           if 'Fuzzy' in i]
  1157                                                       colRenamer = {n + 'Fuzzy': n for n in fuzzyNamesBase}
  1158                                                       fuzzyNamesBasePresent = [
  1159                                                           i
  1160                                                           for i in fuzzyNamesBase
  1161                                                           if i in allWaveforms.columns]
  1162                                                       allWaveforms.drop(columns=fuzzyNamesBasePresent, inplace=True)
  1163                                                       allWaveforms.rename(columns=colRenamer, inplace=True)
  1164                                                       idxLabels = np.unique(
  1165                                                           [i.replace('Fuzzy', '') for i in idxLabels])
  1166                                                   #
  1167                                                   allWaveforms.set_index(
  1168                                                       list(idxLabels),
  1169                                                       inplace=True)
  1170                                                   if isinstance(allWaveforms.columns, pd.MultiIndex):
  1171                                                       allWaveforms.columns = allWaveforms.columns.remove_unused_levels()
  1172                                               #
  1173                                               if transposeToColumns == 'feature':
  1174                                                   zipNames = zip(pd.unique(allWaveforms.columns.get_level_values('feature')).tolist(), unitNames)
  1175                                                   try:
  1176                                                       assert np.all([i == j for i, j in zipNames]), 'columns out of requested order!'
  1177                                                   except Exception:
  1178                                                       traceback.print_exc()
  1179                                                       allWaveforms.reindex(columns=unitNames)
  1180                                               if isinstance(allWaveforms.columns, pd.MultiIndex):
  1181                                                   allWaveforms.columns = allWaveforms.columns.remove_unused_levels()
  1182                                               allWaveforms.sort_index(
  1183                                                   axis='columns', inplace=True, kind='mergesort')
  1184                                               return allWaveforms

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getAsigsAlignedToEvents at line 1186

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1186                                           @profile
  1187                                           def getAsigsAlignedToEvents(
  1188                                                   eventBlock=None, signalBlock=None,
  1189                                                   chansToTrigger=None, chanQuery=None,
  1190                                                   eventName=None, windowSize=None,
  1191                                                   minNReps=None,
  1192                                                   appendToExisting=False,
  1193                                                   checkReferences=True, verbose=False,
  1194                                                   fileName=None, folderPath=None, chunkSize=None
  1195                                                   ):
  1196                                               #  get signals from same block as events?
  1197                                               if signalBlock is None:
  1198                                                   signalBlock = eventBlock
  1199                                               #  channels to trigger
  1200                                               if chansToTrigger is None:
  1201                                                   chansToTrigger = listChanNames(
  1202                                                       signalBlock, chanQuery, objType=ChannelIndex, condition='hasAsigs')
  1203                                               #  allocate block for spiketrains
  1204                                               masterBlock = Block()
  1205                                               try:
  1206                                                   masterBlock.name = signalBlock.annotations['neo_name']
  1207                                                   masterBlock.annotate(nix_name=signalBlock.annotations['neo_name'])
  1208                                               except Exception:
  1209                                                   masterBlock.name = signalBlock.name
  1210                                                   masterBlock.annotate(neo_name=signalBlock.name)
  1211                                                   masterBlock.annotate(nix_name=signalBlock.name)
  1212                                               #  make channels and units for triggered time series
  1213                                               for chanName in chansToTrigger:
  1214                                                   chanIdx = ChannelIndex(name=chanName + '#0', index=[0])
  1215                                                   chanIdx.annotate(nix_name=chanIdx.name)
  1216                                                   thisUnit = Unit(name=chanIdx.name)
  1217                                                   thisUnit.annotate(nix_name=chanIdx.name)
  1218                                                   chanIdx.units.append(thisUnit)
  1219                                                   thisUnit.channel_index = chanIdx
  1220                                                   masterBlock.channel_indexes.append(chanIdx)
  1221                                                   sigChanIdxList = signalBlock.filter(
  1222                                                       objects=ChannelIndex, name=chanName)
  1223                                                   if len(sigChanIdxList):
  1224                                                       sigChanIdx = sigChanIdxList[0]
  1225                                                       if sigChanIdx.coordinates is not None:
  1226                                                           coordUnits = sigChanIdx.coordinates[0][0].units
  1227                                                           chanIdx.coordinates = np.asarray(sigChanIdx.coordinates) * coordUnits
  1228                                                           thisUnit.annotations['parentChanXCoords'] = float(chanIdx.coordinates[:, 0].magnitude)
  1229                                                           thisUnit.annotations['parentChanYCoords'] = float(chanIdx.coordinates[:, 1].magnitude)
  1230                                                           thisUnit.annotations['parentChanCoordinateUnits'] = '{}'.format(coordUnits)
  1231                                               #
  1232                                               totalNSegs = 0
  1233                                               #  print([evSeg.events[3].name for evSeg in eventBlock.segments])
  1234                                               allAlignEventsList = []
  1235                                               for segIdx, eventSeg in enumerate(eventBlock.segments):
  1236                                                   thisEventName = 'seg{}_{}'.format(segIdx, eventName)
  1237                                                   try:
  1238                                                       assert len(eventSeg.filter(name=thisEventName)) == 1
  1239                                                   except Exception:
  1240                                                       traceback.print_exc()
  1241                                                   allEvIn = eventSeg.filter(name=thisEventName)[0]
  1242                                                   if isinstance(allEvIn, EventProxy):
  1243                                                       allAlignEvents = loadObjArrayAnn(allEvIn.load())
  1244                                                   elif isinstance(allEvIn, Event):
  1245                                                       allAlignEvents = allEvIn
  1246                                                   else:
  1247                                                       raise(Exception(
  1248                                                           '{} must be an Event or EventProxy!'
  1249                                                           .format(eventName)))
  1250                                                   allAlignEventsList.append(allAlignEvents)
  1251                                               allAlignEventsDF = unitSpikeTrainArrayAnnToDF(allAlignEventsList)
  1252                                               #
  1253                                               breakDownData = (
  1254                                                   allAlignEventsDF
  1255                                                   .groupby(minNReps['categories'])
  1256                                                   .agg('count')
  1257                                                   .iloc[:, 0]
  1258                                                   )
  1259                                               try:
  1260                                                   breakDownData[breakDownData > minNReps['n']].to_csv(
  1261                                                       os.path.join(
  1262                                                           folderPath, 'numRepetitionsEachCondition.csv'
  1263                                                       ), header=True
  1264                                                   )
  1265                                               except Exception:
  1266                                                   traceback.print_exc()
  1267                                               allAlignEventsDF.loc[:, 'keepMask'] = False
  1268                                               for name, group in allAlignEventsDF.groupby(minNReps['categories']):
  1269                                                   allAlignEventsDF.loc[group.index, 'keepMask'] = (
  1270                                                       breakDownData[name] > minNReps['n'])
  1271                                               for segIdx, group in allAlignEventsDF.groupby('segment'):
  1272                                                   allAlignEventsList[segIdx].array_annotations['keepMask'] = group['keepMask'].to_numpy()
  1273                                               #
  1274                                               for segIdx, eventSeg in enumerate(eventBlock.segments):
  1275                                                   if verbose:
  1276                                                       print(
  1277                                                           'getAsigsAlignedToEvents on segment {} of {}'
  1278                                                           .format(segIdx + 1, len(eventBlock.segments)))
  1279                                                   allAlignEvents = allAlignEventsList[segIdx]
  1280                                                   if chunkSize is None:
  1281                                                       alignEventGroups = [allAlignEvents]
  1282                                                   else:
  1283                                                       nChunks = max(
  1284                                                           int(np.floor(allAlignEvents.shape[0] / chunkSize)),
  1285                                                           1)
  1286                                                       alignEventGroups = []
  1287                                                       for i in range(nChunks):
  1288                                                           if not (i == (nChunks - 1)):
  1289                                                               # not last one
  1290                                                               alignEventGroups.append(
  1291                                                                   allAlignEvents[i * chunkSize: (i + 1) * chunkSize])
  1292                                                           else:
  1293                                                               alignEventGroups.append(
  1294                                                                   allAlignEvents[i * chunkSize:])
  1295                                                   signalSeg = signalBlock.segments[segIdx]
  1296                                                   for subSegIdx, alignEvents in enumerate(alignEventGroups):
  1297                                                       # seg to contain triggered time series
  1298                                                       if verbose:
  1299                                                           print(
  1300                                                               'getAsigsAlignedToEvents on subSegment {} of {}'
  1301                                                               .format(subSegIdx + 1, len(alignEventGroups)))
  1302                                                       if not alignEvents.shape[0] > 0:
  1303                                                           continue
  1304                                                       newSeg = Segment(name='seg{}_'.format(int(totalNSegs)))
  1305                                                       newSeg.annotate(nix_name=newSeg.name)
  1306                                                       masterBlock.segments.append(newSeg)
  1307                                                       for chanName in chansToTrigger:
  1308                                                           asigName = 'seg{}_{}'.format(segIdx, chanName)
  1309                                                           if verbose:
  1310                                                               print(
  1311                                                                   'getAsigsAlignedToEvents on channel {}'
  1312                                                                   .format(chanName))
  1313                                                           assert len(signalSeg.filter(name=asigName)) == 1
  1314                                                           asig = signalSeg.filter(name=asigName)[0]
  1315                                                           nominalWinLen = int(
  1316                                                               (windowSize[1] - windowSize[0]) *
  1317                                                               asig.sampling_rate - 1)
  1318                                                           validMask = (
  1319                                                               ((
  1320                                                                   alignEvents + windowSize[1] +
  1321                                                                   asig.sampling_rate ** (-1)) < asig.t_stop) &
  1322                                                               ((
  1323                                                                   alignEvents + windowSize[0] -
  1324                                                                   asig.sampling_rate ** (-1)) > asig.t_start)
  1325                                                               )
  1326                                                           thisKeepMask = alignEvents.array_annotations['keepMask']
  1327                                                           fullMask = (validMask & thisKeepMask)
  1328                                                           alignEvents = alignEvents[fullMask]
  1329                                                           # array_annotations get sliced with the event, but regular anns do not
  1330                                                           for annName in alignEvents.annotations['arrayAnnNames']:
  1331                                                               alignEvents.annotations[annName] = (
  1332                                                                   alignEvents.array_annotations[annName])
  1333                                                           if isinstance(asig, AnalogSignalProxy):
  1334                                                               if checkReferences:
  1335                                                                   da = (
  1336                                                                       asig
  1337                                                                       ._rawio
  1338                                                                       .da_list['blocks'][0]['segments'][segIdx]['data'])
  1339                                                                   print('segIdx {}, asig.name {}'.format(
  1340                                                                       segIdx, asig.name))
  1341                                                                   print('asig._global_channel_indexes = {}'.format(
  1342                                                                       asig._global_channel_indexes))
  1343                                                                   print('asig references {}'.format(
  1344                                                                       da[asig._global_channel_indexes[0]]))
  1345                                                                   try:
  1346                                                                       assert (
  1347                                                                           asig.name
  1348                                                                           in da[asig._global_channel_indexes[0]].name)
  1349                                                                   except Exception:
  1350                                                                       traceback.print_exc()
  1351                                                               rawWaveforms = [
  1352                                                                   asig.load(
  1353                                                                       time_slice=(t + windowSize[0], t + windowSize[1]))
  1354                                                                   for t in alignEvents]
  1355                                                               if any([rW.shape[0] < nominalWinLen for rW in rawWaveforms]):
  1356                                                                   rawWaveforms = [
  1357                                                                       asig.load(
  1358                                                                           time_slice=(t + windowSize[0], t + windowSize[1] + asig.sampling_period))
  1359                                                                       for t in alignEvents]
  1360                                                           elif isinstance(asig, AnalogSignal):
  1361                                                               rawWaveforms = []
  1362                                                               for t in alignEvents:
  1363                                                                   asigMask = (asig.times > t + windowSize[0]) & (asig.times < t + windowSize[1])
  1364                                                                   rawWaveforms.append(asig[asigMask[:, np.newaxis]])
  1365                                                           else:
  1366                                                               raise(Exception('{} must be an AnalogSignal or AnalogSignalProxy!'.format(asigName)))
  1367                                                           #
  1368                                                           samplingRate = asig.sampling_rate
  1369                                                           waveformUnits = rawWaveforms[0].units
  1370                                                           #  fix length if roundoff error
  1371                                                           #  minLen = min([rW.shape[0] for rW in rawWaveforms])
  1372                                                           rawWaveforms = [rW[:nominalWinLen] for rW in rawWaveforms]
  1373                                                           #
  1374                                                           spikeWaveforms = (
  1375                                                               np.hstack([rW.magnitude for rW in rawWaveforms])
  1376                                                               .transpose()[:, np.newaxis, :] * waveformUnits
  1377                                                               )
  1378                                                           #
  1379                                                           thisUnit = masterBlock.filter(
  1380                                                               objects=Unit, name=chanName + '#0')[0]
  1381                                                           skipEventAnnNames = (
  1382                                                               ['nix_name', 'neo_name']
  1383                                                               )
  1384                                                           stAnn = {
  1385                                                               k: v
  1386                                                               for k, v in alignEvents.annotations.items()
  1387                                                               if k not in skipEventAnnNames
  1388                                                               }
  1389                                                           skipAsigAnnNames = (
  1390                                                               ['channel_id', 'nix_name', 'neo_name']
  1391                                                               )
  1392                                                           stAnn.update({
  1393                                                               k: v
  1394                                                               for k, v in asig.annotations.items()
  1395                                                               if k not in skipAsigAnnNames
  1396                                                           })
  1397                                                           st = SpikeTrain(
  1398                                                               name='seg{}_{}'.format(int(totalNSegs), thisUnit.name),
  1399                                                               times=alignEvents.times,
  1400                                                               waveforms=spikeWaveforms,
  1401                                                               t_start=asig.t_start, t_stop=asig.t_stop,
  1402                                                               left_sweep=windowSize[0] * (-1),
  1403                                                               sampling_rate=samplingRate,
  1404                                                               **stAnn
  1405                                                               )
  1406                                                           st.annotate(nix_name=st.name)
  1407                                                           st.annotations['unitAnnotations'] = json.dumps(
  1408                                                               thisUnit.annotations.copy())
  1409                                                           thisUnit.spiketrains.append(st)
  1410                                                           newSeg.spiketrains.append(st)
  1411                                                           st.unit = thisUnit
  1412                                                       totalNSegs += 1
  1413                                               try:
  1414                                                   eventBlock.filter(
  1415                                                       objects=EventProxy)[0]._rawio.file.close()
  1416                                               except Exception:
  1417                                                   traceback.print_exc()
  1418                                               if signalBlock is not eventBlock:
  1419                                                   try:
  1420                                                       signalBlock.filter(
  1421                                                           objects=AnalogSignalProxy)[0]._rawio.file.close()
  1422                                                   except Exception:
  1423                                                       traceback.print_exc()
  1424                                               triggeredPath = os.path.join(
  1425                                                   folderPath, fileName + '.nix')
  1426                                               if not os.path.exists(triggeredPath):
  1427                                                   appendToExisting = False
  1428                                           
  1429                                               if appendToExisting:
  1430                                                   allSegs = list(range(len(masterBlock.segments)))
  1431                                                   addBlockToNIX(
  1432                                                       masterBlock, neoSegIdx=allSegs,
  1433                                                       writeSpikes=True,
  1434                                                       fileName=fileName,
  1435                                                       folderPath=folderPath,
  1436                                                       purgeNixNames=False,
  1437                                                       nixBlockIdx=0, nixSegIdx=allSegs)
  1438                                               else:
  1439                                                   if os.path.exists(triggeredPath):
  1440                                                       os.remove(triggeredPath)
  1441                                                   masterBlock = purgeNixAnn(masterBlock)
  1442                                                   writer = NixIO(filename=triggeredPath)
  1443                                                   writer.write_block(masterBlock, use_obj_names=True)
  1444                                                   writer.close()
  1445                                               return masterBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: alignedAsigDFtoSpikeTrain at line 1447

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1447                                           @profile
  1448                                           def alignedAsigDFtoSpikeTrain(
  1449                                                   allWaveforms, dataBlock=None, matchSamplingRate=True):
  1450                                               masterBlock = Block()
  1451                                               masterBlock.name = dataBlock.annotations['neo_name']
  1452                                               masterBlock.annotate(nix_name=dataBlock.annotations['neo_name'])
  1453                                               for segIdx, group in allWaveforms.groupby('segment'):
  1454                                                   print('Saving trajectoriess for segment {}'.format(segIdx))
  1455                                                   dataSeg = dataBlock.segments[segIdx]
  1456                                                   exSt = dataSeg.spiketrains[0]
  1457                                                   if isinstance(exSt, SpikeTrainProxy):
  1458                                                       print(
  1459                                                           'alignedAsigDFtoSpikeTrain basing seg {} on {}'
  1460                                                           .format(segIdx, exSt.name))
  1461                                                       stProxy = exSt
  1462                                                       exSt = loadStProxy(stProxy)
  1463                                                       exSt = loadObjArrayAnn(exSt)
  1464                                                   print('exSt.left_sweep is {}'.format(exSt.left_sweep))
  1465                                                   wfBins = ((np.arange(exSt.waveforms.shape[2]) / (exSt.sampling_rate)) - exSt.left_sweep).magnitude
  1466                                                   # seg to contain triggered time series
  1467                                                   newSeg = Segment(name=dataSeg.annotations['neo_name'])
  1468                                                   newSeg.annotate(nix_name=dataSeg.annotations['neo_name'])
  1469                                                   masterBlock.segments.append(newSeg)
  1470                                                   #
  1471                                                   if group.columns.name == 'bin':
  1472                                                       grouper = group.groupby('feature')
  1473                                                       colsAre = 'bin'
  1474                                                   elif group.columns.name == 'feature':
  1475                                                       grouper = group.iteritems()
  1476                                                       colsAre = 'feature'
  1477                                                   for featName, featGroup in grouper:
  1478                                                       print('Saving {}...'.format(featName))
  1479                                                       if featName[-2:] == '#0':
  1480                                                           cleanFeatName = featName
  1481                                                       else:
  1482                                                           cleanFeatName = featName + '#0'
  1483                                                       if segIdx == 0:
  1484                                                           #  allocate units
  1485                                                           chanIdx = ChannelIndex(
  1486                                                               name=cleanFeatName, index=[0])
  1487                                                           chanIdx.annotate(nix_name=chanIdx.name)
  1488                                                           thisUnit = Unit(name=chanIdx.name)
  1489                                                           thisUnit.annotate(nix_name=chanIdx.name)
  1490                                                           chanIdx.units.append(thisUnit)
  1491                                                           thisUnit.channel_index = chanIdx
  1492                                                           masterBlock.channel_indexes.append(chanIdx)
  1493                                                       else:
  1494                                                           thisUnit = masterBlock.filter(
  1495                                                               objects=Unit, name=cleanFeatName)[0]
  1496                                                       if colsAre == 'bin':
  1497                                                           spikeWaveformsDF = featGroup
  1498                                                       elif colsAre == 'feature':
  1499                                                           if isinstance(featGroup, pd.Series):
  1500                                                               featGroup = featGroup.to_frame(name=featName)
  1501                                                               featGroup.columns.name = 'feature'
  1502                                                           spikeWaveformsDF = transposeSpikeDF(
  1503                                                               featGroup,
  1504                                                               'bin', fastTranspose=True)
  1505                                                       if matchSamplingRate:
  1506                                                           if len(spikeWaveformsDF.columns) != len(wfBins):
  1507                                                               wfDF = spikeWaveformsDF.reset_index(drop=True).T
  1508                                                               wfDF = hf.interpolateDF(wfDF, wfBins)
  1509                                                               spikeWaveformsDF = wfDF.T.set_index(spikeWaveformsDF.index)
  1510                                                       spikeWaveforms = spikeWaveformsDF.to_numpy()[:, np.newaxis, :]
  1511                                                       arrAnnDF = spikeWaveformsDF.index.to_frame()
  1512                                                       spikeTimes = arrAnnDF['t']
  1513                                                       arrAnnDF.drop(columns='t', inplace=True)
  1514                                                       arrAnn = {}
  1515                                                       colsToKeep = arrAnnDF.columns.drop(['originalIndex', 'feature', 'segment', 'lag'])
  1516                                                       for cName in colsToKeep:
  1517                                                           values = arrAnnDF[cName].to_numpy()
  1518                                                           if isinstance(values[0], str):
  1519                                                               values = values.astype('U')
  1520                                                           arrAnn.update({str(cName): values.flatten()})
  1521                                                       arrayAnnNames = {
  1522                                                           'arrayAnnNames': list(arrAnn.keys())}
  1523                                                       st = SpikeTrain(
  1524                                                           name='seg{}_{}'.format(int(segIdx), thisUnit.name),
  1525                                                           times=spikeTimes.to_numpy() * exSt.units,
  1526                                                           waveforms=spikeWaveforms * pq.dimensionless,
  1527                                                           t_start=exSt.t_start, t_stop=exSt.t_stop,
  1528                                                           left_sweep=exSt.left_sweep,
  1529                                                           sampling_rate=exSt.sampling_rate,
  1530                                                           **arrAnn, **arrayAnnNames
  1531                                                           )
  1532                                                       st.annotate(nix_name=st.name)
  1533                                                       thisUnit.spiketrains.append(st)
  1534                                                       newSeg.spiketrains.append(st)
  1535                                                       st.unit = thisUnit
  1536                                               return masterBlock

Total time: 0.686681 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: dataFrameToAnalogSignals at line 1538

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1538                                           @profile
  1539                                           def dataFrameToAnalogSignals(
  1540                                                   df,
  1541                                                   block=None, seg=None,
  1542                                                   idxT='NSPTime',
  1543                                                   probeName='insTD', samplingRate=500*pq.Hz,
  1544                                                   timeUnits=pq.s, measureUnits=pq.mV,
  1545                                                   dataCol=['channel_0', 'channel_1'],
  1546                                                   useColNames=False, forceColNames=None,
  1547                                                   namePrefix='', nameSuffix='', verbose=False):
  1548         1         18.0     18.0      0.0      if block is None:
  1549         1         14.0     14.0      0.0          assert seg is None
  1550         1        925.0    925.0      0.0          block = Block(name=probeName)
  1551         1        699.0    699.0      0.0          seg = Segment(name='seg0_' + probeName)
  1552         1         22.0     22.0      0.0          block.segments.append(seg)
  1553         1         12.0     12.0      0.0      if verbose:
  1554         1        814.0    814.0      0.0          print('in dataFrameToAnalogSignals...')
  1555        57       2586.0     45.4      0.0      for idx, colName in enumerate(dataCol):
  1556        56        458.0      8.2      0.0          if verbose:
  1557        56      37417.0    668.2      0.5              print('    {}'.format(colName))
  1558        56        573.0     10.2      0.0          if forceColNames is not None:
  1559                                                       chanName = forceColNames[idx]
  1560        56        401.0      7.2      0.0          elif useColNames:
  1561        56        554.0      9.9      0.0              chanName = namePrefix + colName + nameSuffix
  1562                                                   else:
  1563                                                       chanName = namePrefix + (probeName.lower() + '{}'.format(idx)) + nameSuffix
  1564                                                   #
  1565        56        527.0      9.4      0.0          chanIdx = ChannelIndex(
  1566        56        407.0      7.3      0.0              name=chanName,
  1567                                                       # index=None,
  1568        56      39785.0    710.4      0.6              index=np.asarray([idx]),
  1569                                                       # channel_names=np.asarray([chanName])
  1570                                                       )
  1571        56       1090.0     19.5      0.0          block.channel_indexes.append(chanIdx)
  1572        56        518.0      9.2      0.0          asig = AnalogSignal(
  1573        56    3165444.0  56525.8     46.1              df[colName].to_numpy() * measureUnits,
  1574        56       2008.0     35.9      0.0              name='seg0_' + chanName,
  1575        56        576.0     10.3      0.0              sampling_rate=samplingRate,
  1576        56    3453031.0  61661.3     50.3              dtype=np.float32,
  1577                                                       # **ann
  1578                                                       )
  1579        56       1843.0     32.9      0.0          if idxT is not None:
  1580        56      90388.0   1614.1      1.3              asig.t_start = df[idxT].iloc[0] * timeUnits
  1581                                                   else:
  1582                                                       asig.t_start = df.index[0] * timeUnits
  1583        56        733.0     13.1      0.0          asig.channel_index = chanIdx
  1584                                                   # assign ownership to containers
  1585        56       1165.0     20.8      0.0          chanIdx.analogsignals.append(asig)
  1586        56        964.0     17.2      0.0          seg.analogsignals.append(asig)
  1587        56      32854.0    586.7      0.5          chanIdx.create_relationship()
  1588                                               # assign parent to children
  1589         1      29680.0  29680.0      0.4      block.create_relationship()
  1590         1       1291.0   1291.0      0.0      seg.create_relationship()
  1591         1         12.0     12.0      0.0      return block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: eventDataFrameToEvents at line 1593

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1593                                           @profile
  1594                                           def eventDataFrameToEvents(
  1595                                                   eventDF, idxT=None,
  1596                                                   annCol=None,
  1597                                                   eventName='', tUnits=pq.s,
  1598                                                   makeList=True
  1599                                                   ):
  1600                                               if makeList:
  1601                                                   eventList = []
  1602                                                   for colName in annCol:
  1603                                                       originalDType = type(eventDF[colName].to_numpy()[0]).__name__
  1604                                                       event = Event(
  1605                                                           name=eventName + colName,
  1606                                                           times=eventDF[idxT].to_numpy() * tUnits,
  1607                                                           labels=eventDF[colName].astype(originalDType).to_numpy()
  1608                                                           )
  1609                                                       event.annotate(originalDType=originalDType)
  1610                                                       eventList.append(event)
  1611                                                   return eventList
  1612                                               else:
  1613                                                   if annCol is None:
  1614                                                       annCol = eventDF.drop(columns=idxT).columns
  1615                                                   event = Event(
  1616                                                       name=eventName,
  1617                                                       times=eventDF[idxT].to_numpy() * tUnits,
  1618                                                       labels=np.asarray(eventDF.index)
  1619                                                       )
  1620                                                   event.annotations.update(
  1621                                                       {
  1622                                                           'arrayAnnNames': [],
  1623                                                           'arrayAnnDTypes': []
  1624                                                           })
  1625                                                   for colName in annCol:
  1626                                                       originalDType = type(eventDF[colName].to_numpy()[0]).__name__
  1627                                                       arrayAnn = eventDF[colName].astype(originalDType).to_numpy()
  1628                                                       event.array_annotations.update(
  1629                                                           {colName: arrayAnn})
  1630                                                       event.annotations['arrayAnnNames'].append(colName)
  1631                                                       event.annotations['arrayAnnDTypes'].append(originalDType)
  1632                                                       event.annotations.update(
  1633                                                           {colName: arrayAnn})
  1634                                                   return event

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: eventsToDataFrame at line 1636

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1636                                           @profile
  1637                                           def eventsToDataFrame(
  1638                                                   events, idxT='t', names=None
  1639                                                   ):
  1640                                               eventDict = {}
  1641                                               calculatedT = False
  1642                                               for event in events:
  1643                                                   if names is not None:
  1644                                                       if event.name not in names:
  1645                                                           continue
  1646                                                   if len(event.times):
  1647                                                       if not calculatedT:
  1648                                                           t = pd.Series(event.times.magnitude)
  1649                                                           calculatedT = True
  1650                                                       try:
  1651                                                           values = event.array_annotations['labels']
  1652                                                       except Exception:
  1653                                                           values = event.labels
  1654                                                       if isinstance(values[0], bytes):
  1655                                                           #  event came from hdf, need to recover dtype
  1656                                                           if 'originalDType' in event.annotations:
  1657                                                               dtypeStr = event.annotations['originalDType'].split(';')[-1]
  1658                                                               if 'np.' not in dtypeStr:
  1659                                                                   dtypeStr = 'np.' + dtypeStr
  1660                                                               originalDType = eval(dtypeStr)
  1661                                                               values = np.asarray(values, dtype=originalDType)
  1662                                                           else:
  1663                                                               values = np.asarray(values, dtype=np.str)
  1664                                                       #  print(values.dtype)
  1665                                                       eventDict.update({
  1666                                                           event.name: pd.Series(values)})
  1667                                               eventDict.update({idxT: t})
  1668                                               return pd.concat(eventDict, axis=1)

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadSpikeMats at line 1670

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1670                                           @profile
  1671                                           def loadSpikeMats(
  1672                                                   dataPath, rasterOpts,
  1673                                                   alignTimes=None, chans=None, loadAll=False,
  1674                                                   absoluteBins=False, transposeSpikeMat=False,
  1675                                                   checkReferences=False,
  1676                                                   aggregateFun=None):
  1677                                           
  1678                                               reader = nixio_fr.NixIO(filename=dataPath)
  1679                                               chanNames = reader.header['signal_channels']['name']
  1680                                               
  1681                                               if chans is not None:
  1682                                                   sigMask = np.isin(chanNames, chans)
  1683                                                   chanNames = chanNames[sigMask]
  1684                                                   
  1685                                               chanIdx = reader.channel_name_to_index(chanNames)
  1686                                               
  1687                                               if not loadAll:
  1688                                                   assert alignTimes is not None
  1689                                                   spikeMats = {i: None for i in alignTimes.index}
  1690                                                   validTrials = pd.Series(True, index=alignTimes.index)
  1691                                               else:
  1692                                                   spikeMats = {
  1693                                                       i: None for i in range(reader.segment_count(block_index=0))}
  1694                                                   validTrials = None
  1695                                               
  1696                                               for segIdx in range(reader.segment_count(block_index=0)):
  1697                                                   if checkReferences:
  1698                                                       for i, cIdx in enumerate(chanIdx):
  1699                                                           da = reader.da_list['blocks'][0]['segments'][segIdx]['data'][cIdx]
  1700                                                           print('name {}, da.name {}'.format(chanNames[i], da.name))
  1701                                                           try:
  1702                                                               assert chanNames[i] in da.name, 'reference problem!!'
  1703                                                           except Exception:
  1704                                                               traceback.print_exc()
  1705                                                   tStart = reader.get_signal_t_start(
  1706                                                       block_index=0, seg_index=segIdx)
  1707                                                   fs = reader.get_signal_sampling_rate(
  1708                                                       channel_indexes=chanIdx
  1709                                                       )
  1710                                                   sigSize = reader.get_signal_size(
  1711                                                       block_index=0, seg_index=segIdx
  1712                                                       )
  1713                                                   tStop = sigSize / fs + tStart
  1714                                                   #  convert to indices early to avoid floating point problems
  1715                                                   
  1716                                                   intervalIdx = int(round(rasterOpts['binInterval'] * fs))
  1717                                                   #  halfIntervalIdx = int(round(intervalIdx / 2))
  1718                                                   
  1719                                                   widthIdx = int(round(rasterOpts['binWidth'] * fs))
  1720                                                   halfWidthIdx = int(round(widthIdx / 2))
  1721                                                   
  1722                                                   if rasterOpts['smoothKernelWidth'] is not None:
  1723                                                       kernWidthIdx = int(round(rasterOpts['smoothKernelWidth'] * fs))
  1724                                                   
  1725                                                   theBins = None
  1726                                           
  1727                                                   if not loadAll:
  1728                                                       winStartIdx = int(round(rasterOpts['windowSize'][0] * fs))
  1729                                                       winStopIdx = int(round(rasterOpts['windowSize'][1] * fs))
  1730                                                       timeMask = (alignTimes > tStart) & (alignTimes < tStop)
  1731                                                       maskedTimes = alignTimes[timeMask]
  1732                                                   else:
  1733                                                       #  irrelevant, will load all
  1734                                                       maskedTimes = pd.Series(np.nan)
  1735                                           
  1736                                                   for idx, tOnset in maskedTimes.iteritems():
  1737                                                       if not loadAll:
  1738                                                           idxOnset = int(round((tOnset - tStart) * fs))
  1739                                                           #  can't not be ints
  1740                                                           iStart = idxOnset + winStartIdx - int(3 * halfWidthIdx)
  1741                                                           iStop = idxOnset + winStopIdx + int(3 * halfWidthIdx)
  1742                                                       else:
  1743                                                           winStartIdx = 0
  1744                                                           iStart = 0
  1745                                                           iStop = sigSize
  1746                                           
  1747                                                       if iStart < 0:
  1748                                                           #  near the first edge
  1749                                                           validTrials[idx] = False
  1750                                                       elif (sigSize < iStop):
  1751                                                           #  near the ending edge
  1752                                                           validTrials[idx] = False
  1753                                                       else:
  1754                                                           #  valid slices
  1755                                                           try:
  1756                                                               rawSpikeMat = pd.DataFrame(
  1757                                                                   reader.get_analogsignal_chunk(
  1758                                                                       block_index=0, seg_index=segIdx,
  1759                                                                       i_start=iStart, i_stop=iStop,
  1760                                                                       channel_names=chanNames))
  1761                                                           except Exception:
  1762                                                               traceback.print_exc()
  1763                                                               #
  1764                                                           if aggregateFun is None:
  1765                                                               procSpikeMat = rawSpikeMat.rolling(
  1766                                                                   window=3 * widthIdx, center=True,
  1767                                                                   win_type='gaussian'
  1768                                                                   ).mean(std=halfWidthIdx)
  1769                                                           else:
  1770                                                               procSpikeMat = rawSpikeMat.rolling(
  1771                                                                   window=widthIdx, center=True
  1772                                                                   ).apply(
  1773                                                                       aggregateFun,
  1774                                                                       raw=True,
  1775                                                                       kwargs={'fs': fs, 'nSamp': widthIdx})
  1776                                                           #
  1777                                                           if rasterOpts['smoothKernelWidth'] is not None:
  1778                                                               procSpikeMat = (
  1779                                                                   procSpikeMat
  1780                                                                   .rolling(
  1781                                                                       window=3 * kernWidthIdx, center=True,
  1782                                                                       win_type='gaussian')
  1783                                                                   .mean(std=kernWidthIdx/2)
  1784                                                                   .dropna().iloc[::intervalIdx, :]
  1785                                                               )
  1786                                                           else:
  1787                                                               procSpikeMat = (
  1788                                                                   procSpikeMat
  1789                                                                   .dropna().iloc[::intervalIdx, :]
  1790                                                               )
  1791                                           
  1792                                                           procSpikeMat.columns = chanNames
  1793                                                           procSpikeMat.columns.name = 'unit'
  1794                                                           if theBins is None:
  1795                                                               theBins = np.asarray(
  1796                                                                   procSpikeMat.index + winStartIdx) / fs
  1797                                                           if absoluteBins:
  1798                                                               procSpikeMat.index = theBins + idxOnset / fs
  1799                                                           else:
  1800                                                               procSpikeMat.index = theBins
  1801                                                           procSpikeMat.index.name = 'bin'
  1802                                                           if loadAll:
  1803                                                               smIdx = segIdx
  1804                                                           else:
  1805                                                               smIdx = idx
  1806                                                               
  1807                                                           spikeMats[smIdx] = procSpikeMat
  1808                                                           if transposeSpikeMat:
  1809                                                               spikeMats[smIdx] = spikeMats[smIdx].transpose()
  1810                                                       #  plt.imshow(rawSpikeMat.to_numpy(), aspect='equal'); plt.show()
  1811                                               return spikeMats, validTrials

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: synchronizeINStoNSP at line 1813

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1813                                           @profile
  1814                                           def synchronizeINStoNSP(
  1815                                                   tapTimestampsNSP=None, tapTimestampsINS=None,
  1816                                                   precalculatedFun=None,
  1817                                                   NSPTimeRanges=(None, None),
  1818                                                   td=None, accel=None, insBlock=None, trialSegment=None, degree=1,
  1819                                                   trimSpiketrains=False
  1820                                                   ):
  1821                                               print('Trial Segment {}'.format(trialSegment))
  1822                                               if precalculatedFun is None:
  1823                                                   assert ((tapTimestampsNSP is not None) & (tapTimestampsINS is not None))
  1824                                                   # sanity check that the intervals match
  1825                                                   insDiff = tapTimestampsINS.diff().dropna().values
  1826                                                   nspDiff = tapTimestampsNSP.diff().dropna().values
  1827                                                   print('On the INS, the diff() between taps was\n{}'.format(insDiff))
  1828                                                   print('On the NSP, the diff() between taps was\n{}'.format(nspDiff))
  1829                                                   print('This amounts to a msec difference of\n{}'.format(
  1830                                                       (insDiff - nspDiff) * 1e3))
  1831                                                   if (insDiff - nspDiff > 20e-3).any():
  1832                                                       raise(Exception('Tap trains too different!'))
  1833                                                   #
  1834                                                   if degree > 0:
  1835                                                       synchPolyCoeffsINStoNSP = np.polyfit(
  1836                                                           x=tapTimestampsINS.values, y=tapTimestampsNSP.values,
  1837                                                           deg=degree)
  1838                                                   else:
  1839                                                       timeOffset = tapTimestampsNSP.values - tapTimestampsINS.values
  1840                                                       synchPolyCoeffsINStoNSP = np.array([1, np.mean(timeOffset)])
  1841                                                   timeInterpFunINStoNSP = np.poly1d(synchPolyCoeffsINStoNSP)
  1842                                               else:
  1843                                                   timeInterpFunINStoNSP = precalculatedFun
  1844                                               if td is not None:
  1845                                                   td.loc[:, 'NSPTime'] = pd.Series(
  1846                                                       timeInterpFunINStoNSP(td['t']), index=td['t'].index)
  1847                                                   td.loc[:, 'NSPTime'] = timeInterpFunINStoNSP(td['t'].to_numpy())
  1848                                               if accel is not None:
  1849                                                   accel.loc[:, 'NSPTime'] = pd.Series(
  1850                                                       timeInterpFunINStoNSP(accel['t']), index=accel['t'].index)
  1851                                               if insBlock is not None:
  1852                                                   # allUnits = [st.unit for st in insBlock.segments[0].spiketrains]
  1853                                                   # [un.name for un in insBlock.filter(objects=Unit)]
  1854                                                   for unit in insBlock.filter(objects=Unit):
  1855                                                       tStart = NSPTimeRanges[0]
  1856                                                       tStop = NSPTimeRanges[1]
  1857                                                       uniqueSt = []
  1858                                                       for st in unit.spiketrains:
  1859                                                           if st not in uniqueSt:
  1860                                                               uniqueSt.append(st)
  1861                                                           else:
  1862                                                               continue
  1863                                                           print('Synchronizing {}'.format(st.name))
  1864                                                           if len(st.times):
  1865                                                               segMaskSt = np.array(
  1866                                                                   st.array_annotations['trialSegment'],
  1867                                                                   dtype=np.int) == trialSegment
  1868                                                               st.magnitude[segMaskSt] = (
  1869                                                                   timeInterpFunINStoNSP(st.times[segMaskSt].magnitude))
  1870                                                               if trimSpiketrains:
  1871                                                                   print('Trimming spiketrain')
  1872                                                                   #  kludgey fix for weirdness concerning t_start
  1873                                                                   st.t_start = min(tStart, st.times[0] * 0.999)
  1874                                                                   st.t_stop = min(tStop, st.times[-1] * 1.001)
  1875                                                                   validMask = st < st.t_stop
  1876                                                                   if ~validMask.all():
  1877                                                                       print('Deleted some spikes')
  1878                                                                       st = st[validMask]
  1879                                                                       # delete invalid spikes
  1880                                                                       if 'arrayAnnNames' in st.annotations.keys():
  1881                                                                           for key in st.annotations['arrayAnnNames']:
  1882                                                                               try:
  1883                                                                                   # st.annotations[key] = np.array(st.array_annotations[key])
  1884                                                                                   st.annotations[key] = np.delete(st.annotations[key], ~validMask)
  1885                                                                               except Exception:
  1886                                                                                   traceback.print_exc()
  1887                                                                                   pdb.set_trace()
  1888                                                           else:
  1889                                                               if trimSpiketrains:
  1890                                                                   st.t_start = tStart
  1891                                                                   st.t_stop = tStop
  1892                                                   #
  1893                                                   allEvents = [
  1894                                                       ev
  1895                                                       for ev in insBlock.filter(objects=Event)
  1896                                                       if ('ins' in ev.name) and ('concatenate' not in ev.name)]
  1897                                                   concatEvents = [
  1898                                                       ev
  1899                                                       for ev in insBlock.filter(objects=Event)
  1900                                                       if ('ins' in ev.name) and ('concatenate' in ev.name)]
  1901                                                   eventsDF = eventsToDataFrame(allEvents, idxT='t')
  1902                                                   newNames = {i: childBaseName(i, 'seg') for i in eventsDF.columns}
  1903                                                   eventsDF.rename(columns=newNames, inplace=True)
  1904                                                   segMask = hf.getStimSerialTrialSegMask(eventsDF, trialSegment)
  1905                                                   evTStart = eventsDF.loc[segMask, 't'].min() * pq.s
  1906                                                   evTStop = eventsDF.loc[segMask, 't'].max() * pq.s
  1907                                                   # print('allEvents[0].shape = {}'.format(allEvents[0].shape))
  1908                                                   # print('allEvents[0].magnitude[segMask][0] = {}'.format(allEvents[0].magnitude[segMask][0]))
  1909                                                   for event in (allEvents + concatEvents):
  1910                                                       if trimSpiketrains:
  1911                                                           thisSegMask = (event.times >= evTStart) & (event.times <= evTStop)
  1912                                                       else:
  1913                                                           thisSegMask = (event.times >= evTStart) & (event.times < evTStop)
  1914                                                       event.magnitude[thisSegMask] = (
  1915                                                           timeInterpFunINStoNSP(event.times[thisSegMask].magnitude))
  1916                                                   # print('allEvents[0].magnitude[segMask][0] = {}'.format(allEvents[0].magnitude[segMask][0]))
  1917                                                   # if len(concatEvents) > trialSegment:
  1918                                                   #     concatEvents[trialSegment].magnitude[:] = timeInterpFunINStoNSP(
  1919                                                   #         concatEvents[trialSegment].times[:].magnitude)
  1920                                               return td, accel, insBlock, timeInterpFunINStoNSP

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: findSegsIncluding at line 1922

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1922                                           @profile
  1923                                           def findSegsIncluding(
  1924                                                   block, timeSlice=None):
  1925                                               segBoundsList = []
  1926                                               for segIdx, seg in enumerate(block.segments):
  1927                                                   segBoundsList.append(pd.DataFrame({
  1928                                                       't_start': seg.t_start,
  1929                                                       't_stop': seg.t_stop
  1930                                                       }, index=[segIdx]))
  1931                                           
  1932                                               segBounds = pd.concat(segBoundsList)
  1933                                               if timeSlice[0] is not None:
  1934                                                   segMask = (segBounds['t_start'] * pq.s >= timeSlice[0]) & (
  1935                                                       segBounds['t_stop'] * pq.s <= timeSlice[1])
  1936                                                   requestedSegs = segBounds.loc[segMask, :]
  1937                                               else:
  1938                                                   timeSlice = (None, None)
  1939                                                   requestedSegs = segBounds
  1940                                               return segBounds, requestedSegs

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: findSegsIncluded at line 1942

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1942                                           @profile
  1943                                           def findSegsIncluded(
  1944                                                   block, timeSlice=None):
  1945                                               segBoundsList = []
  1946                                               for segIdx, seg in enumerate(block.segments):
  1947                                                   segBoundsList.append(pd.DataFrame({
  1948                                                       't_start': seg.t_start,
  1949                                                       't_stop': seg.t_stop
  1950                                                       }, index=[segIdx]))
  1951                                           
  1952                                               segBounds = pd.concat(segBoundsList)
  1953                                               if timeSlice[0] is not None:
  1954                                                   segMask = (segBounds['t_start'] * pq.s <= timeSlice[0]) | (
  1955                                                       segBounds['t_stop'] * pq.s >= timeSlice[1])
  1956                                                   requestedSegs = segBounds.loc[segMask, :]
  1957                                               else:
  1958                                                   timeSlice = (None, None)
  1959                                                   requestedSegs = segBounds
  1960                                               return segBounds, requestedSegs

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getElecLookupTable at line 1962

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1962                                           @profile
  1963                                           def getElecLookupTable(
  1964                                                   block, elecIds=None):
  1965                                               lookupTableList = []
  1966                                               for metaIdx, chanIdx in enumerate(block.channel_indexes):
  1967                                                   if chanIdx.analogsignals:
  1968                                                       #  print(chanIdx.name)
  1969                                                       lookupTableList.append(pd.DataFrame({
  1970                                                           'channelNames': np.asarray(chanIdx.channel_names, dtype=np.str),
  1971                                                           'index': chanIdx.index,
  1972                                                           'metaIndex': metaIdx * chanIdx.index**0,
  1973                                                           'localIndex': (
  1974                                                               list(range(chanIdx.analogsignals[0].shape[1])))
  1975                                                           }))
  1976                                               lookupTable = pd.concat(lookupTableList, ignore_index=True)
  1977                                           
  1978                                               if elecIds is None:
  1979                                                   requestedIndices = lookupTable
  1980                                               else:
  1981                                                   if isinstance(elecIds[0], str):
  1982                                                       idxMask = lookupTable['channelNames'].isin(elecIds)
  1983                                                       requestedIndices = lookupTable.loc[idxMask, :]
  1984                                               return lookupTable, requestedIndices

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getNIXData at line 1986

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1986                                           @profile
  1987                                           def getNIXData(
  1988                                                   fileName=None,
  1989                                                   folderPath=None,
  1990                                                   reader=None, blockIdx=0,
  1991                                                   elecIds=None, startTime_s=None,
  1992                                                   dataLength_s=None, downsample=1,
  1993                                                   signal_group_mode='group-by-same-units',
  1994                                                   closeReader=False):
  1995                                               #  Open file and extract headers
  1996                                               if reader is None:
  1997                                                   assert (fileName is not None) and (folderPath is not None)
  1998                                                   filePath = os.path.join(folderPath, fileName) + '.nix'
  1999                                                   reader = nixio_fr.NixIO(filename=filePath)
  2000                                           
  2001                                               block = reader.read_block(
  2002                                                   block_index=blockIdx, lazy=True,
  2003                                                   signal_group_mode=signal_group_mode)
  2004                                           
  2005                                               for segIdx, seg in enumerate(block.segments):
  2006                                                   seg.events = [i.load() for i in seg.events]
  2007                                                   seg.epochs = [i.load() for i in seg.epochs]
  2008                                           
  2009                                               # find elecIds
  2010                                               lookupTable, requestedIndices = getElecLookupTable(
  2011                                                   block, elecIds=elecIds)
  2012                                           
  2013                                               # find segments that contain the requested times
  2014                                               if dataLength_s is not None:
  2015                                                   assert startTime_s is not None
  2016                                                   timeSlice = (
  2017                                                       startTime_s * pq.s,
  2018                                                       (startTime_s + dataLength_s) * pq.s)
  2019                                               else:
  2020                                                   timeSlice = (None, None)
  2021                                               segBounds, requestedSegs = findSegsIncluding(block, timeSlice)
  2022                                               #
  2023                                               data = pd.DataFrame(columns=elecIds + ['t'])
  2024                                               for segIdx in requestedSegs.index:
  2025                                                   seg = block.segments[segIdx]
  2026                                                   if dataLength_s is not None:
  2027                                                       timeSlice = (
  2028                                                           max(timeSlice[0], seg.t_start),
  2029                                                           min(timeSlice[1], seg.t_stop)
  2030                                                           )
  2031                                                   else:
  2032                                                       timeSlice = (seg.t_start, seg.t_stop)
  2033                                                   segData = pd.DataFrame()
  2034                                                   for metaIdx in pd.unique(requestedIndices['metaIndex']):
  2035                                                       metaIdxMatch = requestedIndices['metaIndex'] == metaIdx
  2036                                                       theseRequestedIndices = requestedIndices.loc[
  2037                                                           metaIdxMatch, :]
  2038                                                       theseElecIds = theseRequestedIndices['channelNames']
  2039                                                       asig = seg.analogsignals[metaIdx]
  2040                                                       thisTimeSlice = (
  2041                                                           max(timeSlice[0], asig.t_start),
  2042                                                           min(timeSlice[1], asig.t_stop)
  2043                                                           )
  2044                                                       reqData = asig.load(
  2045                                                           time_slice=thisTimeSlice,
  2046                                                           channel_indexes=theseRequestedIndices['localIndex'].to_numpy())
  2047                                                       segData = pd.concat((
  2048                                                               segData,
  2049                                                               pd.DataFrame(
  2050                                                                   reqData.magnitude, columns=theseElecIds.to_numpy())),
  2051                                                           axis=1)
  2052                                                   segT = reqData.times
  2053                                                   segData['t'] = segT
  2054                                                   data = pd.concat(
  2055                                                       (data, segData),
  2056                                                       axis=0, ignore_index=True)
  2057                                               channelData = {
  2058                                                   'data': data,
  2059                                                   't': data['t']
  2060                                                   }
  2061                                               if closeReader:
  2062                                                   reader.file.close()
  2063                                                   block = None
  2064                                                   # closing the reader breaks its connection to the block
  2065                                               return channelData, block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: childBaseName at line 2067

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2067                                           @profile
  2068                                           def childBaseName(
  2069                                                   childName, searchTerm):
  2070                                               if searchTerm in childName:
  2071                                                   baseName = '_'.join(childName.split('_')[1:])
  2072                                               else:
  2073                                                   baseName = childName
  2074                                               return baseName

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: readBlockFixNames at line 2076

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2076                                           @profile
  2077                                           def readBlockFixNames(
  2078                                                   rawioReader,
  2079                                                   block_index=0, signal_group_mode='split-all',
  2080                                                   lazy=True, mapDF=None, reduceChannelIndexes=False,
  2081                                                   loadList=None, purgeNixNames=False
  2082                                                   ):
  2083                                               headerSignalChan = pd.DataFrame(
  2084                                                   rawioReader.header['signal_channels']).set_index('id')
  2085                                               headerUnitChan = pd.DataFrame(
  2086                                                   rawioReader.header['unit_channels']).set_index('id')
  2087                                               dataBlock = rawioReader.read_block(
  2088                                                   block_index=block_index, lazy=lazy,
  2089                                                   signal_group_mode=signal_group_mode)
  2090                                               if dataBlock.name is None:
  2091                                                   if 'neo_name' in dataBlock.annotations:
  2092                                                       dataBlock.name = dataBlock.annotations['neo_name']
  2093                                               #  on first segment, rename the chan_indexes and units
  2094                                               seg0 = dataBlock.segments[0]
  2095                                               asigLikeList = (
  2096                                                   seg0.filter(objects=AnalogSignalProxy) +
  2097                                                   seg0.filter(objects=AnalogSignal))
  2098                                               if mapDF is not None:
  2099                                                   if headerSignalChan.size > 0:
  2100                                                       asigNameChanger = {}
  2101                                                       for nevID in mapDF['nevID']:
  2102                                                           if int(nevID) in headerSignalChan.index:
  2103                                                               labelFromMap = (
  2104                                                                   mapDF
  2105                                                                   .loc[mapDF['nevID'] == nevID, 'label']
  2106                                                                   .iloc[0])
  2107                                                               asigNameChanger[
  2108                                                                   headerSignalChan.loc[int(nevID), 'name']] = labelFromMap
  2109                                                   else:
  2110                                                       asigOrigNames = np.unique(
  2111                                                           [i.split('#')[0] for i in headerUnitChan['name']])
  2112                                                       asigNameChanger = {}
  2113                                                       for origName in asigOrigNames:
  2114                                                           # ripple specific
  2115                                                           formattedName = origName.replace('.', '_').replace(' raw', '')
  2116                                                           if mapDF['label'].str.contains(formattedName).any():
  2117                                                               asigNameChanger[origName] = formattedName
  2118                                               else:
  2119                                                   asigNameChanger = dict()
  2120                                               for asig in asigLikeList:
  2121                                                   asigBaseName = childBaseName(asig.name, 'seg')
  2122                                                   asig.name = (
  2123                                                       asigNameChanger[asigBaseName]
  2124                                                       if asigBaseName in asigNameChanger
  2125                                                       else asigBaseName)
  2126                                                   if mapDF is not None:
  2127                                                       if (mapDF['label'] == asig.name).any():
  2128                                                           asig.annotations['xCoords'] = float(mapDF.loc[mapDF['label'] == asig.name, 'xcoords'].iloc[0])
  2129                                                           asig.annotations['yCoords'] = float(mapDF.loc[mapDF['label'] == asig.name, 'ycoords'].iloc[0])
  2130                                                           asig.annotations['zCoords'] = float(mapDF.loc[mapDF['label'] == asig.name, 'zcoords'].iloc[0])
  2131                                                   if 'Channel group ' in asig.channel_index.name:
  2132                                                       newChanName = (
  2133                                                           asigNameChanger[asigBaseName]
  2134                                                           if asigBaseName in asigNameChanger
  2135                                                           else asigBaseName)
  2136                                                       asig.channel_index.name = newChanName
  2137                                                       if 'neo_name' in asig.channel_index.annotations:
  2138                                                           asig.channel_index.annotations['neo_name'] = newChanName
  2139                                                       if 'nix_name' in asig.channel_index.annotations:
  2140                                                           asig.channel_index.annotations['nix_name'] = newChanName
  2141                                                       if mapDF is not None:
  2142                                                           try:
  2143                                                               asig.channel_index.coordinates = np.asarray([
  2144                                                                   asig.annotations['xCoords'], asig.annotations['yCoords'], asig.annotations['zCoords']
  2145                                                               ])[np.newaxis, :] * pq.um
  2146                                                           except Exception:
  2147                                                               pass
  2148                                               spikeTrainLikeList = (
  2149                                                   seg0.filter(objects=SpikeTrainProxy) +
  2150                                                   seg0.filter(objects=SpikeTrain))
  2151                                               # add channels for channelIndex that has no asigs but has spikes
  2152                                               nExtraChans = 0
  2153                                               for stp in spikeTrainLikeList:
  2154                                                   stpBaseName = childBaseName(stp.name, 'seg')
  2155                                                   nameParser = re.search(r'ch(\d*)#(\d*)', stpBaseName)
  2156                                                   if nameParser is not None:
  2157                                                       # first time at this unit, rename it
  2158                                                       chanId = int(nameParser.group(1))
  2159                                                       unitId = int(nameParser.group(2))
  2160                                                       if chanId >= 5121:
  2161                                                           isRippleStimChan = True
  2162                                                           chanId = chanId - 5120
  2163                                                       else:
  2164                                                           isRippleStimChan = False
  2165                                                       ####################
  2166                                                       # asigBaseName = headerSignalChan.loc[chanId, 'name']
  2167                                                       # if mapDF is not None:
  2168                                                       #     if asigBaseName in asigNameChanger:
  2169                                                       #         chanIdLabel = (
  2170                                                       #             asigNameChanger[asigBaseName]
  2171                                                       #             if asigBaseName in asigNameChanger
  2172                                                       #             else asigBaseName)
  2173                                                       #     else:
  2174                                                       #         chanIdLabel = asigBaseName
  2175                                                       # else:
  2176                                                       #     chanIdLabel = asigBaseName
  2177                                                       ###################
  2178                                                       # if swapMaps is not None:
  2179                                                       #     nameCandidates = (swapMaps['to'].loc[swapMaps['to']['nevID'] == chanId, 'label']).to_list()
  2180                                                       # elif mapDF is not None:
  2181                                                       #     nameCandidates = (mapDF.loc[mapDF['nevID'] == chanId, 'label']).to_list()
  2182                                                       # else:
  2183                                                       #     nameCandidates = []
  2184                                                       ##############################
  2185                                                       if mapDF is not None:
  2186                                                           nameCandidates = (
  2187                                                               mapDF
  2188                                                               .loc[mapDF['nevID'] == chanId, 'label']
  2189                                                               .to_list())
  2190                                                       else:
  2191                                                           nameCandidates = []
  2192                                                       if len(nameCandidates) == 1:
  2193                                                           chanIdLabel = nameCandidates[0]
  2194                                                       elif chanId in headerSignalChan:
  2195                                                           chanIdLabel = headerSignalChan.loc[chanId, 'name']
  2196                                                       else:
  2197                                                           chanIdLabel = 'ch{}'.format(chanId)
  2198                                                       #
  2199                                                       if isRippleStimChan:
  2200                                                           stp.name = '{}_stim#{}'.format(chanIdLabel, unitId)
  2201                                                       else:
  2202                                                           stp.name = '{}#{}'.format(chanIdLabel, unitId)
  2203                                                       stp.unit.name = stp.name
  2204                                                   ########################################
  2205                                                   # sanitize ripple names ####
  2206                                                   stp.name = stp.name.replace('.', '_').replace(' raw', '')
  2207                                                   stp.unit.name = stp.unit.name.replace('.', '_').replace(' raw', '')
  2208                                                   ###########################################
  2209                                                   if 'ChannelIndex for ' in stp.unit.channel_index.name:
  2210                                                       newChanName = stp.name.replace('_stim#0', '')
  2211                                                       # remove unit #
  2212                                                       newChanName = re.sub(r'#\d', '', newChanName)
  2213                                                       stp.unit.channel_index.name = newChanName
  2214                                                       # units and analogsignals have different channel_indexes when loaded by nix
  2215                                                       # add them to each other's parent list
  2216                                                       allMatchingChIdx = dataBlock.filter(
  2217                                                           objects=ChannelIndex, name=newChanName)
  2218                                                       if (len(allMatchingChIdx) > 1) and reduceChannelIndexes:
  2219                                                           assert len(allMatchingChIdx) == 2
  2220                                                           targetChIdx = [
  2221                                                               ch
  2222                                                               for ch in allMatchingChIdx
  2223                                                               if ch is not stp.unit.channel_index][0]
  2224                                                           oldChIdx = stp.unit.channel_index
  2225                                                           targetChIdx.units.append(stp.unit)
  2226                                                           stp.unit.channel_index = targetChIdx
  2227                                                           oldChIdx.units.remove(stp.unit)
  2228                                                           if not (len(oldChIdx.units) or len(oldChIdx.analogsignals)):
  2229                                                               dataBlock.channel_indexes.remove(oldChIdx)
  2230                                                           del oldChIdx
  2231                                                           targetChIdx.create_relationship()
  2232                                                       elif reduceChannelIndexes:
  2233                                                           if newChanName not in headerSignalChan['name']:
  2234                                                               stp.unit.channel_index.index = np.asarray(
  2235                                                                   [headerSignalChan['name'].size + nExtraChans])
  2236                                                               stp.unit.channel_index.channel_ids = np.asarray(
  2237                                                                   [headerSignalChan['name'].size + nExtraChans])
  2238                                                               stp.unit.channel_index.channel_names = np.asarray(
  2239                                                                   [newChanName])
  2240                                                               nExtraChans += 1
  2241                                                           if 'neo_name' not in allMatchingChIdx[0].annotations:
  2242                                                               allMatchingChIdx[0].annotations['neo_name'] = allMatchingChIdx[0].name
  2243                                                           if 'nix_name' not in allMatchingChIdx[0].annotations:
  2244                                                               allMatchingChIdx[0].annotations['nix_name'] = allMatchingChIdx[0].name
  2245                                                   stp.unit.channel_index.name = stp.unit.channel_index.name.replace('.', '_').replace(' raw', '')
  2246                                               #  rename the children
  2247                                               typesNeedRenaming = [
  2248                                                   SpikeTrainProxy, AnalogSignalProxy, EventProxy,
  2249                                                   SpikeTrain, AnalogSignal, Event]
  2250                                               for segIdx, seg in enumerate(dataBlock.segments):
  2251                                                   if seg.name is None:
  2252                                                       seg.name = 'seg{}_'.format(segIdx)
  2253                                                   else:
  2254                                                       if 'seg{}_'.format(segIdx) not in seg.name:
  2255                                                           seg.name = (
  2256                                                               'seg{}_{}'
  2257                                                               .format(
  2258                                                                   segIdx,
  2259                                                                   childBaseName(seg.name, 'seg')))
  2260                                                   for objType in typesNeedRenaming:
  2261                                                       for child in seg.filter(objects=objType):
  2262                                                           if 'seg{}_'.format(segIdx) not in child.name:
  2263                                                               child.name = (
  2264                                                                   'seg{}_{}'
  2265                                                                   .format(
  2266                                                                       segIdx, childBaseName(child.name, 'seg')))
  2267                                                           #  todo: decide if below is needed
  2268                                                           #  elif 'seg' in child.name:
  2269                                                           #      childBaseName = '_'.join(child.name.split('_')[1:])
  2270                                                           #      child.name = 'seg{}_{}'.format(segIdx, childBaseName)
  2271                                               # [i.name for i in dataBlock.filter(objects=Unit)]
  2272                                               # [i.name for i in dataBlock.filter(objects=ChannelIndex)]
  2273                                               # [i.name for i in dataBlock.filter(objects=SpikeTrain)]
  2274                                               # [i.name for i in dataBlock.filter(objects=SpikeTrainProxy)]
  2275                                               if lazy:
  2276                                                   for stP in dataBlock.filter(objects=SpikeTrainProxy):
  2277                                                       if 'unitAnnotations' in stP.annotations:
  2278                                                           unAnnStr = stP.annotations['unitAnnotations']
  2279                                                           stP.unit.annotations.update(json.loads(unAnnStr))
  2280                                               if (loadList is not None) and lazy:
  2281                                                   if 'asigs' in loadList:
  2282                                                       loadAsigList(
  2283                                                           dataBlock, listOfAsigProxyNames=loadList['asigs'],
  2284                                                           replaceInParents=True)
  2285                                                   if 'events' in loadList:
  2286                                                       loadEventList(
  2287                                                           dataBlock,
  2288                                                           listOfEventNames=loadList['events'],
  2289                                                           replaceInParents=True)
  2290                                                   if 'spiketrains' in loadList:
  2291                                                       loadSpikeTrainList(
  2292                                                           dataBlock,
  2293                                                           listOfSpikeTrainNames=loadList['spiketrains'],
  2294                                                           replaceInParents=True)
  2295                                               if purgeNixNames:
  2296                                                   dataBlock = purgeNixAnn(dataBlock)
  2297                                               return dataBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadSpikeTrainList at line 2299

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2299                                           @profile
  2300                                           def loadSpikeTrainList(
  2301                                                   dataBlock, listOfSpikeTrainNames=None,
  2302                                                   replaceInParents=True):
  2303                                               listOfSpikeTrains = []
  2304                                               if listOfSpikeTrainNames is None:
  2305                                                   listOfSpikeTrainNames = [
  2306                                                       stp.name
  2307                                                       for stp in dataBlock.filter(objects=SpikeTrainProxy)]
  2308                                               for stP in dataBlock.filter(objects=SpikeTrainProxy):
  2309                                                   if stP.name in listOfSpikeTrainNames:
  2310                                                       st = loadObjArrayAnn(stP.load())
  2311                                                       listOfSpikeTrains.append(st)
  2312                                                       if replaceInParents:
  2313                                                           seg = stP.segment
  2314                                                           segStNames = [s.name for s in seg.spiketrains]
  2315                                                           idxInSeg = segStNames.index(stP.name)
  2316                                                           seg.spiketrains[idxInSeg] = st
  2317                                                           #
  2318                                                           unit = stP.unit
  2319                                                           unitStNames = [s.name for s in unit.spiketrains]
  2320                                                           st.unit = unit
  2321                                                           idxInUnit = unitStNames.index(stP.name)
  2322                                                           unit.spiketrains[idxInUnit] = st
  2323                                               return listOfSpikeTrains

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadEventList at line 2325

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2325                                           @profile
  2326                                           def loadEventList(
  2327                                                   dataBlock,
  2328                                                   listOfEventNames=None, replaceInParents=True):
  2329                                               listOfEvents = []
  2330                                               if listOfEventNames is None:
  2331                                                   listOfEventNames = [
  2332                                                       evp.name
  2333                                                       for evp in dataBlock.filter(objects=EventProxy)]
  2334                                               for evP in dataBlock.filter(objects=EventProxy):
  2335                                                   if evP.name in listOfEventNames:
  2336                                                       ev = loadObjArrayAnn(evP.load())
  2337                                                       listOfEvents.append(ev)
  2338                                                       if replaceInParents:
  2339                                                           seg = evP.segment
  2340                                                           segEvNames = [e.name for e in seg.events]
  2341                                                           idxInSeg = segEvNames.index(evP.name)
  2342                                                           seg.events[idxInSeg] = ev
  2343                                               return listOfEvents

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadAsigList at line 2345

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2345                                           @profile
  2346                                           def loadAsigList(
  2347                                                   dataBlock, listOfAsigProxyNames=None, replaceInParents=True):
  2348                                               listOfAsigs = []
  2349                                               if listOfAsigProxyNames is None:
  2350                                                   listOfAsigProxyNames = [
  2351                                                       asigp.name
  2352                                                       for asigp in dataBlock.filter(objects=AnalogSignalProxy)]
  2353                                               for asigP in dataBlock.filter(objects=AnalogSignalProxy):
  2354                                                   if asigP.name in listOfAsigProxyNames:
  2355                                                       asig = asigP.load()
  2356                                                       asig.annotations = asigP.annotations.copy()
  2357                                                       listOfAsigs.append(asig)
  2358                                                       #
  2359                                                       if replaceInParents:
  2360                                                           seg = asigP.segment
  2361                                                           segAsigNames = [ag.name for ag in seg.analogsignals]
  2362                                                           asig.segment = seg
  2363                                                           idxInSeg = segAsigNames.index(asigP.name)
  2364                                                           seg.analogsignals[idxInSeg] = asig
  2365                                                           #
  2366                                                           chIdx = asigP.channel_index
  2367                                                           chIdxAsigNames = [ag.name for ag in chIdx.analogsignals]
  2368                                                           asig.channel_index = chIdx
  2369                                                           idxInChIdx = chIdxAsigNames.index(asigP.name)
  2370                                                           chIdx.analogsignals[idxInChIdx] = asig
  2371                                               return listOfAsigs

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: addBlockToNIX at line 2373

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2373                                           @profile
  2374                                           def addBlockToNIX(
  2375                                                   newBlock, neoSegIdx=[0],
  2376                                                   writeAsigs=True, writeSpikes=True, writeEvents=True,
  2377                                                   asigNameList=None,
  2378                                                   purgeNixNames=False,
  2379                                                   fileName=None,
  2380                                                   folderPath=None,
  2381                                                   nixBlockIdx=0, nixSegIdx=[0],
  2382                                                   ):
  2383                                               #  base file name
  2384                                               trialBasePath = os.path.join(folderPath, fileName)
  2385                                               if writeAsigs:
  2386                                                   # peek at file to ensure compatibility
  2387                                                   reader = nixio_fr.NixIO(filename=trialBasePath + '.nix')
  2388                                                   tempBlock = reader.read_block(
  2389                                                       block_index=nixBlockIdx,
  2390                                                       lazy=True, signal_group_mode='split-all')
  2391                                                   checkCompatible = {i: False for i in nixSegIdx}
  2392                                                   forceShape = {i: None for i in nixSegIdx}
  2393                                                   forceType = {i: None for i in nixSegIdx}
  2394                                                   forceFS = {i: None for i in nixSegIdx}
  2395                                                   for nixIdx in nixSegIdx:
  2396                                                       tempAsigList = tempBlock.segments[nixIdx].filter(
  2397                                                           objects=AnalogSignalProxy)
  2398                                                       if len(tempAsigList) > 0:
  2399                                                           tempAsig = tempAsigList[0]
  2400                                                           checkCompatible[nixIdx] = True
  2401                                                           forceType[nixIdx] = tempAsig.dtype
  2402                                                           forceShape[nixIdx] = tempAsig.shape[0]  # ? docs say shape[1], but that's confusing
  2403                                                           forceFS[nixIdx] = tempAsig.sampling_rate
  2404                                                   reader.file.close()
  2405                                               #  if newBlock was loaded from a nix file, strip the old nix_names away:
  2406                                               #  todo: replace with function from this module
  2407                                               if purgeNixNames:
  2408                                                   newBlock = purgeNixAnn(newBlock)
  2409                                               #
  2410                                               writer = NixIO(filename=trialBasePath + '.nix')
  2411                                               nixblock = writer.nix_file.blocks[nixBlockIdx]
  2412                                               nixblockName = nixblock.name
  2413                                               if 'nix_name' in newBlock.annotations.keys():
  2414                                                   try:
  2415                                                       assert newBlock.annotations['nix_name'] == nixblockName
  2416                                                   except Exception:
  2417                                                       newBlock.annotations['nix_name'] = nixblockName
  2418                                               else:
  2419                                                   newBlock.annotate(nix_name=nixblockName)
  2420                                               #
  2421                                               for idx, segIdx in enumerate(neoSegIdx):
  2422                                                   nixIdx = nixSegIdx[idx]
  2423                                                   newSeg = newBlock.segments[segIdx]
  2424                                                   nixgroup = nixblock.groups[nixIdx]
  2425                                                   nixSegName = nixgroup.name
  2426                                                   if 'nix_name' in newSeg.annotations.keys():
  2427                                                       try:
  2428                                                           assert newSeg.annotations['nix_name'] == nixSegName
  2429                                                       except Exception:
  2430                                                           newSeg.annotations['nix_name'] = nixSegName
  2431                                                   else:
  2432                                                       newSeg.annotate(nix_name=nixSegName)
  2433                                                   #
  2434                                                   if writeEvents:
  2435                                                       eventList = newSeg.events
  2436                                                       eventOrder = np.argsort([i.name for i in eventList])
  2437                                                       for event in [eventList[i] for i in eventOrder]:
  2438                                                           event = writer._write_event(event, nixblock, nixgroup)
  2439                                                   #
  2440                                                   if writeAsigs:
  2441                                                       asigList = newSeg.filter(objects=AnalogSignal)
  2442                                                       asigOrder = np.argsort([i.name for i in asigList])
  2443                                                       for asig in [asigList[i] for i in asigOrder]:
  2444                                                           if checkCompatible[nixIdx]:
  2445                                                               assert asig.dtype == forceType[nixIdx]
  2446                                                               assert asig.sampling_rate == forceFS[nixIdx]
  2447                                                               #  print('asig.shape[0] = {}'.format(asig.shape[0]))
  2448                                                               #  print('forceShape[nixIdx] = {}'.format(forceShape[nixIdx]))
  2449                                                               assert asig.shape[0] == forceShape[nixIdx]
  2450                                                           asig = writer._write_analogsignal(asig, nixblock, nixgroup)
  2451                                                       #  for isig in newSeg.filter(objects=IrregularlySampledSignal):
  2452                                                       #      isig = writer._write_irregularlysampledsignal(
  2453                                                       #          isig, nixblock, nixgroup)
  2454                                                   #
  2455                                                   if writeSpikes:
  2456                                                       stList = newSeg.filter(objects=SpikeTrain)
  2457                                                       stOrder = np.argsort([i.name for i in stList])
  2458                                                       for st in [stList[i] for i in stOrder]:
  2459                                                           st = writer._write_spiketrain(st, nixblock, nixgroup)
  2460                                               #
  2461                                               for chanIdx in newBlock.filter(objects=ChannelIndex):
  2462                                                   chanIdx = writer._write_channelindex(chanIdx, nixblock)
  2463                                                   #  auto descends into units inside of _write_channelindex
  2464                                               writer._create_source_links(newBlock, nixblock)
  2465                                               writer.close()
  2466                                               print('Done adding block to Nix.')
  2467                                               return newBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadStProxy at line 2469

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2469                                           @profile
  2470                                           def loadStProxy(stProxy):
  2471                                               try:
  2472                                                   st = stProxy.load(
  2473                                                       magnitude_mode='rescaled',
  2474                                                       load_waveforms=True)
  2475                                               except Exception:
  2476                                                   st = stProxy.load(
  2477                                                       magnitude_mode='rescaled',
  2478                                                       load_waveforms=False)
  2479                                                   st.waveforms = np.asarray([]).reshape((0, 0, 0))*pq.mV
  2480                                               return st

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: preproc at line 2482

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2482                                           @profile
  2483                                           def preproc(
  2484                                                   fileName='Trial001',
  2485                                                   rawFolderPath='./',
  2486                                                   outputFolderPath='./', mapDF=None,
  2487                                                   # swapMaps=None,
  2488                                                   electrodeArrayName='utah',
  2489                                                   fillOverflow=True, removeJumps=True,
  2490                                                   removeMeanAcross=False,
  2491                                                   linearDetrend=False,
  2492                                                   interpolateOutliers=False, calcOutliers=False,
  2493                                                   outlierMaskFilterOpts=None,
  2494                                                   outlierThreshold=1,
  2495                                                   calcArtifactTrace=False,
  2496                                                   motorEncoderMask=None,
  2497                                                   calcAverageLFP=False,
  2498                                                   eventInfo=None,
  2499                                                   spikeSourceType='', spikePath=None,
  2500                                                   chunkSize=1800, equalChunks=True, chunkList=None, chunkOffset=0,
  2501                                                   writeMode='rw',
  2502                                                   signal_group_mode='split-all', trialInfo=None,
  2503                                                   asigNameList=None, ainpNameList=None, nameSuffix='',
  2504                                                   saveFromAsigNameList=True,
  2505                                                   calcRigEvents=True, normalizeByImpedance=False,
  2506                                                   LFPFilterOpts=None, encoderCountPerDegree=180e2,
  2507                                                   outlierRemovalDebugFlag=False, impedanceFilePath=None
  2508                                                   ):
  2509                                               #  base file name
  2510                                               rawBasePath = os.path.join(rawFolderPath, fileName)
  2511                                               outputFilePath = os.path.join(
  2512                                                   outputFolderPath,
  2513                                                   fileName + nameSuffix + '.nix')
  2514                                               if os.path.exists(outputFilePath):
  2515                                                   os.remove(outputFilePath)
  2516                                               #  instantiate reader, get metadata
  2517                                               print('Loading\n{}\n'.format(rawBasePath))
  2518                                               reader = BlackrockIO(
  2519                                                   filename=rawBasePath, nsx_to_load=5)
  2520                                               reader.parse_header()
  2521                                               # metadata = reader.header
  2522                                               #  absolute section index
  2523                                               dummyBlock = readBlockFixNames(
  2524                                                   reader,
  2525                                                   block_index=0, lazy=True,
  2526                                                   signal_group_mode=signal_group_mode,
  2527                                                   mapDF=mapDF, reduceChannelIndexes=True,
  2528                                                   # swapMaps=swapMaps
  2529                                                   )
  2530                                               segLen = dummyBlock.segments[0].analogsignals[0].shape[0] / (
  2531                                                   dummyBlock.segments[0].analogsignals[0].sampling_rate)
  2532                                               nChunks = math.ceil(segLen / chunkSize)
  2533                                               #
  2534                                               if equalChunks:
  2535                                                   actualChunkSize = (segLen / nChunks).magnitude
  2536                                               else:
  2537                                                   actualChunkSize = chunkSize
  2538                                               if chunkList is None:
  2539                                                   chunkList = range(nChunks)
  2540                                               chunkingMetadata = {}
  2541                                               for chunkIdx in chunkList:
  2542                                                   print('preproc on chunk {}'.format(chunkIdx))
  2543                                                   #  instantiate spike reader if requested
  2544                                                   if spikeSourceType == 'tdc':
  2545                                                       if spikePath is None:
  2546                                                           spikePath = os.path.join(
  2547                                                               outputFolderPath, 'tdc_' + fileName,
  2548                                                               'tdc_' + fileName + '.nix')
  2549                                                       print('loading {}'.format(spikePath))
  2550                                                       spikeReader = nixio_fr.NixIO(filename=spikePath)
  2551                                                   else:
  2552                                                       spikeReader = None
  2553                                                   #  absolute section index
  2554                                                   block = readBlockFixNames(
  2555                                                       reader,
  2556                                                       block_index=0, lazy=True,
  2557                                                       signal_group_mode=signal_group_mode,
  2558                                                       mapDF=mapDF, reduceChannelIndexes=True,
  2559                                                       # swapMaps=swapMaps
  2560                                                       )
  2561                                                   if spikeReader is not None:
  2562                                                       spikeBlock = readBlockFixNames(
  2563                                                           spikeReader, block_index=0, lazy=True,
  2564                                                           signal_group_mode=signal_group_mode,
  2565                                                           mapDF=mapDF, reduceChannelIndexes=True,
  2566                                                           # swapMaps=swapMaps
  2567                                                           )
  2568                                                       spikeBlock = purgeNixAnn(spikeBlock)
  2569                                                   else:
  2570                                                       spikeBlock = None
  2571                                                   #
  2572                                                   #  instantiate writer
  2573                                                   if (nChunks == 1) or (len(chunkList) == 1):
  2574                                                       partNameSuffix = ""
  2575                                                       thisChunkOutFilePath = outputFilePath
  2576                                                   else:
  2577                                                       partNameSuffix = '_pt{:0>3}'.format(chunkIdx)
  2578                                                       thisChunkOutFilePath = (
  2579                                                           outputFilePath
  2580                                                           .replace('.nix', partNameSuffix + '.nix'))
  2581                                                   #
  2582                                                   if os.path.exists(thisChunkOutFilePath):
  2583                                                       os.remove(thisChunkOutFilePath)
  2584                                                   writer = NixIO(
  2585                                                       filename=thisChunkOutFilePath, mode=writeMode)
  2586                                                   chunkTStart = chunkIdx * actualChunkSize + chunkOffset
  2587                                                   chunkTStop = (chunkIdx + 1) * actualChunkSize + chunkOffset
  2588                                                   chunkingMetadata[chunkIdx] = {
  2589                                                       'filename': thisChunkOutFilePath,
  2590                                                       'partNameSuffix': partNameSuffix,
  2591                                                       'chunkTStart': chunkTStart,
  2592                                                       'chunkTStop': chunkTStop}
  2593                                                   block.annotate(chunkTStart=chunkTStart)
  2594                                                   block.annotate(chunkTStop=chunkTStop)
  2595                                                   block.annotate(
  2596                                                       recDatetimeStr=(
  2597                                                           block
  2598                                                           .rec_datetime
  2599                                                           .replace(tzinfo=timezone.utc)
  2600                                                           .isoformat())
  2601                                                       )
  2602                                                   #
  2603                                                   preprocBlockToNix(
  2604                                                       block, writer,
  2605                                                       chunkTStart=chunkTStart,
  2606                                                       chunkTStop=chunkTStop,
  2607                                                       fillOverflow=fillOverflow,
  2608                                                       removeJumps=removeJumps,
  2609                                                       interpolateOutliers=interpolateOutliers,
  2610                                                       calcOutliers=calcOutliers,
  2611                                                       outlierThreshold=outlierThreshold,
  2612                                                       outlierMaskFilterOpts=outlierMaskFilterOpts,
  2613                                                       calcArtifactTrace=calcArtifactTrace,
  2614                                                       linearDetrend=linearDetrend,
  2615                                                       motorEncoderMask=motorEncoderMask,
  2616                                                       electrodeArrayName=electrodeArrayName,
  2617                                                       calcAverageLFP=calcAverageLFP,
  2618                                                       eventInfo=eventInfo,
  2619                                                       asigNameList=asigNameList, ainpNameList=ainpNameList,
  2620                                                       saveFromAsigNameList=saveFromAsigNameList,
  2621                                                       spikeSourceType=spikeSourceType,
  2622                                                       spikeBlock=spikeBlock,
  2623                                                       calcRigEvents=calcRigEvents,
  2624                                                       normalizeByImpedance=normalizeByImpedance,
  2625                                                       removeMeanAcross=removeMeanAcross,
  2626                                                       LFPFilterOpts=LFPFilterOpts,
  2627                                                       encoderCountPerDegree=encoderCountPerDegree,
  2628                                                       outlierRemovalDebugFlag=outlierRemovalDebugFlag,
  2629                                                       impedanceFilePath=impedanceFilePath,
  2630                                                       )
  2631                                                   #### diagnostics
  2632                                                   diagnosticFolder = os.path.join(
  2633                                                       outputFolderPath,
  2634                                                       'preprocDiagnostics',
  2635                                                       # fileName + nameSuffix + partNameSuffix
  2636                                                       )
  2637                                                   if not os.path.exists(diagnosticFolder):
  2638                                                       os.mkdir(diagnosticFolder)
  2639                                                   asigDiagnostics = {}
  2640                                                   outlierDiagnostics = {}
  2641                                                   diagnosticText = ''
  2642                                                   for asig in block.filter(objects=AnalogSignal):
  2643                                                       annNames = ['mean_removal_r2', 'mean_removal_group']
  2644                                                       for annName in annNames:
  2645                                                           if annName in asig.annotations:
  2646                                                               if asig.name not in asigDiagnostics:
  2647                                                                   asigDiagnostics[asig.name] = {}
  2648                                                               asigDiagnostics[asig.name].update({
  2649                                                                   annName: asig.annotations[annName]})
  2650                                                       annNames = [
  2651                                                           'outlierProportion', 'nDim',
  2652                                                           'noveltyThreshold', 'outlierThreshold'
  2653                                                           ]
  2654                                                       for annName in annNames:
  2655                                                           if annName in asig.annotations:
  2656                                                               if asig.name not in outlierDiagnostics:
  2657                                                                   outlierDiagnostics[asig.name] = {}
  2658                                                               outlierDiagnostics[asig.name].update({
  2659                                                                   annName: '{}'.format(asig.annotations[annName])
  2660                                                               })
  2661                                                   if removeMeanAcross:
  2662                                                       asigDiagnosticsDF = pd.DataFrame(asigDiagnostics).T
  2663                                                       asigDiagnosticsDF.sort_values(by='mean_removal_r2', inplace=True)
  2664                                                       diagnosticText += '<h2>LFP Diagnostics</h2>\n'
  2665                                                       diagnosticText += asigDiagnosticsDF.to_html()
  2666                                                       fig, ax = plt.subplots()
  2667                                                       sns.distplot(asigDiagnosticsDF['mean_removal_r2'], ax=ax)
  2668                                                       ax.set_ylabel('Count of analog signals')
  2669                                                       ax.set_xlabel('R^2 of regressing mean against signal')
  2670                                                       fig.savefig(os.path.join(
  2671                                                               diagnosticFolder,
  2672                                                               fileName + nameSuffix + partNameSuffix + '_meanRemovalR2.png'
  2673                                                           ))
  2674                                                   if interpolateOutliers:
  2675                                                       outlierDiagnosticsDF = pd.DataFrame(outlierDiagnostics).T
  2676                                                       diagnosticText += '<h2>Outlier Diagnostics</h2>\n'
  2677                                                       diagnosticText += outlierDiagnosticsDF.to_html()
  2678                                                   diagnosticTextPath = os.path.join(
  2679                                                       diagnosticFolder,
  2680                                                       fileName + nameSuffix + partNameSuffix + '_asigDiagnostics.html'
  2681                                                       )
  2682                                                   with open(diagnosticTextPath, 'w') as _f:
  2683                                                       _f.write(diagnosticText)
  2684                                                   writer.close()
  2685                                               chunkingInfoPath = os.path.join(
  2686                                                   outputFolderPath,
  2687                                                   fileName + nameSuffix +
  2688                                                   '_chunkingInfo.json'
  2689                                                   )
  2690                                               if os.path.exists(chunkingInfoPath):
  2691                                                   os.remove(chunkingInfoPath)
  2692                                               with open(chunkingInfoPath, 'w') as f:
  2693                                                   json.dump(chunkingMetadata, f)
  2694                                               return

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: preprocBlockToNix at line 2696

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2696                                           @profile
  2697                                           def preprocBlockToNix(
  2698                                                   block, writer,
  2699                                                   chunkTStart=None,
  2700                                                   chunkTStop=None,
  2701                                                   eventInfo=None,
  2702                                                   fillOverflow=False, calcAverageLFP=False,
  2703                                                   interpolateOutliers=False, calcOutliers=False,
  2704                                                   calcArtifactTrace=False,
  2705                                                   outlierMaskFilterOpts=None,
  2706                                                   useMeanToCenter=False,   # mean center? median center?
  2707                                                   linearDetrend=False,
  2708                                                   zScoreEachTrace=False,
  2709                                                   outlierThreshold=1,
  2710                                                   motorEncoderMask=None,
  2711                                                   electrodeArrayName='utah',
  2712                                                   removeJumps=False, trackMemory=True,
  2713                                                   asigNameList=None, ainpNameList=None,
  2714                                                   saveFromAsigNameList=True,
  2715                                                   spikeSourceType='', spikeBlock=None,
  2716                                                   calcRigEvents=True,
  2717                                                   normalizeByImpedance=True,
  2718                                                   impedanceFilePath=None,
  2719                                                   removeMeanAcross=False,
  2720                                                   LFPFilterOpts=None, encoderCountPerDegree=180e2,
  2721                                                   outlierRemovalDebugFlag=False,
  2722                                                   ):
  2723                                               #  prune out nev spike placeholders
  2724                                               #  (will get added back on a chunk by chunk basis,
  2725                                               #  if not pruning units)
  2726                                               if spikeSourceType == 'nev':
  2727                                                   pruneOutUnits = False
  2728                                               else:
  2729                                                   pruneOutUnits = True
  2730                                               #
  2731                                               for chanIdx in block.channel_indexes:
  2732                                                   if chanIdx.units:
  2733                                                       for unit in chanIdx.units:
  2734                                                           if unit.spiketrains:
  2735                                                               unit.spiketrains = []
  2736                                                       if pruneOutUnits:
  2737                                                           chanIdx.units = []
  2738                                               #
  2739                                               if spikeBlock is not None:
  2740                                                   for chanIdx in spikeBlock.channel_indexes:
  2741                                                       if chanIdx.units:
  2742                                                           for unit in chanIdx.units:
  2743                                                               if unit.spiketrains:
  2744                                                                   unit.spiketrains = []
  2745                                               #  precalculate new segment
  2746                                               seg = block.segments[0]
  2747                                               #  remove chanIndexes assigned to units; makes more sense to
  2748                                               #  only use chanIdx for asigs and spikes on that asig
  2749                                               #  block.channel_indexes = (
  2750                                               #      [chanIdx for chanIdx in block.channel_indexes if (
  2751                                               #          chanIdx.analogsignals)])
  2752                                               if calcAverageLFP:
  2753                                                   lastIndex = len(block.channel_indexes)
  2754                                                   lastID = block.channel_indexes[-1].channel_ids[0] + 1
  2755                                                   if asigNameList is None:
  2756                                                       asigNameList = [
  2757                                                           [
  2758                                                               childBaseName(a.name, 'seg')
  2759                                                               for a in seg.analogsignals
  2760                                                               if not (('ainp' in a.name) or ('analog' in a.name))]
  2761                                                           ]
  2762                                                   nMeanChans = len(asigNameList)
  2763                                                   #
  2764                                                   meanChIdxList = []
  2765                                                   for meanChIdx in range(nMeanChans):
  2766                                                       tempChIdx = ChannelIndex(
  2767                                                           index=[lastIndex + meanChIdx],
  2768                                                           channel_names=['{}_rawAverage_{}'.format(electrodeArrayName, meanChIdx)],
  2769                                                           channel_ids=[lastID + meanChIdx],
  2770                                                           name='{}_rawAverage_{}'.format(electrodeArrayName, meanChIdx),
  2771                                                           file_origin=block.channel_indexes[-1].file_origin
  2772                                                           )
  2773                                                       tempChIdx.merge_annotations(block.channel_indexes[-1])
  2774                                                       block.channel_indexes.append(tempChIdx)
  2775                                                       meanChIdxList.append(tempChIdx)
  2776                                                       lastIndex += 1
  2777                                                       lastID += 1
  2778                                                   lastIndex = len(block.channel_indexes)
  2779                                                   lastID = block.channel_indexes[-1].channel_ids[0] + 1
  2780                                                   # if calcArtifactTrace:
  2781                                                   if True:
  2782                                                       artChIdxList = []
  2783                                                       for artChIdx in range(nMeanChans):
  2784                                                           tempChIdx = ChannelIndex(
  2785                                                               index=[lastIndex + artChIdx],
  2786                                                               channel_names=['{}_artifact_{}'.format(electrodeArrayName, artChIdx)],
  2787                                                               channel_ids=[lastID + artChIdx],
  2788                                                               name='{}_artifact_{}'.format(electrodeArrayName, artChIdx),
  2789                                                               file_origin=block.channel_indexes[-1].file_origin
  2790                                                               )
  2791                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2792                                                           block.channel_indexes.append(tempChIdx)
  2793                                                           artChIdxList.append(tempChIdx)
  2794                                                           lastIndex += 1
  2795                                                           lastID += 1
  2796                                                   # if calcOutliers:
  2797                                                   if True:
  2798                                                       devChIdxList = []
  2799                                                       for devChIdx in range(nMeanChans):
  2800                                                           tempChIdx = ChannelIndex(
  2801                                                               index=[lastIndex + devChIdx],
  2802                                                               channel_names=['{}_deviation_{}'.format(electrodeArrayName, devChIdx)],
  2803                                                               channel_ids=[lastID + devChIdx],
  2804                                                               name='{}_deviation_{}'.format(electrodeArrayName, devChIdx),
  2805                                                               file_origin=block.channel_indexes[-1].file_origin
  2806                                                               )
  2807                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2808                                                           block.channel_indexes.append(tempChIdx)
  2809                                                           devChIdxList.append(tempChIdx)
  2810                                                           lastIndex += 1
  2811                                                           lastID += 1
  2812                                                       smDevChIdxList = []
  2813                                                       for devChIdx in range(nMeanChans):
  2814                                                           tempChIdx = ChannelIndex(
  2815                                                               index=[lastIndex + devChIdx],
  2816                                                               channel_names=['{}_smoothed_deviation_{}'.format(electrodeArrayName, devChIdx)],
  2817                                                               channel_ids=[lastID + devChIdx],
  2818                                                               name='{}_smoothed_deviation_{}'.format(electrodeArrayName, devChIdx),
  2819                                                               file_origin=block.channel_indexes[-1].file_origin
  2820                                                               )
  2821                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2822                                                           block.channel_indexes.append(tempChIdx)
  2823                                                           smDevChIdxList.append(tempChIdx)
  2824                                                           lastIndex += 1
  2825                                                           lastID += 1
  2826                                                       outMaskChIdxList = []
  2827                                                       for outMaskChIdx in range(nMeanChans):
  2828                                                           tempChIdx = ChannelIndex(
  2829                                                               index=[lastIndex + outMaskChIdx],
  2830                                                               channel_names=['{}_outlierMask_{}'.format(
  2831                                                                   electrodeArrayName, outMaskChIdx)],
  2832                                                               channel_ids=[lastID + outMaskChIdx],
  2833                                                               name='{}_outlierMask_{}'.format(
  2834                                                                   electrodeArrayName, outMaskChIdx),
  2835                                                               file_origin=block.channel_indexes[-1].file_origin
  2836                                                               )
  2837                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2838                                                           block.channel_indexes.append(tempChIdx)
  2839                                                           outMaskChIdxList.append(tempChIdx)
  2840                                                           lastIndex += 1
  2841                                                           lastID += 1
  2842                                               #  delete asig and irsig proxies from channel index list
  2843                                               for metaIdx, chanIdx in enumerate(block.channel_indexes):
  2844                                                   if chanIdx.analogsignals:
  2845                                                       chanIdx.analogsignals = []
  2846                                                   if chanIdx.irregularlysampledsignals:
  2847                                                       chanIdx.irregularlysampledsignals = []
  2848                                               newSeg = Segment(
  2849                                                       index=0, name=seg.name,
  2850                                                       description=seg.description,
  2851                                                       file_origin=seg.file_origin,
  2852                                                       file_datetime=seg.file_datetime,
  2853                                                       rec_datetime=seg.rec_datetime,
  2854                                                       **seg.annotations
  2855                                                   )
  2856                                               block.segments = [newSeg]
  2857                                               block, nixblock = writer.write_block_meta(block)
  2858                                               # descend into Segments
  2859                                               if impedanceFilePath is not None:
  2860                                                   try:
  2861                                                       impedances = prb_meta.getLatestImpedance(
  2862                                                           block=block, impedanceFilePath=impedanceFilePath)
  2863                                                       averageImpedance = impedances['impedance'].median()
  2864                                                   except Exception:
  2865                                                       traceback.print_exc()
  2866                                               # for segIdx, seg in enumerate(oldSegList):
  2867                                               if spikeBlock is not None:
  2868                                                   spikeSeg = spikeBlock.segments[0]
  2869                                               else:
  2870                                                   spikeSeg = seg
  2871                                               #
  2872                                               if trackMemory:
  2873                                                   print('memory usage: {:.1f} MB'.format(
  2874                                                       prf.memory_usage_psutil()))
  2875                                               newSeg, nixgroup = writer._write_segment_meta(newSeg, nixblock)
  2876                                               #  trim down list of analog signals if necessary
  2877                                               asigNameListSeg = []
  2878                                               if (removeMeanAcross or calcAverageLFP):
  2879                                                   meanGroups = {}
  2880                                               for subListIdx, subList in enumerate(asigNameList):
  2881                                                   subListSeg = [
  2882                                                       'seg{}_{}'.format(0, a)
  2883                                                       for a in subList]
  2884                                                   asigNameListSeg += subListSeg
  2885                                                   if (removeMeanAcross or calcAverageLFP):
  2886                                                       meanGroups[subListIdx] = subListSeg
  2887                                               aSigList = []
  2888                                               # [asig.name for asig in seg.analogsignals]
  2889                                               for a in seg.analogsignals:
  2890                                                   # if np.any([n in a.name for n in asigNameListSeg]):
  2891                                                   if a.name in asigNameListSeg:
  2892                                                       aSigList.append(a)
  2893                                               if ainpNameList is not None:
  2894                                                   ainpNameListSeg = [
  2895                                                       'seg{}_{}'.format(0, a)
  2896                                                       for a in ainpNameList]
  2897                                                   ainpList = []
  2898                                                   for a in seg.analogsignals:
  2899                                                       if np.any([n == a.name for n in ainpNameListSeg]):
  2900                                                           ainpList.append(a)
  2901                                               else:
  2902                                                   ainpList = [
  2903                                                       a
  2904                                                       for a in seg.analogsignals
  2905                                                       if (('ainp' in a.name) or ('analog' in a.name))]
  2906                                                   ainpNameListSeg = [a.name for a in aSigList]
  2907                                               nAsigs = len(aSigList)
  2908                                               if LFPFilterOpts is not None:
  2909                                                   def filterFun(sig, filterCoeffs=None):
  2910                                                       # sig[:] = signal.sosfiltfilt(
  2911                                                       sig[:] = signal.sosfilt(
  2912                                                           filterCoeffs, sig.magnitude.flatten())[:, np.newaxis] * sig.units
  2913                                                       return sig
  2914                                                   filterCoeffs = hf.makeFilterCoeffsSOS(
  2915                                                       LFPFilterOpts, float(seg.analogsignals[0].sampling_rate))
  2916                                                   if False:
  2917                                                       fig, ax1, ax2 = hf.plotFilterResponse(
  2918                                                           filterCoeffs,
  2919                                                           float(seg.analogsignals[0].sampling_rate))
  2920                                                       fig2, ax3, ax4 = hf.plotFilterImpulseResponse(
  2921                                                           LFPFilterOpts,
  2922                                                           float(seg.analogsignals[0].sampling_rate))
  2923                                                       plt.show()
  2924                                               # first pass through asigs, if removing mean across channels
  2925                                               if (removeMeanAcross or calcAverageLFP):
  2926                                                   for aSigIdx, aSigProxy in enumerate(seg.analogsignals):
  2927                                                       if aSigIdx == 0:
  2928                                                           # check bounds
  2929                                                           tStart = max(chunkTStart * pq.s, aSigProxy.t_start)
  2930                                                           tStop = min(chunkTStop * pq.s, aSigProxy.t_stop)
  2931                                                       loadThisOne = (aSigProxy in aSigList)
  2932                                                       if loadThisOne:
  2933                                                           if trackMemory:
  2934                                                               print(
  2935                                                                   'Extracting asig for mean, memory usage: {:.1f} MB'.format(
  2936                                                                       prf.memory_usage_psutil()))
  2937                                                           chanIdx = aSigProxy.channel_index
  2938                                                           asig = aSigProxy.load(
  2939                                                               time_slice=(tStart, tStop),
  2940                                                               magnitude_mode='rescaled')
  2941                                                           if 'tempLFPStore' not in locals():
  2942                                                               tempLFPStore = pd.DataFrame(
  2943                                                                   np.zeros(
  2944                                                                       (asig.shape[0], nAsigs),
  2945                                                                       dtype=np.float32),
  2946                                                                   columns=asigNameListSeg)
  2947                                                           if 'dummyAsig' not in locals():
  2948                                                               dummyAsig = asig.copy()
  2949                                                           #  perform requested preproc operations
  2950                                                           #  if LFPFilterOpts is not None:
  2951                                                           #      asig[:] = filterFun(
  2952                                                           #          asig, filterCoeffs=filterCoeffs)
  2953                                                           if normalizeByImpedance:
  2954                                                               elNmMatchMsk = impedances['elec'] == chanIdx.name
  2955                                                               '''
  2956                                                               asig.magnitude[:] = (
  2957                                                                   (asig.magnitude - np.median(asig.magnitude)) /
  2958                                                                   np.min(
  2959                                                                       impedances.loc[elNmMatchMsk, 'impedance']
  2960                                                                       ))
  2961                                                               '''
  2962                                                               asig.magnitude[:] = (
  2963                                                                   (asig.magnitude) * averageImpedance /
  2964                                                                   np.min(
  2965                                                                       impedances.loc[elNmMatchMsk, 'impedance']
  2966                                                                       ))
  2967                                                           # if fillOverflow:
  2968                                                           #     # fill in overflow:
  2969                                                           #     '''
  2970                                                           #     timeSection['data'], overflowMask = hf.fillInOverflow(
  2971                                                           #         timeSection['data'], fillMethod = 'average')
  2972                                                           #     badData.update({'overflow': overflowMask})
  2973                                                           #     '''
  2974                                                           #     pass
  2975                                                           # if removeJumps:
  2976                                                           #     # find unusual jumps in derivative or amplitude
  2977                                                           #     '''
  2978                                                           #     timeSection['data'], newBadData = hf.fillInJumps(timeSection['data'],
  2979                                                           #     timeSection['samp_per_s'], smoothing_ms = 0.5, nStdDiff = 50,
  2980                                                           #     nStdAmp = 100)
  2981                                                           #     badData.update(newBadData)
  2982                                                           #     '''
  2983                                                           #     pass
  2984                                                           tempLFPStore.loc[:, aSigProxy.name] = asig.magnitude.flatten()
  2985                                                           del asig
  2986                                                           gc.collect()
  2987                                                   # end of first pass
  2988                                                   if (removeMeanAcross or calcAverageLFP):
  2989                                                       centerLFP = np.zeros(
  2990                                                           (tempLFPStore.shape[0], len(asigNameList)),
  2991                                                           dtype=np.float32)
  2992                                                       spreadLFP = np.zeros(
  2993                                                           (tempLFPStore.shape[0], len(asigNameList)),
  2994                                                           dtype=np.float32)
  2995                                                       # if calcOutliers:
  2996                                                       if True:
  2997                                                           if outlierMaskFilterOpts is not None:
  2998                                                               filterCoeffsOutlierMask = hf.makeFilterCoeffsSOS(
  2999                                                                   outlierMaskFilterOpts, float(dummyAsig.sampling_rate))
  3000                                                           lfpDeviation = np.zeros(
  3001                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3002                                                               dtype=np.float32)
  3003                                                           smoothedDeviation = np.zeros(
  3004                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3005                                                               dtype=np.float32)
  3006                                                           outlierMask = np.zeros(
  3007                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3008                                                               dtype=np.bool)
  3009                                                           outlierMetadata = {}
  3010                                                       # if calcArtifactTrace:
  3011                                                       if True:
  3012                                                           artifactSignal = np.zeros(
  3013                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3014                                                               dtype=np.float32)
  3015                                                       ###############
  3016                                                       # tempLFPStore.iloc[:, 0] = np.nan  # for debugging axes
  3017                                                       #############
  3018                                                       plotDevFilterDebug = False
  3019                                                       if plotDevFilterDebug:
  3020                                                           try:
  3021                                                               devFiltDebugMask = (dummyAsig.times > 90 * pq.s) & (dummyAsig.times < 92 * pq.s)
  3022                                                           except Exception:
  3023                                                               pdb.set_trace()
  3024                                                           plotColIdx = 1
  3025                                                           ddfFig, ddfAx = plt.subplots(len(asigNameList), 1)
  3026                                                           ddfFig2, ddfAx2 = plt.subplots()
  3027                                                           ddfFig3, ddfAx3 = plt.subplots(
  3028                                                               1, len(asigNameList),
  3029                                                               sharey=True)
  3030                                                           if len(asigNameList) == 1:
  3031                                                               ddfAx = np.asarray([ddfAx])
  3032                                                               ddfAx3 = np.asarray([ddfAx3])
  3033                                                       for subListIdx, subList in enumerate(asigNameList):
  3034                                                           columnsForThisGroup = meanGroups[subListIdx]
  3035                                                           if trackMemory:
  3036                                                               print(
  3037                                                                   'asig group {}: calculating mean, memory usage: {:.1f} MB'.format(
  3038                                                                       subListIdx, prf.memory_usage_psutil()))
  3039                                                               print('this group contains\n{}'.format(columnsForThisGroup))
  3040                                                           if plotDevFilterDebug:
  3041                                                               ddfAx3[subListIdx].plot(
  3042                                                                   dummyAsig.times[devFiltDebugMask],
  3043                                                                   tempLFPStore.loc[:, columnsForThisGroup].iloc[devFiltDebugMask, plotColIdx],
  3044                                                                   label='original ch'
  3045                                                                   )
  3046                                                           if fillOverflow:
  3047                                                               print('Filling overflow...')
  3048                                                               # fill in overflow:
  3049                                                               tempLFPStore.loc[:, columnsForThisGroup], pltHandles = hf.fillInOverflow2(
  3050                                                                   tempLFPStore.loc[:, columnsForThisGroup].to_numpy(),
  3051                                                                   overFlowFillType='average',
  3052                                                                   overFlowThreshold=8000,
  3053                                                                   debuggingPlots=plotDevFilterDebug
  3054                                                                   )
  3055                                                               if plotDevFilterDebug:
  3056                                                                   pltHandles['ax'].set_title('ch grp {}'.format(subListIdx))
  3057                                                                   ddfAx3[subListIdx].plot(
  3058                                                                       dummyAsig.times[devFiltDebugMask],
  3059                                                                       tempLFPStore.loc[:, columnsForThisGroup].iloc[devFiltDebugMask, plotColIdx],
  3060                                                                       label='filled ch'
  3061                                                                       )
  3062                                                           # zscore of each trace
  3063                                                           if zScoreEachTrace:
  3064                                                               print('About to calculate zscore of each trace (along columns) for prelim outlier detection')
  3065                                                               columnZScore = pd.DataFrame(
  3066                                                                   stats.zscore(
  3067                                                                       tempLFPStore.loc[:, columnsForThisGroup],
  3068                                                                       axis=1),
  3069                                                                   index=tempLFPStore.index,
  3070                                                                   columns=columnsForThisGroup
  3071                                                                   )
  3072                                                               excludeFromMeanMask = columnZScore.abs() > 6
  3073                                                               if useMeanToCenter:
  3074                                                                   centerLFP[:, subListIdx] = (
  3075                                                                       tempLFPStore
  3076                                                                       .loc[:, columnsForThisGroup]
  3077                                                                       .mask(excludeFromMeanMask)
  3078                                                                       .mean(axis=1).to_numpy()
  3079                                                                       )
  3080                                                               else:
  3081                                                                   centerLFP[:, subListIdx] = (
  3082                                                                       tempLFPStore
  3083                                                                       .loc[:, columnsForThisGroup]
  3084                                                                       .mask(excludeFromMeanMask)
  3085                                                                       .median(axis=1).to_numpy()
  3086                                                                       )
  3087                                                           else:
  3088                                                               if useMeanToCenter:
  3089                                                                   centerLFP[:, subListIdx] = (
  3090                                                                       tempLFPStore
  3091                                                                       .loc[:, columnsForThisGroup]
  3092                                                                       .mean(axis=1).to_numpy()
  3093                                                                       )
  3094                                                               else:
  3095                                                                   centerLFP[:, subListIdx] = (
  3096                                                                       tempLFPStore
  3097                                                                       .loc[:, columnsForThisGroup]
  3098                                                                       .median(axis=1).to_numpy()
  3099                                                                       )
  3100                                                           if calcArtifactTrace:
  3101                                                               if LFPFilterOpts is not None:
  3102                                                                   print('applying LFPFilterOpts to cached asigs for artifact ID')
  3103                                                                   # tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfilt(
  3104                                                                   tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfiltfilt(
  3105                                                                       filterCoeffs, tempLFPStore.loc[:, columnsForThisGroup],
  3106                                                                       axis=0)
  3107                                                                   if useMeanToCenter:
  3108                                                                       tempCenter = (
  3109                                                                           tempLFPStore
  3110                                                                           .loc[:, columnsForThisGroup]
  3111                                                                           .mean(axis=1).diff().fillna(0)
  3112                                                                           )
  3113                                                                   else:
  3114                                                                       tempCenter = (
  3115                                                                           tempLFPStore
  3116                                                                           .loc[:, columnsForThisGroup]
  3117                                                                           .median(axis=1).diff().fillna(0)
  3118                                                                           )
  3119                                                               artifactSignal[:, subListIdx] = np.abs(stats.zscore(tempCenter.to_numpy()))
  3120                                                           if calcOutliers:
  3121                                                               if plotDevFilterDebug:
  3122                                                                   ddfAx[subListIdx].plot(
  3123                                                                       dummyAsig.times[devFiltDebugMask],
  3124                                                                       centerLFP[devFiltDebugMask, subListIdx],
  3125                                                                       label='mean of ch group'
  3126                                                                       )
  3127                                                               # filter the traces, if needed
  3128                                                               if LFPFilterOpts is not None:
  3129                                                                   print('applying LFPFilterOpts to cached asigs before outlier detection')
  3130                                                                   # tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfiltfilt(
  3131                                                                   tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfilt(
  3132                                                                       filterCoeffs, tempLFPStore.loc[:, columnsForThisGroup],
  3133                                                                       axis=0)
  3134                                                                   if plotDevFilterDebug:
  3135                                                                       ddfAx3[subListIdx].plot(
  3136                                                                           dummyAsig.times[devFiltDebugMask],
  3137                                                                           tempLFPStore.loc[:, columnsForThisGroup].iloc[devFiltDebugMask, plotColIdx],
  3138                                                                           label='filtered ch'
  3139                                                                           )
  3140                                                               ##################################
  3141                                                               print('Whitening cached traces before outlier detection')
  3142                                                               whitenByPCA = True
  3143                                                               if whitenByPCA:
  3144                                                                   projector = PCA(
  3145                                                                       n_components=None, whiten=True)
  3146                                                                   pcs = projector.fit_transform(
  3147                                                                       tempLFPStore.loc[:, columnsForThisGroup])
  3148                                                                   explVarMask = (
  3149                                                                       np.cumsum(projector.explained_variance_ratio_) < 1 - 1e-2)
  3150                                                                   explVarMask[0] = True  # (keep at least 1)
  3151                                                                   pcs = pcs[:, explVarMask]
  3152                                                                   nDim = pcs.shape[1]
  3153                                                                   lfpDeviation[:, subListIdx] = (pcs ** 2).sum(axis=1)
  3154                                                               else:  # whiten by mahalanobis distance
  3155                                                                   est = EmpiricalCovariance()
  3156                                                                   est.fit(tempLFPStore.loc[:, columnsForThisGroup].to_numpy())
  3157                                                                   lfpDeviation[:, subListIdx] = est.mahalanobis(
  3158                                                                       tempLFPStore.loc[:, columnsForThisGroup].to_numpy())
  3159                                                                   nDim = tempLFPStore.loc[:, columnsForThisGroup].shape[1]
  3160                                                               #
  3161                                                               transformedDeviation = stats.norm.isf(stats.chi2.sf(lfpDeviation[:, subListIdx], nDim))
  3162                                                               infMask = np.isinf(transformedDeviation)
  3163                                                               if infMask.any():
  3164                                                                   transformedDeviation[infMask] = transformedDeviation[~infMask].max()
  3165                                                               debugProbaTrans = False
  3166                                                               if debugProbaTrans:
  3167                                                                   fig, ax = plt.subplots()
  3168                                                                   tAx = ax.twinx()
  3169                                                                   plotMask = (dummyAsig.times >= 60 * pq.s) & (dummyAsig.times < 95 * pq.s)
  3170                                                                   ax.plot(dummyAsig.times[plotMask], transformedDeviation[plotMask], c='b', label='transformed deviation')
  3171                                                                   tAx.plot(dummyAsig.times[plotMask], lfpDeviation[plotMask, subListIdx], c='r', label='original deviation')
  3172                                                                   ax.legend(loc='upper left')
  3173                                                                   tAx.legend(loc='upper right')
  3174                                                                   plt.show()
  3175                                                               lfpDeviation[:, subListIdx] = transformedDeviation
  3176                                                               noveltyThreshold = stats.norm.interval(outlierThreshold)[1]
  3177                                                               # chi2Bounds = stats.chi2.interval(outlierThreshold, nDim)
  3178                                                               # lfpDeviation[:, subListIdx] = lfpDeviation[:, subListIdx] / chi2Bounds[1]
  3179                                                               # print('nDim = {}, chi2Lim = {}'.format(nDim, chi2Bounds))
  3180                                                               # noveltyThreshold = 1
  3181                                                               #
  3182                                                               outlierMetadata[subListIdx] = {
  3183                                                                   'nDim': nDim,
  3184                                                                   'noveltyThreshold': noveltyThreshold,
  3185                                                                   'outlierThreshold': outlierThreshold
  3186                                                                   }
  3187                                                               # smoothedDeviation = signal.sosfilt(
  3188                                                               print('Smoothing deviation')
  3189                                                               tempSmDev = signal.sosfiltfilt(
  3190                                                                   filterCoeffsOutlierMask, lfpDeviation[:, subListIdx])
  3191                                                               smoothedDeviation[:, subListIdx] = tempSmDev
  3192                                                               if plotDevFilterDebug:
  3193                                                                   ddfAx[subListIdx].plot(
  3194                                                                       dummyAsig.times[devFiltDebugMask],
  3195                                                                       lfpDeviation[devFiltDebugMask, subListIdx],
  3196                                                                       label='original deviation (ch grp {})'.format(subListIdx))
  3197                                                                   ddfAx[subListIdx].plot(
  3198                                                                       dummyAsig.times[devFiltDebugMask],
  3199                                                                       smoothedDeviation[devFiltDebugMask, subListIdx],
  3200                                                                       label='filtered deviation (ch grp {})'.format(subListIdx))
  3201                                                               ##
  3202                                                               print('Calculating outlier mask')
  3203                                                               outlierMask[:, subListIdx] = (
  3204                                                                   smoothedDeviation[:, subListIdx] > noveltyThreshold)
  3205                                                               if plotDevFilterDebug:
  3206                                                                   ddfAx[subListIdx].axhline(noveltyThreshold, c='r')
  3207                                                       if plotDevFilterDebug and calcOutliers:
  3208                                                           for subListIdx, subList in enumerate(asigNameList):
  3209                                                               ddfAx[subListIdx].legend(loc='upper right')
  3210                                                               ddfAx[subListIdx].set_title('Deviation')
  3211                                                               ddfAx3[subListIdx].legend(loc='upper right')
  3212                                                               ddfAx3[subListIdx].set_title('Example channel')
  3213                                                               ddfAx2.plot(
  3214                                                                   dummyAsig.times[devFiltDebugMask],
  3215                                                                   smoothedDeviation[devFiltDebugMask, subListIdx],
  3216                                                                   label='ch grp {}'.format(subListIdx))
  3217                                                               ddfAx2.set_title('Smoothed Deviation')
  3218                                                           ddfAx2.legend(loc='upper right')
  3219                                                           plt.show()
  3220                                                       #############
  3221                                                       del tempLFPStore
  3222                                                       gc.collect()
  3223                                               if (removeMeanAcross or calcAverageLFP):
  3224                                                   for mIdx, meanChIdx in enumerate(meanChIdxList):
  3225                                                       meanAsig = AnalogSignal(
  3226                                                           centerLFP[:, mIdx],
  3227                                                           units=dummyAsig.units,
  3228                                                           sampling_rate=dummyAsig.sampling_rate,
  3229                                                           # name='seg{}_{}'.format(idx, meanChIdx.name)
  3230                                                           name='seg{}_{}'.format(0, meanChIdx.name),
  3231                                                           t_start=tStart
  3232                                                       )
  3233                                                       # assign ownership to containers
  3234                                                       meanChIdx.analogsignals.append(meanAsig)
  3235                                                       newSeg.analogsignals.append(meanAsig)
  3236                                                       # assign parent to children
  3237                                                       meanChIdx.create_relationship()
  3238                                                       newSeg.create_relationship()
  3239                                                       # write out to file
  3240                                                       if LFPFilterOpts is not None:
  3241                                                           meanAsig[:] = filterFun(
  3242                                                               meanAsig, filterCoeffs=filterCoeffs)
  3243                                                       meanAsig = writer._write_analogsignal(
  3244                                                           meanAsig, nixblock, nixgroup)
  3245                                                   # if calcArtifactTrace:
  3246                                                   if True:
  3247                                                       for mIdx, artChIdx in enumerate(artChIdxList):
  3248                                                           artAsig = AnalogSignal(
  3249                                                               artifactSignal[:, mIdx],
  3250                                                               units=dummyAsig.units,
  3251                                                               sampling_rate=dummyAsig.sampling_rate,
  3252                                                               # name='seg{}_{}'.format(idx, devChIdx.name)
  3253                                                               name='seg{}_{}'.format(0, artChIdx.name),
  3254                                                               t_start=tStart
  3255                                                               )
  3256                                                           # assign ownership to containers
  3257                                                           artChIdx.analogsignals.append(artAsig)
  3258                                                           newSeg.analogsignals.append(artAsig)
  3259                                                           # assign parent to children
  3260                                                           artChIdx.create_relationship()
  3261                                                           newSeg.create_relationship()
  3262                                                           # write out to file
  3263                                                           artAsig = writer._write_analogsignal(
  3264                                                               artAsig, nixblock, nixgroup)
  3265                                                           #########################################################
  3266                                                   # if calcOutliers:
  3267                                                   if True:
  3268                                                       for mIdx, devChIdx in enumerate(devChIdxList):
  3269                                                           devAsig = AnalogSignal(
  3270                                                               lfpDeviation[:, mIdx],
  3271                                                               units=dummyAsig.units,
  3272                                                               sampling_rate=dummyAsig.sampling_rate,
  3273                                                               # name='seg{}_{}'.format(idx, devChIdx.name)
  3274                                                               name='seg{}_{}'.format(0, devChIdx.name),
  3275                                                               t_start=tStart
  3276                                                               )
  3277                                                           # assign ownership to containers
  3278                                                           devChIdx.analogsignals.append(devAsig)
  3279                                                           newSeg.analogsignals.append(devAsig)
  3280                                                           # assign parent to children
  3281                                                           devChIdx.create_relationship()
  3282                                                           newSeg.create_relationship()
  3283                                                           # write out to file
  3284                                                           devAsig = writer._write_analogsignal(
  3285                                                               devAsig, nixblock, nixgroup)
  3286                                                           #########################################################
  3287                                                       for mIdx, smDevChIdx in enumerate(smDevChIdxList):
  3288                                                           smDevAsig = AnalogSignal(
  3289                                                               smoothedDeviation[:, mIdx],
  3290                                                               units=dummyAsig.units,
  3291                                                               sampling_rate=dummyAsig.sampling_rate,
  3292                                                               # name='seg{}_{}'.format(idx, devChIdx.name)
  3293                                                               name='seg{}_{}'.format(0, smDevChIdx.name),
  3294                                                               t_start=tStart
  3295                                                               )
  3296                                                           # assign ownership to containers
  3297                                                           smDevChIdx.analogsignals.append(smDevAsig)
  3298                                                           newSeg.analogsignals.append(smDevAsig)
  3299                                                           # assign parent to children
  3300                                                           smDevChIdx.create_relationship()
  3301                                                           newSeg.create_relationship()
  3302                                                           # write out to file
  3303                                                           smDevAsig = writer._write_analogsignal(
  3304                                                               smDevAsig, nixblock, nixgroup)
  3305                                                           #########################################################
  3306                                                       for mIdx, outMaskChIdx in enumerate(outMaskChIdxList):
  3307                                                           outMaskAsig = AnalogSignal(
  3308                                                               outlierMask[:, mIdx],
  3309                                                               units=dummyAsig.units,
  3310                                                               sampling_rate=dummyAsig.sampling_rate,
  3311                                                               # name='seg{}_{}'.format(idx, outMaskChIdx.name)
  3312                                                               name='seg{}_{}'.format(0, outMaskChIdx.name),
  3313                                                               t_start=tStart, dtype=np.float32
  3314                                                               )
  3315                                                           outMaskAsig.annotations['outlierProportion'] = np.mean(outlierMask[:, mIdx])
  3316                                                           if calcOutliers:
  3317                                                               outMaskAsig.annotations.update(outlierMetadata[mIdx])
  3318                                                           # assign ownership to containers
  3319                                                           outMaskChIdx.analogsignals.append(outMaskAsig)
  3320                                                           newSeg.analogsignals.append(outMaskAsig)
  3321                                                           # assign parent to children
  3322                                                           outMaskChIdx.create_relationship()
  3323                                                           newSeg.create_relationship()
  3324                                                           # write out to file
  3325                                                           outMaskAsig = writer._write_analogsignal(
  3326                                                               outMaskAsig, nixblock, nixgroup)
  3327                                                   #
  3328                                                   w0 = 60
  3329                                                   bandQ = 20
  3330                                                   bw = w0/bandQ
  3331                                                   noiseSos = signal.iirfilter(
  3332                                                       N=8, Wn=[w0 - bw/2, w0 + bw/2],
  3333                                                       btype='band', ftype='butter',
  3334                                                       analog=False, fs=float(dummyAsig.sampling_rate),
  3335                                                       output='sos')
  3336                                                   # signal.hilbert does not have an option to zero pad
  3337                                                   nextLen = fftpack.helper.next_fast_len(dummyAsig.shape[0])
  3338                                                   deficit = int(nextLen - dummyAsig.shape[0])
  3339                                                   lDef = int(np.floor(deficit / 2))
  3340                                                   rDef = int(np.ceil(deficit / 2)) + 1
  3341                                                   temp = np.pad(
  3342                                                       dummyAsig.magnitude.flatten(),
  3343                                                       (lDef, rDef), mode='constant')
  3344                                                   # lineNoise = signal.sosfiltfilt(
  3345                                                   lineNoise = signal.sosfilt(
  3346                                                       noiseSos, temp, axis=0)
  3347                                                   lineNoiseH = signal.hilbert(lineNoise)
  3348                                                   lineNoise = lineNoise[lDef:-rDef]
  3349                                                   lineNoiseH = lineNoiseH[lDef:-rDef]
  3350                                                   lineNoisePhase = np.angle(lineNoiseH)
  3351                                                   lineNoisePhaseDF = pd.DataFrame(
  3352                                                       lineNoisePhase,
  3353                                                       index=dummyAsig.times,
  3354                                                       columns=['phase']
  3355                                                       )
  3356                                                   plotHilbert = False
  3357                                                   if plotHilbert:
  3358                                                       lineNoiseFreq = (
  3359                                                           np.diff(np.unwrap(lineNoisePhase)) /
  3360                                                           (2.0*np.pi) * float(dummyAsig.sampling_rate))
  3361                                                       lineNoiseEnvelope = np.abs(lineNoiseH)
  3362                                                       i1 = 300000; i2 = 330000
  3363                                                       fig, ax = plt.subplots(2, 1, sharex=True)
  3364                                                       ax[0].plot(dummyAsig.times[devFiltDebugMask], dummyAsig.magnitude[devFiltDebugMask, :])
  3365                                                       ax[0].plot(dummyAsig.times[devFiltDebugMask], lineNoise[devFiltDebugMask])
  3366                                                       ax[0].plot(dummyAsig.times[devFiltDebugMask], lineNoiseEnvelope[devFiltDebugMask])
  3367                                                       axFr = ax[1].twinx()
  3368                                                       ax[1].plot(
  3369                                                           dummyAsig.times[devFiltDebugMask], lineNoisePhase[devFiltDebugMask],
  3370                                                           c='r', label='phase')
  3371                                                       ax[1].legend()
  3372                                                       axFr.plot(
  3373                                                           dummyAsig.times[devFiltDebugMask], lineNoiseFreq[devFiltDebugMask],
  3374                                                           label='freq')
  3375                                                       axFr.set_ylim([59, 61])
  3376                                                       axFr.legend()
  3377                                                       plt.show()
  3378                                               # second pass through asigs, to save
  3379                                               for aSigIdx, aSigProxy in enumerate(seg.analogsignals):
  3380                                                   if aSigIdx == 0:
  3381                                                       # check bounds
  3382                                                       tStart = max(chunkTStart * pq.s, aSigProxy.t_start)
  3383                                                       tStop = min(chunkTStop * pq.s, aSigProxy.t_stop)
  3384                                                   loadThisOne = (
  3385                                                       (saveFromAsigNameList and (aSigProxy in aSigList)) or
  3386                                                       (aSigProxy in ainpList)
  3387                                                       )
  3388                                                   if loadThisOne:
  3389                                                       if trackMemory:
  3390                                                           print('writing asig {} ({}) memory usage: {:.1f} MB'.format(
  3391                                                               aSigIdx, aSigProxy.name, prf.memory_usage_psutil()))
  3392                                                       chanIdx = aSigProxy.channel_index
  3393                                                       asig = aSigProxy.load(
  3394                                                           time_slice=(tStart, tStop),
  3395                                                           magnitude_mode='rescaled')
  3396                                                       #  link AnalogSignal and ID providing channel_index
  3397                                                       asig.channel_index = chanIdx
  3398                                                       #  perform requested preproc operations
  3399                                                       if 'impedances' in locals():
  3400                                                           elNmMatchMsk = impedances['elec'] == chanIdx.name
  3401                                                           if elNmMatchMsk.any():
  3402                                                               originalImpedance = np.min(
  3403                                                                   impedances.loc[elNmMatchMsk, 'impedance']
  3404                                                                   )
  3405                                                               asig.annotations['originalImpedance'] = originalImpedance
  3406                                                               if normalizeByImpedance and (aSigProxy not in ainpList):
  3407                                                                   '''
  3408                                                                   asig.magnitude[:] = (
  3409                                                                       (asig.magnitude - np.median(asig.magnitude)) /
  3410                                                                       np.min(
  3411                                                                           impedances.loc[elNmMatchMsk, 'impedance']
  3412                                                                           )
  3413                                                                       )
  3414                                                                   '''
  3415                                                                   print('Normalizing {} by {} kOhms'.format(asig.name, originalImpedance))
  3416                                                                   asig.magnitude[:] = (
  3417                                                                       (asig.magnitude * averageImpedance) / originalImpedance
  3418                                                                       )
  3419                                                       if fillOverflow:
  3420                                                           # fill in overflow:
  3421                                                           asig.magnitude[:], _ = hf.fillInOverflow2(
  3422                                                               asig.magnitude[:],
  3423                                                               overFlowFillType='average',
  3424                                                               overFlowThreshold=8000,
  3425                                                               debuggingPlots=False
  3426                                                               )
  3427                                                       if removeJumps:
  3428                                                           # find unusual jumps in derivative or amplitude
  3429                                                           '''
  3430                                                           timeSection['data'], newBadData = hf.fillInJumps(timeSection['data'],
  3431                                                           timeSection['samp_per_s'], smoothing_ms = 0.5, nStdDiff = 50,
  3432                                                           nStdAmp = 100)
  3433                                                           badData.update(newBadData)
  3434                                                           '''
  3435                                                           pass
  3436                                                       if calcAverageLFP and (aSigProxy not in ainpList):
  3437                                                           for k, cols in meanGroups.items():
  3438                                                               if asig.name in cols:
  3439                                                                   whichColumnToSubtract = k
  3440                                                           noiseModel = np.polyfit(
  3441                                                               centerLFP[:, whichColumnToSubtract],
  3442                                                               asig.magnitude.flatten(), 1, full=True)
  3443                                                           rSq = 1 - noiseModel[1][0] / np.sum(asig.magnitude.flatten() ** 2)
  3444                                                           asig.annotations['mean_removal_r2'] = rSq
  3445                                                           asig.annotations['mean_removal_group'] = whichColumnToSubtract
  3446                                                           if linearDetrend:
  3447                                                               noiseTerm = np.polyval(
  3448                                                                   noiseModel[0],
  3449                                                                   centerLFP[:, whichColumnToSubtract])
  3450                                                           else:
  3451                                                               noiseTerm = centerLFP[:, whichColumnToSubtract]
  3452                                                           ###
  3453                                                           plotMeanSubtraction = False
  3454                                                           if plotMeanSubtraction:
  3455                                                               i1 = 300000; i2 = 330000
  3456                                                               fig, ax = plt.subplots(1, 1)
  3457                                                               ax.plot(asig.times[devFiltDebugMask], asig.magnitude[devFiltDebugMask, :], label='channel')
  3458                                                               ax.plot(asig.times[devFiltDebugMask], centerLFP[devFiltDebugMask, whichColumnToSubtract], label='mean')
  3459                                                               ax.plot(asig.times[devFiltDebugMask], noiseTerm[devFiltDebugMask], label='adjusted mean')
  3460                                                               ax.legend()
  3461                                                               plt.show()
  3462                                                           ###
  3463                                                           if removeMeanAcross:
  3464                                                               asig.magnitude[:] = np.atleast_2d(
  3465                                                                   asig.magnitude.flatten() - noiseTerm).transpose()
  3466                                                               # asig.magnitude[:] = (
  3467                                                               #     asig.magnitude - np.median(asig.magnitude))
  3468                                                       if (LFPFilterOpts is not None) and (aSigProxy not in ainpList):
  3469                                                           asig.magnitude[:] = filterFun(asig, filterCoeffs=filterCoeffs)
  3470                                                       if (interpolateOutliers) and (aSigProxy not in ainpList) and (not outlierRemovalDebugFlag):
  3471                                                           for k, cols in meanGroups.items():
  3472                                                               if asig.name in cols:
  3473                                                                   whichColumnToSubtract = k
  3474                                                           tempSer = pd.Series(asig.magnitude.flatten())
  3475                                                           tempSer.loc[outlierMask[:, whichColumnToSubtract]] = np.nan
  3476                                                           tempSer = (
  3477                                                               tempSer
  3478                                                               .interpolate(method='linear', limit_area='inside')
  3479                                                               .fillna(method='ffill')
  3480                                                               .fillna(method='bfill')
  3481                                                               )
  3482                                                           asig.magnitude[:, 0] = tempSer.to_numpy()
  3483                                                       # pdb.set_trace()
  3484                                                       if (aSigProxy in aSigList) or (aSigProxy in ainpList):
  3485                                                           # assign ownership to containers
  3486                                                           chanIdx.analogsignals.append(asig)
  3487                                                           newSeg.analogsignals.append(asig)
  3488                                                           # assign parent to children
  3489                                                           chanIdx.create_relationship()
  3490                                                           newSeg.create_relationship()
  3491                                                           # write out to file
  3492                                                           asig = writer._write_analogsignal(
  3493                                                               asig, nixblock, nixgroup)
  3494                                                       del asig
  3495                                                       gc.collect()
  3496                                               for irSigIdx, irSigProxy in enumerate(
  3497                                                       seg.irregularlysampledsignals):
  3498                                                   chanIdx = irSigProxy.channel_index
  3499                                                   #
  3500                                                   isig = irSigProxy.load(
  3501                                                       time_slice=(tStart, tStop),
  3502                                                       magnitude_mode='rescaled')
  3503                                                   #  link irregularlysampledSignal
  3504                                                   #  and ID providing channel_index
  3505                                                   isig.channel_index = chanIdx
  3506                                                   # assign ownership to containers
  3507                                                   chanIdx.irregularlysampledsignals.append(isig)
  3508                                                   newSeg.irregularlysampledsignals.append(isig)
  3509                                                   # assign parent to children
  3510                                                   chanIdx.create_relationship()
  3511                                                   newSeg.create_relationship()
  3512                                                   # write out to file
  3513                                                   isig = writer._write_irregularlysampledsignal(
  3514                                                       isig, nixblock, nixgroup)
  3515                                                   del isig
  3516                                                   gc.collect()
  3517                                               #
  3518                                               if len(spikeSourceType):
  3519                                                   for stIdx, stProxy in enumerate(spikeSeg.spiketrains):
  3520                                                       if trackMemory:
  3521                                                           print('writing spiketrains mem usage: {}'.format(
  3522                                                               prf.memory_usage_psutil()))
  3523                                                       unit = stProxy.unit
  3524                                                       st = loadStProxy(stProxy)
  3525                                                       #  have to manually slice tStop and tStart because
  3526                                                       #  array annotations are not saved natively in the nix file
  3527                                                       #  (we're getting them as plain annotations)
  3528                                                       timeMask = np.asarray(
  3529                                                           (st.times >= tStart) & (st.times < tStop),
  3530                                                           dtype=np.bool)
  3531                                                       try:
  3532                                                           if 'arrayAnnNames' in st.annotations:
  3533                                                               for key in st.annotations['arrayAnnNames']:
  3534                                                                   st.annotations[key] = np.asarray(
  3535                                                                       st.annotations[key])[timeMask]
  3536                                                           st = st[timeMask]
  3537                                                           st.t_start = tStart
  3538                                                           st.t_stop = tStop
  3539                                                       except Exception:
  3540                                                           traceback.print_exc()
  3541                                                       #  tdc may or may not have the same channel ids, but
  3542                                                       #  it will have consistent channel names
  3543                                                       nameParser = re.search(
  3544                                                           r'([a-zA-Z0-9]*)#(\d*)', unit.name)
  3545                                                       chanLabel = nameParser.group(1)
  3546                                                       unitId = nameParser.group(2)
  3547                                                       #
  3548                                                       chIdxName = unit.name.replace('_stim', '').split('#')[0]
  3549                                                       chanIdx = block.filter(objects=ChannelIndex, name=chIdxName)[0]
  3550                                                       # [i.name for i in block.filter(objects=ChannelIndex)]
  3551                                                       # [i.name for i in spikeBlock.filter(objects=Unit)]
  3552                                                       #  print(unit.name)
  3553                                                       if not (unit in chanIdx.units):
  3554                                                           # first time at this unit, add to its chanIdx
  3555                                                           unit.channel_index = chanIdx
  3556                                                           chanIdx.units.append(unit)
  3557                                                       #  except Exception:
  3558                                                       #      traceback.print_exc()
  3559                                                       st.name = 'seg{}_{}'.format(0, unit.name)
  3560                                                       # st.name = 'seg{}_{}'.format(idx, unit.name)
  3561                                                       #  link SpikeTrain and ID providing unit
  3562                                                       if calcAverageLFP:
  3563                                                           if 'arrayAnnNames' in st.annotations:
  3564                                                               st.annotations['arrayAnnNames'] = list(st.annotations['arrayAnnNames'])
  3565                                                           else:
  3566                                                               st.annotations['arrayAnnNames'] = []
  3567                                                           st.annotations['arrayAnnNames'].append('phase60hz')
  3568                                                           phase60hz = hf.interpolateDF(
  3569                                                               lineNoisePhaseDF,
  3570                                                               newX=st.times, columns=['phase']).to_numpy().flatten()
  3571                                                           st.annotations.update({'phase60hz': phase60hz})
  3572                                                           plotPhaseDist = False
  3573                                                           if plotPhaseDist:
  3574                                                               sns.distplot(phase60hz)
  3575                                                               plt.show()
  3576                                                       st.unit = unit
  3577                                                       # assign ownership to containers
  3578                                                       unit.spiketrains.append(st)
  3579                                                       newSeg.spiketrains.append(st)
  3580                                                       # assign parent to children
  3581                                                       unit.create_relationship()
  3582                                                       newSeg.create_relationship()
  3583                                                       # write out to file
  3584                                                       st = writer._write_spiketrain(st, nixblock, nixgroup)
  3585                                                       del st
  3586                                               #  process proprio trial related events
  3587                                               if calcRigEvents:
  3588                                                   print('Processing rig events...')
  3589                                                   analogData = []
  3590                                                   for key, value in eventInfo['inputIDs'].items():
  3591                                                       searchName = 'seg{}_'.format(0) + value
  3592                                                       ainpAsig = seg.filter(
  3593                                                           objects=AnalogSignalProxy,
  3594                                                           name=searchName)[0]
  3595                                                       ainpData = ainpAsig.load(
  3596                                                           time_slice=(tStart, tStop),
  3597                                                           magnitude_mode='rescaled')
  3598                                                       analogData.append(
  3599                                                           pd.DataFrame(ainpData.magnitude, columns=[key]))
  3600                                                       del ainpData
  3601                                                       gc.collect()
  3602                                                   motorData = pd.concat(analogData, axis=1)
  3603                                                   del analogData
  3604                                                   gc.collect()
  3605                                                   if motorEncoderMask is not None:
  3606                                                       ainpData = ainpAsig.load(
  3607                                                           time_slice=(tStart, tStop),
  3608                                                           magnitude_mode='rescaled')
  3609                                                       ainpTime = ainpData.times.magnitude
  3610                                                       meTimeMask = np.zeros_like(ainpTime, dtype=np.bool)
  3611                                                       for meTimeBounds in motorEncoderMask:
  3612                                                           meTimeMask = (
  3613                                                               meTimeMask |
  3614                                                               (
  3615                                                                   (ainpTime > meTimeBounds[0]) &
  3616                                                                   (ainpTime < meTimeBounds[1])
  3617                                                                   )
  3618                                                               )
  3619                                                       columnsToOverride = ['A-', 'A+', 'B-', 'B+', 'Z-', 'Z+']
  3620                                                       for colName in columnsToOverride:
  3621                                                           motorData.loc[~meTimeMask, colName] = motorData.loc[:, colName].quantile(q=0.05)
  3622                                                       del ainpData, ainpTime
  3623                                                       gc.collect()
  3624                                                   motorData = mea.processMotorData(
  3625                                                       motorData, ainpAsig.sampling_rate.magnitude,
  3626                                                       encoderCountPerDegree=encoderCountPerDegree
  3627                                                       )
  3628                                                   keepCols = [
  3629                                                       'position', 'velocity', 'velocityCat',
  3630                                                       'rightBut_int', 'leftBut_int',
  3631                                                       'rightLED_int', 'leftLED_int', 'simiTrigs_int']
  3632                                                   for colName in keepCols:
  3633                                                       if trackMemory:
  3634                                                           print('writing motorData memory usage: {:.1f} MB'.format(
  3635                                                               prf.memory_usage_psutil()))
  3636                                                       chanIdx = ChannelIndex(
  3637                                                           name=colName,
  3638                                                           index=np.asarray([0]),
  3639                                                           channel_names=np.asarray([0]))
  3640                                                       block.channel_indexes.append(chanIdx)
  3641                                                       motorAsig = AnalogSignal(
  3642                                                           motorData[colName].to_numpy() * pq.mV,
  3643                                                           name=colName,
  3644                                                           sampling_rate=ainpAsig.sampling_rate,
  3645                                                           dtype=np.float32)
  3646                                                       motorAsig.t_start = ainpAsig.t_start
  3647                                                       motorAsig.channel_index = chanIdx
  3648                                                       # assign ownership to containers
  3649                                                       chanIdx.analogsignals.append(motorAsig)
  3650                                                       newSeg.analogsignals.append(motorAsig)
  3651                                                       chanIdx.create_relationship()
  3652                                                       newSeg.create_relationship()
  3653                                                       # write out to file
  3654                                                       motorAsig = writer._write_analogsignal(
  3655                                                           motorAsig, nixblock, nixgroup)
  3656                                                       del motorAsig
  3657                                                       gc.collect()
  3658                                                   _, trialEvents = mea.getTrials(
  3659                                                       motorData, ainpAsig.sampling_rate.magnitude,
  3660                                                       float(tStart.magnitude), trialType=None)
  3661                                                   trialEvents.fillna(0)
  3662                                                   trialEvents.rename(
  3663                                                       columns={
  3664                                                           'Label': 'rig_property',
  3665                                                           'Details': 'rig_value'},
  3666                                                       inplace=True)
  3667                                                   del motorData
  3668                                                   gc.collect()
  3669                                                   eventList = eventDataFrameToEvents(
  3670                                                       trialEvents,
  3671                                                       idxT='Time',
  3672                                                       annCol=['rig_property', 'rig_value'])
  3673                                                   for event in eventList:
  3674                                                       if trackMemory:
  3675                                                           print(
  3676                                                               'writing motor events memory usage: {:.1f} MB'
  3677                                                               .format(prf.memory_usage_psutil()))
  3678                                                       event.segment = newSeg
  3679                                                       newSeg.events.append(event)
  3680                                                       newSeg.create_relationship()
  3681                                                       # write out to file
  3682                                                       event = writer._write_event(event, nixblock, nixgroup)
  3683                                                       del event
  3684                                                       gc.collect()
  3685                                                   del trialEvents, eventList
  3686                                               #
  3687                                               for eventProxy in seg.events:
  3688                                                   event = eventProxy.load(
  3689                                                       time_slice=(tStart, tStop))
  3690                                                   event.t_start = tStart
  3691                                                   event.t_stop = tStop
  3692                                                   event.segment = newSeg
  3693                                                   newSeg.events.append(event)
  3694                                                   newSeg.create_relationship()
  3695                                                   # write out to file
  3696                                                   event = writer._write_event(event, nixblock, nixgroup)
  3697                                                   del event
  3698                                                   gc.collect()
  3699                                               #
  3700                                               for epochProxy in seg.epochs:
  3701                                                   epoch = epochProxy.load(
  3702                                                       time_slice=(tStart, tStop))
  3703                                                   epoch.t_start = tStart
  3704                                                   epoch.t_stop = tStop
  3705                                                   epoch.segment = newSeg
  3706                                                   newSeg.events.append(epoch)
  3707                                                   newSeg.create_relationship()
  3708                                                   # write out to file
  3709                                                   epoch = writer._write_epoch(epoch, nixblock, nixgroup)
  3710                                                   del epoch
  3711                                                   gc.collect()
  3712                                               #
  3713                                               chanIdxDiscardNames = []
  3714                                               # descend into ChannelIndexes
  3715                                               for chanIdx in block.channel_indexes:
  3716                                                   if chanIdx.analogsignals or chanIdx.units:
  3717                                                       chanIdx = writer._write_channelindex(chanIdx, nixblock)
  3718                                                   else:
  3719                                                       chanIdxDiscardNames.append(chanIdx.name)
  3720                                               block.channel_indexes = [
  3721                                                   i
  3722                                                   for i in block.channel_indexes
  3723                                                   if i.name not in chanIdxDiscardNames
  3724                                                   ]
  3725                                               writer._create_source_links(block, nixblock)
  3726                                               return

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: purgeNixAnn at line 3728

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3728                                           @profile
  3729                                           def purgeNixAnn(
  3730                                                   block, annNames=['nix_name', 'neo_name']):
  3731                                               for annName in annNames:
  3732                                                   block.annotations.pop(annName, None)
  3733                                               for child in block.children_recur:
  3734                                                   if child.annotations:
  3735                                                       child.annotations = {
  3736                                                           k: v
  3737                                                           for k, v in child.annotations.items()
  3738                                                           if k not in annNames}
  3739                                               for child in block.data_children_recur:
  3740                                                   if child.annotations:
  3741                                                       child.annotations = {
  3742                                                           k: v
  3743                                                           for k, v in child.annotations.items()
  3744                                                           if k not in annNames}
  3745                                               return block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadContainerArrayAnn at line 3747

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3747                                           @profile
  3748                                           def loadContainerArrayAnn(
  3749                                                   container=None, trainList=None
  3750                                                   ):
  3751                                               assert (container is not None) or (trainList is not None)
  3752                                               #
  3753                                               spikesAndEvents = []
  3754                                               returnObj = []
  3755                                               if container is not None:
  3756                                                   #  need the line below! (RD: don't remember why, consider removing)
  3757                                                   container.create_relationship()
  3758                                                   #
  3759                                                   spikesAndEvents += (
  3760                                                       container.filter(objects=SpikeTrain) +
  3761                                                       container.filter(objects=Event)
  3762                                                       )
  3763                                                   returnObj.append(container)
  3764                                               if trainList is not None:
  3765                                                   spikesAndEvents += trainList
  3766                                                   returnObj.append(trainList)
  3767                                               #
  3768                                               if len(returnObj) == 1:
  3769                                                   returnObj = returnObj[0]
  3770                                               else:
  3771                                                   returnObj = tuple(returnObj)
  3772                                               #
  3773                                               for st in spikesAndEvents:
  3774                                                   st = loadObjArrayAnn(st)
  3775                                               return returnObj

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadObjArrayAnn at line 3777

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3777                                           @profile
  3778                                           def loadObjArrayAnn(st):
  3779                                               if 'arrayAnnNames' in st.annotations.keys():
  3780                                                   if isinstance(st.annotations['arrayAnnNames'], str):
  3781                                                       st.annotations['arrayAnnNames'] = [st.annotations['arrayAnnNames']]
  3782                                                   elif isinstance(st.annotations['arrayAnnNames'], tuple):
  3783                                                       st.annotations['arrayAnnNames'] = [i for i in st.annotations['arrayAnnNames']]
  3784                                                   #
  3785                                                   for key in st.annotations['arrayAnnNames']:
  3786                                                       #  fromRaw, the ann come back as tuple, need to recast
  3787                                                       try:
  3788                                                           if len(st.times) == 1:
  3789                                                               st.annotations[key] = np.atleast_1d(st.annotations[key]).flatten()
  3790                                                           st.array_annotations.update(
  3791                                                               {key: np.asarray(st.annotations[key])})
  3792                                                           st.annotations[key] = np.asarray(st.annotations[key])
  3793                                                       except Exception:
  3794                                                           print('Error with {}'.format(st.name))
  3795                                                           traceback.print_exc()
  3796                                                           pdb.set_trace()
  3797                                               if hasattr(st, 'waveforms'):
  3798                                                   if st.waveforms is None:
  3799                                                       st.waveforms = np.asarray([]).reshape((0, 0, 0)) * pq.mV
  3800                                                   elif not len(st.waveforms):
  3801                                                       st.waveforms = np.asarray([]).reshape((0, 0, 0)) * pq.mV
  3802                                               return st

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadWithArrayAnn at line 3804

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3804                                           @profile
  3805                                           def loadWithArrayAnn(
  3806                                                   dataPath, fromRaw=False,
  3807                                                   mapDF=None, reduceChannelIndexes=False):
  3808                                               if fromRaw:
  3809                                                   reader = nixio_fr.NixIO(filename=dataPath)
  3810                                                   block = readBlockFixNames(
  3811                                                       reader, lazy=False,
  3812                                                       mapDF=mapDF,
  3813                                                       reduceChannelIndexes=reduceChannelIndexes)
  3814                                               else:
  3815                                                   reader = NixIO(filename=dataPath)
  3816                                                   block = reader.read_block()
  3817                                                   # [un.name for un in block.filter(objects=Unit)]
  3818                                                   # [len(un.spiketrains) for un in block.filter(objects=Unit)]
  3819                                               
  3820                                               block = loadContainerArrayAnn(container=block)
  3821                                               
  3822                                               if fromRaw:
  3823                                                   reader.file.close()
  3824                                               else:
  3825                                                   reader.close()
  3826                                               return block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: blockFromPath at line 3828

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3828                                           @profile
  3829                                           def blockFromPath(
  3830                                                   dataPath, lazy=False, mapDF=None,
  3831                                                   reduceChannelIndexes=False, loadList=None,
  3832                                                   purgeNixNames=False, chunkingInfoPath=None):
  3833                                               chunkingMetadata = None
  3834                                               if chunkingInfoPath is not None:
  3835                                                   if os.path.exists(chunkingInfoPath):
  3836                                                       with open(chunkingInfoPath, 'r') as f:
  3837                                                           chunkingMetadata = json.load(f)
  3838                                               if chunkingMetadata is None:
  3839                                                   chunkingMetadata = {
  3840                                                       '0': {
  3841                                                           'filename': dataPath,
  3842                                                           'partNameSuffix': '',
  3843                                                           'chunkTStart': 0,
  3844                                                           'chunkTStop': 'NaN'
  3845                                                       }}
  3846                                               for idx, (chunkIdxStr, chunkMeta) in enumerate(chunkingMetadata.items()):   
  3847                                                   thisDataPath = chunkMeta['filename']
  3848                                                   assert os.path.exists(thisDataPath)
  3849                                                   if idx == 0:
  3850                                                       if lazy:
  3851                                                           dataReader = nixio_fr.NixIO(
  3852                                                               filename=thisDataPath)
  3853                                                           dataBlock = readBlockFixNames(
  3854                                                               dataReader, lazy=lazy, mapDF=mapDF,
  3855                                                               reduceChannelIndexes=reduceChannelIndexes,
  3856                                                               purgeNixNames=purgeNixNames, loadList=loadList)
  3857                                                       else:
  3858                                                           dataReader = None
  3859                                                           dataBlock = loadWithArrayAnn(thisDataPath)
  3860                                                   else:
  3861                                                       if lazy:
  3862                                                           dataReader2 = nixio_fr.NixIO(
  3863                                                               filename=thisDataPath)
  3864                                                           dataBlock2 = readBlockFixNames(
  3865                                                               dataReader2, lazy=lazy, mapDF=mapDF,
  3866                                                               reduceChannelIndexes=reduceChannelIndexes, loadList=loadList)
  3867                                                       else:
  3868                                                           dataReader2 = None
  3869                                                           dataBlock2 = loadWithArrayAnn(thisDataPath)
  3870                                                       maxSegIdx = len(dataBlock.segments)
  3871                                                       typesNeedRenaming = [
  3872                                                           SpikeTrainProxy, AnalogSignalProxy, EventProxy,
  3873                                                           SpikeTrain, AnalogSignal, Event]
  3874                                                       for segIdx, seg in enumerate(dataBlock2.segments):
  3875                                                           if seg.name is None:
  3876                                                               seg.name = 'seg{}_'.format(maxSegIdx + segIdx)
  3877                                                           else:
  3878                                                               if 'seg{}_'.format(maxSegIdx + segIdx) not in seg.name:
  3879                                                                   seg.name = (
  3880                                                                       'seg{}_{}'
  3881                                                                       .format(
  3882                                                                           maxSegIdx + segIdx,
  3883                                                                           childBaseName(seg.name, 'seg')))
  3884                                                           for objType in typesNeedRenaming:
  3885                                                               for child in seg.filter(objects=objType):
  3886                                                                   if 'seg{}_'.format(maxSegIdx + segIdx) not in child.name:
  3887                                                                       child.name = (
  3888                                                                           'seg{}_{}'
  3889                                                                           .format(
  3890                                                                               maxSegIdx + segIdx, childBaseName(child.name, 'seg')))
  3891                                                       dataBlock.merge(dataBlock2)
  3892                                               return dataReader, dataBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: calcBinarizedArray at line 3894

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3894                                           @profile
  3895                                           def calcBinarizedArray(
  3896                                                   dataBlock, samplingRate,
  3897                                                   binnedSpikePath=None,
  3898                                                   saveToFile=True, matchT=None):
  3899                                               #
  3900                                               spikeMatBlock = Block(name=dataBlock.name + '_binarized')
  3901                                               spikeMatBlock.merge_annotations(dataBlock)
  3902                                               #
  3903                                               allSpikeTrains = [
  3904                                                   i for i in dataBlock.filter(objects=SpikeTrain)]
  3905                                               #
  3906                                               for st in allSpikeTrains:
  3907                                                   chanList = spikeMatBlock.filter(
  3908                                                       objects=ChannelIndex, name=st.unit.name)
  3909                                                   if not len(chanList):
  3910                                                       chanIdx = ChannelIndex(name=st.unit.name, index=np.asarray([0]))
  3911                                                       #  print(chanIdx.name)
  3912                                                       spikeMatBlock.channel_indexes.append(chanIdx)
  3913                                                       thisUnit = Unit(name=st.unit.name)
  3914                                                       chanIdx.units.append(thisUnit)
  3915                                                       thisUnit.channel_index = chanIdx
  3916                                               #
  3917                                               for segIdx, seg in enumerate(dataBlock.segments):
  3918                                                   newSeg = Segment(name='seg{}_{}'.format(segIdx, spikeMatBlock.name))
  3919                                                   newSeg.merge_annotations(seg)
  3920                                                   spikeMatBlock.segments.append(newSeg)
  3921                                                   #  tStart = dataBlock.segments[0].t_start
  3922                                                   #  tStop = dataBlock.segments[0].t_stop
  3923                                                   tStart = seg.t_start
  3924                                                   tStop = seg.t_stop
  3925                                                   # make dummy binary spike train, in case ths chan didn't fire
  3926                                                   segSpikeTrains = [
  3927                                                       i for i in seg.filter(objects=SpikeTrain) if '#' in i.name]
  3928                                                   dummyBin = binarize(
  3929                                                       segSpikeTrains[0],
  3930                                                       sampling_rate=samplingRate,
  3931                                                       t_start=tStart,
  3932                                                       t_stop=tStop + samplingRate ** -1) * 0
  3933                                                   for chanIdx in spikeMatBlock.channel_indexes:
  3934                                                       #  print(chanIdx.name)
  3935                                                       stList = seg.filter(
  3936                                                           objects=SpikeTrain,
  3937                                                           name='seg{}_{}'.format(segIdx, chanIdx.name)
  3938                                                           )
  3939                                                       if len(stList):
  3940                                                           st = stList[0]
  3941                                                           print('binarizing {}'.format(st.name))
  3942                                                           stBin = binarize(
  3943                                                               st,
  3944                                                               sampling_rate=samplingRate,
  3945                                                               t_start=tStart,
  3946                                                               t_stop=tStop + samplingRate ** -1)
  3947                                                           spikeMatBlock.segments[segIdx].spiketrains.append(st)
  3948                                                           #  to do: link st to spikematblock's chidx and units
  3949                                                           assert len(chanIdx.filter(objects=Unit)) == 1
  3950                                                           thisUnit = chanIdx.filter(objects=Unit)[0]
  3951                                                           thisUnit.spiketrains.append(st)
  3952                                                           st.unit = thisUnit
  3953                                                           st.segment = spikeMatBlock.segments[segIdx]
  3954                                                       else:
  3955                                                           print('{} has no spikes'.format(st.name))
  3956                                                           stBin = dummyBin
  3957                                                       skipStAnnNames = [
  3958                                                           'nix_name', 'neo_name', 'arrayAnnNames']
  3959                                                       if 'arrayAnnNames' in st.annotations:
  3960                                                           skipStAnnNames += list(st.annotations['arrayAnnNames'])
  3961                                                       asigAnn = {
  3962                                                           k: v
  3963                                                           for k, v in st.annotations.items()
  3964                                                           if k not in skipStAnnNames
  3965                                                           }
  3966                                                       asig = AnalogSignal(
  3967                                                           stBin * samplingRate,
  3968                                                           name='seg{}_{}_raster'.format(segIdx, st.unit.name),
  3969                                                           sampling_rate=samplingRate,
  3970                                                           dtype=np.int,
  3971                                                           **asigAnn)
  3972                                                       if matchT is not None:
  3973                                                           asig = asig[:matchT.shape[0], :]
  3974                                                       asig.t_start = tStart
  3975                                                       asig.annotate(binWidth=1 / samplingRate.magnitude)
  3976                                                       chanIdx.analogsignals.append(asig)
  3977                                                       asig.channel_index = chanIdx
  3978                                                       spikeMatBlock.segments[segIdx].analogsignals.append(asig)
  3979                                               #
  3980                                               for chanIdx in spikeMatBlock.channel_indexes:
  3981                                                   chanIdx.name = chanIdx.name + '_raster'
  3982                                               #
  3983                                               spikeMatBlock.create_relationship()
  3984                                               spikeMatBlock = purgeNixAnn(spikeMatBlock)
  3985                                               if saveToFile:
  3986                                                   if os.path.exists(binnedSpikePath):
  3987                                                       os.remove(binnedSpikePath)
  3988                                                   writer = NixIO(filename=binnedSpikePath)
  3989                                                   writer.write_block(spikeMatBlock, use_obj_names=True)
  3990                                                   writer.close()
  3991                                               return spikeMatBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: calcFR at line 3993

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3993                                           @profile
  3994                                           def calcFR(
  3995                                                   binnedPath, dataPath,
  3996                                                   suffix='fr', aggregateFun=None,
  3997                                                   chanNames=None, rasterOpts=None, verbose=False
  3998                                                   ):
  3999                                               print('Loading rasters...')
  4000                                               masterSpikeMats, _ = loadSpikeMats(
  4001                                                   binnedPath, rasterOpts,
  4002                                                   aggregateFun=aggregateFun,
  4003                                                   chans=chanNames,
  4004                                                   loadAll=True, checkReferences=False)
  4005                                               print('Loading data file...')
  4006                                               dataReader = nixio_fr.NixIO(
  4007                                                   filename=dataPath)
  4008                                               dataBlock = dataReader.read_block(
  4009                                                   block_index=0, lazy=True,
  4010                                                   signal_group_mode='split-all')
  4011                                               masterBlock = Block()
  4012                                               masterBlock.name = dataBlock.annotations['neo_name']
  4013                                               #
  4014                                               for segIdx, segSpikeMat in masterSpikeMats.items():
  4015                                                   print('Calculating FR for segment {}'.format(segIdx))
  4016                                                   spikeMatDF = segSpikeMat.reset_index().rename(
  4017                                                       columns={'bin': 't'})
  4018                                           
  4019                                                   dataSeg = dataBlock.segments[segIdx]
  4020                                                   dummyAsig = dataSeg.filter(
  4021                                                       objects=AnalogSignalProxy)[0].load(channel_indexes=[0])
  4022                                                   samplingRate = dummyAsig.sampling_rate
  4023                                                   newT = dummyAsig.times.magnitude
  4024                                                   spikeMatDF['t'] = spikeMatDF['t'] + newT[0]
  4025                                           
  4026                                                   segSpikeMatInterp = hf.interpolateDF(
  4027                                                       spikeMatDF, pd.Series(newT),
  4028                                                       kind='linear', fill_value=(0, 0),
  4029                                                       x='t')
  4030                                                   spikeMatBlockInterp = dataFrameToAnalogSignals(
  4031                                                       segSpikeMatInterp,
  4032                                                       idxT='t', useColNames=True,
  4033                                                       dataCol=segSpikeMatInterp.drop(columns='t').columns,
  4034                                                       samplingRate=samplingRate)
  4035                                                   spikeMatBlockInterp.name = dataBlock.annotations['neo_name']
  4036                                                   spikeMatBlockInterp.annotate(
  4037                                                       nix_name=dataBlock.annotations['neo_name'])
  4038                                                   spikeMatBlockInterp.segments[0].name = dataSeg.annotations['neo_name']
  4039                                                   spikeMatBlockInterp.segments[0].annotate(
  4040                                                       nix_name=dataSeg.annotations['neo_name'])
  4041                                                   asigList = spikeMatBlockInterp.filter(objects=AnalogSignal)
  4042                                                   for asig in asigList:
  4043                                                       asig.annotate(binWidth=rasterOpts['binWidth'])
  4044                                                       if '_raster' in asig.name:
  4045                                                           asig.name = asig.name.replace('_raster', '_' + suffix)
  4046                                                       asig.name = 'seg{}_{}'.format(segIdx, childBaseName(asig.name, 'seg'))
  4047                                                       asig.annotate(nix_name=asig.name)
  4048                                                   chanIdxList = spikeMatBlockInterp.filter(objects=ChannelIndex)
  4049                                                   for chanIdx in chanIdxList:
  4050                                                       if '_raster' in chanIdx.name:
  4051                                                           chanIdx.name = chanIdx.name.replace('_raster', '_' + suffix)
  4052                                                       chanIdx.annotate(nix_name=chanIdx.name)
  4053                                           
  4054                                                   # masterBlock.merge(spikeMatBlockInterp)
  4055                                                   frBlockPath = dataPath.replace('_analyze.nix', '_fr.nix')
  4056                                                   writer = NixIO(filename=frBlockPath)
  4057                                                   writer.write_block(spikeMatBlockInterp, use_obj_names=True)
  4058                                                   writer.close()
  4059                                               #
  4060                                               dataReader.file.close()
  4061                                               return masterBlock

Timer unit: 1e-07 s

Total time: 0.235801 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: analogSignalsToDataFrame at line 43

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    43                                           @profile
    44                                           def analogSignalsToDataFrame(
    45                                                   analogsignals, idxT='t', useChanNames=False):
    46         1         25.0     25.0      0.0      asigList = []
    47        57        735.0     12.9      0.0      for asig in analogsignals:
    48        56        648.0     11.6      0.0          if asig.shape[1] == 1:
    49        56        293.0      5.2      0.0              if useChanNames:
    50        56       1020.0     18.2      0.0                  colNames = [str(asig.channel_index.name)]
    51                                                       else:
    52                                                           colNames = [str(asig.name)]
    53                                                   else:
    54                                                       colNames = [
    55                                                           asig.name +
    56                                                           '_{}'.format(i) for i in
    57                                                           asig.channel_index.channel_ids
    58                                                           ]
    59        56        330.0      5.9      0.0          asigList.append(
    60        56        440.0      7.9      0.0              pd.DataFrame(
    61        56       1742.0     31.1      0.1                  asig.magnitude, columns=colNames,
    62        56     355474.0   6347.8     15.1                  index=range(asig.shape[0])))
    63         1          6.0      6.0      0.0      asigList.append(
    64         1          8.0      8.0      0.0          pd.DataFrame(
    65         1     278863.0 278863.0     11.8              asig.times.magnitude, columns=[idxT],
    66         1       8840.0   8840.0      0.4              index=range(asig.shape[0])))
    67         1    1709585.0 1709585.0     72.5      return pd.concat(asigList, axis=1)

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: listChanNames at line 69

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    69                                           @profile
    70                                           def listChanNames(
    71                                                   dataBlock, chanQuery,
    72                                                   objType=AnalogSignalProxy, condition=None):
    73                                               allChanList = [
    74                                                   i.name
    75                                                   for i in dataBlock.filter(objects=objType)]
    76                                               if condition == 'hasAsigs':
    77                                                   allChanList = [
    78                                                       i
    79                                                       for i in allChanList
    80                                                       if len(dataBlock.filter(objects=objType, name=i)[0].analogsignals)
    81                                                   ]
    82                                               chansToTrigger = pd.DataFrame(
    83                                                   np.unique(allChanList),
    84                                                   columns=['chanName'])
    85                                               if chanQuery is not None:
    86                                                   chansToTrigger = chansToTrigger.query(
    87                                                       chanQuery, engine='python')['chanName'].to_list()
    88                                               else:
    89                                                   chansToTrigger = chansToTrigger['chanName'].to_list()
    90                                               return chansToTrigger

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: spikeDictToSpikeTrains at line 92

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    92                                           @profile
    93                                           def spikeDictToSpikeTrains(
    94                                                   spikes, block=None, seg=None,
    95                                                   probeName='insTD', t_stop=None,
    96                                                   waveformUnits=pq.uV,
    97                                                   sampling_rate=3e4 * pq.Hz):
    98                                           
    99                                               if block is None:
   100                                                   assert seg is None
   101                                                   block = Block()
   102                                                   seg = Segment(name=probeName + ' segment')
   103                                                   block.segments.append(seg)
   104                                           
   105                                               if t_stop is None:
   106                                                   t_stop = hf.getLastSpikeTime(spikes) + 1
   107                                           
   108                                               for idx, chanName in enumerate(spikes['ChannelID']):
   109                                                   #  unique units on this channel
   110                                                   unitsOnThisChan = pd.unique(spikes['Classification'][idx])
   111                                                   nixChanName = probeName + '{}'.format(chanName)
   112                                                   chanIdx = ChannelIndex(
   113                                                       name=nixChanName,
   114                                                       index=np.asarray([idx]),
   115                                                       channel_names=np.asarray([nixChanName]))
   116                                                   block.channel_indexes.append(chanIdx)
   117                                                   
   118                                                   for unitIdx, unitName in enumerate(unitsOnThisChan):
   119                                                       unitMask = spikes['Classification'][idx] == unitName
   120                                                       # this unit's spike timestamps
   121                                                       theseTimes = spikes['TimeStamps'][idx][unitMask]
   122                                                       # this unit's waveforms
   123                                                       if len(spikes['Waveforms'][idx].shape) == 3:
   124                                                           theseWaveforms = spikes['Waveforms'][idx][unitMask, :, :]
   125                                                           theseWaveforms = np.swapaxes(theseWaveforms, 1, 2)
   126                                                       elif len(spikes['Waveforms'][idx].shape) == 2:
   127                                                           theseWaveforms = (
   128                                                               spikes['Waveforms'][idx][unitMask, np.newaxis, :])
   129                                                       else:
   130                                                           raise(Exception('spikes[Waveforms] has bad shape'))
   131                                           
   132                                                       unitName = '{}#{}'.format(nixChanName, unitIdx)
   133                                                       unit = Unit(name=unitName)
   134                                                       unit.channel_index = chanIdx
   135                                                       chanIdx.units.append(unit)
   136                                           
   137                                                       train = SpikeTrain(
   138                                                           times=theseTimes, t_stop=t_stop, units='sec',
   139                                                           name=unitName, sampling_rate=sampling_rate,
   140                                                           waveforms=theseWaveforms*waveformUnits,
   141                                                           left_sweep=0, dtype=np.float32)
   142                                                       unit.spiketrains.append(train)
   143                                                       seg.spiketrains.append(train)
   144                                           
   145                                                       unit.create_relationship()
   146                                                   chanIdx.create_relationship()
   147                                               seg.create_relationship()
   148                                               block.create_relationship()
   149                                               return block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: spikeTrainsToSpikeDict at line 151

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   151                                           @profile
   152                                           def spikeTrainsToSpikeDict(
   153                                                   spiketrains):
   154                                               nCh = len(spiketrains)
   155                                               spikes = {
   156                                                   'ChannelID': [i for i in range(nCh)],
   157                                                   'Classification': [np.asarray([]) for i in range(nCh)],
   158                                                   'NEUEVWAV_HeaderIndices': [None for i in range(nCh)],
   159                                                   'TimeStamps': [np.asarray([]) for i in range(nCh)],
   160                                                   'Units': 'uV',
   161                                                   'Waveforms': [np.asarray([]) for i in range(nCh)],
   162                                                   'basic_headers': {'TimeStampResolution': 3e4},
   163                                                   'extended_headers': []
   164                                                   }
   165                                               for idx, st in enumerate(spiketrains):
   166                                                   spikes['ChannelID'][idx] = st.name
   167                                                   if len(spikes['TimeStamps'][idx]):
   168                                                       spikes['TimeStamps'][idx] = np.stack((
   169                                                           spikes['TimeStamps'][idx],
   170                                                           st.times.magnitude), axis=-1)
   171                                                   else:
   172                                                       spikes['TimeStamps'][idx] = st.times.magnitude
   173                                                   
   174                                                   theseWaveforms = np.swapaxes(
   175                                                       st.waveforms, 1, 2)
   176                                                   theseWaveforms = np.atleast_2d(np.squeeze(
   177                                                       theseWaveforms))
   178                                                       
   179                                                   if len(spikes['Waveforms'][idx]):
   180                                                       spikes['Waveforms'][idx] = np.stack((
   181                                                           spikes['Waveforms'][idx],
   182                                                           theseWaveforms.magnitude), axis=-1)
   183                                                   else:
   184                                                       spikes['Waveforms'][idx] = theseWaveforms.magnitude
   185                                                   
   186                                                   classVals = st.times.magnitude ** 0 * idx
   187                                                   if len(spikes['Classification'][idx]):
   188                                                       spikes['Classification'][idx] = np.stack((
   189                                                           spikes['Classification'][idx],
   190                                                           classVals), axis=-1)
   191                                                   else:
   192                                                       spikes['Classification'][idx] = classVals
   193                                               return spikes

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: channelIndexesToSpikeDict at line 195

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   195                                           @profile
   196                                           def channelIndexesToSpikeDict(
   197                                                   channel_indexes):
   198                                               nCh = len(channel_indexes)
   199                                               spikes = {
   200                                                   'ChannelID': [i for i in range(nCh)],
   201                                                   'Classification': [np.asarray([]) for i in range(nCh)],
   202                                                   'NEUEVWAV_HeaderIndices': [None for i in range(nCh)],
   203                                                   'TimeStamps': [np.asarray([]) for i in range(nCh)],
   204                                                   'Units': 'uV',
   205                                                   'Waveforms': [np.asarray([]) for i in range(nCh)],
   206                                                   'basic_headers': {'TimeStampResolution': 3e4},
   207                                                   'extended_headers': []
   208                                                   }
   209                                               #  allocate fields for annotations
   210                                               for dummyCh in channel_indexes:
   211                                                   if len(dummyCh.units):
   212                                                       dummyUnit = dummyCh.units[0]
   213                                                       if len(dummyUnit.spiketrains):
   214                                                           if len(dummyUnit.spiketrains[0].times):
   215                                                               break
   216                                               dummySt = [
   217                                                   st
   218                                                   for st in dummyUnit.spiketrains
   219                                                   if len(st.times)][0]
   220                                               #  allocate fields for array annotations (per spike)
   221                                               if dummySt.array_annotations:
   222                                                   for key in dummySt.array_annotations.keys():
   223                                                       spikes.update({key: [np.asarray([]) for i in range(nCh)]})
   224                                                   
   225                                               maxUnitIdx = 0
   226                                               for idx, chIdx in enumerate(channel_indexes):
   227                                                   spikes['ChannelID'][idx] = chIdx.name
   228                                                   for unitIdx, thisUnit in enumerate(chIdx.units):
   229                                                       for stIdx, st in enumerate(thisUnit.spiketrains):
   230                                                           if not len(st.times):
   231                                                               continue
   232                                                           #  print(
   233                                                           #      'unit {} has {} spiketrains'.format(
   234                                                           #          thisUnit.name,
   235                                                           #          len(thisUnit.spiketrains)))
   236                                                           if len(spikes['TimeStamps'][idx]):
   237                                                               spikes['TimeStamps'][idx] = np.concatenate((
   238                                                                   spikes['TimeStamps'][idx],
   239                                                                   st.times.magnitude), axis=0)
   240                                                           else:
   241                                                               spikes['TimeStamps'][idx] = st.times.magnitude
   242                                                           #  reshape waveforms to comply with BRM convention
   243                                                           theseWaveforms = np.swapaxes(
   244                                                               st.waveforms, 1, 2)
   245                                                           theseWaveforms = np.atleast_2d(np.squeeze(
   246                                                               theseWaveforms))
   247                                                           #  append waveforms
   248                                                           if len(spikes['Waveforms'][idx]):
   249                                                               try:
   250                                                                   spikes['Waveforms'][idx] = np.concatenate((
   251                                                                       spikes['Waveforms'][idx],
   252                                                                       theseWaveforms.magnitude), axis=0)
   253                                                               except Exception:
   254                                                                   traceback.print_exc()
   255                                                           else:
   256                                                               spikes['Waveforms'][idx] = theseWaveforms.magnitude
   257                                                           #  give each unit a global index
   258                                                           classVals = st.times.magnitude ** 0 * maxUnitIdx
   259                                                           st.array_annotations.update({'Classification': classVals})
   260                                                           #  expand array_annotations into spikes dict
   261                                                           for key, value in st.array_annotations.items():
   262                                                               if len(spikes[key][idx]):
   263                                                                   spikes[key][idx] = np.concatenate((
   264                                                                       spikes[key][idx],
   265                                                                       value), axis=0)
   266                                                               else:
   267                                                                   spikes[key][idx] = value
   268                                                           for key, value in st.annotations.items():
   269                                                               if key not in spikes['basic_headers']:
   270                                                                   spikes['basic_headers'].update({key: {}})
   271                                                               try:
   272                                                                   spikes['basic_headers'][key].update({maxUnitIdx: value})
   273                                                               except Exception:
   274                                                                   pass
   275                                                           maxUnitIdx += 1
   276                                               return spikes

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: unitSpikeTrainArrayAnnToDF at line 278

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   278                                           @profile
   279                                           def unitSpikeTrainArrayAnnToDF(
   280                                                   spikeTrainContainer):
   281                                               #  list contains different segments
   282                                               if isinstance(spikeTrainContainer, ChannelIndex):
   283                                                   assert len(spikeTrainContainer.units) == 0
   284                                                   spiketrains = spikeTrainContainer.units[0].spiketrains
   285                                               elif isinstance(spikeTrainContainer, Unit):
   286                                                   spiketrains = spikeTrainContainer.spiketrains
   287                                               elif isinstance(spikeTrainContainer, list):
   288                                                   spiketrains = spikeTrainContainer
   289                                               fullAnnotationsDict = {}
   290                                               for segIdx, st in enumerate(spiketrains):
   291                                                   theseAnnDF = pd.DataFrame(st.array_annotations)
   292                                                   theseAnnDF['t'] = st.times.magnitude
   293                                                   fullAnnotationsDict.update({segIdx: theseAnnDF})
   294                                               annotationsDF = pd.concat(
   295                                                   fullAnnotationsDict, names=['segment', 'index'], sort=True)
   296                                               return annotationsDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getSpikeDFMetadata at line 298

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   298                                           @profile
   299                                           def getSpikeDFMetadata(spikeDF, metaDataCols):
   300                                               spikeDF.reset_index(inplace=True)
   301                                               metaDataCols = np.atleast_1d(metaDataCols)
   302                                               spikeDF.index.name = 'metaDataIdx'
   303                                               metaDataDF = spikeDF.loc[:, metaDataCols].copy()
   304                                               newSpikeDF = spikeDF.drop(columns=metaDataCols).reset_index()
   305                                               return newSpikeDF, metaDataDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: transposeSpikeDF at line 307

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   307                                           @profile
   308                                           def transposeSpikeDF(
   309                                                   spikeDF, transposeToColumns,
   310                                                   fastTranspose=False):
   311                                               newColumnNames = np.atleast_1d(transposeToColumns).tolist()
   312                                               originalColumnNames = np.atleast_1d(spikeDF.columns.names)
   313                                               metaDataCols = np.setdiff1d(spikeDF.index.names, newColumnNames).tolist()
   314                                               if fastTranspose:
   315                                                   #  fast but memory inefficient
   316                                                   return spikeDF.stack().unstack(transposeToColumns)
   317                                               else:
   318                                                   raise(Warning('Caution! transposeSpikeDF might not be working, needs testing RD 06252019'))
   319                                                   #  stash annotations, transpose, recover annotations
   320                                                   newSpikeDF, metaDataDF = getSpikeDFMetadata(spikeDF, metaDataCols)
   321                                                   del spikeDF
   322                                                   gc.collect()
   323                                                   #
   324                                                   newSpikeDF = newSpikeDF.stack().unstack(newColumnNames)
   325                                                   newSpikeDF.reset_index(inplace=True)
   326                                                   #  set the index
   327                                                   newIdxLabels = np.concatenate(
   328                                                       [originalColumnNames, metaDataCols]).tolist()
   329                                                   newSpikeDF.loc[:, metaDataCols] = (
   330                                                       metaDataDF
   331                                                       .loc[newSpikeDF['metaDataIdx'].to_list(), metaDataCols]
   332                                                       .to_numpy())
   333                                                   newSpikeDF = (
   334                                                       newSpikeDF
   335                                                       .drop(columns=['metaDataIdx'])
   336                                                       .set_index(newIdxLabels))
   337                                                   return newSpikeDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: concatenateBlocks at line 339

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   339                                           @profile
   340                                           def concatenateBlocks(
   341                                                   asigBlocks, spikeBlocks, eventBlocks, chunkingMetadata,
   342                                                   samplingRate, chanQuery, lazy, trackMemory, verbose
   343                                                   ):
   344                                               # Scan ahead through all files and ensure that
   345                                               # spikeTrains and units are present across all assembled files
   346                                               channelIndexCache = {}
   347                                               unitCache = {}
   348                                               asigCache = []
   349                                               asigAnnCache = {}
   350                                               spiketrainCache = {}
   351                                               eventCache = {}
   352                                               # get list of channels and units
   353                                               for idx, (chunkIdxStr, chunkMeta) in enumerate(chunkingMetadata.items()):
   354                                                   gc.collect()
   355                                                   chunkIdx = int(chunkIdxStr)
   356                                                   asigBlock = asigBlocks[chunkIdx]
   357                                                   asigSeg = asigBlock.segments[0]
   358                                                   spikeBlock = spikeBlocks[chunkIdx]
   359                                                   eventBlock = eventBlocks[chunkIdx]
   360                                                   eventSeg = eventBlock.segments[0]
   361                                                   for chIdx in asigBlock.filter(objects=ChannelIndex):
   362                                                       chAlreadyThere = (chIdx.name in channelIndexCache.keys())
   363                                                       if not chAlreadyThere:
   364                                                           newChIdx = copy(chIdx)
   365                                                           newChIdx.analogsignals = []
   366                                                           newChIdx.units = []
   367                                                           channelIndexCache[chIdx.name] = newChIdx
   368                                                   for unit in (spikeBlock.filter(objects=Unit)):
   369                                                       if lazy:
   370                                                           theseSpiketrains = []
   371                                                           for stP in unit.spiketrains:
   372                                                               st = loadStProxy(stP)
   373                                                               if len(st.times) > 0:
   374                                                                   theseSpiketrains.append(st)
   375                                                       else:
   376                                                           theseSpiketrains = [
   377                                                               st
   378                                                               for st in unit.spiketrains
   379                                                               if len(st.times)
   380                                                               ]
   381                                                       for st in theseSpiketrains:
   382                                                           st = loadObjArrayAnn(st)
   383                                                           if len(st.times):
   384                                                               st.magnitude[:] = st.times.magnitude + spikeBlock.annotations['chunkTStart']
   385                                                               st.t_start = min(0 * pq.s, st.times[0] * 0.999)
   386                                                               st.t_stop = max(
   387                                                                   st.t_stop + spikeBlock.annotations['chunkTStart'] * pq.s,
   388                                                                   st.times[-1] * 1.001)
   389                                                           else:
   390                                                               st.t_start += spikeBlock.annotations['chunkTStart'] * pq.s
   391                                                               st.t_stop += spikeBlock.annotations['chunkTStart'] * pq.s
   392                                                       uAlreadyThere = (unit.name in unitCache.keys())
   393                                                       if not uAlreadyThere:
   394                                                           newUnit = copy(unit)
   395                                                           newUnit.spiketrains = []
   396                                                           newUnit.annotations['parentChanName'] = unit.channel_index.name
   397                                                           unitCache[unit.name] = newUnit
   398                                                           spiketrainCache[unit.name] = theseSpiketrains
   399                                                       else:
   400                                                           spiketrainCache[unit.name] = spiketrainCache[unit.name] + theseSpiketrains
   401                                                   #
   402                                                   if lazy:
   403                                                       evList = [
   404                                                           evP.load()
   405                                                           for evP in eventSeg.events]
   406                                                   else:
   407                                                       evList = eventSeg.events
   408                                                   for event in evList:
   409                                                       event.magnitude[:] = event.magnitude + eventBlock.annotations['chunkTStart']
   410                                                       if event.name in eventCache.keys():
   411                                                           eventCache[event.name].append(event)
   412                                                       else:
   413                                                           eventCache[event.name] = [event]
   414                                                   # take the requested analog signal channels
   415                                                   if lazy:
   416                                                       tdChanNames = listChanNames(
   417                                                           asigBlock, chanQuery, objType=AnalogSignalProxy)
   418                                                       #############
   419                                                       # tdChanNames = ['seg0_utah1', 'seg0_utah10']
   420                                                       ##############
   421                                                       asigList = []
   422                                                       for asigP in asigSeg.analogsignals:
   423                                                           if asigP.name in tdChanNames:
   424                                                               asig = asigP.load()
   425                                                               asig.channel_index = asigP.channel_index
   426                                                               asigList.append(asig)
   427                                                               if trackMemory:
   428                                                                   print('loading {} from proxy object. memory usage: {:.1f} MB'.format(
   429                                                                       asigP.name, prf.memory_usage_psutil()))
   430                                                   else:
   431                                                       tdChanNames = listChanNames(
   432                                                           asigBlock, chanQuery, objType=AnalogSignal)
   433                                                       asigList = [
   434                                                           asig
   435                                                           for asig in asigSeg.analogsignals
   436                                                           if asig.name in tdChanNames
   437                                                           ]
   438                                                   for asig in asigList:
   439                                                       if asig.size > 0:
   440                                                           dummyAsig = asig
   441                                                   if idx == 0:
   442                                                       outputBlock = Block(
   443                                                           name=asigBlock.name,
   444                                                           file_origin=asigBlock.file_origin,
   445                                                           file_datetime=asigBlock.file_datetime,
   446                                                           rec_datetime=asigBlock.rec_datetime,
   447                                                           **asigBlock.annotations
   448                                                       )
   449                                                       newSeg = Segment(
   450                                                           index=0, name=asigSeg.name,
   451                                                           description=asigSeg.description,
   452                                                           file_origin=asigSeg.file_origin,
   453                                                           file_datetime=asigSeg.file_datetime,
   454                                                           rec_datetime=asigSeg.rec_datetime,
   455                                                           **asigSeg.annotations
   456                                                       )
   457                                                       outputBlock.segments = [newSeg]
   458                                                       for asig in asigList:
   459                                                           asigAnnCache[asig.name] = asig.annotations
   460                                                           asigAnnCache[asig.name]['parentChanName'] = asig.channel_index.name
   461                                                       asigUnits = dummyAsig.units
   462                                                   tdDF = analogSignalsToDataFrame(asigList)
   463                                                   del asigList  # asigs saved to dataframe, no longer needed
   464                                                   tdDF.loc[:, 't'] += asigBlock.annotations['chunkTStart']
   465                                                   tdDF.set_index('t', inplace=True)
   466                                                   if samplingRate != dummyAsig.sampling_rate:
   467                                                       lowPassOpts = {
   468                                                           'low': {
   469                                                               'Wn': float(samplingRate / 2),
   470                                                               'N': 4,
   471                                                               'btype': 'low',
   472                                                               'ftype': 'bessel'
   473                                                           }
   474                                                       }
   475                                                       newT = pd.Series(
   476                                                           np.arange(
   477                                                               dummyAsig.t_start + asigBlock.annotations['chunkTStart'] * pq.s,
   478                                                               dummyAsig.t_stop + asigBlock.annotations['chunkTStart'] * pq.s,
   479                                                               1/samplingRate))
   480                                                       if samplingRate < dummyAsig.sampling_rate:
   481                                                           filterCoeffs = hf.makeFilterCoeffsSOS(
   482                                                               lowPassOpts, float(dummyAsig.sampling_rate))
   483                                                           if trackMemory:
   484                                                               print('Filtering analog data before downsampling. memory usage: {:.1f} MB'.format(
   485                                                                   prf.memory_usage_psutil()))
   486                                                           '''
   487                                                           ### check that axis=0 is the correct option
   488                                                           dummyDF = tdDF.iloc[:, :4].copy()
   489                                                           filteredAsigs0 = signal.sosfiltfilt( filterCoeffs, dummyDF.to_numpy(), axis=0)
   490                                                           filteredAsigs1 = signal.sosfiltfilt( filterCoeffs, dummyDF.to_numpy(), axis=1)
   491                                                           ###
   492                                                           '''
   493                                                           filteredAsigs = signal.sosfiltfilt(
   494                                                               filterCoeffs, tdDF.to_numpy(),
   495                                                               axis=0)
   496                                                           tdDF = pd.DataFrame(
   497                                                               filteredAsigs,
   498                                                               index=tdDF.index,
   499                                                               columns=tdDF.columns)
   500                                                           if trackMemory:
   501                                                               print('Just finished analog data filtering before downsampling. memory usage: {:.1f} MB'.format(
   502                                                                   prf.memory_usage_psutil()))
   503                                                       tdInterp = hf.interpolateDF(
   504                                                           tdDF, newT,
   505                                                           kind='linear', fill_value='extrapolate',
   506                                                           verbose=verbose)
   507                                                       # free up memory used by full resolution asigs
   508                                                       del tdDF
   509                                                   else:
   510                                                       tdInterp = tdDF
   511                                                   #
   512                                                   asigCache.append(tdInterp)
   513                                                   #
   514                                                   print('Finished chunk {}'.format(chunkIdxStr))
   515                                               allTdDF = pd.concat(asigCache)
   516                                               # TODO: check for nans, if, for example a signal is partially missing
   517                                               allTdDF.fillna(method='bfill', inplace=True)
   518                                               allTdDF.fillna(method='ffill', inplace=True)
   519                                               for asigName in allTdDF.columns:
   520                                                   newAsig = AnalogSignal(
   521                                                       allTdDF[asigName].to_numpy() * asigUnits,
   522                                                       name=asigName,
   523                                                       sampling_rate=samplingRate,
   524                                                       dtype=np.float32,
   525                                                       **asigAnnCache[asigName])
   526                                                   chIdxName = asigAnnCache[asigName]['parentChanName']
   527                                                   chIdx = channelIndexCache[chIdxName]
   528                                                   # cross-assign ownership to containers
   529                                                   chIdx.analogsignals.append(newAsig)
   530                                                   newSeg.analogsignals.append(newAsig)
   531                                                   newAsig.channel_index = chIdx
   532                                                   newAsig.segment = newSeg
   533                                               #
   534                                               for uName, unit in unitCache.items():
   535                                                   # concatenate spike times, waveforms, etc.
   536                                                   if len(spiketrainCache[unit.name]):
   537                                                       consolidatedTimes = np.concatenate([
   538                                                               st.times.magnitude
   539                                                               for st in spiketrainCache[unit.name]
   540                                                           ])
   541                                                       # TODO:   decide whether to include this step
   542                                                       #         which snaps the spike times to the nearest
   543                                                       #         *sampled* data point
   544                                                       #
   545                                                       # consolidatedTimes, timesIndex = hf.closestSeries(
   546                                                       #     takeFrom=pd.Series(consolidatedTimes),
   547                                                       #     compareTo=pd.Series(allTdDF.index))
   548                                                       #
   549                                                       # find an example spiketrain with array_annotations
   550                                                       for st in spiketrainCache[unit.name]:
   551                                                           if len(st.times):
   552                                                               dummySt = st
   553                                                               break
   554                                                       consolidatedAnn = {
   555                                                           key: np.array([])
   556                                                           for key, value in dummySt.array_annotations.items()
   557                                                           }
   558                                                       for key, value in consolidatedAnn.items():
   559                                                           consolidatedAnn[key] = np.concatenate([
   560                                                               st.annotations[key]
   561                                                               for st in spiketrainCache[unit.name]
   562                                                           ])
   563                                                       consolidatedWaveforms = np.concatenate([
   564                                                           st.waveforms
   565                                                           for st in spiketrainCache[unit.name]
   566                                                           ])
   567                                                       spikeTStop = max([
   568                                                           st.t_stop
   569                                                           for st in spiketrainCache[unit.name]
   570                                                           ])
   571                                                       spikeTStart = max([
   572                                                           st.t_start
   573                                                           for st in spiketrainCache[unit.name]
   574                                                           ])
   575                                                       spikeAnnotations = {
   576                                                           key: value
   577                                                           for key, value in dummySt.annotations.items()
   578                                                           if key not in dummySt.annotations['arrayAnnNames']
   579                                                       }
   580                                                       newSt = SpikeTrain(
   581                                                           name=dummySt.name,
   582                                                           times=consolidatedTimes, units='sec', t_stop=spikeTStop,
   583                                                           waveforms=consolidatedWaveforms * dummySt.waveforms.units,
   584                                                           left_sweep=dummySt.left_sweep,
   585                                                           sampling_rate=dummySt.sampling_rate,
   586                                                           t_start=spikeTStart, **spikeAnnotations,
   587                                                           array_annotations=consolidatedAnn)
   588                                                       # cross-assign ownership to containers
   589                                                       unit.spiketrains.append(newSt)
   590                                                       newSt.unit = unit
   591                                                       newSeg.spiketrains.append(newSt)
   592                                                       newSt.segment = newSeg
   593                                                       # link chIdxes and Units
   594                                                       if unit.annotations['parentChanName'] in channelIndexCache:
   595                                                           chIdx = channelIndexCache[unit.annotations['parentChanName']]
   596                                                           if unit not in chIdx.units:
   597                                                               chIdx.units.append(unit)
   598                                                               unit.channel_index = chIdx
   599                                                       else:
   600                                                           newChIdx = ChannelIndex(
   601                                                               name=unit.annotations['parentChanName'], index=0)
   602                                                           channelIndexCache[unit.annotations['parentChanName']] = newChIdx
   603                                                           if unit not in newChIdx.units:
   604                                                               newChIdx.units.append(unit)
   605                                                               unit.channel_index = newChIdx
   606                                               #
   607                                               for evName, eventList in eventCache.items():
   608                                                   consolidatedTimes = np.concatenate([
   609                                                       ev.times.magnitude
   610                                                       for ev in eventList
   611                                                       ])
   612                                                   consolidatedLabels = np.concatenate([
   613                                                       ev.labels
   614                                                       for ev in eventList
   615                                                       ])
   616                                                   newEvent = Event(
   617                                                       name=evName,
   618                                                       times=consolidatedTimes * pq.s,
   619                                                       labels=consolidatedLabels
   620                                                       )
   621                                                   # if len(newEvent):
   622                                                   newEvent.segment = newSeg
   623                                                   newSeg.events.append(newEvent)
   624                                               for chIdxName, chIdx in channelIndexCache.items():
   625                                                   if len(chIdx.analogsignals) or len(chIdx.units):
   626                                                       outputBlock.channel_indexes.append(chIdx)
   627                                                       chIdx.block = outputBlock
   628                                               #
   629                                               outputBlock = purgeNixAnn(outputBlock)
   630                                               createRelationship = False
   631                                               if createRelationship:
   632                                                   outputBlock.create_relationship()
   633                                               return outputBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: concatenateEventsContainer at line 660

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   660                                           @profile
   661                                           def concatenateEventsContainer(eventContainer, linkParents=True):
   662                                               if isinstance(eventContainer, dict):
   663                                                   listOfEvents = list(eventContainer.values())
   664                                               else:
   665                                                   listOfEvents = eventContainer
   666                                               nonEmptyEvents = [ev for ev in listOfEvents if len(ev.times)]
   667                                               if not len(nonEmptyEvents) > 0:
   668                                                   return listOfEvents[0]
   669                                               masterEvent = listOfEvents[0]
   670                                               for evIdx, ev in enumerate(listOfEvents[1:]):
   671                                                   try:
   672                                                       masterEvent = masterEvent.merge(ev)
   673                                                   except Exception:
   674                                                       traceback.print_exc()
   675                                                       pdb.set_trace()
   676                                               if masterEvent.array_annotations is not None:
   677                                                   arrayAnnNames = list(masterEvent.array_annotations.keys())
   678                                                   masterEvent.annotations.update(masterEvent.array_annotations)
   679                                                   masterEvent.annotations['arrayAnnNames'] = arrayAnnNames
   680                                               if linkParents:
   681                                                   masterEvent.segment = listOfEvents[0].segment
   682                                                   if isinstance(masterEvent, SpikeTrain):
   683                                                       masterEvent.unit = listOfEvents[0].unit
   684                                               return masterEvent

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: unitSpikeTrainWaveformsToDF at line 743

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   743                                           @profile
   744                                           def unitSpikeTrainWaveformsToDF(
   745                                                   spikeTrainContainer,
   746                                                   dataQuery=None,
   747                                                   transposeToColumns='bin', fastTranspose=True,
   748                                                   lags=None, decimate=1, rollingWindow=None,
   749                                                   getMetaData=True, verbose=False,
   750                                                   whichSegments=None, windowSize=None, procFun=None):
   751                                               #  list contains different segments from *one* unit
   752                                               if isinstance(spikeTrainContainer, ChannelIndex):
   753                                                   assert len(spikeTrainContainer.units) == 0
   754                                                   spiketrains = spikeTrainContainer.units[0].spiketrains
   755                                               elif isinstance(spikeTrainContainer, Unit):
   756                                                   spiketrains = spikeTrainContainer.spiketrains
   757                                               else:
   758                                                   raise(Exception('not a valid container'))
   759                                               # TODO check if really need to assert uniqueness?
   760                                               uniqueSpiketrains = []
   761                                               for st in spiketrains:
   762                                                   if not np.any([st is i for i in uniqueSpiketrains]):
   763                                                       uniqueSpiketrains.append(st)
   764                                               #  subsampling options
   765                                               decimate = int(decimate)
   766                                               if whichSegments is not None:
   767                                                   uniqueSpiketrains = [
   768                                                       uniqueSpiketrains[i]
   769                                                       for i in whichSegments
   770                                                   ]
   771                                               #
   772                                               waveformsList = []
   773                                               #
   774                                               for segIdx, stIn in enumerate(uniqueSpiketrains):
   775                                                   if verbose:
   776                                                       print('extracting spiketrain from {}'.format(stIn.segment))
   777                                                   #  make sure is not a proxyObj
   778                                                   if isinstance(stIn, SpikeTrainProxy):
   779                                                       st = loadStProxy(stIn)
   780                                                       if (getMetaData) or (dataQuery is not None):
   781                                                           # if there's a query, get metadata temporarily to resolve it
   782                                                           st = loadObjArrayAnn(st)
   783                                                   else:
   784                                                       st = stIn
   785                                                   #  extract bins spaced by decimate argument
   786                                                   if not st.times.any():
   787                                                       continue
   788                                                   if verbose:
   789                                                       print('extracting wf from {}'.format(stIn.segment))
   790                                                   wf = np.asarray(
   791                                                       np.squeeze(st.waveforms),
   792                                                       dtype='float32')
   793                                                   if wf.ndim == 3:
   794                                                       print('Waveforms from more than one channel!')
   795                                                       if wf.shape[1] > 0:
   796                                                           wf = wf[:, 0, :]
   797                                                   wfDF = pd.DataFrame(wf)
   798                                                   samplingRate = st.sampling_rate
   799                                                   bins = (
   800                                                       np.asarray(wfDF.columns) / samplingRate -
   801                                                       st.left_sweep)
   802                                                   wfDF.columns = np.around(bins.magnitude, decimals=6)
   803                                                   if windowSize is not None:
   804                                                       winMask = (
   805                                                           (wfDF.columns >= windowSize[0]) &
   806                                                           (wfDF.columns <= windowSize[1]))
   807                                                       wfDF = wfDF.loc[:, winMask]
   808                                                   if procFun is not None:
   809                                                       wfDF = procFun(wfDF, st)
   810                                                   idxLabels = ['segment', 'originalIndex', 't']
   811                                                   wfDF.loc[:, 't'] = np.asarray(st.times.magnitude)
   812                                                   if (getMetaData) or (dataQuery is not None):
   813                                                       # if there's a query, get metadata temporarily to resolve it
   814                                                       annDict = {}
   815                                                       for k, values in st.array_annotations.items():
   816                                                           if isinstance(getMetaData, Iterable):
   817                                                               # if selecting metadata fields, check that
   818                                                               # the key is in the provided list
   819                                                               if k not in getMetaData:
   820                                                                   continue
   821                                                           if isinstance(values[0], str):
   822                                                               v = np.asarray(values, dtype='str')
   823                                                           else:
   824                                                               v = np.asarray(values)
   825                                                           annDict.update({k: v})
   826                                                       skipAnnNames = (
   827                                                           st.annotations['arrayAnnNames'] +
   828                                                           [
   829                                                               'arrayAnnNames', 'arrayAnnDTypes',
   830                                                               'nix_name', 'neo_name', 'id',
   831                                                               'cell_label', 'cluster_label', 'max_on_channel', 'binWidth']
   832                                                           )
   833                                                       annDF = pd.DataFrame(annDict)
   834                                                       for k, value in st.annotations.items():
   835                                                           if isinstance(getMetaData, Iterable):
   836                                                               # if selecting metadata fields, check that
   837                                                               # the key is in the provided list
   838                                                               if k not in getMetaData:
   839                                                                   continue
   840                                                           if k not in skipAnnNames:
   841                                                               annDF.loc[:, k] = value
   842                                                       #
   843                                                       if isinstance(getMetaData, Iterable):
   844                                                           doNotFillList = idxLabels + ['feature', 'bin']
   845                                                           fieldsNeedFiller = [
   846                                                               mdn
   847                                                               for mdn in getMetaData
   848                                                               if (mdn not in doNotFillList) and (mdn not in annDF.columns)]
   849                                                           for mdName in fieldsNeedFiller:
   850                                                               annDF.loc[:, mdName] = 'NA'
   851                                                       annColumns = annDF.columns.to_list()
   852                                                       if getMetaData:
   853                                                           for annNm in annColumns:
   854                                                               if annNm not in idxLabels:
   855                                                                   idxLabels.append(annNm)
   856                                                           # idxLabels += annColumns
   857                                                       spikeDF = annDF.join(wfDF)
   858                                                   else:
   859                                                       spikeDF = wfDF
   860                                                       del wfDF, st
   861                                                   spikeDF.loc[:, 'segment'] = segIdx
   862                                                   spikeDF.loc[:, 'originalIndex'] = spikeDF.index
   863                                                   spikeDF.columns.name = 'bin'
   864                                                   #
   865                                                   if dataQuery is not None:
   866                                                       spikeDF.query(dataQuery, inplace=True)
   867                                                       if not getMetaData:
   868                                                           spikeDF.drop(columns=annColumns, inplace=True)
   869                                                   waveformsList.append(spikeDF)
   870                                               #
   871                                               zeroLagWaveformsDF = pd.concat(waveformsList, axis='index')
   872                                               if verbose:
   873                                                   prf.print_memory_usage('before transposing waveforms')
   874                                               # TODO implement lags and rolling window addition here
   875                                               metaDF = zeroLagWaveformsDF.loc[:, idxLabels].copy()
   876                                               zeroLagWaveformsDF.drop(columns=idxLabels, inplace=True)
   877                                               if lags is None:
   878                                                   lags = [0]
   879                                               laggedWaveformsDict = {
   880                                                   (spikeTrainContainer.name, k): None for k in lags}
   881                                               for lag in lags:
   882                                                   if isinstance(lag, int):
   883                                                       shiftedWaveform = zeroLagWaveformsDF.shift(
   884                                                           lag, axis='columns')
   885                                                       if rollingWindow is not None:
   886                                                           halfRollingWin = int(np.ceil(rollingWindow/2))
   887                                                           seekIdx = slice(
   888                                                               halfRollingWin, -halfRollingWin+1, decimate)
   889                                                           # seekIdx = slice(None, None, decimate)
   890                                                           #shiftedWaveform = (
   891                                                           #    shiftedWaveform
   892                                                           #    .rolling(
   893                                                           #        window=rollingWindow, win_type='gaussian',
   894                                                           #        axis='columns', center=True)
   895                                                           #    .mean(std=halfRollingWin))
   896                                                           shiftedWaveform = (
   897                                                               shiftedWaveform
   898                                                               .rolling(
   899                                                                   window=rollingWindow, 
   900                                                                   axis='columns', center=True)
   901                                                               .mean())
   902                                                       else:
   903                                                           halfRollingWin = 0
   904                                                           seekIdx = slice(None, None, decimate)
   905                                                           if False:
   906                                                               oldShiftedWaveform = zeroLagWaveformsDF.shift(
   907                                                                   lag, axis='columns')
   908                                                               plt.plot(oldShiftedWaveform.iloc[0, :])
   909                                                               plt.plot(shiftedWaveform.iloc[0, :])
   910                                                               plt.show()
   911                                                       laggedWaveformsDict[
   912                                                           (spikeTrainContainer.name, lag)] = (
   913                                                               shiftedWaveform.iloc[:, seekIdx].copy())
   914                                                   if isinstance(lag, tuple):
   915                                                       halfRollingWin = int(np.ceil(lag[1]/2))
   916                                                       seekIdx = slice(
   917                                                           halfRollingWin, -halfRollingWin+1, decimate)
   918                                                       # seekIdx = slice(None, None, decimate)
   919                                                       shiftedWaveform = (
   920                                                           zeroLagWaveformsDF
   921                                                           .shift(lag[0], axis='columns')
   922                                                           .rolling(
   923                                                               window=lag[1], win_type='gaussian',
   924                                                               axis='columns', center=True)
   925                                                           .mean(std=halfRollingWin))
   926                                                       laggedWaveformsDict[
   927                                                           (spikeTrainContainer.name, lag)] = (
   928                                                               shiftedWaveform.iloc[:, seekIdx].copy())
   929                                               #
   930                                               if transposeToColumns == 'feature':
   931                                                   # stack the bin, name the feature column
   932                                                   # 
   933                                                   for idx, (key, value) in enumerate(laggedWaveformsDict.items()):
   934                                                       if idx == 0:
   935                                                           stackedIndexDF = pd.concat(
   936                                                               [metaDF, value], axis='columns')
   937                                                           stackedIndexDF.set_index(idxLabels, inplace=True)
   938                                                           # don't drop nans for now - might need to keep track of them
   939                                                           # if we need to equalize to another array later
   940                                                           newIndex = stackedIndexDF.stack(dropna=False).index
   941                                                           idxLabels.append('bin')
   942                                                       laggedWaveformsDict[key] = value.stack(dropna=False).to_frame(name=key).reset_index(drop=True)
   943                                                   waveformsDF = pd.concat(
   944                                                       laggedWaveformsDict.values(),
   945                                                       axis='columns')
   946                                                   waveformsDF.columns.names = ['feature', 'lag']
   947                                                   waveformsDF.index = newIndex
   948                                                   waveformsDF.columns.name = 'feature'
   949                                               elif transposeToColumns == 'bin':
   950                                                   # add the feature column
   951                                                   waveformsDF = pd.concat(
   952                                                       laggedWaveformsDict,
   953                                                       names=['feature', 'lag', 'originalDummy']).reset_index()
   954                                                   waveformsDF = pd.concat(
   955                                                       [
   956                                                           metaDF.reset_index(drop=True),
   957                                                           waveformsDF.drop(columns='originalDummy')],
   958                                                       axis='columns')
   959                                                   idxLabels += ['feature', 'lag']
   960                                                   waveformsDF.columns.name = 'bin'
   961                                                   waveformsDF.set_index(idxLabels, inplace=True)
   962                                               #
   963                                               if transposeToColumns != waveformsDF.columns.name:
   964                                                   waveformsDF = transposeSpikeDF(
   965                                                       waveformsDF, transposeToColumns,
   966                                                       fastTranspose=fastTranspose)
   967                                               return waveformsDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: concatenateUnitSpikeTrainWaveformsDF at line 969

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   969                                           @profile
   970                                           def concatenateUnitSpikeTrainWaveformsDF(
   971                                                   units, dataQuery=None,
   972                                                   transposeToColumns='bin', concatOn='index',
   973                                                   fastTranspose=True, getMetaData=True, verbose=False,
   974                                                   addLags=None, decimate=1, rollingWindow=None,
   975                                                   metaDataToCategories=False, windowSize=None,
   976                                                   whichSegments=None, procFun=None):
   977                                               allUnits = []
   978                                               for thisUnit in units:
   979                                                   hasAnySpikes = []
   980                                                   for stIn in thisUnit.spiketrains:
   981                                                       if isinstance(stIn, SpikeTrainProxy):
   982                                                           st = stIn.load(
   983                                                               magnitude_mode='rescaled',
   984                                                               load_waveforms=False)
   985                                                       else:
   986                                                           st = stIn
   987                                                       hasAnySpikes.append(st.times.any())
   988                                                   if np.any(hasAnySpikes):
   989                                                       allUnits.append(thisUnit)
   990                                               waveformsList = []
   991                                               for idx, thisUnit in enumerate(allUnits):
   992                                                   if verbose:
   993                                                       print('concatenating unitDF {}'.format(thisUnit.name))
   994                                                   lags = None
   995                                                   if addLags is not None:
   996                                                       if thisUnit.name in addLags:
   997                                                           lags = addLags[thisUnit.name]
   998                                                   unitWaveforms = unitSpikeTrainWaveformsToDF(
   999                                                       thisUnit, dataQuery=dataQuery,
  1000                                                       transposeToColumns=transposeToColumns,
  1001                                                       fastTranspose=fastTranspose, getMetaData=getMetaData,
  1002                                                       lags=lags, decimate=decimate, rollingWindow=rollingWindow,
  1003                                                       verbose=verbose, windowSize=windowSize,
  1004                                                       whichSegments=whichSegments, procFun=procFun)
  1005                                                   if idx == 0:
  1006                                                       idxLabels = unitWaveforms.index.names
  1007                                                   if (concatOn == 'columns') and (idx > 0):
  1008                                                       # other than first time, we already have the metadata
  1009                                                       unitWaveforms.reset_index(drop=True, inplace=True)
  1010                                                   else:
  1011                                                       # first time, or if concatenating indices,
  1012                                                       # keep the the metadata
  1013                                                       unitWaveforms.reset_index(inplace=True)
  1014                                                       if metaDataToCategories:
  1015                                                           # convert metadata to categoricals to free memory
  1016                                                           #
  1017                                                           unitWaveforms[idxLabels] = (
  1018                                                               unitWaveforms[idxLabels]
  1019                                                               .astype('category')
  1020                                                               )
  1021                                                   waveformsList.append(unitWaveforms)
  1022                                                   del unitWaveforms
  1023                                                   if verbose:
  1024                                                       print('memory usage: {:.1f} MB'.format(prf.memory_usage_psutil()))
  1025                                               if verbose:
  1026                                                   print(
  1027                                                       'about to join all, memory usage: {:.1f} MB'
  1028                                                       .format(prf.memory_usage_psutil()))
  1029                                               #  if concatenating indexes, reset the index of the result
  1030                                               #  ignoreIndex = (concatOn == 'index')
  1031                                               allWaveforms = pd.concat(
  1032                                                   waveformsList, axis=concatOn,
  1033                                                   # ignore_index=ignoreIndex
  1034                                                   )
  1035                                               del waveformsList
  1036                                               if verbose:
  1037                                                   print(
  1038                                                       'finished concatenating, memory usage: {:.1f} MB'
  1039                                                       .format(prf.memory_usage_psutil()))
  1040                                               try:
  1041                                                   allWaveforms.set_index(idxLabels, inplace=True)
  1042                                                   allWaveforms.sort_index(
  1043                                                       level=['segment', 'originalIndex', 't'],
  1044                                                       axis='index', inplace=True, kind='mergesort')
  1045                                                   allWaveforms.sort_index(
  1046                                                       axis='columns', inplace=True, kind='mergesort')
  1047                                               except Exception:
  1048                                                   pdb.set_trace()
  1049                                               return allWaveforms

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: alignedAsigsToDF at line 1051

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1051                                           @profile
  1052                                           def alignedAsigsToDF(
  1053                                                   dataBlock, unitNames=None,
  1054                                                   unitQuery=None, dataQuery=None,
  1055                                                   collapseSizes=False, verbose=False,
  1056                                                   duplicateControlsByProgram=False,
  1057                                                   amplitudeColumn='amplitude',
  1058                                                   programColumn='program',
  1059                                                   electrodeColumn='electrode',
  1060                                                   transposeToColumns='bin', concatOn='index', fastTranspose=True,
  1061                                                   addLags=None, decimate=1, rollingWindow=None,
  1062                                                   whichSegments=None, windowSize=None,
  1063                                                   getMetaData=True, metaDataToCategories=True,
  1064                                                   outlierTrials=None, invertOutlierMask=False,
  1065                                                   makeControlProgram=False, removeFuzzyName=False, procFun=None):
  1066                                               #  channels to trigger
  1067                                               if unitNames is None:
  1068                                                   unitNames = listChanNames(dataBlock, unitQuery, objType=Unit)
  1069                                               allUnits = []
  1070                                               for uName in unitNames:
  1071                                                   allUnits += dataBlock.filter(objects=Unit, name=uName)
  1072                                               allWaveforms = concatenateUnitSpikeTrainWaveformsDF(
  1073                                                   allUnits, dataQuery=dataQuery,
  1074                                                   transposeToColumns=transposeToColumns, concatOn=concatOn,
  1075                                                   fastTranspose=fastTranspose,
  1076                                                   addLags=addLags, decimate=decimate, rollingWindow=rollingWindow,
  1077                                                   verbose=verbose, whichSegments=whichSegments,
  1078                                                   windowSize=windowSize, procFun=procFun,
  1079                                                   getMetaData=getMetaData, metaDataToCategories=metaDataToCategories)
  1080                                               #
  1081                                               manipulateIndex = np.any(
  1082                                                   [
  1083                                                       collapseSizes, duplicateControlsByProgram,
  1084                                                       makeControlProgram, removeFuzzyName
  1085                                                       ])
  1086                                               if outlierTrials is not None:
  1087                                                   def rejectionLookup(entry):
  1088                                                       key = []
  1089                                                       for subKey in outlierTrials.index.names:
  1090                                                           keyIdx = allWaveforms.index.names.index(subKey)
  1091                                                           key.append(entry[keyIdx])
  1092                                                       # print(key)
  1093                                                       # outlierTrials.iloc[1, :]
  1094                                                       # allWaveforms.iloc[1, :]
  1095                                                       return outlierTrials[tuple(key)]
  1096                                                   #
  1097                                                   outlierMask = np.asarray(
  1098                                                       allWaveforms.index.map(rejectionLookup),
  1099                                                       dtype=np.bool)
  1100                                                   if invertOutlierMask:
  1101                                                       outlierMask = ~outlierMask
  1102                                                   allWaveforms = allWaveforms.loc[~outlierMask, :]
  1103                                               if manipulateIndex and getMetaData:
  1104                                                   idxLabels = allWaveforms.index.names
  1105                                                   allWaveforms.reset_index(inplace=True)
  1106                                                   # 
  1107                                                   if collapseSizes:
  1108                                                       try:
  1109                                                           allWaveforms.loc[allWaveforms['pedalSizeCat'] == 'XL', 'pedalSizeCat'] = 'L'
  1110                                                           allWaveforms.loc[allWaveforms['pedalSizeCat'] == 'XS', 'pedalSizeCat'] = 'S'
  1111                                                       except Exception:
  1112                                                           traceback.print_exc()
  1113                                                   if makeControlProgram:
  1114                                                       try:
  1115                                                           allWaveforms.loc[allWaveforms[amplitudeColumn] == 0, programColumn] = 999
  1116                                                           allWaveforms.loc[allWaveforms[amplitudeColumn] == 0, electrodeColumn] = 'control'
  1117                                                       except Exception:
  1118                                                           traceback.print_exc()
  1119                                                   if duplicateControlsByProgram:
  1120                                                       #
  1121                                                       noStimWaveforms = (
  1122                                                           allWaveforms
  1123                                                           .loc[allWaveforms[amplitudeColumn] == 0, :]
  1124                                                           )
  1125                                                       stimWaveforms = (
  1126                                                           allWaveforms
  1127                                                           .loc[allWaveforms[amplitudeColumn] != 0, :]
  1128                                                           .copy()
  1129                                                           )
  1130                                                       uniqProgs = stimWaveforms[programColumn].unique()
  1131                                                       progElecLookup = {}
  1132                                                       #pdb.set_trace()
  1133                                                       for progIdx in uniqProgs:
  1134                                                           theseStimDF = stimWaveforms.loc[
  1135                                                               stimWaveforms[programColumn] == progIdx,
  1136                                                               electrodeColumn]
  1137                                                           elecIdx = theseStimDF.iloc[0]
  1138                                                           progElecLookup.update({progIdx: elecIdx})
  1139                                                       #
  1140                                                       if makeControlProgram:
  1141                                                           uniqProgs = np.append(uniqProgs, 999)
  1142                                                           progElecLookup.update({999: 'control'})
  1143                                                       #
  1144                                                       for progIdx in uniqProgs:
  1145                                                           dummyWaveforms = noStimWaveforms.copy()
  1146                                                           dummyWaveforms.loc[:, programColumn] = progIdx
  1147                                                           dummyWaveforms.loc[:, electrodeColumn] = progElecLookup[progIdx]
  1148                                                           stimWaveforms = pd.concat([stimWaveforms, dummyWaveforms])
  1149                                                       stimWaveforms.reset_index(drop=True, inplace=True)
  1150                                                       allWaveforms = stimWaveforms
  1151                                                   #
  1152                                                   if removeFuzzyName:
  1153                                                       fuzzyNamesBase = [
  1154                                                           i.replace('Fuzzy', '')
  1155                                                           for i in idxLabels
  1156                                                           if 'Fuzzy' in i]
  1157                                                       colRenamer = {n + 'Fuzzy': n for n in fuzzyNamesBase}
  1158                                                       fuzzyNamesBasePresent = [
  1159                                                           i
  1160                                                           for i in fuzzyNamesBase
  1161                                                           if i in allWaveforms.columns]
  1162                                                       allWaveforms.drop(columns=fuzzyNamesBasePresent, inplace=True)
  1163                                                       allWaveforms.rename(columns=colRenamer, inplace=True)
  1164                                                       idxLabels = np.unique(
  1165                                                           [i.replace('Fuzzy', '') for i in idxLabels])
  1166                                                   #
  1167                                                   allWaveforms.set_index(
  1168                                                       list(idxLabels),
  1169                                                       inplace=True)
  1170                                                   if isinstance(allWaveforms.columns, pd.MultiIndex):
  1171                                                       allWaveforms.columns = allWaveforms.columns.remove_unused_levels()
  1172                                               #
  1173                                               if transposeToColumns == 'feature':
  1174                                                   zipNames = zip(pd.unique(allWaveforms.columns.get_level_values('feature')).tolist(), unitNames)
  1175                                                   try:
  1176                                                       assert np.all([i == j for i, j in zipNames]), 'columns out of requested order!'
  1177                                                   except Exception:
  1178                                                       traceback.print_exc()
  1179                                                       allWaveforms.reindex(columns=unitNames)
  1180                                               if isinstance(allWaveforms.columns, pd.MultiIndex):
  1181                                                   allWaveforms.columns = allWaveforms.columns.remove_unused_levels()
  1182                                               allWaveforms.sort_index(
  1183                                                   axis='columns', inplace=True, kind='mergesort')
  1184                                               return allWaveforms

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getAsigsAlignedToEvents at line 1186

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1186                                           @profile
  1187                                           def getAsigsAlignedToEvents(
  1188                                                   eventBlock=None, signalBlock=None,
  1189                                                   chansToTrigger=None, chanQuery=None,
  1190                                                   eventName=None, windowSize=None,
  1191                                                   minNReps=None,
  1192                                                   appendToExisting=False,
  1193                                                   checkReferences=True, verbose=False,
  1194                                                   fileName=None, folderPath=None, chunkSize=None
  1195                                                   ):
  1196                                               #  get signals from same block as events?
  1197                                               if signalBlock is None:
  1198                                                   signalBlock = eventBlock
  1199                                               #  channels to trigger
  1200                                               if chansToTrigger is None:
  1201                                                   chansToTrigger = listChanNames(
  1202                                                       signalBlock, chanQuery, objType=ChannelIndex, condition='hasAsigs')
  1203                                               #  allocate block for spiketrains
  1204                                               masterBlock = Block()
  1205                                               try:
  1206                                                   masterBlock.name = signalBlock.annotations['neo_name']
  1207                                                   masterBlock.annotate(nix_name=signalBlock.annotations['neo_name'])
  1208                                               except Exception:
  1209                                                   masterBlock.name = signalBlock.name
  1210                                                   masterBlock.annotate(neo_name=signalBlock.name)
  1211                                                   masterBlock.annotate(nix_name=signalBlock.name)
  1212                                               #  make channels and units for triggered time series
  1213                                               for chanName in chansToTrigger:
  1214                                                   chanIdx = ChannelIndex(name=chanName + '#0', index=[0])
  1215                                                   chanIdx.annotate(nix_name=chanIdx.name)
  1216                                                   thisUnit = Unit(name=chanIdx.name)
  1217                                                   thisUnit.annotate(nix_name=chanIdx.name)
  1218                                                   chanIdx.units.append(thisUnit)
  1219                                                   thisUnit.channel_index = chanIdx
  1220                                                   masterBlock.channel_indexes.append(chanIdx)
  1221                                                   sigChanIdxList = signalBlock.filter(
  1222                                                       objects=ChannelIndex, name=chanName)
  1223                                                   if len(sigChanIdxList):
  1224                                                       sigChanIdx = sigChanIdxList[0]
  1225                                                       if sigChanIdx.coordinates is not None:
  1226                                                           coordUnits = sigChanIdx.coordinates[0][0].units
  1227                                                           chanIdx.coordinates = np.asarray(sigChanIdx.coordinates) * coordUnits
  1228                                                           thisUnit.annotations['parentChanXCoords'] = float(chanIdx.coordinates[:, 0].magnitude)
  1229                                                           thisUnit.annotations['parentChanYCoords'] = float(chanIdx.coordinates[:, 1].magnitude)
  1230                                                           thisUnit.annotations['parentChanCoordinateUnits'] = '{}'.format(coordUnits)
  1231                                               #
  1232                                               totalNSegs = 0
  1233                                               #  print([evSeg.events[3].name for evSeg in eventBlock.segments])
  1234                                               allAlignEventsList = []
  1235                                               for segIdx, eventSeg in enumerate(eventBlock.segments):
  1236                                                   thisEventName = 'seg{}_{}'.format(segIdx, eventName)
  1237                                                   try:
  1238                                                       assert len(eventSeg.filter(name=thisEventName)) == 1
  1239                                                   except Exception:
  1240                                                       traceback.print_exc()
  1241                                                   allEvIn = eventSeg.filter(name=thisEventName)[0]
  1242                                                   if isinstance(allEvIn, EventProxy):
  1243                                                       allAlignEvents = loadObjArrayAnn(allEvIn.load())
  1244                                                   elif isinstance(allEvIn, Event):
  1245                                                       allAlignEvents = allEvIn
  1246                                                   else:
  1247                                                       raise(Exception(
  1248                                                           '{} must be an Event or EventProxy!'
  1249                                                           .format(eventName)))
  1250                                                   allAlignEventsList.append(allAlignEvents)
  1251                                               allAlignEventsDF = unitSpikeTrainArrayAnnToDF(allAlignEventsList)
  1252                                               #
  1253                                               breakDownData = (
  1254                                                   allAlignEventsDF
  1255                                                   .groupby(minNReps['categories'])
  1256                                                   .agg('count')
  1257                                                   .iloc[:, 0]
  1258                                                   )
  1259                                               try:
  1260                                                   breakDownData[breakDownData > minNReps['n']].to_csv(
  1261                                                       os.path.join(
  1262                                                           folderPath, 'numRepetitionsEachCondition.csv'
  1263                                                       ), header=True
  1264                                                   )
  1265                                               except Exception:
  1266                                                   traceback.print_exc()
  1267                                               allAlignEventsDF.loc[:, 'keepMask'] = False
  1268                                               for name, group in allAlignEventsDF.groupby(minNReps['categories']):
  1269                                                   allAlignEventsDF.loc[group.index, 'keepMask'] = (
  1270                                                       breakDownData[name] > minNReps['n'])
  1271                                               for segIdx, group in allAlignEventsDF.groupby('segment'):
  1272                                                   allAlignEventsList[segIdx].array_annotations['keepMask'] = group['keepMask'].to_numpy()
  1273                                               #
  1274                                               for segIdx, eventSeg in enumerate(eventBlock.segments):
  1275                                                   if verbose:
  1276                                                       print(
  1277                                                           'getAsigsAlignedToEvents on segment {} of {}'
  1278                                                           .format(segIdx + 1, len(eventBlock.segments)))
  1279                                                   allAlignEvents = allAlignEventsList[segIdx]
  1280                                                   if chunkSize is None:
  1281                                                       alignEventGroups = [allAlignEvents]
  1282                                                   else:
  1283                                                       nChunks = max(
  1284                                                           int(np.floor(allAlignEvents.shape[0] / chunkSize)),
  1285                                                           1)
  1286                                                       alignEventGroups = []
  1287                                                       for i in range(nChunks):
  1288                                                           if not (i == (nChunks - 1)):
  1289                                                               # not last one
  1290                                                               alignEventGroups.append(
  1291                                                                   allAlignEvents[i * chunkSize: (i + 1) * chunkSize])
  1292                                                           else:
  1293                                                               alignEventGroups.append(
  1294                                                                   allAlignEvents[i * chunkSize:])
  1295                                                   signalSeg = signalBlock.segments[segIdx]
  1296                                                   for subSegIdx, alignEvents in enumerate(alignEventGroups):
  1297                                                       # seg to contain triggered time series
  1298                                                       if verbose:
  1299                                                           print(
  1300                                                               'getAsigsAlignedToEvents on subSegment {} of {}'
  1301                                                               .format(subSegIdx + 1, len(alignEventGroups)))
  1302                                                       if not alignEvents.shape[0] > 0:
  1303                                                           continue
  1304                                                       newSeg = Segment(name='seg{}_'.format(int(totalNSegs)))
  1305                                                       newSeg.annotate(nix_name=newSeg.name)
  1306                                                       masterBlock.segments.append(newSeg)
  1307                                                       for chanName in chansToTrigger:
  1308                                                           asigName = 'seg{}_{}'.format(segIdx, chanName)
  1309                                                           if verbose:
  1310                                                               print(
  1311                                                                   'getAsigsAlignedToEvents on channel {}'
  1312                                                                   .format(chanName))
  1313                                                           assert len(signalSeg.filter(name=asigName)) == 1
  1314                                                           asig = signalSeg.filter(name=asigName)[0]
  1315                                                           nominalWinLen = int(
  1316                                                               (windowSize[1] - windowSize[0]) *
  1317                                                               asig.sampling_rate - 1)
  1318                                                           validMask = (
  1319                                                               ((
  1320                                                                   alignEvents + windowSize[1] +
  1321                                                                   asig.sampling_rate ** (-1)) < asig.t_stop) &
  1322                                                               ((
  1323                                                                   alignEvents + windowSize[0] -
  1324                                                                   asig.sampling_rate ** (-1)) > asig.t_start)
  1325                                                               )
  1326                                                           thisKeepMask = alignEvents.array_annotations['keepMask']
  1327                                                           fullMask = (validMask & thisKeepMask)
  1328                                                           alignEvents = alignEvents[fullMask]
  1329                                                           # array_annotations get sliced with the event, but regular anns do not
  1330                                                           for annName in alignEvents.annotations['arrayAnnNames']:
  1331                                                               alignEvents.annotations[annName] = (
  1332                                                                   alignEvents.array_annotations[annName])
  1333                                                           if isinstance(asig, AnalogSignalProxy):
  1334                                                               if checkReferences:
  1335                                                                   da = (
  1336                                                                       asig
  1337                                                                       ._rawio
  1338                                                                       .da_list['blocks'][0]['segments'][segIdx]['data'])
  1339                                                                   print('segIdx {}, asig.name {}'.format(
  1340                                                                       segIdx, asig.name))
  1341                                                                   print('asig._global_channel_indexes = {}'.format(
  1342                                                                       asig._global_channel_indexes))
  1343                                                                   print('asig references {}'.format(
  1344                                                                       da[asig._global_channel_indexes[0]]))
  1345                                                                   try:
  1346                                                                       assert (
  1347                                                                           asig.name
  1348                                                                           in da[asig._global_channel_indexes[0]].name)
  1349                                                                   except Exception:
  1350                                                                       traceback.print_exc()
  1351                                                               rawWaveforms = [
  1352                                                                   asig.load(
  1353                                                                       time_slice=(t + windowSize[0], t + windowSize[1]))
  1354                                                                   for t in alignEvents]
  1355                                                               if any([rW.shape[0] < nominalWinLen for rW in rawWaveforms]):
  1356                                                                   rawWaveforms = [
  1357                                                                       asig.load(
  1358                                                                           time_slice=(t + windowSize[0], t + windowSize[1] + asig.sampling_period))
  1359                                                                       for t in alignEvents]
  1360                                                           elif isinstance(asig, AnalogSignal):
  1361                                                               rawWaveforms = []
  1362                                                               for t in alignEvents:
  1363                                                                   asigMask = (asig.times > t + windowSize[0]) & (asig.times < t + windowSize[1])
  1364                                                                   rawWaveforms.append(asig[asigMask[:, np.newaxis]])
  1365                                                           else:
  1366                                                               raise(Exception('{} must be an AnalogSignal or AnalogSignalProxy!'.format(asigName)))
  1367                                                           #
  1368                                                           samplingRate = asig.sampling_rate
  1369                                                           waveformUnits = rawWaveforms[0].units
  1370                                                           #  fix length if roundoff error
  1371                                                           #  minLen = min([rW.shape[0] for rW in rawWaveforms])
  1372                                                           rawWaveforms = [rW[:nominalWinLen] for rW in rawWaveforms]
  1373                                                           #
  1374                                                           spikeWaveforms = (
  1375                                                               np.hstack([rW.magnitude for rW in rawWaveforms])
  1376                                                               .transpose()[:, np.newaxis, :] * waveformUnits
  1377                                                               )
  1378                                                           #
  1379                                                           thisUnit = masterBlock.filter(
  1380                                                               objects=Unit, name=chanName + '#0')[0]
  1381                                                           skipEventAnnNames = (
  1382                                                               ['nix_name', 'neo_name']
  1383                                                               )
  1384                                                           stAnn = {
  1385                                                               k: v
  1386                                                               for k, v in alignEvents.annotations.items()
  1387                                                               if k not in skipEventAnnNames
  1388                                                               }
  1389                                                           skipAsigAnnNames = (
  1390                                                               ['channel_id', 'nix_name', 'neo_name']
  1391                                                               )
  1392                                                           stAnn.update({
  1393                                                               k: v
  1394                                                               for k, v in asig.annotations.items()
  1395                                                               if k not in skipAsigAnnNames
  1396                                                           })
  1397                                                           st = SpikeTrain(
  1398                                                               name='seg{}_{}'.format(int(totalNSegs), thisUnit.name),
  1399                                                               times=alignEvents.times,
  1400                                                               waveforms=spikeWaveforms,
  1401                                                               t_start=asig.t_start, t_stop=asig.t_stop,
  1402                                                               left_sweep=windowSize[0] * (-1),
  1403                                                               sampling_rate=samplingRate,
  1404                                                               **stAnn
  1405                                                               )
  1406                                                           st.annotate(nix_name=st.name)
  1407                                                           st.annotations['unitAnnotations'] = json.dumps(
  1408                                                               thisUnit.annotations.copy())
  1409                                                           thisUnit.spiketrains.append(st)
  1410                                                           newSeg.spiketrains.append(st)
  1411                                                           st.unit = thisUnit
  1412                                                       totalNSegs += 1
  1413                                               try:
  1414                                                   eventBlock.filter(
  1415                                                       objects=EventProxy)[0]._rawio.file.close()
  1416                                               except Exception:
  1417                                                   traceback.print_exc()
  1418                                               if signalBlock is not eventBlock:
  1419                                                   try:
  1420                                                       signalBlock.filter(
  1421                                                           objects=AnalogSignalProxy)[0]._rawio.file.close()
  1422                                                   except Exception:
  1423                                                       traceback.print_exc()
  1424                                               triggeredPath = os.path.join(
  1425                                                   folderPath, fileName + '.nix')
  1426                                               if not os.path.exists(triggeredPath):
  1427                                                   appendToExisting = False
  1428                                           
  1429                                               if appendToExisting:
  1430                                                   allSegs = list(range(len(masterBlock.segments)))
  1431                                                   addBlockToNIX(
  1432                                                       masterBlock, neoSegIdx=allSegs,
  1433                                                       writeSpikes=True,
  1434                                                       fileName=fileName,
  1435                                                       folderPath=folderPath,
  1436                                                       purgeNixNames=False,
  1437                                                       nixBlockIdx=0, nixSegIdx=allSegs)
  1438                                               else:
  1439                                                   if os.path.exists(triggeredPath):
  1440                                                       os.remove(triggeredPath)
  1441                                                   masterBlock = purgeNixAnn(masterBlock)
  1442                                                   writer = NixIO(filename=triggeredPath)
  1443                                                   writer.write_block(masterBlock, use_obj_names=True)
  1444                                                   writer.close()
  1445                                               return masterBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: alignedAsigDFtoSpikeTrain at line 1447

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1447                                           @profile
  1448                                           def alignedAsigDFtoSpikeTrain(
  1449                                                   allWaveforms, dataBlock=None, matchSamplingRate=True):
  1450                                               masterBlock = Block()
  1451                                               masterBlock.name = dataBlock.annotations['neo_name']
  1452                                               masterBlock.annotate(nix_name=dataBlock.annotations['neo_name'])
  1453                                               for segIdx, group in allWaveforms.groupby('segment'):
  1454                                                   print('Saving trajectoriess for segment {}'.format(segIdx))
  1455                                                   dataSeg = dataBlock.segments[segIdx]
  1456                                                   exSt = dataSeg.spiketrains[0]
  1457                                                   if isinstance(exSt, SpikeTrainProxy):
  1458                                                       print(
  1459                                                           'alignedAsigDFtoSpikeTrain basing seg {} on {}'
  1460                                                           .format(segIdx, exSt.name))
  1461                                                       stProxy = exSt
  1462                                                       exSt = loadStProxy(stProxy)
  1463                                                       exSt = loadObjArrayAnn(exSt)
  1464                                                   print('exSt.left_sweep is {}'.format(exSt.left_sweep))
  1465                                                   wfBins = ((np.arange(exSt.waveforms.shape[2]) / (exSt.sampling_rate)) - exSt.left_sweep).magnitude
  1466                                                   # seg to contain triggered time series
  1467                                                   newSeg = Segment(name=dataSeg.annotations['neo_name'])
  1468                                                   newSeg.annotate(nix_name=dataSeg.annotations['neo_name'])
  1469                                                   masterBlock.segments.append(newSeg)
  1470                                                   #
  1471                                                   if group.columns.name == 'bin':
  1472                                                       grouper = group.groupby('feature')
  1473                                                       colsAre = 'bin'
  1474                                                   elif group.columns.name == 'feature':
  1475                                                       grouper = group.iteritems()
  1476                                                       colsAre = 'feature'
  1477                                                   for featName, featGroup in grouper:
  1478                                                       print('Saving {}...'.format(featName))
  1479                                                       if featName[-2:] == '#0':
  1480                                                           cleanFeatName = featName
  1481                                                       else:
  1482                                                           cleanFeatName = featName + '#0'
  1483                                                       if segIdx == 0:
  1484                                                           #  allocate units
  1485                                                           chanIdx = ChannelIndex(
  1486                                                               name=cleanFeatName, index=[0])
  1487                                                           chanIdx.annotate(nix_name=chanIdx.name)
  1488                                                           thisUnit = Unit(name=chanIdx.name)
  1489                                                           thisUnit.annotate(nix_name=chanIdx.name)
  1490                                                           chanIdx.units.append(thisUnit)
  1491                                                           thisUnit.channel_index = chanIdx
  1492                                                           masterBlock.channel_indexes.append(chanIdx)
  1493                                                       else:
  1494                                                           thisUnit = masterBlock.filter(
  1495                                                               objects=Unit, name=cleanFeatName)[0]
  1496                                                       if colsAre == 'bin':
  1497                                                           spikeWaveformsDF = featGroup
  1498                                                       elif colsAre == 'feature':
  1499                                                           if isinstance(featGroup, pd.Series):
  1500                                                               featGroup = featGroup.to_frame(name=featName)
  1501                                                               featGroup.columns.name = 'feature'
  1502                                                           spikeWaveformsDF = transposeSpikeDF(
  1503                                                               featGroup,
  1504                                                               'bin', fastTranspose=True)
  1505                                                       if matchSamplingRate:
  1506                                                           if len(spikeWaveformsDF.columns) != len(wfBins):
  1507                                                               wfDF = spikeWaveformsDF.reset_index(drop=True).T
  1508                                                               wfDF = hf.interpolateDF(wfDF, wfBins)
  1509                                                               spikeWaveformsDF = wfDF.T.set_index(spikeWaveformsDF.index)
  1510                                                       spikeWaveforms = spikeWaveformsDF.to_numpy()[:, np.newaxis, :]
  1511                                                       arrAnnDF = spikeWaveformsDF.index.to_frame()
  1512                                                       spikeTimes = arrAnnDF['t']
  1513                                                       arrAnnDF.drop(columns='t', inplace=True)
  1514                                                       arrAnn = {}
  1515                                                       colsToKeep = arrAnnDF.columns.drop(['originalIndex', 'feature', 'segment', 'lag'])
  1516                                                       for cName in colsToKeep:
  1517                                                           values = arrAnnDF[cName].to_numpy()
  1518                                                           if isinstance(values[0], str):
  1519                                                               values = values.astype('U')
  1520                                                           arrAnn.update({str(cName): values.flatten()})
  1521                                                       arrayAnnNames = {
  1522                                                           'arrayAnnNames': list(arrAnn.keys())}
  1523                                                       st = SpikeTrain(
  1524                                                           name='seg{}_{}'.format(int(segIdx), thisUnit.name),
  1525                                                           times=spikeTimes.to_numpy() * exSt.units,
  1526                                                           waveforms=spikeWaveforms * pq.dimensionless,
  1527                                                           t_start=exSt.t_start, t_stop=exSt.t_stop,
  1528                                                           left_sweep=exSt.left_sweep,
  1529                                                           sampling_rate=exSt.sampling_rate,
  1530                                                           **arrAnn, **arrayAnnNames
  1531                                                           )
  1532                                                       st.annotate(nix_name=st.name)
  1533                                                       thisUnit.spiketrains.append(st)
  1534                                                       newSeg.spiketrains.append(st)
  1535                                                       st.unit = thisUnit
  1536                                               return masterBlock

Total time: 0.398358 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: dataFrameToAnalogSignals at line 1538

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1538                                           @profile
  1539                                           def dataFrameToAnalogSignals(
  1540                                                   df,
  1541                                                   block=None, seg=None,
  1542                                                   idxT='NSPTime',
  1543                                                   probeName='insTD', samplingRate=500*pq.Hz,
  1544                                                   timeUnits=pq.s, measureUnits=pq.mV,
  1545                                                   dataCol=['channel_0', 'channel_1'],
  1546                                                   useColNames=False, forceColNames=None,
  1547                                                   namePrefix='', nameSuffix='', verbose=False):
  1548         1         21.0     21.0      0.0      if block is None:
  1549         1          9.0      9.0      0.0          assert seg is None
  1550         1        473.0    473.0      0.0          block = Block(name=probeName)
  1551         1        367.0    367.0      0.0          seg = Segment(name='seg0_' + probeName)
  1552         1         12.0     12.0      0.0          block.segments.append(seg)
  1553         1          6.0      6.0      0.0      if verbose:
  1554         1        606.0    606.0      0.0          print('in dataFrameToAnalogSignals...')
  1555        57        979.0     17.2      0.0      for idx, colName in enumerate(dataCol):
  1556        56        471.0      8.4      0.0          if verbose:
  1557        56      40115.0    716.3      1.0              print('    {}'.format(colName))
  1558        56        750.0     13.4      0.0          if forceColNames is not None:
  1559        56        633.0     11.3      0.0              chanName = forceColNames[idx]
  1560                                                   elif useColNames:
  1561                                                       chanName = namePrefix + colName + nameSuffix
  1562                                                   else:
  1563                                                       chanName = namePrefix + (probeName.lower() + '{}'.format(idx)) + nameSuffix
  1564                                                   #
  1565        56        663.0     11.8      0.0          chanIdx = ChannelIndex(
  1566        56        570.0     10.2      0.0              name=chanName,
  1567                                                       # index=None,
  1568        56      40356.0    720.6      1.0              index=np.asarray([idx]),
  1569                                                       # channel_names=np.asarray([chanName])
  1570                                                       )
  1571        56       1161.0     20.7      0.0          block.channel_indexes.append(chanIdx)
  1572        56        475.0      8.5      0.0          asig = AnalogSignal(
  1573        56    1652175.0  29503.1     41.5              df[colName].to_numpy() * measureUnits,
  1574        56       1884.0     33.6      0.0              name='seg0_' + chanName,
  1575        56        590.0     10.5      0.0              sampling_rate=samplingRate,
  1576        56    2100592.0  37510.6     52.7              dtype=np.float32,
  1577                                                       # **ann
  1578                                                       )
  1579        56       1622.0     29.0      0.0          if idxT is not None:
  1580        56      84993.0   1517.7      2.1              asig.t_start = df[idxT].iloc[0] * timeUnits
  1581                                                   else:
  1582                                                       asig.t_start = df.index[0] * timeUnits
  1583        56        776.0     13.9      0.0          asig.channel_index = chanIdx
  1584                                                   # assign ownership to containers
  1585        56       1245.0     22.2      0.0          chanIdx.analogsignals.append(asig)
  1586        56       1077.0     19.2      0.0          seg.analogsignals.append(asig)
  1587        56      33441.0    597.2      0.8          chanIdx.create_relationship()
  1588                                               # assign parent to children
  1589         1      16796.0  16796.0      0.4      block.create_relationship()
  1590         1        713.0    713.0      0.0      seg.create_relationship()
  1591         1          7.0      7.0      0.0      return block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: eventDataFrameToEvents at line 1593

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1593                                           @profile
  1594                                           def eventDataFrameToEvents(
  1595                                                   eventDF, idxT=None,
  1596                                                   annCol=None,
  1597                                                   eventName='', tUnits=pq.s,
  1598                                                   makeList=True
  1599                                                   ):
  1600                                               if makeList:
  1601                                                   eventList = []
  1602                                                   for colName in annCol:
  1603                                                       originalDType = type(eventDF[colName].to_numpy()[0]).__name__
  1604                                                       event = Event(
  1605                                                           name=eventName + colName,
  1606                                                           times=eventDF[idxT].to_numpy() * tUnits,
  1607                                                           labels=eventDF[colName].astype(originalDType).to_numpy()
  1608                                                           )
  1609                                                       event.annotate(originalDType=originalDType)
  1610                                                       eventList.append(event)
  1611                                                   return eventList
  1612                                               else:
  1613                                                   if annCol is None:
  1614                                                       annCol = eventDF.drop(columns=idxT).columns
  1615                                                   event = Event(
  1616                                                       name=eventName,
  1617                                                       times=eventDF[idxT].to_numpy() * tUnits,
  1618                                                       labels=np.asarray(eventDF.index)
  1619                                                       )
  1620                                                   event.annotations.update(
  1621                                                       {
  1622                                                           'arrayAnnNames': [],
  1623                                                           'arrayAnnDTypes': []
  1624                                                           })
  1625                                                   for colName in annCol:
  1626                                                       originalDType = type(eventDF[colName].to_numpy()[0]).__name__
  1627                                                       arrayAnn = eventDF[colName].astype(originalDType).to_numpy()
  1628                                                       event.array_annotations.update(
  1629                                                           {colName: arrayAnn})
  1630                                                       event.annotations['arrayAnnNames'].append(colName)
  1631                                                       event.annotations['arrayAnnDTypes'].append(originalDType)
  1632                                                       event.annotations.update(
  1633                                                           {colName: arrayAnn})
  1634                                                   return event

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: eventsToDataFrame at line 1636

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1636                                           @profile
  1637                                           def eventsToDataFrame(
  1638                                                   events, idxT='t', names=None
  1639                                                   ):
  1640                                               eventDict = {}
  1641                                               calculatedT = False
  1642                                               for event in events:
  1643                                                   if names is not None:
  1644                                                       if event.name not in names:
  1645                                                           continue
  1646                                                   if len(event.times):
  1647                                                       if not calculatedT:
  1648                                                           t = pd.Series(event.times.magnitude)
  1649                                                           calculatedT = True
  1650                                                       try:
  1651                                                           values = event.array_annotations['labels']
  1652                                                       except Exception:
  1653                                                           values = event.labels
  1654                                                       if isinstance(values[0], bytes):
  1655                                                           #  event came from hdf, need to recover dtype
  1656                                                           if 'originalDType' in event.annotations:
  1657                                                               dtypeStr = event.annotations['originalDType'].split(';')[-1]
  1658                                                               if 'np.' not in dtypeStr:
  1659                                                                   dtypeStr = 'np.' + dtypeStr
  1660                                                               originalDType = eval(dtypeStr)
  1661                                                               values = np.asarray(values, dtype=originalDType)
  1662                                                           else:
  1663                                                               values = np.asarray(values, dtype=np.str)
  1664                                                       #  print(values.dtype)
  1665                                                       eventDict.update({
  1666                                                           event.name: pd.Series(values)})
  1667                                               eventDict.update({idxT: t})
  1668                                               return pd.concat(eventDict, axis=1)

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadSpikeMats at line 1670

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1670                                           @profile
  1671                                           def loadSpikeMats(
  1672                                                   dataPath, rasterOpts,
  1673                                                   alignTimes=None, chans=None, loadAll=False,
  1674                                                   absoluteBins=False, transposeSpikeMat=False,
  1675                                                   checkReferences=False,
  1676                                                   aggregateFun=None):
  1677                                           
  1678                                               reader = nixio_fr.NixIO(filename=dataPath)
  1679                                               chanNames = reader.header['signal_channels']['name']
  1680                                               
  1681                                               if chans is not None:
  1682                                                   sigMask = np.isin(chanNames, chans)
  1683                                                   chanNames = chanNames[sigMask]
  1684                                                   
  1685                                               chanIdx = reader.channel_name_to_index(chanNames)
  1686                                               
  1687                                               if not loadAll:
  1688                                                   assert alignTimes is not None
  1689                                                   spikeMats = {i: None for i in alignTimes.index}
  1690                                                   validTrials = pd.Series(True, index=alignTimes.index)
  1691                                               else:
  1692                                                   spikeMats = {
  1693                                                       i: None for i in range(reader.segment_count(block_index=0))}
  1694                                                   validTrials = None
  1695                                               
  1696                                               for segIdx in range(reader.segment_count(block_index=0)):
  1697                                                   if checkReferences:
  1698                                                       for i, cIdx in enumerate(chanIdx):
  1699                                                           da = reader.da_list['blocks'][0]['segments'][segIdx]['data'][cIdx]
  1700                                                           print('name {}, da.name {}'.format(chanNames[i], da.name))
  1701                                                           try:
  1702                                                               assert chanNames[i] in da.name, 'reference problem!!'
  1703                                                           except Exception:
  1704                                                               traceback.print_exc()
  1705                                                   tStart = reader.get_signal_t_start(
  1706                                                       block_index=0, seg_index=segIdx)
  1707                                                   fs = reader.get_signal_sampling_rate(
  1708                                                       channel_indexes=chanIdx
  1709                                                       )
  1710                                                   sigSize = reader.get_signal_size(
  1711                                                       block_index=0, seg_index=segIdx
  1712                                                       )
  1713                                                   tStop = sigSize / fs + tStart
  1714                                                   #  convert to indices early to avoid floating point problems
  1715                                                   
  1716                                                   intervalIdx = int(round(rasterOpts['binInterval'] * fs))
  1717                                                   #  halfIntervalIdx = int(round(intervalIdx / 2))
  1718                                                   
  1719                                                   widthIdx = int(round(rasterOpts['binWidth'] * fs))
  1720                                                   halfWidthIdx = int(round(widthIdx / 2))
  1721                                                   
  1722                                                   if rasterOpts['smoothKernelWidth'] is not None:
  1723                                                       kernWidthIdx = int(round(rasterOpts['smoothKernelWidth'] * fs))
  1724                                                   
  1725                                                   theBins = None
  1726                                           
  1727                                                   if not loadAll:
  1728                                                       winStartIdx = int(round(rasterOpts['windowSize'][0] * fs))
  1729                                                       winStopIdx = int(round(rasterOpts['windowSize'][1] * fs))
  1730                                                       timeMask = (alignTimes > tStart) & (alignTimes < tStop)
  1731                                                       maskedTimes = alignTimes[timeMask]
  1732                                                   else:
  1733                                                       #  irrelevant, will load all
  1734                                                       maskedTimes = pd.Series(np.nan)
  1735                                           
  1736                                                   for idx, tOnset in maskedTimes.iteritems():
  1737                                                       if not loadAll:
  1738                                                           idxOnset = int(round((tOnset - tStart) * fs))
  1739                                                           #  can't not be ints
  1740                                                           iStart = idxOnset + winStartIdx - int(3 * halfWidthIdx)
  1741                                                           iStop = idxOnset + winStopIdx + int(3 * halfWidthIdx)
  1742                                                       else:
  1743                                                           winStartIdx = 0
  1744                                                           iStart = 0
  1745                                                           iStop = sigSize
  1746                                           
  1747                                                       if iStart < 0:
  1748                                                           #  near the first edge
  1749                                                           validTrials[idx] = False
  1750                                                       elif (sigSize < iStop):
  1751                                                           #  near the ending edge
  1752                                                           validTrials[idx] = False
  1753                                                       else:
  1754                                                           #  valid slices
  1755                                                           try:
  1756                                                               rawSpikeMat = pd.DataFrame(
  1757                                                                   reader.get_analogsignal_chunk(
  1758                                                                       block_index=0, seg_index=segIdx,
  1759                                                                       i_start=iStart, i_stop=iStop,
  1760                                                                       channel_names=chanNames))
  1761                                                           except Exception:
  1762                                                               traceback.print_exc()
  1763                                                               #
  1764                                                           if aggregateFun is None:
  1765                                                               procSpikeMat = rawSpikeMat.rolling(
  1766                                                                   window=3 * widthIdx, center=True,
  1767                                                                   win_type='gaussian'
  1768                                                                   ).mean(std=halfWidthIdx)
  1769                                                           else:
  1770                                                               procSpikeMat = rawSpikeMat.rolling(
  1771                                                                   window=widthIdx, center=True
  1772                                                                   ).apply(
  1773                                                                       aggregateFun,
  1774                                                                       raw=True,
  1775                                                                       kwargs={'fs': fs, 'nSamp': widthIdx})
  1776                                                           #
  1777                                                           if rasterOpts['smoothKernelWidth'] is not None:
  1778                                                               procSpikeMat = (
  1779                                                                   procSpikeMat
  1780                                                                   .rolling(
  1781                                                                       window=3 * kernWidthIdx, center=True,
  1782                                                                       win_type='gaussian')
  1783                                                                   .mean(std=kernWidthIdx/2)
  1784                                                                   .dropna().iloc[::intervalIdx, :]
  1785                                                               )
  1786                                                           else:
  1787                                                               procSpikeMat = (
  1788                                                                   procSpikeMat
  1789                                                                   .dropna().iloc[::intervalIdx, :]
  1790                                                               )
  1791                                           
  1792                                                           procSpikeMat.columns = chanNames
  1793                                                           procSpikeMat.columns.name = 'unit'
  1794                                                           if theBins is None:
  1795                                                               theBins = np.asarray(
  1796                                                                   procSpikeMat.index + winStartIdx) / fs
  1797                                                           if absoluteBins:
  1798                                                               procSpikeMat.index = theBins + idxOnset / fs
  1799                                                           else:
  1800                                                               procSpikeMat.index = theBins
  1801                                                           procSpikeMat.index.name = 'bin'
  1802                                                           if loadAll:
  1803                                                               smIdx = segIdx
  1804                                                           else:
  1805                                                               smIdx = idx
  1806                                                               
  1807                                                           spikeMats[smIdx] = procSpikeMat
  1808                                                           if transposeSpikeMat:
  1809                                                               spikeMats[smIdx] = spikeMats[smIdx].transpose()
  1810                                                       #  plt.imshow(rawSpikeMat.to_numpy(), aspect='equal'); plt.show()
  1811                                               return spikeMats, validTrials

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: synchronizeINStoNSP at line 1813

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1813                                           @profile
  1814                                           def synchronizeINStoNSP(
  1815                                                   tapTimestampsNSP=None, tapTimestampsINS=None,
  1816                                                   precalculatedFun=None,
  1817                                                   NSPTimeRanges=(None, None),
  1818                                                   td=None, accel=None, insBlock=None, trialSegment=None, degree=1,
  1819                                                   trimSpiketrains=False
  1820                                                   ):
  1821                                               print('Trial Segment {}'.format(trialSegment))
  1822                                               if precalculatedFun is None:
  1823                                                   assert ((tapTimestampsNSP is not None) & (tapTimestampsINS is not None))
  1824                                                   # sanity check that the intervals match
  1825                                                   insDiff = tapTimestampsINS.diff().dropna().values
  1826                                                   nspDiff = tapTimestampsNSP.diff().dropna().values
  1827                                                   print('On the INS, the diff() between taps was\n{}'.format(insDiff))
  1828                                                   print('On the NSP, the diff() between taps was\n{}'.format(nspDiff))
  1829                                                   print('This amounts to a msec difference of\n{}'.format(
  1830                                                       (insDiff - nspDiff) * 1e3))
  1831                                                   if (insDiff - nspDiff > 20e-3).any():
  1832                                                       raise(Exception('Tap trains too different!'))
  1833                                                   #
  1834                                                   if degree > 0:
  1835                                                       synchPolyCoeffsINStoNSP = np.polyfit(
  1836                                                           x=tapTimestampsINS.values, y=tapTimestampsNSP.values,
  1837                                                           deg=degree)
  1838                                                   else:
  1839                                                       timeOffset = tapTimestampsNSP.values - tapTimestampsINS.values
  1840                                                       synchPolyCoeffsINStoNSP = np.array([1, np.mean(timeOffset)])
  1841                                                   timeInterpFunINStoNSP = np.poly1d(synchPolyCoeffsINStoNSP)
  1842                                               else:
  1843                                                   timeInterpFunINStoNSP = precalculatedFun
  1844                                               if td is not None:
  1845                                                   td.loc[:, 'NSPTime'] = pd.Series(
  1846                                                       timeInterpFunINStoNSP(td['t']), index=td['t'].index)
  1847                                                   td.loc[:, 'NSPTime'] = timeInterpFunINStoNSP(td['t'].to_numpy())
  1848                                               if accel is not None:
  1849                                                   accel.loc[:, 'NSPTime'] = pd.Series(
  1850                                                       timeInterpFunINStoNSP(accel['t']), index=accel['t'].index)
  1851                                               if insBlock is not None:
  1852                                                   # allUnits = [st.unit for st in insBlock.segments[0].spiketrains]
  1853                                                   # [un.name for un in insBlock.filter(objects=Unit)]
  1854                                                   for unit in insBlock.filter(objects=Unit):
  1855                                                       tStart = NSPTimeRanges[0]
  1856                                                       tStop = NSPTimeRanges[1]
  1857                                                       uniqueSt = []
  1858                                                       for st in unit.spiketrains:
  1859                                                           if st not in uniqueSt:
  1860                                                               uniqueSt.append(st)
  1861                                                           else:
  1862                                                               continue
  1863                                                           print('Synchronizing {}'.format(st.name))
  1864                                                           if len(st.times):
  1865                                                               segMaskSt = np.array(
  1866                                                                   st.array_annotations['trialSegment'],
  1867                                                                   dtype=np.int) == trialSegment
  1868                                                               st.magnitude[segMaskSt] = (
  1869                                                                   timeInterpFunINStoNSP(st.times[segMaskSt].magnitude))
  1870                                                               if trimSpiketrains:
  1871                                                                   print('Trimming spiketrain')
  1872                                                                   #  kludgey fix for weirdness concerning t_start
  1873                                                                   st.t_start = min(tStart, st.times[0] * 0.999)
  1874                                                                   st.t_stop = min(tStop, st.times[-1] * 1.001)
  1875                                                                   validMask = st < st.t_stop
  1876                                                                   if ~validMask.all():
  1877                                                                       print('Deleted some spikes')
  1878                                                                       st = st[validMask]
  1879                                                                       # delete invalid spikes
  1880                                                                       if 'arrayAnnNames' in st.annotations.keys():
  1881                                                                           for key in st.annotations['arrayAnnNames']:
  1882                                                                               try:
  1883                                                                                   # st.annotations[key] = np.array(st.array_annotations[key])
  1884                                                                                   st.annotations[key] = np.delete(st.annotations[key], ~validMask)
  1885                                                                               except Exception:
  1886                                                                                   traceback.print_exc()
  1887                                                                                   pdb.set_trace()
  1888                                                           else:
  1889                                                               if trimSpiketrains:
  1890                                                                   st.t_start = tStart
  1891                                                                   st.t_stop = tStop
  1892                                                   #
  1893                                                   allEvents = [
  1894                                                       ev
  1895                                                       for ev in insBlock.filter(objects=Event)
  1896                                                       if ('ins' in ev.name) and ('concatenate' not in ev.name)]
  1897                                                   concatEvents = [
  1898                                                       ev
  1899                                                       for ev in insBlock.filter(objects=Event)
  1900                                                       if ('ins' in ev.name) and ('concatenate' in ev.name)]
  1901                                                   eventsDF = eventsToDataFrame(allEvents, idxT='t')
  1902                                                   newNames = {i: childBaseName(i, 'seg') for i in eventsDF.columns}
  1903                                                   eventsDF.rename(columns=newNames, inplace=True)
  1904                                                   segMask = hf.getStimSerialTrialSegMask(eventsDF, trialSegment)
  1905                                                   evTStart = eventsDF.loc[segMask, 't'].min() * pq.s
  1906                                                   evTStop = eventsDF.loc[segMask, 't'].max() * pq.s
  1907                                                   # print('allEvents[0].shape = {}'.format(allEvents[0].shape))
  1908                                                   # print('allEvents[0].magnitude[segMask][0] = {}'.format(allEvents[0].magnitude[segMask][0]))
  1909                                                   for event in (allEvents + concatEvents):
  1910                                                       if trimSpiketrains:
  1911                                                           thisSegMask = (event.times >= evTStart) & (event.times <= evTStop)
  1912                                                       else:
  1913                                                           thisSegMask = (event.times >= evTStart) & (event.times < evTStop)
  1914                                                       event.magnitude[thisSegMask] = (
  1915                                                           timeInterpFunINStoNSP(event.times[thisSegMask].magnitude))
  1916                                                   # print('allEvents[0].magnitude[segMask][0] = {}'.format(allEvents[0].magnitude[segMask][0]))
  1917                                                   # if len(concatEvents) > trialSegment:
  1918                                                   #     concatEvents[trialSegment].magnitude[:] = timeInterpFunINStoNSP(
  1919                                                   #         concatEvents[trialSegment].times[:].magnitude)
  1920                                               return td, accel, insBlock, timeInterpFunINStoNSP

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: findSegsIncluding at line 1922

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1922                                           @profile
  1923                                           def findSegsIncluding(
  1924                                                   block, timeSlice=None):
  1925                                               segBoundsList = []
  1926                                               for segIdx, seg in enumerate(block.segments):
  1927                                                   segBoundsList.append(pd.DataFrame({
  1928                                                       't_start': seg.t_start,
  1929                                                       't_stop': seg.t_stop
  1930                                                       }, index=[segIdx]))
  1931                                           
  1932                                               segBounds = pd.concat(segBoundsList)
  1933                                               if timeSlice[0] is not None:
  1934                                                   segMask = (segBounds['t_start'] * pq.s >= timeSlice[0]) & (
  1935                                                       segBounds['t_stop'] * pq.s <= timeSlice[1])
  1936                                                   requestedSegs = segBounds.loc[segMask, :]
  1937                                               else:
  1938                                                   timeSlice = (None, None)
  1939                                                   requestedSegs = segBounds
  1940                                               return segBounds, requestedSegs

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: findSegsIncluded at line 1942

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1942                                           @profile
  1943                                           def findSegsIncluded(
  1944                                                   block, timeSlice=None):
  1945                                               segBoundsList = []
  1946                                               for segIdx, seg in enumerate(block.segments):
  1947                                                   segBoundsList.append(pd.DataFrame({
  1948                                                       't_start': seg.t_start,
  1949                                                       't_stop': seg.t_stop
  1950                                                       }, index=[segIdx]))
  1951                                           
  1952                                               segBounds = pd.concat(segBoundsList)
  1953                                               if timeSlice[0] is not None:
  1954                                                   segMask = (segBounds['t_start'] * pq.s <= timeSlice[0]) | (
  1955                                                       segBounds['t_stop'] * pq.s >= timeSlice[1])
  1956                                                   requestedSegs = segBounds.loc[segMask, :]
  1957                                               else:
  1958                                                   timeSlice = (None, None)
  1959                                                   requestedSegs = segBounds
  1960                                               return segBounds, requestedSegs

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getElecLookupTable at line 1962

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1962                                           @profile
  1963                                           def getElecLookupTable(
  1964                                                   block, elecIds=None):
  1965                                               lookupTableList = []
  1966                                               for metaIdx, chanIdx in enumerate(block.channel_indexes):
  1967                                                   if chanIdx.analogsignals:
  1968                                                       #  print(chanIdx.name)
  1969                                                       lookupTableList.append(pd.DataFrame({
  1970                                                           'channelNames': np.asarray(chanIdx.channel_names, dtype=np.str),
  1971                                                           'index': chanIdx.index,
  1972                                                           'metaIndex': metaIdx * chanIdx.index**0,
  1973                                                           'localIndex': (
  1974                                                               list(range(chanIdx.analogsignals[0].shape[1])))
  1975                                                           }))
  1976                                               lookupTable = pd.concat(lookupTableList, ignore_index=True)
  1977                                           
  1978                                               if elecIds is None:
  1979                                                   requestedIndices = lookupTable
  1980                                               else:
  1981                                                   if isinstance(elecIds[0], str):
  1982                                                       idxMask = lookupTable['channelNames'].isin(elecIds)
  1983                                                       requestedIndices = lookupTable.loc[idxMask, :]
  1984                                               return lookupTable, requestedIndices

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getNIXData at line 1986

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1986                                           @profile
  1987                                           def getNIXData(
  1988                                                   fileName=None,
  1989                                                   folderPath=None,
  1990                                                   reader=None, blockIdx=0,
  1991                                                   elecIds=None, startTime_s=None,
  1992                                                   dataLength_s=None, downsample=1,
  1993                                                   signal_group_mode='group-by-same-units',
  1994                                                   closeReader=False):
  1995                                               #  Open file and extract headers
  1996                                               if reader is None:
  1997                                                   assert (fileName is not None) and (folderPath is not None)
  1998                                                   filePath = os.path.join(folderPath, fileName) + '.nix'
  1999                                                   reader = nixio_fr.NixIO(filename=filePath)
  2000                                           
  2001                                               block = reader.read_block(
  2002                                                   block_index=blockIdx, lazy=True,
  2003                                                   signal_group_mode=signal_group_mode)
  2004                                           
  2005                                               for segIdx, seg in enumerate(block.segments):
  2006                                                   seg.events = [i.load() for i in seg.events]
  2007                                                   seg.epochs = [i.load() for i in seg.epochs]
  2008                                           
  2009                                               # find elecIds
  2010                                               lookupTable, requestedIndices = getElecLookupTable(
  2011                                                   block, elecIds=elecIds)
  2012                                           
  2013                                               # find segments that contain the requested times
  2014                                               if dataLength_s is not None:
  2015                                                   assert startTime_s is not None
  2016                                                   timeSlice = (
  2017                                                       startTime_s * pq.s,
  2018                                                       (startTime_s + dataLength_s) * pq.s)
  2019                                               else:
  2020                                                   timeSlice = (None, None)
  2021                                               segBounds, requestedSegs = findSegsIncluding(block, timeSlice)
  2022                                               #
  2023                                               data = pd.DataFrame(columns=elecIds + ['t'])
  2024                                               for segIdx in requestedSegs.index:
  2025                                                   seg = block.segments[segIdx]
  2026                                                   if dataLength_s is not None:
  2027                                                       timeSlice = (
  2028                                                           max(timeSlice[0], seg.t_start),
  2029                                                           min(timeSlice[1], seg.t_stop)
  2030                                                           )
  2031                                                   else:
  2032                                                       timeSlice = (seg.t_start, seg.t_stop)
  2033                                                   segData = pd.DataFrame()
  2034                                                   for metaIdx in pd.unique(requestedIndices['metaIndex']):
  2035                                                       metaIdxMatch = requestedIndices['metaIndex'] == metaIdx
  2036                                                       theseRequestedIndices = requestedIndices.loc[
  2037                                                           metaIdxMatch, :]
  2038                                                       theseElecIds = theseRequestedIndices['channelNames']
  2039                                                       asig = seg.analogsignals[metaIdx]
  2040                                                       thisTimeSlice = (
  2041                                                           max(timeSlice[0], asig.t_start),
  2042                                                           min(timeSlice[1], asig.t_stop)
  2043                                                           )
  2044                                                       reqData = asig.load(
  2045                                                           time_slice=thisTimeSlice,
  2046                                                           channel_indexes=theseRequestedIndices['localIndex'].to_numpy())
  2047                                                       segData = pd.concat((
  2048                                                               segData,
  2049                                                               pd.DataFrame(
  2050                                                                   reqData.magnitude, columns=theseElecIds.to_numpy())),
  2051                                                           axis=1)
  2052                                                   segT = reqData.times
  2053                                                   segData['t'] = segT
  2054                                                   data = pd.concat(
  2055                                                       (data, segData),
  2056                                                       axis=0, ignore_index=True)
  2057                                               channelData = {
  2058                                                   'data': data,
  2059                                                   't': data['t']
  2060                                                   }
  2061                                               if closeReader:
  2062                                                   reader.file.close()
  2063                                                   block = None
  2064                                                   # closing the reader breaks its connection to the block
  2065                                               return channelData, block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: childBaseName at line 2067

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2067                                           @profile
  2068                                           def childBaseName(
  2069                                                   childName, searchTerm):
  2070                                               if searchTerm in childName:
  2071                                                   baseName = '_'.join(childName.split('_')[1:])
  2072                                               else:
  2073                                                   baseName = childName
  2074                                               return baseName

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: readBlockFixNames at line 2076

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2076                                           @profile
  2077                                           def readBlockFixNames(
  2078                                                   rawioReader,
  2079                                                   block_index=0, signal_group_mode='split-all',
  2080                                                   lazy=True, mapDF=None, reduceChannelIndexes=False,
  2081                                                   loadList=None, purgeNixNames=False
  2082                                                   ):
  2083                                               headerSignalChan = pd.DataFrame(
  2084                                                   rawioReader.header['signal_channels']).set_index('id')
  2085                                               headerUnitChan = pd.DataFrame(
  2086                                                   rawioReader.header['unit_channels']).set_index('id')
  2087                                               dataBlock = rawioReader.read_block(
  2088                                                   block_index=block_index, lazy=lazy,
  2089                                                   signal_group_mode=signal_group_mode)
  2090                                               if dataBlock.name is None:
  2091                                                   if 'neo_name' in dataBlock.annotations:
  2092                                                       dataBlock.name = dataBlock.annotations['neo_name']
  2093                                               #  on first segment, rename the chan_indexes and units
  2094                                               seg0 = dataBlock.segments[0]
  2095                                               asigLikeList = (
  2096                                                   seg0.filter(objects=AnalogSignalProxy) +
  2097                                                   seg0.filter(objects=AnalogSignal))
  2098                                               if mapDF is not None:
  2099                                                   if headerSignalChan.size > 0:
  2100                                                       asigNameChanger = {}
  2101                                                       for nevID in mapDF['nevID']:
  2102                                                           if int(nevID) in headerSignalChan.index:
  2103                                                               labelFromMap = (
  2104                                                                   mapDF
  2105                                                                   .loc[mapDF['nevID'] == nevID, 'label']
  2106                                                                   .iloc[0])
  2107                                                               asigNameChanger[
  2108                                                                   headerSignalChan.loc[int(nevID), 'name']] = labelFromMap
  2109                                                   else:
  2110                                                       asigOrigNames = np.unique(
  2111                                                           [i.split('#')[0] for i in headerUnitChan['name']])
  2112                                                       asigNameChanger = {}
  2113                                                       for origName in asigOrigNames:
  2114                                                           # ripple specific
  2115                                                           formattedName = origName.replace('.', '_').replace(' raw', '')
  2116                                                           if mapDF['label'].str.contains(formattedName).any():
  2117                                                               asigNameChanger[origName] = formattedName
  2118                                               else:
  2119                                                   asigNameChanger = dict()
  2120                                               for asig in asigLikeList:
  2121                                                   asigBaseName = childBaseName(asig.name, 'seg')
  2122                                                   asig.name = (
  2123                                                       asigNameChanger[asigBaseName]
  2124                                                       if asigBaseName in asigNameChanger
  2125                                                       else asigBaseName)
  2126                                                   if mapDF is not None:
  2127                                                       if (mapDF['label'] == asig.name).any():
  2128                                                           asig.annotations['xCoords'] = float(mapDF.loc[mapDF['label'] == asig.name, 'xcoords'].iloc[0])
  2129                                                           asig.annotations['yCoords'] = float(mapDF.loc[mapDF['label'] == asig.name, 'ycoords'].iloc[0])
  2130                                                           asig.annotations['zCoords'] = float(mapDF.loc[mapDF['label'] == asig.name, 'zcoords'].iloc[0])
  2131                                                   if 'Channel group ' in asig.channel_index.name:
  2132                                                       newChanName = (
  2133                                                           asigNameChanger[asigBaseName]
  2134                                                           if asigBaseName in asigNameChanger
  2135                                                           else asigBaseName)
  2136                                                       asig.channel_index.name = newChanName
  2137                                                       if 'neo_name' in asig.channel_index.annotations:
  2138                                                           asig.channel_index.annotations['neo_name'] = newChanName
  2139                                                       if 'nix_name' in asig.channel_index.annotations:
  2140                                                           asig.channel_index.annotations['nix_name'] = newChanName
  2141                                                       if mapDF is not None:
  2142                                                           try:
  2143                                                               asig.channel_index.coordinates = np.asarray([
  2144                                                                   asig.annotations['xCoords'], asig.annotations['yCoords'], asig.annotations['zCoords']
  2145                                                               ])[np.newaxis, :] * pq.um
  2146                                                           except Exception:
  2147                                                               pass
  2148                                               spikeTrainLikeList = (
  2149                                                   seg0.filter(objects=SpikeTrainProxy) +
  2150                                                   seg0.filter(objects=SpikeTrain))
  2151                                               # add channels for channelIndex that has no asigs but has spikes
  2152                                               nExtraChans = 0
  2153                                               for stp in spikeTrainLikeList:
  2154                                                   stpBaseName = childBaseName(stp.name, 'seg')
  2155                                                   nameParser = re.search(r'ch(\d*)#(\d*)', stpBaseName)
  2156                                                   if nameParser is not None:
  2157                                                       # first time at this unit, rename it
  2158                                                       chanId = int(nameParser.group(1))
  2159                                                       unitId = int(nameParser.group(2))
  2160                                                       if chanId >= 5121:
  2161                                                           isRippleStimChan = True
  2162                                                           chanId = chanId - 5120
  2163                                                       else:
  2164                                                           isRippleStimChan = False
  2165                                                       ####################
  2166                                                       # asigBaseName = headerSignalChan.loc[chanId, 'name']
  2167                                                       # if mapDF is not None:
  2168                                                       #     if asigBaseName in asigNameChanger:
  2169                                                       #         chanIdLabel = (
  2170                                                       #             asigNameChanger[asigBaseName]
  2171                                                       #             if asigBaseName in asigNameChanger
  2172                                                       #             else asigBaseName)
  2173                                                       #     else:
  2174                                                       #         chanIdLabel = asigBaseName
  2175                                                       # else:
  2176                                                       #     chanIdLabel = asigBaseName
  2177                                                       ###################
  2178                                                       # if swapMaps is not None:
  2179                                                       #     nameCandidates = (swapMaps['to'].loc[swapMaps['to']['nevID'] == chanId, 'label']).to_list()
  2180                                                       # elif mapDF is not None:
  2181                                                       #     nameCandidates = (mapDF.loc[mapDF['nevID'] == chanId, 'label']).to_list()
  2182                                                       # else:
  2183                                                       #     nameCandidates = []
  2184                                                       ##############################
  2185                                                       if mapDF is not None:
  2186                                                           nameCandidates = (
  2187                                                               mapDF
  2188                                                               .loc[mapDF['nevID'] == chanId, 'label']
  2189                                                               .to_list())
  2190                                                       else:
  2191                                                           nameCandidates = []
  2192                                                       if len(nameCandidates) == 1:
  2193                                                           chanIdLabel = nameCandidates[0]
  2194                                                       elif chanId in headerSignalChan:
  2195                                                           chanIdLabel = headerSignalChan.loc[chanId, 'name']
  2196                                                       else:
  2197                                                           chanIdLabel = 'ch{}'.format(chanId)
  2198                                                       #
  2199                                                       if isRippleStimChan:
  2200                                                           stp.name = '{}_stim#{}'.format(chanIdLabel, unitId)
  2201                                                       else:
  2202                                                           stp.name = '{}#{}'.format(chanIdLabel, unitId)
  2203                                                       stp.unit.name = stp.name
  2204                                                   ########################################
  2205                                                   # sanitize ripple names ####
  2206                                                   stp.name = stp.name.replace('.', '_').replace(' raw', '')
  2207                                                   stp.unit.name = stp.unit.name.replace('.', '_').replace(' raw', '')
  2208                                                   ###########################################
  2209                                                   if 'ChannelIndex for ' in stp.unit.channel_index.name:
  2210                                                       newChanName = stp.name.replace('_stim#0', '')
  2211                                                       # remove unit #
  2212                                                       newChanName = re.sub(r'#\d', '', newChanName)
  2213                                                       stp.unit.channel_index.name = newChanName
  2214                                                       # units and analogsignals have different channel_indexes when loaded by nix
  2215                                                       # add them to each other's parent list
  2216                                                       allMatchingChIdx = dataBlock.filter(
  2217                                                           objects=ChannelIndex, name=newChanName)
  2218                                                       if (len(allMatchingChIdx) > 1) and reduceChannelIndexes:
  2219                                                           assert len(allMatchingChIdx) == 2
  2220                                                           targetChIdx = [
  2221                                                               ch
  2222                                                               for ch in allMatchingChIdx
  2223                                                               if ch is not stp.unit.channel_index][0]
  2224                                                           oldChIdx = stp.unit.channel_index
  2225                                                           targetChIdx.units.append(stp.unit)
  2226                                                           stp.unit.channel_index = targetChIdx
  2227                                                           oldChIdx.units.remove(stp.unit)
  2228                                                           if not (len(oldChIdx.units) or len(oldChIdx.analogsignals)):
  2229                                                               dataBlock.channel_indexes.remove(oldChIdx)
  2230                                                           del oldChIdx
  2231                                                           targetChIdx.create_relationship()
  2232                                                       elif reduceChannelIndexes:
  2233                                                           if newChanName not in headerSignalChan['name']:
  2234                                                               stp.unit.channel_index.index = np.asarray(
  2235                                                                   [headerSignalChan['name'].size + nExtraChans])
  2236                                                               stp.unit.channel_index.channel_ids = np.asarray(
  2237                                                                   [headerSignalChan['name'].size + nExtraChans])
  2238                                                               stp.unit.channel_index.channel_names = np.asarray(
  2239                                                                   [newChanName])
  2240                                                               nExtraChans += 1
  2241                                                           if 'neo_name' not in allMatchingChIdx[0].annotations:
  2242                                                               allMatchingChIdx[0].annotations['neo_name'] = allMatchingChIdx[0].name
  2243                                                           if 'nix_name' not in allMatchingChIdx[0].annotations:
  2244                                                               allMatchingChIdx[0].annotations['nix_name'] = allMatchingChIdx[0].name
  2245                                                   stp.unit.channel_index.name = stp.unit.channel_index.name.replace('.', '_').replace(' raw', '')
  2246                                               #  rename the children
  2247                                               typesNeedRenaming = [
  2248                                                   SpikeTrainProxy, AnalogSignalProxy, EventProxy,
  2249                                                   SpikeTrain, AnalogSignal, Event]
  2250                                               for segIdx, seg in enumerate(dataBlock.segments):
  2251                                                   if seg.name is None:
  2252                                                       seg.name = 'seg{}_'.format(segIdx)
  2253                                                   else:
  2254                                                       if 'seg{}_'.format(segIdx) not in seg.name:
  2255                                                           seg.name = (
  2256                                                               'seg{}_{}'
  2257                                                               .format(
  2258                                                                   segIdx,
  2259                                                                   childBaseName(seg.name, 'seg')))
  2260                                                   for objType in typesNeedRenaming:
  2261                                                       for child in seg.filter(objects=objType):
  2262                                                           if 'seg{}_'.format(segIdx) not in child.name:
  2263                                                               child.name = (
  2264                                                                   'seg{}_{}'
  2265                                                                   .format(
  2266                                                                       segIdx, childBaseName(child.name, 'seg')))
  2267                                                           #  todo: decide if below is needed
  2268                                                           #  elif 'seg' in child.name:
  2269                                                           #      childBaseName = '_'.join(child.name.split('_')[1:])
  2270                                                           #      child.name = 'seg{}_{}'.format(segIdx, childBaseName)
  2271                                               # [i.name for i in dataBlock.filter(objects=Unit)]
  2272                                               # [i.name for i in dataBlock.filter(objects=ChannelIndex)]
  2273                                               # [i.name for i in dataBlock.filter(objects=SpikeTrain)]
  2274                                               # [i.name for i in dataBlock.filter(objects=SpikeTrainProxy)]
  2275                                               if lazy:
  2276                                                   for stP in dataBlock.filter(objects=SpikeTrainProxy):
  2277                                                       if 'unitAnnotations' in stP.annotations:
  2278                                                           unAnnStr = stP.annotations['unitAnnotations']
  2279                                                           stP.unit.annotations.update(json.loads(unAnnStr))
  2280                                               if (loadList is not None) and lazy:
  2281                                                   if 'asigs' in loadList:
  2282                                                       loadAsigList(
  2283                                                           dataBlock, listOfAsigProxyNames=loadList['asigs'],
  2284                                                           replaceInParents=True)
  2285                                                   if 'events' in loadList:
  2286                                                       loadEventList(
  2287                                                           dataBlock,
  2288                                                           listOfEventNames=loadList['events'],
  2289                                                           replaceInParents=True)
  2290                                                   if 'spiketrains' in loadList:
  2291                                                       loadSpikeTrainList(
  2292                                                           dataBlock,
  2293                                                           listOfSpikeTrainNames=loadList['spiketrains'],
  2294                                                           replaceInParents=True)
  2295                                               if purgeNixNames:
  2296                                                   dataBlock = purgeNixAnn(dataBlock)
  2297                                               return dataBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadSpikeTrainList at line 2299

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2299                                           @profile
  2300                                           def loadSpikeTrainList(
  2301                                                   dataBlock, listOfSpikeTrainNames=None,
  2302                                                   replaceInParents=True):
  2303                                               listOfSpikeTrains = []
  2304                                               if listOfSpikeTrainNames is None:
  2305                                                   listOfSpikeTrainNames = [
  2306                                                       stp.name
  2307                                                       for stp in dataBlock.filter(objects=SpikeTrainProxy)]
  2308                                               for stP in dataBlock.filter(objects=SpikeTrainProxy):
  2309                                                   if stP.name in listOfSpikeTrainNames:
  2310                                                       st = loadObjArrayAnn(stP.load())
  2311                                                       listOfSpikeTrains.append(st)
  2312                                                       if replaceInParents:
  2313                                                           seg = stP.segment
  2314                                                           segStNames = [s.name for s in seg.spiketrains]
  2315                                                           idxInSeg = segStNames.index(stP.name)
  2316                                                           seg.spiketrains[idxInSeg] = st
  2317                                                           #
  2318                                                           unit = stP.unit
  2319                                                           unitStNames = [s.name for s in unit.spiketrains]
  2320                                                           st.unit = unit
  2321                                                           idxInUnit = unitStNames.index(stP.name)
  2322                                                           unit.spiketrains[idxInUnit] = st
  2323                                               return listOfSpikeTrains

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadEventList at line 2325

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2325                                           @profile
  2326                                           def loadEventList(
  2327                                                   dataBlock,
  2328                                                   listOfEventNames=None, replaceInParents=True):
  2329                                               listOfEvents = []
  2330                                               if listOfEventNames is None:
  2331                                                   listOfEventNames = [
  2332                                                       evp.name
  2333                                                       for evp in dataBlock.filter(objects=EventProxy)]
  2334                                               for evP in dataBlock.filter(objects=EventProxy):
  2335                                                   if evP.name in listOfEventNames:
  2336                                                       ev = loadObjArrayAnn(evP.load())
  2337                                                       listOfEvents.append(ev)
  2338                                                       if replaceInParents:
  2339                                                           seg = evP.segment
  2340                                                           segEvNames = [e.name for e in seg.events]
  2341                                                           idxInSeg = segEvNames.index(evP.name)
  2342                                                           seg.events[idxInSeg] = ev
  2343                                               return listOfEvents

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadAsigList at line 2345

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2345                                           @profile
  2346                                           def loadAsigList(
  2347                                                   dataBlock, listOfAsigProxyNames=None, replaceInParents=True):
  2348                                               listOfAsigs = []
  2349                                               if listOfAsigProxyNames is None:
  2350                                                   listOfAsigProxyNames = [
  2351                                                       asigp.name
  2352                                                       for asigp in dataBlock.filter(objects=AnalogSignalProxy)]
  2353                                               for asigP in dataBlock.filter(objects=AnalogSignalProxy):
  2354                                                   if asigP.name in listOfAsigProxyNames:
  2355                                                       asig = asigP.load()
  2356                                                       asig.annotations = asigP.annotations.copy()
  2357                                                       listOfAsigs.append(asig)
  2358                                                       #
  2359                                                       if replaceInParents:
  2360                                                           seg = asigP.segment
  2361                                                           segAsigNames = [ag.name for ag in seg.analogsignals]
  2362                                                           asig.segment = seg
  2363                                                           idxInSeg = segAsigNames.index(asigP.name)
  2364                                                           seg.analogsignals[idxInSeg] = asig
  2365                                                           #
  2366                                                           chIdx = asigP.channel_index
  2367                                                           chIdxAsigNames = [ag.name for ag in chIdx.analogsignals]
  2368                                                           asig.channel_index = chIdx
  2369                                                           idxInChIdx = chIdxAsigNames.index(asigP.name)
  2370                                                           chIdx.analogsignals[idxInChIdx] = asig
  2371                                               return listOfAsigs

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: addBlockToNIX at line 2373

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2373                                           @profile
  2374                                           def addBlockToNIX(
  2375                                                   newBlock, neoSegIdx=[0],
  2376                                                   writeAsigs=True, writeSpikes=True, writeEvents=True,
  2377                                                   asigNameList=None,
  2378                                                   purgeNixNames=False,
  2379                                                   fileName=None,
  2380                                                   folderPath=None,
  2381                                                   nixBlockIdx=0, nixSegIdx=[0],
  2382                                                   ):
  2383                                               #  base file name
  2384                                               trialBasePath = os.path.join(folderPath, fileName)
  2385                                               if writeAsigs:
  2386                                                   # peek at file to ensure compatibility
  2387                                                   reader = nixio_fr.NixIO(filename=trialBasePath + '.nix')
  2388                                                   tempBlock = reader.read_block(
  2389                                                       block_index=nixBlockIdx,
  2390                                                       lazy=True, signal_group_mode='split-all')
  2391                                                   checkCompatible = {i: False for i in nixSegIdx}
  2392                                                   forceShape = {i: None for i in nixSegIdx}
  2393                                                   forceType = {i: None for i in nixSegIdx}
  2394                                                   forceFS = {i: None for i in nixSegIdx}
  2395                                                   for nixIdx in nixSegIdx:
  2396                                                       tempAsigList = tempBlock.segments[nixIdx].filter(
  2397                                                           objects=AnalogSignalProxy)
  2398                                                       if len(tempAsigList) > 0:
  2399                                                           tempAsig = tempAsigList[0]
  2400                                                           checkCompatible[nixIdx] = True
  2401                                                           forceType[nixIdx] = tempAsig.dtype
  2402                                                           forceShape[nixIdx] = tempAsig.shape[0]  # ? docs say shape[1], but that's confusing
  2403                                                           forceFS[nixIdx] = tempAsig.sampling_rate
  2404                                                   reader.file.close()
  2405                                               #  if newBlock was loaded from a nix file, strip the old nix_names away:
  2406                                               #  todo: replace with function from this module
  2407                                               if purgeNixNames:
  2408                                                   newBlock = purgeNixAnn(newBlock)
  2409                                               #
  2410                                               writer = NixIO(filename=trialBasePath + '.nix')
  2411                                               nixblock = writer.nix_file.blocks[nixBlockIdx]
  2412                                               nixblockName = nixblock.name
  2413                                               if 'nix_name' in newBlock.annotations.keys():
  2414                                                   try:
  2415                                                       assert newBlock.annotations['nix_name'] == nixblockName
  2416                                                   except Exception:
  2417                                                       newBlock.annotations['nix_name'] = nixblockName
  2418                                               else:
  2419                                                   newBlock.annotate(nix_name=nixblockName)
  2420                                               #
  2421                                               for idx, segIdx in enumerate(neoSegIdx):
  2422                                                   nixIdx = nixSegIdx[idx]
  2423                                                   newSeg = newBlock.segments[segIdx]
  2424                                                   nixgroup = nixblock.groups[nixIdx]
  2425                                                   nixSegName = nixgroup.name
  2426                                                   if 'nix_name' in newSeg.annotations.keys():
  2427                                                       try:
  2428                                                           assert newSeg.annotations['nix_name'] == nixSegName
  2429                                                       except Exception:
  2430                                                           newSeg.annotations['nix_name'] = nixSegName
  2431                                                   else:
  2432                                                       newSeg.annotate(nix_name=nixSegName)
  2433                                                   #
  2434                                                   if writeEvents:
  2435                                                       eventList = newSeg.events
  2436                                                       eventOrder = np.argsort([i.name for i in eventList])
  2437                                                       for event in [eventList[i] for i in eventOrder]:
  2438                                                           event = writer._write_event(event, nixblock, nixgroup)
  2439                                                   #
  2440                                                   if writeAsigs:
  2441                                                       asigList = newSeg.filter(objects=AnalogSignal)
  2442                                                       asigOrder = np.argsort([i.name for i in asigList])
  2443                                                       for asig in [asigList[i] for i in asigOrder]:
  2444                                                           if checkCompatible[nixIdx]:
  2445                                                               assert asig.dtype == forceType[nixIdx]
  2446                                                               assert asig.sampling_rate == forceFS[nixIdx]
  2447                                                               #  print('asig.shape[0] = {}'.format(asig.shape[0]))
  2448                                                               #  print('forceShape[nixIdx] = {}'.format(forceShape[nixIdx]))
  2449                                                               assert asig.shape[0] == forceShape[nixIdx]
  2450                                                           asig = writer._write_analogsignal(asig, nixblock, nixgroup)
  2451                                                       #  for isig in newSeg.filter(objects=IrregularlySampledSignal):
  2452                                                       #      isig = writer._write_irregularlysampledsignal(
  2453                                                       #          isig, nixblock, nixgroup)
  2454                                                   #
  2455                                                   if writeSpikes:
  2456                                                       stList = newSeg.filter(objects=SpikeTrain)
  2457                                                       stOrder = np.argsort([i.name for i in stList])
  2458                                                       for st in [stList[i] for i in stOrder]:
  2459                                                           st = writer._write_spiketrain(st, nixblock, nixgroup)
  2460                                               #
  2461                                               for chanIdx in newBlock.filter(objects=ChannelIndex):
  2462                                                   chanIdx = writer._write_channelindex(chanIdx, nixblock)
  2463                                                   #  auto descends into units inside of _write_channelindex
  2464                                               writer._create_source_links(newBlock, nixblock)
  2465                                               writer.close()
  2466                                               print('Done adding block to Nix.')
  2467                                               return newBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadStProxy at line 2469

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2469                                           @profile
  2470                                           def loadStProxy(stProxy):
  2471                                               try:
  2472                                                   st = stProxy.load(
  2473                                                       magnitude_mode='rescaled',
  2474                                                       load_waveforms=True)
  2475                                               except Exception:
  2476                                                   st = stProxy.load(
  2477                                                       magnitude_mode='rescaled',
  2478                                                       load_waveforms=False)
  2479                                                   st.waveforms = np.asarray([]).reshape((0, 0, 0))*pq.mV
  2480                                               return st

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: preproc at line 2482

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2482                                           @profile
  2483                                           def preproc(
  2484                                                   fileName='Trial001',
  2485                                                   rawFolderPath='./',
  2486                                                   outputFolderPath='./', mapDF=None,
  2487                                                   # swapMaps=None,
  2488                                                   electrodeArrayName='utah',
  2489                                                   fillOverflow=True, removeJumps=True,
  2490                                                   removeMeanAcross=False,
  2491                                                   linearDetrend=False,
  2492                                                   interpolateOutliers=False, calcOutliers=False,
  2493                                                   outlierMaskFilterOpts=None,
  2494                                                   outlierThreshold=1,
  2495                                                   calcArtifactTrace=False,
  2496                                                   motorEncoderMask=None,
  2497                                                   calcAverageLFP=False,
  2498                                                   eventInfo=None,
  2499                                                   spikeSourceType='', spikePath=None,
  2500                                                   chunkSize=1800, equalChunks=True, chunkList=None, chunkOffset=0,
  2501                                                   writeMode='rw',
  2502                                                   signal_group_mode='split-all', trialInfo=None,
  2503                                                   asigNameList=None, ainpNameList=None, nameSuffix='',
  2504                                                   saveFromAsigNameList=True,
  2505                                                   calcRigEvents=True, normalizeByImpedance=False,
  2506                                                   LFPFilterOpts=None, encoderCountPerDegree=180e2,
  2507                                                   outlierRemovalDebugFlag=False, impedanceFilePath=None
  2508                                                   ):
  2509                                               #  base file name
  2510                                               rawBasePath = os.path.join(rawFolderPath, fileName)
  2511                                               outputFilePath = os.path.join(
  2512                                                   outputFolderPath,
  2513                                                   fileName + nameSuffix + '.nix')
  2514                                               if os.path.exists(outputFilePath):
  2515                                                   os.remove(outputFilePath)
  2516                                               #  instantiate reader, get metadata
  2517                                               print('Loading\n{}\n'.format(rawBasePath))
  2518                                               reader = BlackrockIO(
  2519                                                   filename=rawBasePath, nsx_to_load=5)
  2520                                               reader.parse_header()
  2521                                               # metadata = reader.header
  2522                                               #  absolute section index
  2523                                               dummyBlock = readBlockFixNames(
  2524                                                   reader,
  2525                                                   block_index=0, lazy=True,
  2526                                                   signal_group_mode=signal_group_mode,
  2527                                                   mapDF=mapDF, reduceChannelIndexes=True,
  2528                                                   # swapMaps=swapMaps
  2529                                                   )
  2530                                               segLen = dummyBlock.segments[0].analogsignals[0].shape[0] / (
  2531                                                   dummyBlock.segments[0].analogsignals[0].sampling_rate)
  2532                                               nChunks = math.ceil(segLen / chunkSize)
  2533                                               #
  2534                                               if equalChunks:
  2535                                                   actualChunkSize = (segLen / nChunks).magnitude
  2536                                               else:
  2537                                                   actualChunkSize = chunkSize
  2538                                               if chunkList is None:
  2539                                                   chunkList = range(nChunks)
  2540                                               chunkingMetadata = {}
  2541                                               for chunkIdx in chunkList:
  2542                                                   print('preproc on chunk {}'.format(chunkIdx))
  2543                                                   #  instantiate spike reader if requested
  2544                                                   if spikeSourceType == 'tdc':
  2545                                                       if spikePath is None:
  2546                                                           spikePath = os.path.join(
  2547                                                               outputFolderPath, 'tdc_' + fileName,
  2548                                                               'tdc_' + fileName + '.nix')
  2549                                                       print('loading {}'.format(spikePath))
  2550                                                       spikeReader = nixio_fr.NixIO(filename=spikePath)
  2551                                                   else:
  2552                                                       spikeReader = None
  2553                                                   #  absolute section index
  2554                                                   block = readBlockFixNames(
  2555                                                       reader,
  2556                                                       block_index=0, lazy=True,
  2557                                                       signal_group_mode=signal_group_mode,
  2558                                                       mapDF=mapDF, reduceChannelIndexes=True,
  2559                                                       # swapMaps=swapMaps
  2560                                                       )
  2561                                                   if spikeReader is not None:
  2562                                                       spikeBlock = readBlockFixNames(
  2563                                                           spikeReader, block_index=0, lazy=True,
  2564                                                           signal_group_mode=signal_group_mode,
  2565                                                           mapDF=mapDF, reduceChannelIndexes=True,
  2566                                                           # swapMaps=swapMaps
  2567                                                           )
  2568                                                       spikeBlock = purgeNixAnn(spikeBlock)
  2569                                                   else:
  2570                                                       spikeBlock = None
  2571                                                   #
  2572                                                   #  instantiate writer
  2573                                                   if (nChunks == 1) or (len(chunkList) == 1):
  2574                                                       partNameSuffix = ""
  2575                                                       thisChunkOutFilePath = outputFilePath
  2576                                                   else:
  2577                                                       partNameSuffix = '_pt{:0>3}'.format(chunkIdx)
  2578                                                       thisChunkOutFilePath = (
  2579                                                           outputFilePath
  2580                                                           .replace('.nix', partNameSuffix + '.nix'))
  2581                                                   #
  2582                                                   if os.path.exists(thisChunkOutFilePath):
  2583                                                       os.remove(thisChunkOutFilePath)
  2584                                                   writer = NixIO(
  2585                                                       filename=thisChunkOutFilePath, mode=writeMode)
  2586                                                   chunkTStart = chunkIdx * actualChunkSize + chunkOffset
  2587                                                   chunkTStop = (chunkIdx + 1) * actualChunkSize + chunkOffset
  2588                                                   chunkingMetadata[chunkIdx] = {
  2589                                                       'filename': thisChunkOutFilePath,
  2590                                                       'partNameSuffix': partNameSuffix,
  2591                                                       'chunkTStart': chunkTStart,
  2592                                                       'chunkTStop': chunkTStop}
  2593                                                   block.annotate(chunkTStart=chunkTStart)
  2594                                                   block.annotate(chunkTStop=chunkTStop)
  2595                                                   block.annotate(
  2596                                                       recDatetimeStr=(
  2597                                                           block
  2598                                                           .rec_datetime
  2599                                                           .replace(tzinfo=timezone.utc)
  2600                                                           .isoformat())
  2601                                                       )
  2602                                                   #
  2603                                                   preprocBlockToNix(
  2604                                                       block, writer,
  2605                                                       chunkTStart=chunkTStart,
  2606                                                       chunkTStop=chunkTStop,
  2607                                                       fillOverflow=fillOverflow,
  2608                                                       removeJumps=removeJumps,
  2609                                                       interpolateOutliers=interpolateOutliers,
  2610                                                       calcOutliers=calcOutliers,
  2611                                                       outlierThreshold=outlierThreshold,
  2612                                                       outlierMaskFilterOpts=outlierMaskFilterOpts,
  2613                                                       calcArtifactTrace=calcArtifactTrace,
  2614                                                       linearDetrend=linearDetrend,
  2615                                                       motorEncoderMask=motorEncoderMask,
  2616                                                       electrodeArrayName=electrodeArrayName,
  2617                                                       calcAverageLFP=calcAverageLFP,
  2618                                                       eventInfo=eventInfo,
  2619                                                       asigNameList=asigNameList, ainpNameList=ainpNameList,
  2620                                                       saveFromAsigNameList=saveFromAsigNameList,
  2621                                                       spikeSourceType=spikeSourceType,
  2622                                                       spikeBlock=spikeBlock,
  2623                                                       calcRigEvents=calcRigEvents,
  2624                                                       normalizeByImpedance=normalizeByImpedance,
  2625                                                       removeMeanAcross=removeMeanAcross,
  2626                                                       LFPFilterOpts=LFPFilterOpts,
  2627                                                       encoderCountPerDegree=encoderCountPerDegree,
  2628                                                       outlierRemovalDebugFlag=outlierRemovalDebugFlag,
  2629                                                       impedanceFilePath=impedanceFilePath,
  2630                                                       )
  2631                                                   #### diagnostics
  2632                                                   diagnosticFolder = os.path.join(
  2633                                                       outputFolderPath,
  2634                                                       'preprocDiagnostics',
  2635                                                       # fileName + nameSuffix + partNameSuffix
  2636                                                       )
  2637                                                   if not os.path.exists(diagnosticFolder):
  2638                                                       os.mkdir(diagnosticFolder)
  2639                                                   asigDiagnostics = {}
  2640                                                   outlierDiagnostics = {}
  2641                                                   diagnosticText = ''
  2642                                                   for asig in block.filter(objects=AnalogSignal):
  2643                                                       annNames = ['mean_removal_r2', 'mean_removal_group']
  2644                                                       for annName in annNames:
  2645                                                           if annName in asig.annotations:
  2646                                                               if asig.name not in asigDiagnostics:
  2647                                                                   asigDiagnostics[asig.name] = {}
  2648                                                               asigDiagnostics[asig.name].update({
  2649                                                                   annName: asig.annotations[annName]})
  2650                                                       annNames = [
  2651                                                           'outlierProportion', 'nDim',
  2652                                                           'noveltyThreshold', 'outlierThreshold'
  2653                                                           ]
  2654                                                       for annName in annNames:
  2655                                                           if annName in asig.annotations:
  2656                                                               if asig.name not in outlierDiagnostics:
  2657                                                                   outlierDiagnostics[asig.name] = {}
  2658                                                               outlierDiagnostics[asig.name].update({
  2659                                                                   annName: '{}'.format(asig.annotations[annName])
  2660                                                               })
  2661                                                   if removeMeanAcross:
  2662                                                       asigDiagnosticsDF = pd.DataFrame(asigDiagnostics).T
  2663                                                       asigDiagnosticsDF.sort_values(by='mean_removal_r2', inplace=True)
  2664                                                       diagnosticText += '<h2>LFP Diagnostics</h2>\n'
  2665                                                       diagnosticText += asigDiagnosticsDF.to_html()
  2666                                                       fig, ax = plt.subplots()
  2667                                                       sns.distplot(asigDiagnosticsDF['mean_removal_r2'], ax=ax)
  2668                                                       ax.set_ylabel('Count of analog signals')
  2669                                                       ax.set_xlabel('R^2 of regressing mean against signal')
  2670                                                       fig.savefig(os.path.join(
  2671                                                               diagnosticFolder,
  2672                                                               fileName + nameSuffix + partNameSuffix + '_meanRemovalR2.png'
  2673                                                           ))
  2674                                                   if interpolateOutliers:
  2675                                                       outlierDiagnosticsDF = pd.DataFrame(outlierDiagnostics).T
  2676                                                       diagnosticText += '<h2>Outlier Diagnostics</h2>\n'
  2677                                                       diagnosticText += outlierDiagnosticsDF.to_html()
  2678                                                   diagnosticTextPath = os.path.join(
  2679                                                       diagnosticFolder,
  2680                                                       fileName + nameSuffix + partNameSuffix + '_asigDiagnostics.html'
  2681                                                       )
  2682                                                   with open(diagnosticTextPath, 'w') as _f:
  2683                                                       _f.write(diagnosticText)
  2684                                                   writer.close()
  2685                                               chunkingInfoPath = os.path.join(
  2686                                                   outputFolderPath,
  2687                                                   fileName + nameSuffix +
  2688                                                   '_chunkingInfo.json'
  2689                                                   )
  2690                                               if os.path.exists(chunkingInfoPath):
  2691                                                   os.remove(chunkingInfoPath)
  2692                                               with open(chunkingInfoPath, 'w') as f:
  2693                                                   json.dump(chunkingMetadata, f)
  2694                                               return

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: preprocBlockToNix at line 2696

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2696                                           @profile
  2697                                           def preprocBlockToNix(
  2698                                                   block, writer,
  2699                                                   chunkTStart=None,
  2700                                                   chunkTStop=None,
  2701                                                   eventInfo=None,
  2702                                                   fillOverflow=False, calcAverageLFP=False,
  2703                                                   interpolateOutliers=False, calcOutliers=False,
  2704                                                   calcArtifactTrace=False,
  2705                                                   outlierMaskFilterOpts=None,
  2706                                                   useMeanToCenter=False,   # mean center? median center?
  2707                                                   linearDetrend=False,
  2708                                                   zScoreEachTrace=False,
  2709                                                   outlierThreshold=1,
  2710                                                   motorEncoderMask=None,
  2711                                                   electrodeArrayName='utah',
  2712                                                   removeJumps=False, trackMemory=True,
  2713                                                   asigNameList=None, ainpNameList=None,
  2714                                                   saveFromAsigNameList=True,
  2715                                                   spikeSourceType='', spikeBlock=None,
  2716                                                   calcRigEvents=True,
  2717                                                   normalizeByImpedance=True,
  2718                                                   impedanceFilePath=None,
  2719                                                   removeMeanAcross=False,
  2720                                                   LFPFilterOpts=None, encoderCountPerDegree=180e2,
  2721                                                   outlierRemovalDebugFlag=False,
  2722                                                   ):
  2723                                               #  prune out nev spike placeholders
  2724                                               #  (will get added back on a chunk by chunk basis,
  2725                                               #  if not pruning units)
  2726                                               if spikeSourceType == 'nev':
  2727                                                   pruneOutUnits = False
  2728                                               else:
  2729                                                   pruneOutUnits = True
  2730                                               #
  2731                                               for chanIdx in block.channel_indexes:
  2732                                                   if chanIdx.units:
  2733                                                       for unit in chanIdx.units:
  2734                                                           if unit.spiketrains:
  2735                                                               unit.spiketrains = []
  2736                                                       if pruneOutUnits:
  2737                                                           chanIdx.units = []
  2738                                               #
  2739                                               if spikeBlock is not None:
  2740                                                   for chanIdx in spikeBlock.channel_indexes:
  2741                                                       if chanIdx.units:
  2742                                                           for unit in chanIdx.units:
  2743                                                               if unit.spiketrains:
  2744                                                                   unit.spiketrains = []
  2745                                               #  precalculate new segment
  2746                                               seg = block.segments[0]
  2747                                               #  remove chanIndexes assigned to units; makes more sense to
  2748                                               #  only use chanIdx for asigs and spikes on that asig
  2749                                               #  block.channel_indexes = (
  2750                                               #      [chanIdx for chanIdx in block.channel_indexes if (
  2751                                               #          chanIdx.analogsignals)])
  2752                                               if calcAverageLFP:
  2753                                                   lastIndex = len(block.channel_indexes)
  2754                                                   lastID = block.channel_indexes[-1].channel_ids[0] + 1
  2755                                                   if asigNameList is None:
  2756                                                       asigNameList = [
  2757                                                           [
  2758                                                               childBaseName(a.name, 'seg')
  2759                                                               for a in seg.analogsignals
  2760                                                               if not (('ainp' in a.name) or ('analog' in a.name))]
  2761                                                           ]
  2762                                                   nMeanChans = len(asigNameList)
  2763                                                   #
  2764                                                   meanChIdxList = []
  2765                                                   for meanChIdx in range(nMeanChans):
  2766                                                       tempChIdx = ChannelIndex(
  2767                                                           index=[lastIndex + meanChIdx],
  2768                                                           channel_names=['{}_rawAverage_{}'.format(electrodeArrayName, meanChIdx)],
  2769                                                           channel_ids=[lastID + meanChIdx],
  2770                                                           name='{}_rawAverage_{}'.format(electrodeArrayName, meanChIdx),
  2771                                                           file_origin=block.channel_indexes[-1].file_origin
  2772                                                           )
  2773                                                       tempChIdx.merge_annotations(block.channel_indexes[-1])
  2774                                                       block.channel_indexes.append(tempChIdx)
  2775                                                       meanChIdxList.append(tempChIdx)
  2776                                                       lastIndex += 1
  2777                                                       lastID += 1
  2778                                                   lastIndex = len(block.channel_indexes)
  2779                                                   lastID = block.channel_indexes[-1].channel_ids[0] + 1
  2780                                                   # if calcArtifactTrace:
  2781                                                   if True:
  2782                                                       artChIdxList = []
  2783                                                       for artChIdx in range(nMeanChans):
  2784                                                           tempChIdx = ChannelIndex(
  2785                                                               index=[lastIndex + artChIdx],
  2786                                                               channel_names=['{}_artifact_{}'.format(electrodeArrayName, artChIdx)],
  2787                                                               channel_ids=[lastID + artChIdx],
  2788                                                               name='{}_artifact_{}'.format(electrodeArrayName, artChIdx),
  2789                                                               file_origin=block.channel_indexes[-1].file_origin
  2790                                                               )
  2791                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2792                                                           block.channel_indexes.append(tempChIdx)
  2793                                                           artChIdxList.append(tempChIdx)
  2794                                                           lastIndex += 1
  2795                                                           lastID += 1
  2796                                                   # if calcOutliers:
  2797                                                   if True:
  2798                                                       devChIdxList = []
  2799                                                       for devChIdx in range(nMeanChans):
  2800                                                           tempChIdx = ChannelIndex(
  2801                                                               index=[lastIndex + devChIdx],
  2802                                                               channel_names=['{}_deviation_{}'.format(electrodeArrayName, devChIdx)],
  2803                                                               channel_ids=[lastID + devChIdx],
  2804                                                               name='{}_deviation_{}'.format(electrodeArrayName, devChIdx),
  2805                                                               file_origin=block.channel_indexes[-1].file_origin
  2806                                                               )
  2807                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2808                                                           block.channel_indexes.append(tempChIdx)
  2809                                                           devChIdxList.append(tempChIdx)
  2810                                                           lastIndex += 1
  2811                                                           lastID += 1
  2812                                                       smDevChIdxList = []
  2813                                                       for devChIdx in range(nMeanChans):
  2814                                                           tempChIdx = ChannelIndex(
  2815                                                               index=[lastIndex + devChIdx],
  2816                                                               channel_names=['{}_smoothed_deviation_{}'.format(electrodeArrayName, devChIdx)],
  2817                                                               channel_ids=[lastID + devChIdx],
  2818                                                               name='{}_smoothed_deviation_{}'.format(electrodeArrayName, devChIdx),
  2819                                                               file_origin=block.channel_indexes[-1].file_origin
  2820                                                               )
  2821                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2822                                                           block.channel_indexes.append(tempChIdx)
  2823                                                           smDevChIdxList.append(tempChIdx)
  2824                                                           lastIndex += 1
  2825                                                           lastID += 1
  2826                                                       outMaskChIdxList = []
  2827                                                       for outMaskChIdx in range(nMeanChans):
  2828                                                           tempChIdx = ChannelIndex(
  2829                                                               index=[lastIndex + outMaskChIdx],
  2830                                                               channel_names=['{}_outlierMask_{}'.format(
  2831                                                                   electrodeArrayName, outMaskChIdx)],
  2832                                                               channel_ids=[lastID + outMaskChIdx],
  2833                                                               name='{}_outlierMask_{}'.format(
  2834                                                                   electrodeArrayName, outMaskChIdx),
  2835                                                               file_origin=block.channel_indexes[-1].file_origin
  2836                                                               )
  2837                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2838                                                           block.channel_indexes.append(tempChIdx)
  2839                                                           outMaskChIdxList.append(tempChIdx)
  2840                                                           lastIndex += 1
  2841                                                           lastID += 1
  2842                                               #  delete asig and irsig proxies from channel index list
  2843                                               for metaIdx, chanIdx in enumerate(block.channel_indexes):
  2844                                                   if chanIdx.analogsignals:
  2845                                                       chanIdx.analogsignals = []
  2846                                                   if chanIdx.irregularlysampledsignals:
  2847                                                       chanIdx.irregularlysampledsignals = []
  2848                                               newSeg = Segment(
  2849                                                       index=0, name=seg.name,
  2850                                                       description=seg.description,
  2851                                                       file_origin=seg.file_origin,
  2852                                                       file_datetime=seg.file_datetime,
  2853                                                       rec_datetime=seg.rec_datetime,
  2854                                                       **seg.annotations
  2855                                                   )
  2856                                               block.segments = [newSeg]
  2857                                               block, nixblock = writer.write_block_meta(block)
  2858                                               # descend into Segments
  2859                                               if impedanceFilePath is not None:
  2860                                                   try:
  2861                                                       impedances = prb_meta.getLatestImpedance(
  2862                                                           block=block, impedanceFilePath=impedanceFilePath)
  2863                                                       averageImpedance = impedances['impedance'].median()
  2864                                                   except Exception:
  2865                                                       traceback.print_exc()
  2866                                               # for segIdx, seg in enumerate(oldSegList):
  2867                                               if spikeBlock is not None:
  2868                                                   spikeSeg = spikeBlock.segments[0]
  2869                                               else:
  2870                                                   spikeSeg = seg
  2871                                               #
  2872                                               if trackMemory:
  2873                                                   print('memory usage: {:.1f} MB'.format(
  2874                                                       prf.memory_usage_psutil()))
  2875                                               newSeg, nixgroup = writer._write_segment_meta(newSeg, nixblock)
  2876                                               #  trim down list of analog signals if necessary
  2877                                               asigNameListSeg = []
  2878                                               if (removeMeanAcross or calcAverageLFP):
  2879                                                   meanGroups = {}
  2880                                               for subListIdx, subList in enumerate(asigNameList):
  2881                                                   subListSeg = [
  2882                                                       'seg{}_{}'.format(0, a)
  2883                                                       for a in subList]
  2884                                                   asigNameListSeg += subListSeg
  2885                                                   if (removeMeanAcross or calcAverageLFP):
  2886                                                       meanGroups[subListIdx] = subListSeg
  2887                                               aSigList = []
  2888                                               # [asig.name for asig in seg.analogsignals]
  2889                                               for a in seg.analogsignals:
  2890                                                   # if np.any([n in a.name for n in asigNameListSeg]):
  2891                                                   if a.name in asigNameListSeg:
  2892                                                       aSigList.append(a)
  2893                                               if ainpNameList is not None:
  2894                                                   ainpNameListSeg = [
  2895                                                       'seg{}_{}'.format(0, a)
  2896                                                       for a in ainpNameList]
  2897                                                   ainpList = []
  2898                                                   for a in seg.analogsignals:
  2899                                                       if np.any([n == a.name for n in ainpNameListSeg]):
  2900                                                           ainpList.append(a)
  2901                                               else:
  2902                                                   ainpList = [
  2903                                                       a
  2904                                                       for a in seg.analogsignals
  2905                                                       if (('ainp' in a.name) or ('analog' in a.name))]
  2906                                                   ainpNameListSeg = [a.name for a in aSigList]
  2907                                               nAsigs = len(aSigList)
  2908                                               if LFPFilterOpts is not None:
  2909                                                   def filterFun(sig, filterCoeffs=None):
  2910                                                       # sig[:] = signal.sosfiltfilt(
  2911                                                       sig[:] = signal.sosfilt(
  2912                                                           filterCoeffs, sig.magnitude.flatten())[:, np.newaxis] * sig.units
  2913                                                       return sig
  2914                                                   filterCoeffs = hf.makeFilterCoeffsSOS(
  2915                                                       LFPFilterOpts, float(seg.analogsignals[0].sampling_rate))
  2916                                                   if False:
  2917                                                       fig, ax1, ax2 = hf.plotFilterResponse(
  2918                                                           filterCoeffs,
  2919                                                           float(seg.analogsignals[0].sampling_rate))
  2920                                                       fig2, ax3, ax4 = hf.plotFilterImpulseResponse(
  2921                                                           LFPFilterOpts,
  2922                                                           float(seg.analogsignals[0].sampling_rate))
  2923                                                       plt.show()
  2924                                               # first pass through asigs, if removing mean across channels
  2925                                               if (removeMeanAcross or calcAverageLFP):
  2926                                                   for aSigIdx, aSigProxy in enumerate(seg.analogsignals):
  2927                                                       if aSigIdx == 0:
  2928                                                           # check bounds
  2929                                                           tStart = max(chunkTStart * pq.s, aSigProxy.t_start)
  2930                                                           tStop = min(chunkTStop * pq.s, aSigProxy.t_stop)
  2931                                                       loadThisOne = (aSigProxy in aSigList)
  2932                                                       if loadThisOne:
  2933                                                           if trackMemory:
  2934                                                               print(
  2935                                                                   'Extracting asig for mean, memory usage: {:.1f} MB'.format(
  2936                                                                       prf.memory_usage_psutil()))
  2937                                                           chanIdx = aSigProxy.channel_index
  2938                                                           asig = aSigProxy.load(
  2939                                                               time_slice=(tStart, tStop),
  2940                                                               magnitude_mode='rescaled')
  2941                                                           if 'tempLFPStore' not in locals():
  2942                                                               tempLFPStore = pd.DataFrame(
  2943                                                                   np.zeros(
  2944                                                                       (asig.shape[0], nAsigs),
  2945                                                                       dtype=np.float32),
  2946                                                                   columns=asigNameListSeg)
  2947                                                           if 'dummyAsig' not in locals():
  2948                                                               dummyAsig = asig.copy()
  2949                                                           #  perform requested preproc operations
  2950                                                           #  if LFPFilterOpts is not None:
  2951                                                           #      asig[:] = filterFun(
  2952                                                           #          asig, filterCoeffs=filterCoeffs)
  2953                                                           if normalizeByImpedance:
  2954                                                               elNmMatchMsk = impedances['elec'] == chanIdx.name
  2955                                                               '''
  2956                                                               asig.magnitude[:] = (
  2957                                                                   (asig.magnitude - np.median(asig.magnitude)) /
  2958                                                                   np.min(
  2959                                                                       impedances.loc[elNmMatchMsk, 'impedance']
  2960                                                                       ))
  2961                                                               '''
  2962                                                               asig.magnitude[:] = (
  2963                                                                   (asig.magnitude) * averageImpedance /
  2964                                                                   np.min(
  2965                                                                       impedances.loc[elNmMatchMsk, 'impedance']
  2966                                                                       ))
  2967                                                           # if fillOverflow:
  2968                                                           #     # fill in overflow:
  2969                                                           #     '''
  2970                                                           #     timeSection['data'], overflowMask = hf.fillInOverflow(
  2971                                                           #         timeSection['data'], fillMethod = 'average')
  2972                                                           #     badData.update({'overflow': overflowMask})
  2973                                                           #     '''
  2974                                                           #     pass
  2975                                                           # if removeJumps:
  2976                                                           #     # find unusual jumps in derivative or amplitude
  2977                                                           #     '''
  2978                                                           #     timeSection['data'], newBadData = hf.fillInJumps(timeSection['data'],
  2979                                                           #     timeSection['samp_per_s'], smoothing_ms = 0.5, nStdDiff = 50,
  2980                                                           #     nStdAmp = 100)
  2981                                                           #     badData.update(newBadData)
  2982                                                           #     '''
  2983                                                           #     pass
  2984                                                           tempLFPStore.loc[:, aSigProxy.name] = asig.magnitude.flatten()
  2985                                                           del asig
  2986                                                           gc.collect()
  2987                                                   # end of first pass
  2988                                                   if (removeMeanAcross or calcAverageLFP):
  2989                                                       centerLFP = np.zeros(
  2990                                                           (tempLFPStore.shape[0], len(asigNameList)),
  2991                                                           dtype=np.float32)
  2992                                                       spreadLFP = np.zeros(
  2993                                                           (tempLFPStore.shape[0], len(asigNameList)),
  2994                                                           dtype=np.float32)
  2995                                                       # if calcOutliers:
  2996                                                       if True:
  2997                                                           if outlierMaskFilterOpts is not None:
  2998                                                               filterCoeffsOutlierMask = hf.makeFilterCoeffsSOS(
  2999                                                                   outlierMaskFilterOpts, float(dummyAsig.sampling_rate))
  3000                                                           lfpDeviation = np.zeros(
  3001                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3002                                                               dtype=np.float32)
  3003                                                           smoothedDeviation = np.zeros(
  3004                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3005                                                               dtype=np.float32)
  3006                                                           outlierMask = np.zeros(
  3007                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3008                                                               dtype=np.bool)
  3009                                                           outlierMetadata = {}
  3010                                                       # if calcArtifactTrace:
  3011                                                       if True:
  3012                                                           artifactSignal = np.zeros(
  3013                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3014                                                               dtype=np.float32)
  3015                                                       ###############
  3016                                                       # tempLFPStore.iloc[:, 0] = np.nan  # for debugging axes
  3017                                                       #############
  3018                                                       plotDevFilterDebug = False
  3019                                                       if plotDevFilterDebug:
  3020                                                           try:
  3021                                                               devFiltDebugMask = (dummyAsig.times > 90 * pq.s) & (dummyAsig.times < 92 * pq.s)
  3022                                                           except Exception:
  3023                                                               pdb.set_trace()
  3024                                                           plotColIdx = 1
  3025                                                           ddfFig, ddfAx = plt.subplots(len(asigNameList), 1)
  3026                                                           ddfFig2, ddfAx2 = plt.subplots()
  3027                                                           ddfFig3, ddfAx3 = plt.subplots(
  3028                                                               1, len(asigNameList),
  3029                                                               sharey=True)
  3030                                                           if len(asigNameList) == 1:
  3031                                                               ddfAx = np.asarray([ddfAx])
  3032                                                               ddfAx3 = np.asarray([ddfAx3])
  3033                                                       for subListIdx, subList in enumerate(asigNameList):
  3034                                                           columnsForThisGroup = meanGroups[subListIdx]
  3035                                                           if trackMemory:
  3036                                                               print(
  3037                                                                   'asig group {}: calculating mean, memory usage: {:.1f} MB'.format(
  3038                                                                       subListIdx, prf.memory_usage_psutil()))
  3039                                                               print('this group contains\n{}'.format(columnsForThisGroup))
  3040                                                           if plotDevFilterDebug:
  3041                                                               ddfAx3[subListIdx].plot(
  3042                                                                   dummyAsig.times[devFiltDebugMask],
  3043                                                                   tempLFPStore.loc[:, columnsForThisGroup].iloc[devFiltDebugMask, plotColIdx],
  3044                                                                   label='original ch'
  3045                                                                   )
  3046                                                           if fillOverflow:
  3047                                                               print('Filling overflow...')
  3048                                                               # fill in overflow:
  3049                                                               tempLFPStore.loc[:, columnsForThisGroup], pltHandles = hf.fillInOverflow2(
  3050                                                                   tempLFPStore.loc[:, columnsForThisGroup].to_numpy(),
  3051                                                                   overFlowFillType='average',
  3052                                                                   overFlowThreshold=8000,
  3053                                                                   debuggingPlots=plotDevFilterDebug
  3054                                                                   )
  3055                                                               if plotDevFilterDebug:
  3056                                                                   pltHandles['ax'].set_title('ch grp {}'.format(subListIdx))
  3057                                                                   ddfAx3[subListIdx].plot(
  3058                                                                       dummyAsig.times[devFiltDebugMask],
  3059                                                                       tempLFPStore.loc[:, columnsForThisGroup].iloc[devFiltDebugMask, plotColIdx],
  3060                                                                       label='filled ch'
  3061                                                                       )
  3062                                                           # zscore of each trace
  3063                                                           if zScoreEachTrace:
  3064                                                               print('About to calculate zscore of each trace (along columns) for prelim outlier detection')
  3065                                                               columnZScore = pd.DataFrame(
  3066                                                                   stats.zscore(
  3067                                                                       tempLFPStore.loc[:, columnsForThisGroup],
  3068                                                                       axis=1),
  3069                                                                   index=tempLFPStore.index,
  3070                                                                   columns=columnsForThisGroup
  3071                                                                   )
  3072                                                               excludeFromMeanMask = columnZScore.abs() > 6
  3073                                                               if useMeanToCenter:
  3074                                                                   centerLFP[:, subListIdx] = (
  3075                                                                       tempLFPStore
  3076                                                                       .loc[:, columnsForThisGroup]
  3077                                                                       .mask(excludeFromMeanMask)
  3078                                                                       .mean(axis=1).to_numpy()
  3079                                                                       )
  3080                                                               else:
  3081                                                                   centerLFP[:, subListIdx] = (
  3082                                                                       tempLFPStore
  3083                                                                       .loc[:, columnsForThisGroup]
  3084                                                                       .mask(excludeFromMeanMask)
  3085                                                                       .median(axis=1).to_numpy()
  3086                                                                       )
  3087                                                           else:
  3088                                                               if useMeanToCenter:
  3089                                                                   centerLFP[:, subListIdx] = (
  3090                                                                       tempLFPStore
  3091                                                                       .loc[:, columnsForThisGroup]
  3092                                                                       .mean(axis=1).to_numpy()
  3093                                                                       )
  3094                                                               else:
  3095                                                                   centerLFP[:, subListIdx] = (
  3096                                                                       tempLFPStore
  3097                                                                       .loc[:, columnsForThisGroup]
  3098                                                                       .median(axis=1).to_numpy()
  3099                                                                       )
  3100                                                           if calcArtifactTrace:
  3101                                                               if LFPFilterOpts is not None:
  3102                                                                   print('applying LFPFilterOpts to cached asigs for artifact ID')
  3103                                                                   # tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfilt(
  3104                                                                   tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfiltfilt(
  3105                                                                       filterCoeffs, tempLFPStore.loc[:, columnsForThisGroup],
  3106                                                                       axis=0)
  3107                                                                   if useMeanToCenter:
  3108                                                                       tempCenter = (
  3109                                                                           tempLFPStore
  3110                                                                           .loc[:, columnsForThisGroup]
  3111                                                                           .mean(axis=1).diff().fillna(0)
  3112                                                                           )
  3113                                                                   else:
  3114                                                                       tempCenter = (
  3115                                                                           tempLFPStore
  3116                                                                           .loc[:, columnsForThisGroup]
  3117                                                                           .median(axis=1).diff().fillna(0)
  3118                                                                           )
  3119                                                               artifactSignal[:, subListIdx] = np.abs(stats.zscore(tempCenter.to_numpy()))
  3120                                                           if calcOutliers:
  3121                                                               if plotDevFilterDebug:
  3122                                                                   ddfAx[subListIdx].plot(
  3123                                                                       dummyAsig.times[devFiltDebugMask],
  3124                                                                       centerLFP[devFiltDebugMask, subListIdx],
  3125                                                                       label='mean of ch group'
  3126                                                                       )
  3127                                                               # filter the traces, if needed
  3128                                                               if LFPFilterOpts is not None:
  3129                                                                   print('applying LFPFilterOpts to cached asigs before outlier detection')
  3130                                                                   # tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfiltfilt(
  3131                                                                   tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfilt(
  3132                                                                       filterCoeffs, tempLFPStore.loc[:, columnsForThisGroup],
  3133                                                                       axis=0)
  3134                                                                   if plotDevFilterDebug:
  3135                                                                       ddfAx3[subListIdx].plot(
  3136                                                                           dummyAsig.times[devFiltDebugMask],
  3137                                                                           tempLFPStore.loc[:, columnsForThisGroup].iloc[devFiltDebugMask, plotColIdx],
  3138                                                                           label='filtered ch'
  3139                                                                           )
  3140                                                               ##################################
  3141                                                               print('Whitening cached traces before outlier detection')
  3142                                                               whitenByPCA = True
  3143                                                               if whitenByPCA:
  3144                                                                   projector = PCA(
  3145                                                                       n_components=None, whiten=True)
  3146                                                                   pcs = projector.fit_transform(
  3147                                                                       tempLFPStore.loc[:, columnsForThisGroup])
  3148                                                                   explVarMask = (
  3149                                                                       np.cumsum(projector.explained_variance_ratio_) < 1 - 1e-2)
  3150                                                                   explVarMask[0] = True  # (keep at least 1)
  3151                                                                   pcs = pcs[:, explVarMask]
  3152                                                                   nDim = pcs.shape[1]
  3153                                                                   lfpDeviation[:, subListIdx] = (pcs ** 2).sum(axis=1)
  3154                                                               else:  # whiten by mahalanobis distance
  3155                                                                   est = EmpiricalCovariance()
  3156                                                                   est.fit(tempLFPStore.loc[:, columnsForThisGroup].to_numpy())
  3157                                                                   lfpDeviation[:, subListIdx] = est.mahalanobis(
  3158                                                                       tempLFPStore.loc[:, columnsForThisGroup].to_numpy())
  3159                                                                   nDim = tempLFPStore.loc[:, columnsForThisGroup].shape[1]
  3160                                                               #
  3161                                                               transformedDeviation = stats.norm.isf(stats.chi2.sf(lfpDeviation[:, subListIdx], nDim))
  3162                                                               infMask = np.isinf(transformedDeviation)
  3163                                                               if infMask.any():
  3164                                                                   transformedDeviation[infMask] = transformedDeviation[~infMask].max()
  3165                                                               debugProbaTrans = False
  3166                                                               if debugProbaTrans:
  3167                                                                   fig, ax = plt.subplots()
  3168                                                                   tAx = ax.twinx()
  3169                                                                   plotMask = (dummyAsig.times >= 60 * pq.s) & (dummyAsig.times < 95 * pq.s)
  3170                                                                   ax.plot(dummyAsig.times[plotMask], transformedDeviation[plotMask], c='b', label='transformed deviation')
  3171                                                                   tAx.plot(dummyAsig.times[plotMask], lfpDeviation[plotMask, subListIdx], c='r', label='original deviation')
  3172                                                                   ax.legend(loc='upper left')
  3173                                                                   tAx.legend(loc='upper right')
  3174                                                                   plt.show()
  3175                                                               lfpDeviation[:, subListIdx] = transformedDeviation
  3176                                                               noveltyThreshold = stats.norm.interval(outlierThreshold)[1]
  3177                                                               # chi2Bounds = stats.chi2.interval(outlierThreshold, nDim)
  3178                                                               # lfpDeviation[:, subListIdx] = lfpDeviation[:, subListIdx] / chi2Bounds[1]
  3179                                                               # print('nDim = {}, chi2Lim = {}'.format(nDim, chi2Bounds))
  3180                                                               # noveltyThreshold = 1
  3181                                                               #
  3182                                                               outlierMetadata[subListIdx] = {
  3183                                                                   'nDim': nDim,
  3184                                                                   'noveltyThreshold': noveltyThreshold,
  3185                                                                   'outlierThreshold': outlierThreshold
  3186                                                                   }
  3187                                                               # smoothedDeviation = signal.sosfilt(
  3188                                                               print('Smoothing deviation')
  3189                                                               tempSmDev = signal.sosfiltfilt(
  3190                                                                   filterCoeffsOutlierMask, lfpDeviation[:, subListIdx])
  3191                                                               smoothedDeviation[:, subListIdx] = tempSmDev
  3192                                                               if plotDevFilterDebug:
  3193                                                                   ddfAx[subListIdx].plot(
  3194                                                                       dummyAsig.times[devFiltDebugMask],
  3195                                                                       lfpDeviation[devFiltDebugMask, subListIdx],
  3196                                                                       label='original deviation (ch grp {})'.format(subListIdx))
  3197                                                                   ddfAx[subListIdx].plot(
  3198                                                                       dummyAsig.times[devFiltDebugMask],
  3199                                                                       smoothedDeviation[devFiltDebugMask, subListIdx],
  3200                                                                       label='filtered deviation (ch grp {})'.format(subListIdx))
  3201                                                               ##
  3202                                                               print('Calculating outlier mask')
  3203                                                               outlierMask[:, subListIdx] = (
  3204                                                                   smoothedDeviation[:, subListIdx] > noveltyThreshold)
  3205                                                               if plotDevFilterDebug:
  3206                                                                   ddfAx[subListIdx].axhline(noveltyThreshold, c='r')
  3207                                                       if plotDevFilterDebug and calcOutliers:
  3208                                                           for subListIdx, subList in enumerate(asigNameList):
  3209                                                               ddfAx[subListIdx].legend(loc='upper right')
  3210                                                               ddfAx[subListIdx].set_title('Deviation')
  3211                                                               ddfAx3[subListIdx].legend(loc='upper right')
  3212                                                               ddfAx3[subListIdx].set_title('Example channel')
  3213                                                               ddfAx2.plot(
  3214                                                                   dummyAsig.times[devFiltDebugMask],
  3215                                                                   smoothedDeviation[devFiltDebugMask, subListIdx],
  3216                                                                   label='ch grp {}'.format(subListIdx))
  3217                                                               ddfAx2.set_title('Smoothed Deviation')
  3218                                                           ddfAx2.legend(loc='upper right')
  3219                                                           plt.show()
  3220                                                       #############
  3221                                                       del tempLFPStore
  3222                                                       gc.collect()
  3223                                               if (removeMeanAcross or calcAverageLFP):
  3224                                                   for mIdx, meanChIdx in enumerate(meanChIdxList):
  3225                                                       meanAsig = AnalogSignal(
  3226                                                           centerLFP[:, mIdx],
  3227                                                           units=dummyAsig.units,
  3228                                                           sampling_rate=dummyAsig.sampling_rate,
  3229                                                           # name='seg{}_{}'.format(idx, meanChIdx.name)
  3230                                                           name='seg{}_{}'.format(0, meanChIdx.name),
  3231                                                           t_start=tStart
  3232                                                       )
  3233                                                       # assign ownership to containers
  3234                                                       meanChIdx.analogsignals.append(meanAsig)
  3235                                                       newSeg.analogsignals.append(meanAsig)
  3236                                                       # assign parent to children
  3237                                                       meanChIdx.create_relationship()
  3238                                                       newSeg.create_relationship()
  3239                                                       # write out to file
  3240                                                       if LFPFilterOpts is not None:
  3241                                                           meanAsig[:] = filterFun(
  3242                                                               meanAsig, filterCoeffs=filterCoeffs)
  3243                                                       meanAsig = writer._write_analogsignal(
  3244                                                           meanAsig, nixblock, nixgroup)
  3245                                                   # if calcArtifactTrace:
  3246                                                   if True:
  3247                                                       for mIdx, artChIdx in enumerate(artChIdxList):
  3248                                                           artAsig = AnalogSignal(
  3249                                                               artifactSignal[:, mIdx],
  3250                                                               units=dummyAsig.units,
  3251                                                               sampling_rate=dummyAsig.sampling_rate,
  3252                                                               # name='seg{}_{}'.format(idx, devChIdx.name)
  3253                                                               name='seg{}_{}'.format(0, artChIdx.name),
  3254                                                               t_start=tStart
  3255                                                               )
  3256                                                           # assign ownership to containers
  3257                                                           artChIdx.analogsignals.append(artAsig)
  3258                                                           newSeg.analogsignals.append(artAsig)
  3259                                                           # assign parent to children
  3260                                                           artChIdx.create_relationship()
  3261                                                           newSeg.create_relationship()
  3262                                                           # write out to file
  3263                                                           artAsig = writer._write_analogsignal(
  3264                                                               artAsig, nixblock, nixgroup)
  3265                                                           #########################################################
  3266                                                   # if calcOutliers:
  3267                                                   if True:
  3268                                                       for mIdx, devChIdx in enumerate(devChIdxList):
  3269                                                           devAsig = AnalogSignal(
  3270                                                               lfpDeviation[:, mIdx],
  3271                                                               units=dummyAsig.units,
  3272                                                               sampling_rate=dummyAsig.sampling_rate,
  3273                                                               # name='seg{}_{}'.format(idx, devChIdx.name)
  3274                                                               name='seg{}_{}'.format(0, devChIdx.name),
  3275                                                               t_start=tStart
  3276                                                               )
  3277                                                           # assign ownership to containers
  3278                                                           devChIdx.analogsignals.append(devAsig)
  3279                                                           newSeg.analogsignals.append(devAsig)
  3280                                                           # assign parent to children
  3281                                                           devChIdx.create_relationship()
  3282                                                           newSeg.create_relationship()
  3283                                                           # write out to file
  3284                                                           devAsig = writer._write_analogsignal(
  3285                                                               devAsig, nixblock, nixgroup)
  3286                                                           #########################################################
  3287                                                       for mIdx, smDevChIdx in enumerate(smDevChIdxList):
  3288                                                           smDevAsig = AnalogSignal(
  3289                                                               smoothedDeviation[:, mIdx],
  3290                                                               units=dummyAsig.units,
  3291                                                               sampling_rate=dummyAsig.sampling_rate,
  3292                                                               # name='seg{}_{}'.format(idx, devChIdx.name)
  3293                                                               name='seg{}_{}'.format(0, smDevChIdx.name),
  3294                                                               t_start=tStart
  3295                                                               )
  3296                                                           # assign ownership to containers
  3297                                                           smDevChIdx.analogsignals.append(smDevAsig)
  3298                                                           newSeg.analogsignals.append(smDevAsig)
  3299                                                           # assign parent to children
  3300                                                           smDevChIdx.create_relationship()
  3301                                                           newSeg.create_relationship()
  3302                                                           # write out to file
  3303                                                           smDevAsig = writer._write_analogsignal(
  3304                                                               smDevAsig, nixblock, nixgroup)
  3305                                                           #########################################################
  3306                                                       for mIdx, outMaskChIdx in enumerate(outMaskChIdxList):
  3307                                                           outMaskAsig = AnalogSignal(
  3308                                                               outlierMask[:, mIdx],
  3309                                                               units=dummyAsig.units,
  3310                                                               sampling_rate=dummyAsig.sampling_rate,
  3311                                                               # name='seg{}_{}'.format(idx, outMaskChIdx.name)
  3312                                                               name='seg{}_{}'.format(0, outMaskChIdx.name),
  3313                                                               t_start=tStart, dtype=np.float32
  3314                                                               )
  3315                                                           outMaskAsig.annotations['outlierProportion'] = np.mean(outlierMask[:, mIdx])
  3316                                                           if calcOutliers:
  3317                                                               outMaskAsig.annotations.update(outlierMetadata[mIdx])
  3318                                                           # assign ownership to containers
  3319                                                           outMaskChIdx.analogsignals.append(outMaskAsig)
  3320                                                           newSeg.analogsignals.append(outMaskAsig)
  3321                                                           # assign parent to children
  3322                                                           outMaskChIdx.create_relationship()
  3323                                                           newSeg.create_relationship()
  3324                                                           # write out to file
  3325                                                           outMaskAsig = writer._write_analogsignal(
  3326                                                               outMaskAsig, nixblock, nixgroup)
  3327                                                   #
  3328                                                   w0 = 60
  3329                                                   bandQ = 20
  3330                                                   bw = w0/bandQ
  3331                                                   noiseSos = signal.iirfilter(
  3332                                                       N=8, Wn=[w0 - bw/2, w0 + bw/2],
  3333                                                       btype='band', ftype='butter',
  3334                                                       analog=False, fs=float(dummyAsig.sampling_rate),
  3335                                                       output='sos')
  3336                                                   # signal.hilbert does not have an option to zero pad
  3337                                                   nextLen = fftpack.helper.next_fast_len(dummyAsig.shape[0])
  3338                                                   deficit = int(nextLen - dummyAsig.shape[0])
  3339                                                   lDef = int(np.floor(deficit / 2))
  3340                                                   rDef = int(np.ceil(deficit / 2)) + 1
  3341                                                   temp = np.pad(
  3342                                                       dummyAsig.magnitude.flatten(),
  3343                                                       (lDef, rDef), mode='constant')
  3344                                                   # lineNoise = signal.sosfiltfilt(
  3345                                                   lineNoise = signal.sosfilt(
  3346                                                       noiseSos, temp, axis=0)
  3347                                                   lineNoiseH = signal.hilbert(lineNoise)
  3348                                                   lineNoise = lineNoise[lDef:-rDef]
  3349                                                   lineNoiseH = lineNoiseH[lDef:-rDef]
  3350                                                   lineNoisePhase = np.angle(lineNoiseH)
  3351                                                   lineNoisePhaseDF = pd.DataFrame(
  3352                                                       lineNoisePhase,
  3353                                                       index=dummyAsig.times,
  3354                                                       columns=['phase']
  3355                                                       )
  3356                                                   plotHilbert = False
  3357                                                   if plotHilbert:
  3358                                                       lineNoiseFreq = (
  3359                                                           np.diff(np.unwrap(lineNoisePhase)) /
  3360                                                           (2.0*np.pi) * float(dummyAsig.sampling_rate))
  3361                                                       lineNoiseEnvelope = np.abs(lineNoiseH)
  3362                                                       i1 = 300000; i2 = 330000
  3363                                                       fig, ax = plt.subplots(2, 1, sharex=True)
  3364                                                       ax[0].plot(dummyAsig.times[devFiltDebugMask], dummyAsig.magnitude[devFiltDebugMask, :])
  3365                                                       ax[0].plot(dummyAsig.times[devFiltDebugMask], lineNoise[devFiltDebugMask])
  3366                                                       ax[0].plot(dummyAsig.times[devFiltDebugMask], lineNoiseEnvelope[devFiltDebugMask])
  3367                                                       axFr = ax[1].twinx()
  3368                                                       ax[1].plot(
  3369                                                           dummyAsig.times[devFiltDebugMask], lineNoisePhase[devFiltDebugMask],
  3370                                                           c='r', label='phase')
  3371                                                       ax[1].legend()
  3372                                                       axFr.plot(
  3373                                                           dummyAsig.times[devFiltDebugMask], lineNoiseFreq[devFiltDebugMask],
  3374                                                           label='freq')
  3375                                                       axFr.set_ylim([59, 61])
  3376                                                       axFr.legend()
  3377                                                       plt.show()
  3378                                               # second pass through asigs, to save
  3379                                               for aSigIdx, aSigProxy in enumerate(seg.analogsignals):
  3380                                                   if aSigIdx == 0:
  3381                                                       # check bounds
  3382                                                       tStart = max(chunkTStart * pq.s, aSigProxy.t_start)
  3383                                                       tStop = min(chunkTStop * pq.s, aSigProxy.t_stop)
  3384                                                   loadThisOne = (
  3385                                                       (saveFromAsigNameList and (aSigProxy in aSigList)) or
  3386                                                       (aSigProxy in ainpList)
  3387                                                       )
  3388                                                   if loadThisOne:
  3389                                                       if trackMemory:
  3390                                                           print('writing asig {} ({}) memory usage: {:.1f} MB'.format(
  3391                                                               aSigIdx, aSigProxy.name, prf.memory_usage_psutil()))
  3392                                                       chanIdx = aSigProxy.channel_index
  3393                                                       asig = aSigProxy.load(
  3394                                                           time_slice=(tStart, tStop),
  3395                                                           magnitude_mode='rescaled')
  3396                                                       #  link AnalogSignal and ID providing channel_index
  3397                                                       asig.channel_index = chanIdx
  3398                                                       #  perform requested preproc operations
  3399                                                       if 'impedances' in locals():
  3400                                                           elNmMatchMsk = impedances['elec'] == chanIdx.name
  3401                                                           if elNmMatchMsk.any():
  3402                                                               originalImpedance = np.min(
  3403                                                                   impedances.loc[elNmMatchMsk, 'impedance']
  3404                                                                   )
  3405                                                               asig.annotations['originalImpedance'] = originalImpedance
  3406                                                               if normalizeByImpedance and (aSigProxy not in ainpList):
  3407                                                                   '''
  3408                                                                   asig.magnitude[:] = (
  3409                                                                       (asig.magnitude - np.median(asig.magnitude)) /
  3410                                                                       np.min(
  3411                                                                           impedances.loc[elNmMatchMsk, 'impedance']
  3412                                                                           )
  3413                                                                       )
  3414                                                                   '''
  3415                                                                   print('Normalizing {} by {} kOhms'.format(asig.name, originalImpedance))
  3416                                                                   asig.magnitude[:] = (
  3417                                                                       (asig.magnitude * averageImpedance) / originalImpedance
  3418                                                                       )
  3419                                                       if fillOverflow:
  3420                                                           # fill in overflow:
  3421                                                           asig.magnitude[:], _ = hf.fillInOverflow2(
  3422                                                               asig.magnitude[:],
  3423                                                               overFlowFillType='average',
  3424                                                               overFlowThreshold=8000,
  3425                                                               debuggingPlots=False
  3426                                                               )
  3427                                                       if removeJumps:
  3428                                                           # find unusual jumps in derivative or amplitude
  3429                                                           '''
  3430                                                           timeSection['data'], newBadData = hf.fillInJumps(timeSection['data'],
  3431                                                           timeSection['samp_per_s'], smoothing_ms = 0.5, nStdDiff = 50,
  3432                                                           nStdAmp = 100)
  3433                                                           badData.update(newBadData)
  3434                                                           '''
  3435                                                           pass
  3436                                                       if calcAverageLFP and (aSigProxy not in ainpList):
  3437                                                           for k, cols in meanGroups.items():
  3438                                                               if asig.name in cols:
  3439                                                                   whichColumnToSubtract = k
  3440                                                           noiseModel = np.polyfit(
  3441                                                               centerLFP[:, whichColumnToSubtract],
  3442                                                               asig.magnitude.flatten(), 1, full=True)
  3443                                                           rSq = 1 - noiseModel[1][0] / np.sum(asig.magnitude.flatten() ** 2)
  3444                                                           asig.annotations['mean_removal_r2'] = rSq
  3445                                                           asig.annotations['mean_removal_group'] = whichColumnToSubtract
  3446                                                           if linearDetrend:
  3447                                                               noiseTerm = np.polyval(
  3448                                                                   noiseModel[0],
  3449                                                                   centerLFP[:, whichColumnToSubtract])
  3450                                                           else:
  3451                                                               noiseTerm = centerLFP[:, whichColumnToSubtract]
  3452                                                           ###
  3453                                                           plotMeanSubtraction = False
  3454                                                           if plotMeanSubtraction:
  3455                                                               i1 = 300000; i2 = 330000
  3456                                                               fig, ax = plt.subplots(1, 1)
  3457                                                               ax.plot(asig.times[devFiltDebugMask], asig.magnitude[devFiltDebugMask, :], label='channel')
  3458                                                               ax.plot(asig.times[devFiltDebugMask], centerLFP[devFiltDebugMask, whichColumnToSubtract], label='mean')
  3459                                                               ax.plot(asig.times[devFiltDebugMask], noiseTerm[devFiltDebugMask], label='adjusted mean')
  3460                                                               ax.legend()
  3461                                                               plt.show()
  3462                                                           ###
  3463                                                           if removeMeanAcross:
  3464                                                               asig.magnitude[:] = np.atleast_2d(
  3465                                                                   asig.magnitude.flatten() - noiseTerm).transpose()
  3466                                                               # asig.magnitude[:] = (
  3467                                                               #     asig.magnitude - np.median(asig.magnitude))
  3468                                                       if (LFPFilterOpts is not None) and (aSigProxy not in ainpList):
  3469                                                           asig.magnitude[:] = filterFun(asig, filterCoeffs=filterCoeffs)
  3470                                                       if (interpolateOutliers) and (aSigProxy not in ainpList) and (not outlierRemovalDebugFlag):
  3471                                                           for k, cols in meanGroups.items():
  3472                                                               if asig.name in cols:
  3473                                                                   whichColumnToSubtract = k
  3474                                                           tempSer = pd.Series(asig.magnitude.flatten())
  3475                                                           tempSer.loc[outlierMask[:, whichColumnToSubtract]] = np.nan
  3476                                                           tempSer = (
  3477                                                               tempSer
  3478                                                               .interpolate(method='linear', limit_area='inside')
  3479                                                               .fillna(method='ffill')
  3480                                                               .fillna(method='bfill')
  3481                                                               )
  3482                                                           asig.magnitude[:, 0] = tempSer.to_numpy()
  3483                                                       # pdb.set_trace()
  3484                                                       if (aSigProxy in aSigList) or (aSigProxy in ainpList):
  3485                                                           # assign ownership to containers
  3486                                                           chanIdx.analogsignals.append(asig)
  3487                                                           newSeg.analogsignals.append(asig)
  3488                                                           # assign parent to children
  3489                                                           chanIdx.create_relationship()
  3490                                                           newSeg.create_relationship()
  3491                                                           # write out to file
  3492                                                           asig = writer._write_analogsignal(
  3493                                                               asig, nixblock, nixgroup)
  3494                                                       del asig
  3495                                                       gc.collect()
  3496                                               for irSigIdx, irSigProxy in enumerate(
  3497                                                       seg.irregularlysampledsignals):
  3498                                                   chanIdx = irSigProxy.channel_index
  3499                                                   #
  3500                                                   isig = irSigProxy.load(
  3501                                                       time_slice=(tStart, tStop),
  3502                                                       magnitude_mode='rescaled')
  3503                                                   #  link irregularlysampledSignal
  3504                                                   #  and ID providing channel_index
  3505                                                   isig.channel_index = chanIdx
  3506                                                   # assign ownership to containers
  3507                                                   chanIdx.irregularlysampledsignals.append(isig)
  3508                                                   newSeg.irregularlysampledsignals.append(isig)
  3509                                                   # assign parent to children
  3510                                                   chanIdx.create_relationship()
  3511                                                   newSeg.create_relationship()
  3512                                                   # write out to file
  3513                                                   isig = writer._write_irregularlysampledsignal(
  3514                                                       isig, nixblock, nixgroup)
  3515                                                   del isig
  3516                                                   gc.collect()
  3517                                               #
  3518                                               if len(spikeSourceType):
  3519                                                   for stIdx, stProxy in enumerate(spikeSeg.spiketrains):
  3520                                                       if trackMemory:
  3521                                                           print('writing spiketrains mem usage: {}'.format(
  3522                                                               prf.memory_usage_psutil()))
  3523                                                       unit = stProxy.unit
  3524                                                       st = loadStProxy(stProxy)
  3525                                                       #  have to manually slice tStop and tStart because
  3526                                                       #  array annotations are not saved natively in the nix file
  3527                                                       #  (we're getting them as plain annotations)
  3528                                                       timeMask = np.asarray(
  3529                                                           (st.times >= tStart) & (st.times < tStop),
  3530                                                           dtype=np.bool)
  3531                                                       try:
  3532                                                           if 'arrayAnnNames' in st.annotations:
  3533                                                               for key in st.annotations['arrayAnnNames']:
  3534                                                                   st.annotations[key] = np.asarray(
  3535                                                                       st.annotations[key])[timeMask]
  3536                                                           st = st[timeMask]
  3537                                                           st.t_start = tStart
  3538                                                           st.t_stop = tStop
  3539                                                       except Exception:
  3540                                                           traceback.print_exc()
  3541                                                       #  tdc may or may not have the same channel ids, but
  3542                                                       #  it will have consistent channel names
  3543                                                       nameParser = re.search(
  3544                                                           r'([a-zA-Z0-9]*)#(\d*)', unit.name)
  3545                                                       chanLabel = nameParser.group(1)
  3546                                                       unitId = nameParser.group(2)
  3547                                                       #
  3548                                                       chIdxName = unit.name.replace('_stim', '').split('#')[0]
  3549                                                       chanIdx = block.filter(objects=ChannelIndex, name=chIdxName)[0]
  3550                                                       # [i.name for i in block.filter(objects=ChannelIndex)]
  3551                                                       # [i.name for i in spikeBlock.filter(objects=Unit)]
  3552                                                       #  print(unit.name)
  3553                                                       if not (unit in chanIdx.units):
  3554                                                           # first time at this unit, add to its chanIdx
  3555                                                           unit.channel_index = chanIdx
  3556                                                           chanIdx.units.append(unit)
  3557                                                       #  except Exception:
  3558                                                       #      traceback.print_exc()
  3559                                                       st.name = 'seg{}_{}'.format(0, unit.name)
  3560                                                       # st.name = 'seg{}_{}'.format(idx, unit.name)
  3561                                                       #  link SpikeTrain and ID providing unit
  3562                                                       if calcAverageLFP:
  3563                                                           if 'arrayAnnNames' in st.annotations:
  3564                                                               st.annotations['arrayAnnNames'] = list(st.annotations['arrayAnnNames'])
  3565                                                           else:
  3566                                                               st.annotations['arrayAnnNames'] = []
  3567                                                           st.annotations['arrayAnnNames'].append('phase60hz')
  3568                                                           phase60hz = hf.interpolateDF(
  3569                                                               lineNoisePhaseDF,
  3570                                                               newX=st.times, columns=['phase']).to_numpy().flatten()
  3571                                                           st.annotations.update({'phase60hz': phase60hz})
  3572                                                           plotPhaseDist = False
  3573                                                           if plotPhaseDist:
  3574                                                               sns.distplot(phase60hz)
  3575                                                               plt.show()
  3576                                                       st.unit = unit
  3577                                                       # assign ownership to containers
  3578                                                       unit.spiketrains.append(st)
  3579                                                       newSeg.spiketrains.append(st)
  3580                                                       # assign parent to children
  3581                                                       unit.create_relationship()
  3582                                                       newSeg.create_relationship()
  3583                                                       # write out to file
  3584                                                       st = writer._write_spiketrain(st, nixblock, nixgroup)
  3585                                                       del st
  3586                                               #  process proprio trial related events
  3587                                               if calcRigEvents:
  3588                                                   print('Processing rig events...')
  3589                                                   analogData = []
  3590                                                   for key, value in eventInfo['inputIDs'].items():
  3591                                                       searchName = 'seg{}_'.format(0) + value
  3592                                                       ainpAsig = seg.filter(
  3593                                                           objects=AnalogSignalProxy,
  3594                                                           name=searchName)[0]
  3595                                                       ainpData = ainpAsig.load(
  3596                                                           time_slice=(tStart, tStop),
  3597                                                           magnitude_mode='rescaled')
  3598                                                       analogData.append(
  3599                                                           pd.DataFrame(ainpData.magnitude, columns=[key]))
  3600                                                       del ainpData
  3601                                                       gc.collect()
  3602                                                   motorData = pd.concat(analogData, axis=1)
  3603                                                   del analogData
  3604                                                   gc.collect()
  3605                                                   if motorEncoderMask is not None:
  3606                                                       ainpData = ainpAsig.load(
  3607                                                           time_slice=(tStart, tStop),
  3608                                                           magnitude_mode='rescaled')
  3609                                                       ainpTime = ainpData.times.magnitude
  3610                                                       meTimeMask = np.zeros_like(ainpTime, dtype=np.bool)
  3611                                                       for meTimeBounds in motorEncoderMask:
  3612                                                           meTimeMask = (
  3613                                                               meTimeMask |
  3614                                                               (
  3615                                                                   (ainpTime > meTimeBounds[0]) &
  3616                                                                   (ainpTime < meTimeBounds[1])
  3617                                                                   )
  3618                                                               )
  3619                                                       columnsToOverride = ['A-', 'A+', 'B-', 'B+', 'Z-', 'Z+']
  3620                                                       for colName in columnsToOverride:
  3621                                                           motorData.loc[~meTimeMask, colName] = motorData.loc[:, colName].quantile(q=0.05)
  3622                                                       del ainpData, ainpTime
  3623                                                       gc.collect()
  3624                                                   motorData = mea.processMotorData(
  3625                                                       motorData, ainpAsig.sampling_rate.magnitude,
  3626                                                       encoderCountPerDegree=encoderCountPerDegree
  3627                                                       )
  3628                                                   keepCols = [
  3629                                                       'position', 'velocity', 'velocityCat',
  3630                                                       'rightBut_int', 'leftBut_int',
  3631                                                       'rightLED_int', 'leftLED_int', 'simiTrigs_int']
  3632                                                   for colName in keepCols:
  3633                                                       if trackMemory:
  3634                                                           print('writing motorData memory usage: {:.1f} MB'.format(
  3635                                                               prf.memory_usage_psutil()))
  3636                                                       chanIdx = ChannelIndex(
  3637                                                           name=colName,
  3638                                                           index=np.asarray([0]),
  3639                                                           channel_names=np.asarray([0]))
  3640                                                       block.channel_indexes.append(chanIdx)
  3641                                                       motorAsig = AnalogSignal(
  3642                                                           motorData[colName].to_numpy() * pq.mV,
  3643                                                           name=colName,
  3644                                                           sampling_rate=ainpAsig.sampling_rate,
  3645                                                           dtype=np.float32)
  3646                                                       motorAsig.t_start = ainpAsig.t_start
  3647                                                       motorAsig.channel_index = chanIdx
  3648                                                       # assign ownership to containers
  3649                                                       chanIdx.analogsignals.append(motorAsig)
  3650                                                       newSeg.analogsignals.append(motorAsig)
  3651                                                       chanIdx.create_relationship()
  3652                                                       newSeg.create_relationship()
  3653                                                       # write out to file
  3654                                                       motorAsig = writer._write_analogsignal(
  3655                                                           motorAsig, nixblock, nixgroup)
  3656                                                       del motorAsig
  3657                                                       gc.collect()
  3658                                                   _, trialEvents = mea.getTrials(
  3659                                                       motorData, ainpAsig.sampling_rate.magnitude,
  3660                                                       float(tStart.magnitude), trialType=None)
  3661                                                   trialEvents.fillna(0)
  3662                                                   trialEvents.rename(
  3663                                                       columns={
  3664                                                           'Label': 'rig_property',
  3665                                                           'Details': 'rig_value'},
  3666                                                       inplace=True)
  3667                                                   del motorData
  3668                                                   gc.collect()
  3669                                                   eventList = eventDataFrameToEvents(
  3670                                                       trialEvents,
  3671                                                       idxT='Time',
  3672                                                       annCol=['rig_property', 'rig_value'])
  3673                                                   for event in eventList:
  3674                                                       if trackMemory:
  3675                                                           print(
  3676                                                               'writing motor events memory usage: {:.1f} MB'
  3677                                                               .format(prf.memory_usage_psutil()))
  3678                                                       event.segment = newSeg
  3679                                                       newSeg.events.append(event)
  3680                                                       newSeg.create_relationship()
  3681                                                       # write out to file
  3682                                                       event = writer._write_event(event, nixblock, nixgroup)
  3683                                                       del event
  3684                                                       gc.collect()
  3685                                                   del trialEvents, eventList
  3686                                               #
  3687                                               for eventProxy in seg.events:
  3688                                                   event = eventProxy.load(
  3689                                                       time_slice=(tStart, tStop))
  3690                                                   event.t_start = tStart
  3691                                                   event.t_stop = tStop
  3692                                                   event.segment = newSeg
  3693                                                   newSeg.events.append(event)
  3694                                                   newSeg.create_relationship()
  3695                                                   # write out to file
  3696                                                   event = writer._write_event(event, nixblock, nixgroup)
  3697                                                   del event
  3698                                                   gc.collect()
  3699                                               #
  3700                                               for epochProxy in seg.epochs:
  3701                                                   epoch = epochProxy.load(
  3702                                                       time_slice=(tStart, tStop))
  3703                                                   epoch.t_start = tStart
  3704                                                   epoch.t_stop = tStop
  3705                                                   epoch.segment = newSeg
  3706                                                   newSeg.events.append(epoch)
  3707                                                   newSeg.create_relationship()
  3708                                                   # write out to file
  3709                                                   epoch = writer._write_epoch(epoch, nixblock, nixgroup)
  3710                                                   del epoch
  3711                                                   gc.collect()
  3712                                               #
  3713                                               chanIdxDiscardNames = []
  3714                                               # descend into ChannelIndexes
  3715                                               for chanIdx in block.channel_indexes:
  3716                                                   if chanIdx.analogsignals or chanIdx.units:
  3717                                                       chanIdx = writer._write_channelindex(chanIdx, nixblock)
  3718                                                   else:
  3719                                                       chanIdxDiscardNames.append(chanIdx.name)
  3720                                               block.channel_indexes = [
  3721                                                   i
  3722                                                   for i in block.channel_indexes
  3723                                                   if i.name not in chanIdxDiscardNames
  3724                                                   ]
  3725                                               writer._create_source_links(block, nixblock)
  3726                                               return

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: purgeNixAnn at line 3728

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3728                                           @profile
  3729                                           def purgeNixAnn(
  3730                                                   block, annNames=['nix_name', 'neo_name']):
  3731                                               for annName in annNames:
  3732                                                   block.annotations.pop(annName, None)
  3733                                               for child in block.children_recur:
  3734                                                   if child.annotations:
  3735                                                       child.annotations = {
  3736                                                           k: v
  3737                                                           for k, v in child.annotations.items()
  3738                                                           if k not in annNames}
  3739                                               for child in block.data_children_recur:
  3740                                                   if child.annotations:
  3741                                                       child.annotations = {
  3742                                                           k: v
  3743                                                           for k, v in child.annotations.items()
  3744                                                           if k not in annNames}
  3745                                               return block

Total time: 0.0069387 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadContainerArrayAnn at line 3747

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3747                                           @profile
  3748                                           def loadContainerArrayAnn(
  3749                                                   container=None, trainList=None
  3750                                                   ):
  3751         2         13.0      6.5      0.0      assert (container is not None) or (trainList is not None)
  3752                                               #
  3753         2         10.0      5.0      0.0      spikesAndEvents = []
  3754         2          8.0      4.0      0.0      returnObj = []
  3755         2          8.0      4.0      0.0      if container is not None:
  3756                                                   #  need the line below! (RD: don't remember why, consider removing)
  3757         2      20910.0  10455.0     30.1          container.create_relationship()
  3758                                                   #
  3759         2         10.0      5.0      0.0          spikesAndEvents += (
  3760         2      23836.0  11918.0     34.4              container.filter(objects=SpikeTrain) +
  3761         2      22981.0  11490.5     33.1              container.filter(objects=Event)
  3762                                                       )
  3763         2         14.0      7.0      0.0          returnObj.append(container)
  3764         2          9.0      4.5      0.0      if trainList is not None:
  3765                                                   spikesAndEvents += trainList
  3766                                                   returnObj.append(trainList)
  3767                                               #
  3768         2         13.0      6.5      0.0      if len(returnObj) == 1:
  3769         2         12.0      6.0      0.0          returnObj = returnObj[0]
  3770                                               else:
  3771                                                   returnObj = tuple(returnObj)
  3772                                               #
  3773        21         85.0      4.0      0.1      for st in spikesAndEvents:
  3774        19       1470.0     77.4      2.1          st = loadObjArrayAnn(st)
  3775         2          8.0      4.0      0.0      return returnObj

Total time: 5.45e-05 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadObjArrayAnn at line 3777

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3777                                           @profile
  3778                                           def loadObjArrayAnn(st):
  3779        19        167.0      8.8     30.6      if 'arrayAnnNames' in st.annotations.keys():
  3780                                                   if isinstance(st.annotations['arrayAnnNames'], str):
  3781                                                       st.annotations['arrayAnnNames'] = [st.annotations['arrayAnnNames']]
  3782                                                   elif isinstance(st.annotations['arrayAnnNames'], tuple):
  3783                                                       st.annotations['arrayAnnNames'] = [i for i in st.annotations['arrayAnnNames']]
  3784                                                   #
  3785                                                   for key in st.annotations['arrayAnnNames']:
  3786                                                       #  fromRaw, the ann come back as tuple, need to recast
  3787                                                       try:
  3788                                                           if len(st.times) == 1:
  3789                                                               st.annotations[key] = np.atleast_1d(st.annotations[key]).flatten()
  3790                                                           st.array_annotations.update(
  3791                                                               {key: np.asarray(st.annotations[key])})
  3792                                                           st.annotations[key] = np.asarray(st.annotations[key])
  3793                                                       except Exception:
  3794                                                           print('Error with {}'.format(st.name))
  3795                                                           traceback.print_exc()
  3796                                                           pdb.set_trace()
  3797        19        138.0      7.3     25.3      if hasattr(st, 'waveforms'):
  3798        11         64.0      5.8     11.7          if st.waveforms is None:
  3799                                                       st.waveforms = np.asarray([]).reshape((0, 0, 0)) * pq.mV
  3800        11         84.0      7.6     15.4          elif not len(st.waveforms):
  3801                                                       st.waveforms = np.asarray([]).reshape((0, 0, 0)) * pq.mV
  3802        19         92.0      4.8     16.9      return st

Total time: 2.63788 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadWithArrayAnn at line 3804

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3804                                           @profile
  3805                                           def loadWithArrayAnn(
  3806                                                   dataPath, fromRaw=False,
  3807                                                   mapDF=None, reduceChannelIndexes=False):
  3808         2         10.0      5.0      0.0      if fromRaw:
  3809                                                   reader = nixio_fr.NixIO(filename=dataPath)
  3810                                                   block = readBlockFixNames(
  3811                                                       reader, lazy=False,
  3812                                                       mapDF=mapDF,
  3813                                                       reduceChannelIndexes=reduceChannelIndexes)
  3814                                               else:
  3815         2     250488.0 125244.0      0.9          reader = NixIO(filename=dataPath)
  3816         2   24542023.0 12271011.5     93.0          block = reader.read_block()
  3817                                                   # [un.name for un in block.filter(objects=Unit)]
  3818                                                   # [len(un.spiketrains) for un in block.filter(objects=Unit)]
  3819                                               
  3820         2      69922.0  34961.0      0.3      block = loadContainerArrayAnn(container=block)
  3821                                               
  3822         2          8.0      4.0      0.0      if fromRaw:
  3823                                                   reader.file.close()
  3824                                               else:
  3825         2    1516287.0 758143.5      5.7          reader.close()
  3826         2         56.0     28.0      0.0      return block

Total time: 2.6381 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: blockFromPath at line 3828

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3828                                           @profile
  3829                                           def blockFromPath(
  3830                                                   dataPath, lazy=False, mapDF=None,
  3831                                                   reduceChannelIndexes=False, loadList=None,
  3832                                                   purgeNixNames=False, chunkingInfoPath=None):
  3833         2         22.0     11.0      0.0      chunkingMetadata = None
  3834         2         17.0      8.5      0.0      if chunkingInfoPath is not None:
  3835                                                   if os.path.exists(chunkingInfoPath):
  3836                                                       with open(chunkingInfoPath, 'r') as f:
  3837                                                           chunkingMetadata = json.load(f)
  3838         2         16.0      8.0      0.0      if chunkingMetadata is None:
  3839                                                   chunkingMetadata = {
  3840         2         15.0      7.5      0.0              '0': {
  3841         2         14.0      7.0      0.0                  'filename': dataPath,
  3842         2         14.0      7.0      0.0                  'partNameSuffix': '',
  3843         2         14.0      7.0      0.0                  'chunkTStart': 0,
  3844         2         26.0     13.0      0.0                  'chunkTStop': 'NaN'
  3845                                                       }}
  3846         4        109.0     27.2      0.0      for idx, (chunkIdxStr, chunkMeta) in enumerate(chunkingMetadata.items()):   
  3847         2         17.0      8.5      0.0          thisDataPath = chunkMeta['filename']
  3848         2       1210.0    605.0      0.0          assert os.path.exists(thisDataPath)
  3849         2         20.0     10.0      0.0          if idx == 0:
  3850         2         15.0      7.5      0.0              if lazy:
  3851                                                           dataReader = nixio_fr.NixIO(
  3852                                                               filename=thisDataPath)
  3853                                                           dataBlock = readBlockFixNames(
  3854                                                               dataReader, lazy=lazy, mapDF=mapDF,
  3855                                                               reduceChannelIndexes=reduceChannelIndexes,
  3856                                                               purgeNixNames=purgeNixNames, loadList=loadList)
  3857                                                       else:
  3858         2         15.0      7.5      0.0                  dataReader = None
  3859         2   26379454.0 13189727.0    100.0                  dataBlock = loadWithArrayAnn(thisDataPath)
  3860                                                   else:
  3861                                                       if lazy:
  3862                                                           dataReader2 = nixio_fr.NixIO(
  3863                                                               filename=thisDataPath)
  3864                                                           dataBlock2 = readBlockFixNames(
  3865                                                               dataReader2, lazy=lazy, mapDF=mapDF,
  3866                                                               reduceChannelIndexes=reduceChannelIndexes, loadList=loadList)
  3867                                                       else:
  3868                                                           dataReader2 = None
  3869                                                           dataBlock2 = loadWithArrayAnn(thisDataPath)
  3870                                                       maxSegIdx = len(dataBlock.segments)
  3871                                                       typesNeedRenaming = [
  3872                                                           SpikeTrainProxy, AnalogSignalProxy, EventProxy,
  3873                                                           SpikeTrain, AnalogSignal, Event]
  3874                                                       for segIdx, seg in enumerate(dataBlock2.segments):
  3875                                                           if seg.name is None:
  3876                                                               seg.name = 'seg{}_'.format(maxSegIdx + segIdx)
  3877                                                           else:
  3878                                                               if 'seg{}_'.format(maxSegIdx + segIdx) not in seg.name:
  3879                                                                   seg.name = (
  3880                                                                       'seg{}_{}'
  3881                                                                       .format(
  3882                                                                           maxSegIdx + segIdx,
  3883                                                                           childBaseName(seg.name, 'seg')))
  3884                                                           for objType in typesNeedRenaming:
  3885                                                               for child in seg.filter(objects=objType):
  3886                                                                   if 'seg{}_'.format(maxSegIdx + segIdx) not in child.name:
  3887                                                                       child.name = (
  3888                                                                           'seg{}_{}'
  3889                                                                           .format(
  3890                                                                               maxSegIdx + segIdx, childBaseName(child.name, 'seg')))
  3891                                                       dataBlock.merge(dataBlock2)
  3892         2         27.0     13.5      0.0      return dataReader, dataBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: calcBinarizedArray at line 3894

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3894                                           @profile
  3895                                           def calcBinarizedArray(
  3896                                                   dataBlock, samplingRate,
  3897                                                   binnedSpikePath=None,
  3898                                                   saveToFile=True, matchT=None):
  3899                                               #
  3900                                               spikeMatBlock = Block(name=dataBlock.name + '_binarized')
  3901                                               spikeMatBlock.merge_annotations(dataBlock)
  3902                                               #
  3903                                               allSpikeTrains = [
  3904                                                   i for i in dataBlock.filter(objects=SpikeTrain)]
  3905                                               #
  3906                                               for st in allSpikeTrains:
  3907                                                   chanList = spikeMatBlock.filter(
  3908                                                       objects=ChannelIndex, name=st.unit.name)
  3909                                                   if not len(chanList):
  3910                                                       chanIdx = ChannelIndex(name=st.unit.name, index=np.asarray([0]))
  3911                                                       #  print(chanIdx.name)
  3912                                                       spikeMatBlock.channel_indexes.append(chanIdx)
  3913                                                       thisUnit = Unit(name=st.unit.name)
  3914                                                       chanIdx.units.append(thisUnit)
  3915                                                       thisUnit.channel_index = chanIdx
  3916                                               #
  3917                                               for segIdx, seg in enumerate(dataBlock.segments):
  3918                                                   newSeg = Segment(name='seg{}_{}'.format(segIdx, spikeMatBlock.name))
  3919                                                   newSeg.merge_annotations(seg)
  3920                                                   spikeMatBlock.segments.append(newSeg)
  3921                                                   #  tStart = dataBlock.segments[0].t_start
  3922                                                   #  tStop = dataBlock.segments[0].t_stop
  3923                                                   tStart = seg.t_start
  3924                                                   tStop = seg.t_stop
  3925                                                   # make dummy binary spike train, in case ths chan didn't fire
  3926                                                   segSpikeTrains = [
  3927                                                       i for i in seg.filter(objects=SpikeTrain) if '#' in i.name]
  3928                                                   dummyBin = binarize(
  3929                                                       segSpikeTrains[0],
  3930                                                       sampling_rate=samplingRate,
  3931                                                       t_start=tStart,
  3932                                                       t_stop=tStop + samplingRate ** -1) * 0
  3933                                                   for chanIdx in spikeMatBlock.channel_indexes:
  3934                                                       #  print(chanIdx.name)
  3935                                                       stList = seg.filter(
  3936                                                           objects=SpikeTrain,
  3937                                                           name='seg{}_{}'.format(segIdx, chanIdx.name)
  3938                                                           )
  3939                                                       if len(stList):
  3940                                                           st = stList[0]
  3941                                                           print('binarizing {}'.format(st.name))
  3942                                                           stBin = binarize(
  3943                                                               st,
  3944                                                               sampling_rate=samplingRate,
  3945                                                               t_start=tStart,
  3946                                                               t_stop=tStop + samplingRate ** -1)
  3947                                                           spikeMatBlock.segments[segIdx].spiketrains.append(st)
  3948                                                           #  to do: link st to spikematblock's chidx and units
  3949                                                           assert len(chanIdx.filter(objects=Unit)) == 1
  3950                                                           thisUnit = chanIdx.filter(objects=Unit)[0]
  3951                                                           thisUnit.spiketrains.append(st)
  3952                                                           st.unit = thisUnit
  3953                                                           st.segment = spikeMatBlock.segments[segIdx]
  3954                                                       else:
  3955                                                           print('{} has no spikes'.format(st.name))
  3956                                                           stBin = dummyBin
  3957                                                       skipStAnnNames = [
  3958                                                           'nix_name', 'neo_name', 'arrayAnnNames']
  3959                                                       if 'arrayAnnNames' in st.annotations:
  3960                                                           skipStAnnNames += list(st.annotations['arrayAnnNames'])
  3961                                                       asigAnn = {
  3962                                                           k: v
  3963                                                           for k, v in st.annotations.items()
  3964                                                           if k not in skipStAnnNames
  3965                                                           }
  3966                                                       asig = AnalogSignal(
  3967                                                           stBin * samplingRate,
  3968                                                           name='seg{}_{}_raster'.format(segIdx, st.unit.name),
  3969                                                           sampling_rate=samplingRate,
  3970                                                           dtype=np.int,
  3971                                                           **asigAnn)
  3972                                                       if matchT is not None:
  3973                                                           asig = asig[:matchT.shape[0], :]
  3974                                                       asig.t_start = tStart
  3975                                                       asig.annotate(binWidth=1 / samplingRate.magnitude)
  3976                                                       chanIdx.analogsignals.append(asig)
  3977                                                       asig.channel_index = chanIdx
  3978                                                       spikeMatBlock.segments[segIdx].analogsignals.append(asig)
  3979                                               #
  3980                                               for chanIdx in spikeMatBlock.channel_indexes:
  3981                                                   chanIdx.name = chanIdx.name + '_raster'
  3982                                               #
  3983                                               spikeMatBlock.create_relationship()
  3984                                               spikeMatBlock = purgeNixAnn(spikeMatBlock)
  3985                                               if saveToFile:
  3986                                                   if os.path.exists(binnedSpikePath):
  3987                                                       os.remove(binnedSpikePath)
  3988                                                   writer = NixIO(filename=binnedSpikePath)
  3989                                                   writer.write_block(spikeMatBlock, use_obj_names=True)
  3990                                                   writer.close()
  3991                                               return spikeMatBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: calcFR at line 3993

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3993                                           @profile
  3994                                           def calcFR(
  3995                                                   binnedPath, dataPath,
  3996                                                   suffix='fr', aggregateFun=None,
  3997                                                   chanNames=None, rasterOpts=None, verbose=False
  3998                                                   ):
  3999                                               print('Loading rasters...')
  4000                                               masterSpikeMats, _ = loadSpikeMats(
  4001                                                   binnedPath, rasterOpts,
  4002                                                   aggregateFun=aggregateFun,
  4003                                                   chans=chanNames,
  4004                                                   loadAll=True, checkReferences=False)
  4005                                               print('Loading data file...')
  4006                                               dataReader = nixio_fr.NixIO(
  4007                                                   filename=dataPath)
  4008                                               dataBlock = dataReader.read_block(
  4009                                                   block_index=0, lazy=True,
  4010                                                   signal_group_mode='split-all')
  4011                                               masterBlock = Block()
  4012                                               masterBlock.name = dataBlock.annotations['neo_name']
  4013                                               #
  4014                                               for segIdx, segSpikeMat in masterSpikeMats.items():
  4015                                                   print('Calculating FR for segment {}'.format(segIdx))
  4016                                                   spikeMatDF = segSpikeMat.reset_index().rename(
  4017                                                       columns={'bin': 't'})
  4018                                           
  4019                                                   dataSeg = dataBlock.segments[segIdx]
  4020                                                   dummyAsig = dataSeg.filter(
  4021                                                       objects=AnalogSignalProxy)[0].load(channel_indexes=[0])
  4022                                                   samplingRate = dummyAsig.sampling_rate
  4023                                                   newT = dummyAsig.times.magnitude
  4024                                                   spikeMatDF['t'] = spikeMatDF['t'] + newT[0]
  4025                                           
  4026                                                   segSpikeMatInterp = hf.interpolateDF(
  4027                                                       spikeMatDF, pd.Series(newT),
  4028                                                       kind='linear', fill_value=(0, 0),
  4029                                                       x='t')
  4030                                                   spikeMatBlockInterp = dataFrameToAnalogSignals(
  4031                                                       segSpikeMatInterp,
  4032                                                       idxT='t', useColNames=True,
  4033                                                       dataCol=segSpikeMatInterp.drop(columns='t').columns,
  4034                                                       samplingRate=samplingRate)
  4035                                                   spikeMatBlockInterp.name = dataBlock.annotations['neo_name']
  4036                                                   spikeMatBlockInterp.annotate(
  4037                                                       nix_name=dataBlock.annotations['neo_name'])
  4038                                                   spikeMatBlockInterp.segments[0].name = dataSeg.annotations['neo_name']
  4039                                                   spikeMatBlockInterp.segments[0].annotate(
  4040                                                       nix_name=dataSeg.annotations['neo_name'])
  4041                                                   asigList = spikeMatBlockInterp.filter(objects=AnalogSignal)
  4042                                                   for asig in asigList:
  4043                                                       asig.annotate(binWidth=rasterOpts['binWidth'])
  4044                                                       if '_raster' in asig.name:
  4045                                                           asig.name = asig.name.replace('_raster', '_' + suffix)
  4046                                                       asig.name = 'seg{}_{}'.format(segIdx, childBaseName(asig.name, 'seg'))
  4047                                                       asig.annotate(nix_name=asig.name)
  4048                                                   chanIdxList = spikeMatBlockInterp.filter(objects=ChannelIndex)
  4049                                                   for chanIdx in chanIdxList:
  4050                                                       if '_raster' in chanIdx.name:
  4051                                                           chanIdx.name = chanIdx.name.replace('_raster', '_' + suffix)
  4052                                                       chanIdx.annotate(nix_name=chanIdx.name)
  4053                                           
  4054                                                   # masterBlock.merge(spikeMatBlockInterp)
  4055                                                   frBlockPath = dataPath.replace('_analyze.nix', '_fr.nix')
  4056                                                   writer = NixIO(filename=frBlockPath)
  4057                                                   writer.write_block(spikeMatBlockInterp, use_obj_names=True)
  4058                                                   writer.close()
  4059                                               #
  4060                                               dataReader.file.close()
  4061                                               return masterBlock

Timer unit: 1e-07 s

Total time: 87.2706 s
File: C\../../analysis-code/calcISIAnalysisNix.py
Function: calcISIBlockAnalysisNix at line 67

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    67                                           @profile
    68                                           def calcISIBlockAnalysisNix():
    69         1         91.0     91.0      0.0      arguments['chanNames'], arguments['chanQuery'] = ash.processChannelQueryArgs(
    70         1        158.0    158.0      0.0          namedQueries, scratchFolder, **arguments)
    71         1         92.0     92.0      0.0      analysisSubFolder = os.path.join(
    72         1        295.0    295.0      0.0          scratchFolder, arguments['analysisName']
    73                                                   )
    74         1        534.0    534.0      0.0      if not os.path.exists(analysisSubFolder):
    75                                                   os.makedirs(analysisSubFolder, exist_ok=True)
    76         1         86.0     86.0      0.0      if arguments['samplingRate'] is not None:
    77                                                   samplingRate = float(arguments['samplingRate']) * pq.Hz
    78                                               else:
    79         1        575.0    575.0      0.0          samplingRate = float(1 / binOpts['binInterval']) * pq.Hz
    80                                               #
    81         1         96.0     96.0      0.0      delsysBasePath = trialBasePath.replace('.nix', '_delsys_synchronized.nix')
    82                                               # Start parsing autologger info
    83         1         87.0     87.0      0.0      thisJsonPath = trialBasePath.replace('.nix', '_autoStimLog.json')
    84         1        464.0    464.0      0.0      if os.path.exists(thisJsonPath):
    85                                                   #
    86         1        387.0    387.0      0.0          @profile
    87                                                   def parseAutoStimLog(jsonPath):
    88                                                       try:
    89                                                           with open(jsonPath, 'r') as f:
    90                                                               stimLog = json.load(f)
    91                                                       except Exception:
    92                                                           with open(jsonPath, 'r') as f:
    93                                                               stimLogText = f.read()
    94                                                               stimLogText = mdt.fixMalformedJson(stimLogText, jsonType='Log')
    95                                                               stimLog = json.loads(stimLogText)
    96                                                       stimResLookup = {
    97                                                           1: 1 * pq.uA,
    98                                                           2: 2 * pq.uA,
    99                                                           3: 5 * pq.uA,
   100                                                           4: 10 * pq.uA,
   101                                                           5: 20 * pq.uA}
   102                                                       stimDict = {
   103                                                           't': [],
   104                                                           'elec': [],
   105                                                           # 'nominalWaveform': [],
   106                                                           'nominalCurrent': [],
   107                                                           'RateInHz': [],
   108                                                           'stimPeriod': [],
   109                                                           'trainDur': [],
   110                                                           'firstPW': [],
   111                                                           # 'interPhase': [],
   112                                                           'secondPW': [],
   113                                                           'totalPW': [],
   114                                                           'stimRes': []
   115                                                           }
   116                                                       allNominalWaveforms = []
   117                                                       for idx, entry in enumerate(stimLog):
   118                                                           t = entry['t']
   119                                                           if idx == 0:
   120                                                               firstT = t
   121                                                           else:
   122                                                               if t < firstT:
   123                                                                   continue
   124                                                           if 'stimRes' in entry:
   125                                                               ampQuanta = stimResLookup[entry['stimRes']]
   126                                                           else:
   127                                                               ampQuanta = 20 * pq.uA
   128                                                           # print('ampQuanta = {}'.format(ampQuanta))
   129                                                           if 'stimCmd' in entry:
   130                                                               allStimCmd = entry['stimCmd']
   131                                                               if isinstance(allStimCmd, dict):
   132                                                                   # if only one electrode
   133                                                                   allStimCmd = [allStimCmd]
   134                                                               for stimCmd in allStimCmd:
   135                                                                   # each stimCmd represents one electrode
   136                                                                   nominalWaveform = []
   137                                                                   lastAmplitude = 0
   138                                                                   totalLen = 0
   139                                                                   for seqIdx, phase in enumerate(stimCmd['seq']):
   140                                                                       if phase['enable']:
   141                                                                           phAmp = (
   142                                                                               ampQuanta * phase['ampl'] *
   143                                                                               (-1) * ((-1) ** phase['pol'])
   144                                                                               )
   145                                                                           phaseWaveform = [
   146                                                                               phAmp
   147                                                                               for i in range(31 * phase['length'])]
   148                                                                       else:
   149                                                                           phaseWaveform = [
   150                                                                               0
   151                                                                               for i in range(31 * phase['length'])]
   152                                                                       phaseWaveform[:phase['delay']] = [
   153                                                                           lastAmplitude for i in range(phase['delay'])]
   154                                                                       lastAmplitude = phaseWaveform[-1]
   155                                                                       nominalWaveform += phaseWaveform
   156                                                                       totalLen += phase['length']
   157                                                                       if seqIdx == 0:
   158                                                                           stimDict['firstPW'].append(
   159                                                                               (phase['length'] / (3e4)) * pq.s)
   160                                                                       if seqIdx == 1:
   161                                                                           stimDict['secondPW'].append(
   162                                                                               (phase['length'] / (3e4)) * pq.s)
   163                                                                   stimDict['t'].append(t)
   164                                                                   stimDict['stimRes'].append(ampQuanta)
   165                                                                   stimDict['totalPW'].append(
   166                                                                       (totalLen / (3e4)) * pq.s)
   167                                                                   stimDict['elec'].append(
   168                                                                       stimCmd['elec'] * pq.dimensionless)
   169                                                                   allNominalWaveforms.append(
   170                                                                       np.asarray(nominalWaveform))
   171                                                                   nominalIdxMax = np.argmax(
   172                                                                       np.abs(np.asarray(nominalWaveform)))
   173                                                                   stimDict['nominalCurrent'].append(
   174                                                                       nominalWaveform[nominalIdxMax])
   175                                                                   thisStimPeriod = (stimCmd['period'] / (3e4)) * pq.s
   176                                                                   stimDict['stimPeriod'].append(thisStimPeriod)
   177                                                                   stimDict['RateInHz'].append(
   178                                                                       thisStimPeriod ** (-1))
   179                                                                   stimDict['trainDur'].append(
   180                                                                       (stimCmd['repeats'] - 1) * thisStimPeriod)
   181                                                           else:
   182                                                               stimStr = entry['stimString']
   183                                                               stimStrDictRaw = {}
   184                                                               for stimSubStr in stimStr.split(';'):
   185                                                                   if len(stimSubStr):
   186                                                                       splitStr = stimSubStr.split('=')
   187                                                                       stimStrDictRaw[splitStr[0]] = splitStr[1]
   188                                                               stimStrDict = {}
   189                                                               for key, val in stimStrDictRaw.items():
   190                                                                   stimStrDict[key] = [
   191                                                                       float(st)
   192                                                                       for st in val.split(',')
   193                                                                       if len(st)]
   194                                                               stimStrDF = pd.DataFrame(stimStrDict)
   195                                                               stimStrDF['Elect'] = stimStrDF['Elect'].astype(np.int)
   196                                                               stimStrDF.loc[stimStrDF['PL'] == 1, 'Amp'] = (
   197                                                                   stimStrDF.loc[stimStrDF['PL'] == 1, 'Amp'] * (-1))
   198                                                               for rIdx, row in stimStrDF.iterrows():
   199                                                                   stimDict['t'].append(t)
   200                                                                   stimDict['firstPW'].append(
   201                                                                       row['Dur'] * 1e-3 * pq.s)
   202                                                                   stimDict['secondPW'].append(
   203                                                                       row['Dur'] * 1e-3 * pq.s)
   204                                                                   # stimDict['interPhase'].append(
   205                                                                   #     2 * ((3e4) ** -1) * pq.s)  # per page 16 of xippmex manual
   206                                                                   stimDict['totalPW'].append(
   207                                                                       2 * (row['Dur'] * 1e-3 + ((3e4) ** -1)) * pq.s)
   208                                                                   stimDict['nominalCurrent'].append(
   209                                                                       row['Amp'] * ampQuanta)
   210                                                                   stimDict['RateInHz'].append(row['Freq'] * pq.Hz)
   211                                                                   stimDict['stimPeriod'].append(row['Freq'] ** -1)
   212                                                                   stimDict['trainDur'].append(row['TL'] * 1e-3 * pq.s)
   213                                                                   stimDict['elec'].append(
   214                                                                       row['Elect'] * pq.dimensionless)
   215                                                       stimDict['labels'] = np.asarray([
   216                                                           'stim update {}'.format(i)
   217                                                           for i in range(len(stimDict['elec']))])
   218                                                       # (np.asarray(stimDict['t'])/3e4 <= 1).any()
   219                                                       rawStimEventTimes = np.asarray(stimDict.pop('t')) / (30000) * pq.s
   220                                                       # rawStimEventTimes = rawStimEventTimes - rawStimEventTimes[0] + activeTimes.min() * pq.s
   221                                                       # rawStimEventTimes = rawStimEventTimes.magnitude * rawStimEventTimes.units.simplified
   222                                                       stimEvents = Event(
   223                                                           name='seg0_stimEvents',
   224                                                           times=rawStimEventTimes,
   225                                                           labels=stimDict.pop('labels'))
   226                                                       stimEvents.annotations['arrayAnnNames'] = [
   227                                                           k
   228                                                           for k in stimDict.keys()]
   229                                                       stimEvents.annotations['nix_name'] = stimEvents.name
   230                                                       #
   231                                                       for k in stimEvents.annotations['arrayAnnNames']:
   232                                                           stimEvents.array_annotations[k] = stimDict[k]
   233                                                           stimEvents.annotations[k] = stimDict.pop(k)
   234                                                       return stimEvents
   235                                                   #
   236         1    4195805.0 4195805.0      0.5          stimEvents = parseAutoStimLog(thisJsonPath)
   237         1      13208.0  13208.0      0.0          rawStimEventsDF = pd.DataFrame(stimEvents.array_annotations)
   238         1       8805.0   8805.0      0.0          rawStimEventsDF['t'] = stimEvents.times
   239         1        101.0    101.0      0.0          rawStimEventsDF.to_csv(os.path.join(
   240         1     377288.0 377288.0      0.0              analysisSubFolder, ns5FileName + '_unsynched_stim_updates.csv'
   241                                                       ))
   242                                               else:
   243                                                   stimEvents = None
   244                                           
   245         1        591.0    591.0      0.0      if not os.path.exists(trialBasePath):
   246                                                   trialProcessedPath = os.path.join(
   247                                                       processedFolder, ns5FileName + '.nix')
   248                                                   # will throw an error if file was never processed
   249                                                   shutil.copyfile(trialProcessedPath, trialBasePath)
   250                                               #
   251         1    2549119.0 2549119.0      0.3      nspReader = neo.io.nixio_fr.NixIO(filename=trialBasePath)
   252         1    5278273.0 5278273.0      0.6      mapDF = prb_meta.mapToDF(rippleMapFile[int(arguments['blockIdx'])])
   253         1        110.0    110.0      0.0      nspBlock = ns5.readBlockFixNames(
   254         1         84.0     84.0      0.0          nspReader, block_index=0,
   255         1   10771981.0 10771981.0      1.2          reduceChannelIndexes=True
   256                                                   )
   257         1         89.0     89.0      0.0      delsysReader, delsysBlock = ns5.blockFromPath(
   258         1   23448354.0 23448354.0      2.7          delsysBasePath, lazy=True
   259                                                   )
   260         1         89.0     89.0      0.0      delsysChanNames = ns5.listChanNames(
   261         1         86.0     86.0      0.0          delsysBlock, arguments['chanQuery'],
   262         1      73842.0  73842.0      0.0          objType=AnalogSignalProxy)
   263                                               #
   264         1         92.0     92.0      0.0      spikesBlock = hf.extractSignalsFromBlock(
   265         1      11197.0  11197.0      0.0          nspBlock, keepSpikes=True)
   266         1    2362577.0 2362577.0      0.3      spikesBlock = hf.loadBlockProxyObjects(spikesBlock)
   267                                               #  save ins time series
   268         1        107.0    107.0      0.0      tdChanNames = ns5.listChanNames(
   269         1         88.0     88.0      0.0          nspBlock, arguments['chanQuery'],
   270         1      47899.0  47899.0      0.0          objType=AnalogSignalProxy)
   271         1         82.0     82.0      0.0      try:
   272         1        104.0    104.0      0.0          alignTimeBounds = alignTimeBoundsLookup[int(arguments['blockIdx'])]
   273                                               except Exception:
   274                                                   traceback.print_exc()
   275                                                   alignTimeBounds = None
   276                                               #
   277                                               allSpikeTrains = [
   278         1         82.0     82.0      0.0          i
   279         1       6532.0   6532.0      0.0          for i in spikesBlock.filter(objects=SpikeTrain)
   280                                                   if '#' in i.name]
   281         1         80.0     80.0      0.0      if len(allSpikeTrains):
   282         2        162.0     81.0      0.0          for segIdx, dataSeg in enumerate(spikesBlock.segments):
   283         1        507.0    507.0      0.0              spikeList = dataSeg.filter(objects=SpikeTrain)
   284         1       1267.0   1267.0      0.0              spikeList = ns5.loadContainerArrayAnn(trainList=spikeList)
   285                                               # calc binarized and get new time axis
   286                                               allStimTrains = [
   287         1         77.0     77.0      0.0          i
   288         1       6342.0   6342.0      0.0          for i in spikesBlock.filter(objects=SpikeTrain)
   289                                                   if '_stim' in i.name]
   290                                           
   291         1         86.0     86.0      0.0      tdBlock = hf.extractSignalsFromBlock(
   292         1       5954.0   5954.0      0.0          nspBlock, keepSpikes=False, keepSignals=tdChanNames)
   293         1    4000013.0 4000013.0      0.5      tdBlock = hf.loadBlockProxyObjects(tdBlock)
   294         1        103.0    103.0      0.0      delsysLoadedBlock = hf.extractSignalsFromBlock(
   295         1      35992.0  35992.0      0.0          delsysBlock, keepSpikes=False, keepSignals=delsysChanNames)
   296         1    7562605.0 7562605.0      0.9      delsysLoadedBlock = hf.loadBlockProxyObjects(delsysLoadedBlock)
   297                                               #  
   298                                               # if len(allStimTrains):
   299                                               #     for segIdx, dataSeg in enumerate(spikesBlock.segments):
   300                                               #         spikeList = [
   301                                               #             st
   302                                               #             for st in dataSeg.filter(objects=SpikeTrain)
   303                                               #             if '_stim' in st.name]
   304                                               #         for stIdx, st in enumerate(spikeList):
   305                                               #             chanName = st.unit.channel_index.name
   306                                               #             matchingAsig = tdBlock.filter(objects=AnalogSignal, name='seg0_' + chanName)
   307                                               #             if len(matchingAsig):
   308                                               #                 stitchStimArtifact = True
   309                                               #                 if stitchStimArtifact:
   310                                               #                     tIdx = 10
   311                                               #                     winSize = st.sampling_period * st.waveforms.shape[-1]
   312                                               #                     wvfT = np.arange(
   313                                               #                         st.times[tIdx],
   314                                               #                         st.times[tIdx] + winSize,
   315                                               #                         st.sampling_period) * st.sampling_period.units
   316                                               #                     wvfT = wvfT[:st.waveforms.shape[-1]]
   317                                               #                     asigTMask = (
   318                                               #                         (matchingAsig[0].times >= wvfT[0]) &
   319                                               #                         (matchingAsig[0].times < wvfT[0] + winSize))
   320                                               #                     plotAsig = np.squeeze(matchingAsig[0])[asigTMask]
   321                                               #                     plotAsigT = matchingAsig[0].times[asigTMask]
   322                                               #                     plotWvf = np.squeeze(st.waveforms[tIdx, :, :]) * 1e-3
   323                                               #                     fig, ax = plt.subplots()
   324                                               #                     ax.plot(wvfT, plotWvf, 'c.-')
   325                                               #                     twAx = ax.twinx()
   326                                               #                     twAx.plot(plotAsigT, plotAsig, 'm.-')
   327                                               #                     # ax.plot(plotAsigT, plotAsig - plotWvf, '.-')
   328                                               #                     plt.show()
   329         1        101.0    101.0      0.0      if len(allStimTrains):
   330         1         83.0     83.0      0.0          mustDoubleSpikeWvfLen = True
   331                                           
   332         1         83.0     83.0      0.0          def fixRippleStimWvf(sourceArr, destArr, whichIdx, fixFirst=1):
   333                                                       if fixFirst:
   334                                                           for jj in range(fixFirst):
   335                                                               destArr[:, :, jj] = destArr[:, :, fixFirst]
   336                                                       for ii in range(destArr.shape[0]):
   337                                                           destArr[ii, :, :] = destArr[ii, :, :] - sourceArr[ii, :, whichIdx]
   338                                                       return destArr
   339                                           
   340        12       1166.0     97.2      0.0          for stIdx, st in enumerate(allStimTrains):
   341        11       1081.0     98.3      0.0              if stIdx == 0:
   342         1         96.0     96.0      0.0                  originalSpikeWvfLen = st.waveforms.shape[-1]
   343        11      35363.0   3214.8      0.0              theseTimes = pd.Series(st.times)
   344                                                       # if a stim train is longer than 1.7 msec
   345                                                       # it gets split into two spikes
   346        11      73860.0   6714.5      0.0              maskContinued = theseTimes.diff() < 1.8e-3
   347                                                       #
   348        11      17295.0   1572.3      0.0              if maskContinued.any():
   349                                                           # mustDoubleSpikeWvfLen = True
   350                                                           maskContinuedSources = maskContinued.shift(-1).fillna(False)
   351                                                           assert maskContinued.sum() == maskContinuedSources.sum()
   352                                                           secondVolIdx = maskContinued.index[maskContinued]
   353                                                           notADuplicateMask = (~maskContinued).to_numpy()
   354                                                           firstVolIdx = maskContinuedSources.index[maskContinuedSources]
   355                                                           # fix inconsistency in first sample sourceArr, destArr, whichIdx
   356                                                           wvf = pd.DataFrame(np.atleast_2d(
   357                                                               np.squeeze(st.waveforms[notADuplicateMask, :, :])))
   358                                                           wvfDiff = wvf.diff(-1, axis=1).fillna(0)
   359                                                           wvfDiffAbs = wvfDiff.abs()
   360                                                           #
   361                                                           rawMaxIdx = wvfDiffAbs.iloc[:, :5].idxmax(axis=1)
   362                                                           #
   363                                                           firstValidIdx, _ = stats.mode(rawMaxIdx, axis=None)
   364                                                           firstValidIdx = int(firstValidIdx[-1] + 1)
   365                                                           #
   366                                                           st.waveforms[notADuplicateMask, :, :] = fixRippleStimWvf(
   367                                                               sourceArr=st.waveforms[notADuplicateMask, :, :],
   368                                                               destArr=st.waveforms[notADuplicateMask, :, :],
   369                                                               whichIdx=firstValidIdx, fixFirst=firstValidIdx)
   370                                                           st.waveforms[secondVolIdx, :, :] = fixRippleStimWvf(
   371                                                               sourceArr=st.waveforms[secondVolIdx, :, :],
   372                                                               destArr=st.waveforms[secondVolIdx, :, :],
   373                                                               whichIdx=firstValidIdx, fixFirst=firstValidIdx)
   374                                                           st.waveforms[secondVolIdx, :, :] = fixRippleStimWvf(
   375                                                               sourceArr=(-1) * st.waveforms[firstVolIdx, :, :],
   376                                                               destArr=st.waveforms[secondVolIdx, :, :], whichIdx=-1, fixFirst=False)
   377                                                           filledWaveforms = np.concatenate(
   378                                                               [
   379                                                                   st.waveforms[firstVolIdx, :, :],
   380                                                                   st.waveforms[secondVolIdx, :, :]],
   381                                                               axis=-1) * st.waveforms.units
   382                                                           # expand all, to catch single size spikes
   383                                                           #
   384                                                           padding = np.concatenate([
   385                                                               st.waveforms[:, :, -1]
   386                                                               for i in range(st.waveforms.shape[-1])], axis=-1)
   387                                                           newWaveforms = np.concatenate(
   388                                                               [
   389                                                                   st.waveforms, padding[:, np.newaxis, :]],
   390                                                               axis=-1) * st.waveforms.units
   391                                                           newWaveforms[firstVolIdx, :, :] = filledWaveforms
   392                                                           newWaveforms = newWaveforms[notADuplicateMask, :, :]
   393                                                           #
   394                                                           unit = st.unit
   395                                                           uIdx = np.flatnonzero([
   396                                                               np.all(i == st)
   397                                                               for i in unit.spiketrains])[0]
   398                                                           seg = st.segment
   399                                                           segIdx = np.flatnonzero([
   400                                                               np.all(i == st)
   401                                                               for i in seg.spiketrains])[0]
   402                                                           #
   403                                                           newSt = deepcopy(st[notADuplicateMask])
   404                                                           newSt.waveforms = newWaveforms
   405                                                           for k in newSt.array_annotations.keys():
   406                                                               newSt.array_annotations[k] = st.array_annotations[k][notADuplicateMask]
   407                                                               if k in st.annotations:
   408                                                                   newSt.annotations[k] = st.array_annotations[k][notADuplicateMask]
   409                                                           unit.spiketrains[uIdx] = newSt
   410                                                           newSt.unit = unit
   411                                                           seg.spiketrains[segIdx] = newSt
   412                                                           newSt.segment = seg
   413                                                           allStimTrains[stIdx] = newSt
   414                                                           del st
   415                                                           unit.create_relationship()
   416                                                           seg.create_relationship()
   417                                                       else:
   418                                                           # fix inconsistency in first sample sourceArr, destArr, whichIdx
   419        11       1103.0    100.3      0.0                  wvf = pd.DataFrame(np.atleast_2d(
   420        11      33265.0   3024.1      0.0                      np.squeeze(st.waveforms)))
   421        11      93592.0   8508.4      0.0                  wvfDiff = wvf.diff(-1, axis=1).fillna(0)
   422        11      32533.0   2957.5      0.0                  wvfDiffAbs = wvfDiff.abs()
   423                                                           #
   424        11     764670.0  69515.5      0.1                  rawMaxIdx = wvfDiffAbs.iloc[:, :5].idxmax(axis=1)
   425        11      40302.0   3663.8      0.0                  firstValidIdx, _ = stats.mode(rawMaxIdx, axis=None)
   426        11       1278.0    116.2      0.0                  firstValidIdx = int(firstValidIdx[-1] + 1)
   427                                                           #
   428        11        946.0     86.0      0.0                  st.waveforms = fixRippleStimWvf(
   429        11        993.0     90.3      0.0                      st.waveforms, st.waveforms,
   430        11   15248266.0 1386206.0      1.7                      whichIdx=firstValidIdx, fixFirst=firstValidIdx)
   431        11       1232.0    112.0      0.0                  print(
   432        11       1029.0     93.5      0.0                      'on spiketrain {}, waveforms.shape = {}'
   433        11       7650.0    695.5      0.0                      .format(st.name, st.waveforms.shape))
   434                                                       #
   435         1         81.0     81.0      0.0          if mustDoubleSpikeWvfLen:
   436        12       7690.0    640.8      0.0              for stIdx, st in enumerate(spikesBlock.filter(objects=SpikeTrain)):
   437        11        984.0     89.5      0.0                  if st.waveforms.shape[-1] == originalSpikeWvfLen:
   438        11        916.0     83.3      0.0                      st.waveforms = np.concatenate(
   439                                                                   [
   440        11       3746.0    340.5      0.0                              st.waveforms, np.zeros_like(st.waveforms)],
   441        11      32203.0   2927.5      0.0                          axis=-1) * st.waveforms.units
   442                                               #
   443         1         88.0     88.0      0.0      if len(allSpikeTrains):
   444         1         95.0     95.0      0.0          spikeMatBlock = ns5.calcBinarizedArray(
   445         1     122836.0 122836.0      0.0              deepcopy(spikesBlock), samplingRate,
   446         1        124.0    124.0      0.0              binnedSpikePath.format(arguments['analysisName']),
   447         1   21882772.0 21882772.0      2.5              saveToFile=True)
   448         1         99.0     99.0      0.0          newT = pd.Series(
   449         1         90.0     90.0      0.0              spikeMatBlock.filter(
   450         1     156220.0 156220.0      0.0                  objects=AnalogSignal)[0].times.magnitude)
   451                                               else:
   452                                                   dummyT = nspBlock.filter(objects=AnalogSignalProxy)[0]
   453                                                   newT = pd.Series(
   454                                                       np.arange(
   455                                                           dummyT.t_start,
   456                                                           dummyT.t_stop + 1/samplingRate,
   457                                                           1/samplingRate))
   458                                               #
   459         1         93.0     93.0      0.0      etpJsonPath = './isiElectrodeProgramLookup.json'
   460         1       1137.0   1137.0      0.0      if os.path.exists(etpJsonPath):
   461         1        943.0    943.0      0.0          with open(etpJsonPath, 'r') as f:
   462         1        996.0    996.0      0.0              electrodeToProgramLookup = json.load(f)
   463         1        225.0    225.0      0.0              latestProgram = len(electrodeToProgramLookup.keys())
   464                                               else:
   465                                                   electrodeToProgramLookup = {}
   466                                                   latestProgram = 0
   467         1         82.0     82.0      0.0      if stimEvents is not None:
   468         1        112.0    112.0      0.0          stimEvents.segment = spikesBlock.segments[0]
   469         1        106.0    106.0      0.0          spikesBlock.segments[0].events.append(stimEvents)
   470                                               # stimEvents.annotations['nominalWaveforms'] = np.vstack(allNominalWaveforms)
   471         1         83.0     83.0      0.0      if len(allStimTrains):
   472         2        192.0     96.0      0.0          for segIdx, dataSeg in enumerate(spikesBlock.segments):
   473                                                       spikeList = [
   474         1         84.0     84.0      0.0                  st
   475         1        944.0    944.0      0.0                  for st in dataSeg.filter(objects=SpikeTrain)
   476                                                           if '_stim' in st.name]
   477                                                       stimRasters = [
   478         1         83.0     83.0      0.0                  sr
   479         1        185.0    185.0      0.0                  for sr in spikeMatBlock.segments[segIdx].analogsignals
   480                                                           if '_stim' in sr.name]
   481         1         92.0     92.0      0.0              stimRastersDF = ns5.analogSignalsToDataFrame(
   482         1     510589.0 510589.0      0.1                  stimRasters, idxT='t', useChanNames=True)
   483                                                       stimRastersDF.columns = [
   484         1         98.0     98.0      0.0                  cn.replace('_stim#0_raster', '')
   485         1       6652.0   6652.0      0.0                  for cn in stimRastersDF.columns]
   486                                                       # trick to avoid double counting channels that are plugged into the same electrode
   487         1         90.0     90.0      0.0              keepStimRasterList = []
   488        12       1001.0     83.4      0.0              for stIdx, st in enumerate(spikeList):
   489        11       1036.0     94.2      0.0                  chanName = st.unit.channel_index.name
   490                                                           # matchingAsig = nspBlock.filter(objects=AnalogSignalProxy, name='seg0_' + chanName)
   491                                                           # if len(matchingAsig):
   492                                                           #     keepStimRasterList.append(chanName)
   493        11        906.0     82.4      0.0                  keepStimRasterList.append(chanName)
   494         1    1442530.0 1442530.0      0.2              stimActive = stimRastersDF[keepStimRasterList].sum(axis=1) > 0
   495         1      33048.0  33048.0      0.0              activeTimes = stimRastersDF.loc[stimActive, 't']
   496                                                       #
   497         1         94.0     94.0      0.0              if stimEvents is not None:
   498                                                           stimEvents[:] = (
   499                                                               stimEvents.times -
   500                                                               stimEvents.times[0] -
   501         1       3618.0   3618.0      0.0                      20e-3 * pq.s +  # Fudge factor to account for delay between execution and matlab save
   502         1       4569.0   4569.0      0.0                      activeTimes.min() * pq.s)
   503         1      11842.0  11842.0      0.0                  stimEventsDF = pd.DataFrame(stimEvents.array_annotations)
   504         1       8758.0   8758.0      0.0                  stimEventsDF['t'] = stimEvents.times
   505         1        107.0    107.0      0.0                  stimEventsDF.to_csv(os.path.join(
   506         1     159676.0 159676.0      0.0                      analysisSubFolder, ns5FileName + '_exported_stim_updates.csv'
   507                                                               ))
   508                                                       #
   509        12       1062.0     88.5      0.0              for stIdx, st in enumerate(spikeList):
   510                                                           # annotate ripple stim spikes with info from json log
   511        11       1130.0    102.7      0.0                  chanName = st.unit.channel_index.name
   512                                                           # matchingChIdx = nspBlock.filter(objects=ChannelIndex, name=chanName)
   513                                                           #pdb.set_trace()
   514        11      96778.0   8798.0      0.0                  rippleChanNum = int(mapDF.loc[mapDF['label'] == chanName, 'nevID'])
   515        11        986.0     89.6      0.0                  if stimEvents is not None:
   516                                                               # find which events in the stim log reference this spiketrain
   517        11       1820.0    165.5      0.0                      thisStEventsMask = stimEvents.array_annotations['elec'] == rippleChanNum
   518        11        960.0     87.3      0.0                      theseUpdates = pd.DataFrame({
   519                                                                   k: v[thisStEventsMask]
   520        11     120272.0  10933.8      0.0                          for k, v in stimEvents.array_annotations.items()
   521                                                                   })
   522        11      47756.0   4341.5      0.0                      theseUpdates.index = stimEvents[thisStEventsMask].times
   523        11       1023.0     93.0      0.0                      theseUpdates.index.name = 't'
   524                                                               # NOTE: the line below is a workaround for an edge case where the same electrode is
   525                                                               # requested twice in the same command, it should not be needed normally
   526        11      61248.0   5568.0      0.0                      theseUpdates = theseUpdates.loc[~theseUpdates.index.duplicated(), :]
   527                                                               # create entries for each pulse of the spiketrain
   528        11       1064.0     96.7      0.0                      newIndex = np.unique(np.concatenate([
   529        11      33801.0   3072.8      0.0                          stimEvents[thisStEventsMask].times.magnitude,
   530        11      13446.0   1222.4      0.0                          st.times.magnitude]))
   531                                                               #  
   532                                                               # updateTimes = pd.Series(theseUpdates.index)
   533                                                               # nonMonotonicTimes = updateTimes.diff().fillna(1) <= 0
   534                                                               # updateTimes[nonMonotonicTimes][0]
   535                                                               # theseUpdate.loc[theseUpdates.index > updateTimes[nonMonotonicTimes][0], :]
   536        11        952.0     86.5      0.0                      try:
   537        11      71650.0   6513.6      0.0                          allUpdates = theseUpdates.reindex(newIndex, method='ffill')
   538        11       1394.0    126.7      0.0                          stAnnotations = allUpdates.loc[
   539        11     113493.0  10317.5      0.0                              allUpdates.index.isin(st.times.magnitude), :]
   540                                                               except Exception:
   541                                                                   pdb.set_trace()
   542                                                                   traceback.print_exc()
   543                                                           #
   544        11      30493.0   2772.1      0.0                  wvf = pd.DataFrame(np.atleast_2d(np.squeeze(st.waveforms)))
   545        11     113019.0  10274.5      0.0                  wvfDiff = wvf.diff(-1, axis=1).fillna(0)
   546        11      37514.0   3410.4      0.0                  wvfDiffAbs = wvfDiff.abs()
   547        11        992.0     90.2      0.0                  if stimEvents is not None:
   548        11      31732.0   2884.7      0.0                      lastValidIdx = int(stAnnotations['totalPW'].min() * 3e4) - 1
   549        11      24195.0   2199.5      0.0                      idxPeak = int(stAnnotations['firstPW'].min() * 3e4)
   550        11      44440.0   4040.0      0.0                      wvf.iloc[:, lastValidIdx:] = np.nan
   551        11      49687.0   4517.0      0.0                      wvf.fillna(method='ffill', axis=1, inplace=True)
   552        11     112325.0  10211.4      0.0                      wvfDiff = wvf.diff(-1, axis=1).fillna(0)
   553        11      35618.0   3238.0      0.0                      wvfDiffAbs = wvfDiff.abs()
   554                                                           else:
   555                                                               rawMaxIdx = wvfDiffAbs.idxmax(axis=1)
   556                                                               #
   557                                                               if (rawMaxIdx > 2).any():
   558                                                                   lastValidIdx, _ = stats.mode(
   559                                                                       rawMaxIdx[rawMaxIdx > 2], axis=None)
   560                                                                   lastValidIdx = int(lastValidIdx[-1]) - 2
   561                                                               else:
   562                                                                   lastValidIdx = wvf.shape[-1] - 1
   563                                                               #
   564                                                               print(
   565                                                                   'On spikeTrain {}, last valid index is {}'
   566                                                                   .format(st.name, lastValidIdx))
   567                                                               #
   568                                                               wvf.iloc[:, lastValidIdx:] = np.nan
   569                                                               wvf.fillna(method='ffill', axis=1, inplace=True)
   570                                                               wvfDiff = wvf.diff(-1, axis=1).fillna(0)
   571                                                               wvfDiffAbs = wvfDiff.abs()
   572                                                               #
   573                                                               scaler = StandardScaler()
   574                                                               scaler.fit(wvfDiffAbs.iloc[:, 1:lastValidIdx].to_numpy().reshape(-1, 1))
   575                                                               transformWvfDiff = lambda x: np.squeeze(scaler.transform(x.reshape(-1, 1)))
   576                                                               wvfDiffStd = wvfDiffAbs.apply(transformWvfDiff, axis=1, raw=True)
   577                                                               # if arguments['plotting']:
   578                                                               #     plt.plot(wvfDiffStd.T, 'o-'); plt.title('{} standardized abs diff'.format(st.name)); plt.show()
   579                                                               # TODO: check if it's necessary to exclude some samples from being centered
   580                                                               # samplesNeedFix = wvfDiffStd.abs().iloc[:, 0] > 0
   581                                                               # print('{} out of {} samples need fixing'.format(samplesNeedFix.sum(), samplesNeedFix.size))
   582                                                               # wvf.loc[samplesNeedFix, 0] = np.nan
   583                                                               # wvf.fillna(method='bfill', axis=1, inplace=True)
   584                                                               # wvfDiff.loc[samplesNeedFix, 0] = np.nan
   585                                                               # wvfDiff.fillna(method='bfill', axis=1, inplace=True)
   586                                                               # wvfDiffStd.loc[samplesNeedFix, 0] = np.nan
   587                                                               # wvfDiffStd.fillna(method='bfill', axis=1, inplace=True)
   588                                                               # wvf = wvf.apply(lambda x: x - x[0], axis=1, raw=True)
   589                                                               allPeakIdx = wvfDiffStd.iloc[:, :lastValidIdx - 5].idxmax(axis=1)
   590                                                               if (allPeakIdx > 2).any():
   591                                                                   idxPeak, _ = stats.mode(allPeakIdx[allPeakIdx > 2], axis=None)
   592                                                                   idxPeak = int(idxPeak[0])
   593                                                               else:
   594                                                                   idxPeak = int(lastValidIdx/2)
   595                                                           #
   596        11       1006.0     91.5      0.0                  amplitudes = wvf.apply(
   597        11        944.0     85.8      0.0                      lambda x: (x[idxPeak] - x[0]) * 1e-6,
   598        11     438420.0  39856.4      0.1                      axis=1, raw=True).to_numpy() * pq.V
   599        11       1224.0    111.3      0.0                  st.annotations['amplitude'] = amplitudes
   600        11       4857.0    441.5      0.0                  st.array_annotations['amplitude'] = amplitudes
   601        11        958.0     87.1      0.0                  if 'arrayAnnNames' in st.annotations:
   602                                                               st.annotations['arrayAnnNames'].append('amplitude')
   603                                                           else:
   604        11        922.0     83.8      0.0                      st.annotations['arrayAnnNames'] = ['amplitude']
   605                                                           #
   606        11       3242.0    294.7      0.0                  ampWithinSpec = np.abs(amplitudes) < 4
   607                                                           #
   608        11       8071.0    733.7      0.0                  plotMask = st.times > 0 # < 1360
   609        11       1044.0     94.9      0.0                  if arguments['plotting']:
   610                                                               plt.plot(st.sampling_period * np.arange(wvf.shape[1]), wvf.iloc[plotMask, :].T * 1e-6, 'o-'); plt.title('{} fixed wvf peak at {}'.format(st.name, idxPeak*st.sampling_period)); plt.show()
   611                                                               plt.plot(st.sampling_period * np.arange(wvf.shape[1]), (wvfDiffAbs).iloc[:, :].T * 1e-6, 'o-');
   612                                                               plt.plot(st.sampling_period * np.arange(wvf.shape[1]), (wvfDiffAbs).iloc[:, :].mean().T * 1e-6, 'o-', lw=3); plt.title('{} fixed diff peak at {}'.format(st.name, idxPeak*st.sampling_period)); plt.show()
   613        11        995.0     90.5      0.0                  if stimEvents is None:
   614                                                               pws = amplitudes ** 0 * idxPeak * st.sampling_period
   615                                                               st.annotations['firstPW'] = pws
   616                                                               st.array_annotations['firstPW'] = pws
   617                                                               st.annotations['arrayAnnNames'].append('firstPW')
   618                                                               #
   619                                                               secPws = amplitudes ** 0 * (lastValidIdx - idxPeak) * st.sampling_period
   620                                                               st.annotations['secondPW'] = secPws
   621                                                               st.array_annotations['secondPW'] = secPws
   622                                                               st.annotations['arrayAnnNames'].append('secondPW')
   623                                                               #
   624                                                               # interPhases = 2 * amplitudes ** 0 * st.sampling_period
   625                                                               # st.annotations['interPhase'] = interPhases
   626                                                               # st.array_annotations['interPhase'] = interPhases
   627                                                               # st.annotations['arrayAnnNames'].append('interPhase')
   628                                                               #
   629                                                               totalPws = pws + secPws
   630                                                               st.annotations['totalPW'] = totalPws
   631                                                               st.array_annotations['totalPW'] = totalPws
   632                                                               st.annotations['arrayAnnNames'].append('totalPW')
   633                                                               # try to estimate current
   634                                                               matchingAsig = nspBlock.filter(objects=AnalogSignalProxy, name='seg0_' + chanName)
   635                                                               if len(matchingAsig):
   636                                                                   elecImpedance = (
   637                                                                       impedancesRipple
   638                                                                       .loc[impedancesRipple['elec'] == chanName, 'impedance'])
   639                                                                   currents = amplitudes / (elecImpedance.iloc[0] * pq.kOhm)
   640                                                                   st.annotations['nominalCurrent'] = currents
   641                                                                   st.array_annotations['nominalCurrent'] = currents
   642                                                                   if 'arrayAnnNames' in st.annotations:
   643                                                                       st.annotations['arrayAnnNames'].append('nominalCurrent')
   644                                                                   else:
   645                                                                       st.annotations['arrayAnnNames'] = ['nominalCurrent']
   646                                                           else:
   647        99     112627.0   1137.6      0.0                      for annName in stAnnotations.drop('elec', axis='columns'):
   648        88       7911.0     89.9      0.0                          st.annotations['arrayAnnNames'].append(annName)
   649                                                                   st.annotations[annName] = (
   650        88      61368.0    697.4      0.0                              stAnnotations[annName].to_numpy() *
   651        88      27012.0    307.0      0.0                              eventUnits[annName])
   652                                                                   st.array_annotations[annName] = (
   653        88      21567.0    245.1      0.0                              stAnnotations[annName].to_numpy() *
   654        88      44117.0    501.3      0.0                              eventUnits[annName])
   655                                                       # detect stimulation trains
   656         1         91.0     91.0      0.0              peakIdx, _, trainStartIdx, trainEndIdx = hf.findTrains(
   657         1    7216561.0 7216561.0      0.8                  peakTimes=activeTimes, minDistance=5e-3, maxDistance=200e-3)
   658                                                       #  
   659         1       1658.0   1658.0      0.0              trainDurations = trainEndIdx - trainStartIdx
   660                                                       #
   661         1         92.0     92.0      0.0              if len(trainStartIdx):
   662         1         82.0     82.0      0.0                  startCategories = pd.DataFrame(
   663         1      41321.0  41321.0      0.0                      activeTimes[trainStartIdx].to_numpy(),
   664                                                               # index=range(activeTimes[trainStartIdx].size),
   665         1       7205.0   7205.0      0.0                      columns=['t'])
   666         1         96.0     96.0      0.0                  startCategories = startCategories.reindex(columns=[
   667                                                               # 'amplitude',
   668         1         84.0     84.0      0.0                      'nominalCurrent', 'program',
   669         1         82.0     82.0      0.0                      'activeGroup', 'firstPW', 'secondPW',
   670                                                               # 'interPhase',
   671         1         81.0     81.0      0.0                      'totalPW', 'electrode',
   672         1      11091.0  11091.0      0.0                      'RateInHz', 'stimPeriod', 'trainDur', 't'])
   673                                                           #
   674         1         90.0     90.0      0.0                  for idx, (idxStart, idxEnd) in enumerate(
   675       651      82695.0    127.0      0.0                          zip(trainStartIdx, trainEndIdx)):
   676                                                               stimRasterRow = (
   677       650      70535.0    108.5      0.0                          stimRastersDF
   678       650   10631797.0  16356.6      1.2                          .loc[idxStart, keepStimRasterList])
   679       650    3481064.0   5355.5      0.4                      activeChans = stimRasterRow.index[stimRasterRow > 0]
   680       650      88778.0    136.6      0.0                      if not activeChans.empty:
   681       650      67274.0    103.5      0.0                          stimRasterAmplitude = pd.Series(
   682       650    1436439.0   2209.9      0.2                              np.nan, index=activeChans)
   683       650      67715.0    104.2      0.0                          stimRasterCurrent = pd.Series(
   684       650    1321856.0   2033.6      0.2                              np.nan, index=activeChans)
   685      1300     323522.0    248.9      0.0                          for activeChanIdx, activeChan in enumerate(activeChans):
   686                                                                       st = [
   687       650      67729.0    104.2      0.0                                  i
   688       650     156879.0    241.4      0.0                                  for i in spikeList
   689       650      66302.0    102.0      0.0                                  if i.unit.channel_index.name == activeChan][0]
   690                                                                       theseTimesMask = (
   691       650     491164.0    755.6      0.1                                  (st.times >= (
   692       650     629422.0    968.3      0.1                                      stimRastersDF.loc[idxStart, 't'] * pq.s -
   693       650    3897643.0   5996.4      0.4                                      1.1 * samplingRate ** (-1) / 2)) &
   694       650     331497.0    510.0      0.0                                  (st.times <= (
   695       650     569859.0    876.7      0.1                                      stimRastersDF.loc[idxEnd, 't'] * pq.s +
   696       650    3677626.0   5657.9      0.4                                      1.1 * samplingRate ** (-1) / 2))
   697                                                                           )
   698       650     378511.0    582.3      0.0                              theseTimes = st.times[theseTimesMask]
   699       650     133696.0    205.7      0.0                              if not theseTimesMask.sum():
   700                                                                           pdb.set_trace()
   701       650      67858.0    104.4      0.0                              stimRasterAmplitude[activeChan] = np.mean(
   702       650     655612.0   1008.6      0.1                                  st.annotations['amplitude'][theseTimesMask])
   703       650      68081.0    104.7      0.0                              stimRasterCurrent[activeChan] = np.mean(
   704       650     438646.0    674.8      0.1                                  st.annotations['nominalCurrent'][theseTimesMask])
   705       650      66045.0    101.6      0.0                              if activeChanIdx == 0:
   706       650      66346.0    102.1      0.0                                  if stimEvents is None:
   707                                                                               if theseTimes.size == 1:
   708                                                                                   startCategories.loc[
   709                                                                                       idx, 'trainDur'] = 0
   710                                                                                   startCategories.loc[
   711                                                                                       idx, 'RateInHz'] = 0
   712                                                                                   startCategories.loc[
   713                                                                                       idx, 'stimPeriod'] = 1000
   714                                                                               else:
   715                                                                                   startCategories.loc[
   716                                                                                       idx, 'trainDur'] = (
   717                                                                                           theseTimes[-1] -
   718                                                                                           theseTimes[0])
   719                                                                                   # stimPeriod = np.round(np.diff(theseTimes).median(), decimals=6)
   720                                                                                   stimPeriod = np.round(np.median(np.diff(theseTimes)), decimals=6)
   721                                                                                   # stimPeriod = np.median(np.diff(theseTimes))
   722                                                                                   # pdb.set_trace()
   723                                                                                   startCategories.loc[
   724                                                                                       idx, 'stimPeriod'] = stimPeriod
   725                                                                                   startCategories.loc[
   726                                                                                       idx, 'RateInHz'] = stimPeriod ** -1
   727                                                                           else:
   728       650    1094624.0   1684.0      0.1                                      nominalRate = np.median(st.annotations['RateInHz'][theseTimesMask])
   729       650      70014.0    107.7      0.0                                      if len(theseTimes) > 1:
   730       600    1661868.0   2769.8      0.2                                          observedRate = np.median(np.diff(theseTimes)) ** (-1)
   731                                                                               else:
   732        50      23282.0    465.6      0.0                                          observedRate = 3 / pq.s
   733       650      68495.0    105.4      0.0                                      try:
   734       650    3065000.0   4715.4      0.4                                          rateMismatch = np.abs(nominalRate - observedRate)
   735                                                                               except:
   736                                                                                   pdb.set_trace()
   737       650     106126.0    163.3      0.0                                      if not rateMismatch < 1e-6:
   738        20       2784.0    139.2      0.0                                          print(
   739        20       2361.0    118.0      0.0                                              'Rate mismatch warning on {} at time {}: off by {} Hz'
   740        20      20466.0   1023.3      0.0                                              .format(st.name, theseTimes[0], rateMismatch))
   741       650     366195.0    563.4      0.0                                      nominalTrainDur = np.mean(st.annotations['trainDur'][theseTimesMask])
   742       650     785413.0   1208.3      0.1                                      observedTrainDur = (theseTimes[-1] - theseTimes[0])
   743       650     719053.0   1106.2      0.1                                      if not np.abs(nominalTrainDur - observedTrainDur) < 1e-6:
   744        20      13985.0    699.2      0.0                                          print('train Dur Warning on {} at time {}'.format(st.name, theseTimes[0]))
   745                                                                               # assert np.diff(theseTimes).mean()
   746       650    5450018.0   8384.6      0.6                                      startCategories.loc[idx, 'trainDur'] = nominalTrainDur
   747       650    5178806.0   7967.4      0.6                                      startCategories.loc[idx, 'RateInHz'] = nominalRate
   748       650    5447143.0   8380.2      0.6                                      startCategories.loc[idx, 'stimPeriod'] = nominalRate ** -1
   749                                                                           startCategories.loc[
   750       650      73975.0    113.8      0.0                                      idx, 'secondPW'] = np.round(np.mean(
   751       650    5666047.0   8717.0      0.6                                          st.annotations['secondPW'][theseTimesMask]), decimals=9)
   752                                                                           startCategories.loc[
   753       650      71135.0    109.4      0.0                                      idx, 'firstPW'] = np.round(np.mean(
   754       650    5633584.0   8667.1      0.6                                          st.annotations['firstPW'][theseTimesMask]), decimals=9)
   755                                                                           # startCategories.loc[
   756                                                                           #     idx, 'interPhase'] = np.round(np.mean(
   757                                                                           #         st.annotations['interPhase'][theseTimesMask]), decimals=9)
   758                                                                           startCategories.loc[
   759       650      70962.0    109.2      0.0                                      idx, 'totalPW'] = np.round(np.mean(
   760       650    5619474.0   8645.3      0.6                                          st.annotations['totalPW'][theseTimesMask]), decimals=9)
   761       650    4702464.0   7234.6      0.5                          startCategories.loc[idx, 'activeGroup'] = 1
   762       650      69100.0    106.3      0.0                          electrodeShortHand = ''
   763       650    2873474.0   4420.7      0.3                          negativeAmps = stimRasterCurrent < 0
   764                                                                   #
   765       650     943738.0   1451.9      0.1                          if (negativeAmps).any():
   766       650      67238.0    103.4      0.0                              electrodeShortHand += '-'
   767       650    3800917.0   5847.6      0.4                              totalCathode = stimRasterCurrent[negativeAmps].sum()
   768       650    4818558.0   7413.2      0.6                              startCategories.loc[idx, 'nominalCurrent'] = totalCathode
   769       650      70934.0    109.1      0.0                              averageImpedance = np.mean(
   770       650     132998.0    204.6      0.0                                  impedancesRipple.loc[impedancesRipple['elec'].isin(
   771       650    8782022.0  13510.8      1.0                                      stimRasterCurrent[negativeAmps].index), 'impedance'])
   772                                                                       # startCategories.loc[idx, 'amplitude'] = totalCathode * averageImpedance
   773                                                                       # pdb.set_trace()
   774      1300    2757723.0   2121.3      0.3                              for cName in stimRasterCurrent[negativeAmps].index:
   775       650      73837.0    113.6      0.0                                  if cName[:-2] not in electrodeShortHand:
   776       650      69041.0    106.2      0.0                                      electrodeShortHand += cName[:-2]
   777       650    2809410.0   4322.2      0.3                          positiveAmps = stimRasterCurrent > 0
   778       650     938217.0   1443.4      0.1                          if (positiveAmps).any():
   779                                                                       electrodeShortHand += '+'
   780                                                                       totalAnode = stimRasterCurrent[positiveAmps].sum()
   781                                                                       for cName in stimRasterCurrent[positiveAmps].index:
   782                                                                           if cName[:-2] not in electrodeShortHand:
   783                                                                               electrodeShortHand += cName[:-2]
   784                                                                       if np.isnan(startCategories.loc[idx, 'nominalCurrent']):
   785                                                                           startCategories.loc[idx, 'nominalCurrent'] = totalAnode
   786       650    5470015.0   8415.4      0.6                          startCategories.loc[idx, 'electrode'] = electrodeShortHand
   787       650      73673.0    113.3      0.0                          if (electrodeShortHand not in electrodeToProgramLookup):
   788                                                                       electrodeToProgramLookup[electrodeShortHand] = latestProgram
   789                                                                       latestProgram += 1
   790       650    4870670.0   7493.3      0.6                          startCategories.loc[idx, 'program'] = electrodeToProgramLookup[electrodeShortHand]
   791                                                           #
   792         1        270.0    270.0      0.0                  currCats = pd.cut(
   793         1       2307.0   2307.0      0.0                      startCategories['nominalCurrent'],
   794         1     170667.0 170667.0      0.0                      np.arange(-2, 2, 0.2))
   795         1      52708.0  52708.0      0.0                  startCategories['nominalCurrentCat'] = currCats.astype('str')
   796         1       7562.0   7562.0      0.0                  startCategories['RateInHz'] = np.round(startCategories['RateInHz'], decimals=6)
   797         1       7848.0   7848.0      0.0                  stopCategories = startCategories.copy()
   798                                                           #
   799                                                           stopCategories['t'] = (
   800         1      69091.0  69091.0      0.0                      activeTimes[trainEndIdx].to_numpy() +
   801                                                               (
   802         1       1576.0   1576.0      0.0                          stopCategories['firstPW'] +
   803                                                                   # stopCategories['interPhase'] +
   804         1       5507.0   5507.0      0.0                          stopCategories['secondPW']
   805         1       2628.0   2628.0      0.0                      ).to_numpy() * 1e-6)
   806                                                           # maxAmp = startCategories['amplitude'].max()
   807                                                           # minAmp = startCategories['amplitude'].min()
   808                                                           # ampBinRes = 0.2
   809                                                           # ampBins = np.arange(
   810                                                           #     (np.floor(minAmp / ampBinRes) - 1) * ampBinRes,
   811                                                           #     (np.ceil(maxAmp / ampBinRes) + 1) * ampBinRes,
   812                                                           #     ampBinRes)
   813                                                           # ampBins[0] -= 0.01
   814                                                           # ampBins[-1] += 0.01
   815                                                           # ampCats = pd.cut(startCategories['amplitude'], ampBins)
   816                                                           # startCategories['amplitudeCat'] = ampCats.astype(np.str)
   817                                                           # stopCategories['amplitudeCat'] = ampCats.astype(np.str)
   818         1       7873.0   7873.0      0.0                  startCategories['stimCat'] = 'stimOn'
   819         1       6743.0   6743.0      0.0                  stopCategories['stimCat'] = 'stimOff'
   820         1      28421.0  28421.0      0.0                  startCategories.dropna(inplace=True)
   821         1      27287.0  27287.0      0.0                  stopCategories.dropna(inplace=True)
   822                                                   #
   823         1       3120.0   3120.0      0.0          with open(etpJsonPath, 'w') as f:
   824         1       4945.0   4945.0      0.0              json.dump(electrodeToProgramLookup, f)
   825         1         99.0     99.0      0.0          alignEventsDF = pd.concat((
   826         1         86.0     86.0      0.0              startCategories, stopCategories),
   827         1      49283.0  49283.0      0.0              axis=0, ignore_index=True, sort=True)
   828                                                   # remove events outside manually identified time bounds
   829         1         92.0     92.0      0.0          if alignTimeBounds is not None:
   830                                                       keepMask = pd.Series(False, index=alignEventsDF.index)
   831                                                       for atb in alignTimeBounds:
   832                                                           keepMask = (
   833                                                               keepMask |
   834                                                               (
   835                                                                   (alignEventsDF['t'] >= atb[0]) &
   836                                                                   (alignEventsDF['t'] <= atb[1])))
   837                                                   else:
   838         1       2073.0   2073.0      0.0              keepMask = pd.Series(True, index=alignEventsDF.index)
   839         1         90.0     90.0      0.0          alignEventsDF.drop(
   840         1      23539.0  23539.0      0.0              index=alignEventsDF.index[~keepMask], inplace=True)
   841                                                   #
   842         1        220.0    220.0      0.0          if not alignEventsDF.empty:
   843         1      16650.0  16650.0      0.0              alignEventsDF.sort_values('t', inplace=True, kind='mergesort')
   844         1        105.0    105.0      0.0              alignEvents = ns5.eventDataFrameToEvents(
   845         1        111.0    111.0      0.0                  alignEventsDF, idxT='t',
   846         1         84.0     84.0      0.0                  annCol=None,
   847         1         95.0     95.0      0.0                  eventName='seg{}_stimAlignTimes'.format(segIdx),
   848         1      50897.0  50897.0      0.0                  tUnits=pq.s, makeList=False)
   849         1        156.0    156.0      0.0              alignEvents.annotate(nix_name=alignEvents.name)
   850                                                       #
   851         1         85.0     85.0      0.0              concatLabelsDF = alignEventsDF
   852         1         87.0     87.0      0.0              concatLabels = np.array([
   853         1         86.0     86.0      0.0                  '{}'.format(row)
   854         1   12355158.0 12355158.0      1.4                  for rowIdx, row in concatLabelsDF.iterrows()])
   855         1        106.0    106.0      0.0              concatEvents = Event(
   856         1        114.0    114.0      0.0                  name='seg{}_stimAlignTimesConcatenated'.format(segIdx),
   857         1       1145.0   1145.0      0.0                  times=alignEvents.times,
   858         1       7329.0   7329.0      0.0                  labels=concatLabels
   859                                                           )
   860         1        109.0    109.0      0.0              dataSeg.events.append(alignEvents)
   861         1         85.0     85.0      0.0              dataSeg.events.append(concatEvents)
   862         1         87.0     87.0      0.0              alignEvents.segment = dataSeg
   863         1         84.0     84.0      0.0              concatEvents.segment = dataSeg
   864                                               #  Delete stim trains, because they won't be consistent across assembled files
   865                                               # if len(allStimTrains):
   866                                               #     for seg in spikesBlock.segments:
   867                                               #         for st in allStimTrains:
   868                                               #             if st in seg.spiketrains:
   869                                               #                 seg.spiketrains.remove(st)
   870                                               #     allStimUnits = [un for un in spikesBlock.filter(objects=Unit) if '_stim' in un.name]
   871                                               #     del allStimTrains
   872                                               #     # delChanIndices = []
   873                                               #     for chIdx in spikesBlock.channel_indexes:
   874                                               #         for stUn in allStimUnits:
   875                                               #             if stUn in chIdx.units:
   876                                               #                 chIdx.units.remove(stUn)
   877                                               #     del allStimUnits
   878                                               #
   879                                               #
   880         1       1557.0   1557.0      0.0      aSigList = tdBlock.filter(objects=AnalogSignal)
   881                                               #pdb.set_trace()
   882         1    6429827.0 6429827.0      0.7      tdDF = ns5.analogSignalsToDataFrame(aSigList)
   883         1        189.0    189.0      0.0      currentSamplingRate = aSigList[0].sampling_rate
   884                                               #
   885         1       1653.0   1653.0      0.0      if samplingRate != currentSamplingRate:
   886         1        695.0    695.0      0.0          print("Reinterpolating...")
   887         1         93.0     93.0      0.0          tdInterp = hf.interpolateDF(
   888         1         85.0     85.0      0.0              tdDF, newT,
   889         1         84.0     84.0      0.0              kind='linear', fill_value=(0, 0),
   890         1   19939007.0 19939007.0      2.3              x='t', columns=tdChanNames, verbose=arguments['verbose'])
   891                                               else:
   892                                                   tdInterp = tdDF
   893         1      18100.0  18100.0      0.0      delsysASigList = delsysLoadedBlock.filter(objects=AnalogSignal)
   894         1    2408453.0 2408453.0      0.3      delsysDF = ns5.analogSignalsToDataFrame(delsysASigList)
   895         1        135.0    135.0      0.0      currentDelsysSamplingRate = delsysASigList[0].sampling_rate
   896                                               #
   897         1       1477.0   1477.0      0.0      if samplingRate != currentDelsysSamplingRate:
   898         1        713.0    713.0      0.0          print("Reinterpolating...")
   899         1         91.0     91.0      0.0          delsysInterp = hf.interpolateDF(
   900         1         84.0     84.0      0.0              delsysDF, newT,
   901         1         83.0     83.0      0.0              kind='linear', fill_value=(0, 0),
   902         1  389048101.0 389048101.0     44.6              x='t', columns=delsysChanNames, verbose=arguments['verbose'])
   903                                               else:
   904                                                   delsysInterp = delsysDF
   905                                               #
   906         1       1107.0   1107.0      0.0      accCols = [cn for cn in delsysInterp.columns if 'Acc' in cn]
   907         1        121.0    121.0      0.0      if len(accCols):
   908                                                   # fix for bug affecting the mean of the channel
   909         1        113.0    113.0      0.0          if alignTimeBounds is not None:
   910                                                       keepMaskAsig = pd.Series(False, index=delsysInterp.index)
   911                                                       for atb in alignTimeBounds:
   912                                                           keepMaskAsig = (
   913                                                               keepMaskAsig |
   914                                                               (
   915                                                                   (delsysInterp['t'] >= atb[0]) &
   916                                                                   (delsysInterp['t'] <= atb[1])))
   917                                                   else:
   918         1       6303.0   6303.0      0.0              keepMaskAsig = pd.Series(True, index=delsysInterp.index)
   919         1        118.0    118.0      0.0          cornerFrequencyLP = 100
   920         1        126.0    126.0      0.0          sosLP = signal.butter(
   921         1        112.0    112.0      0.0              2, cornerFrequencyLP, 'low',
   922         1      12500.0  12500.0      0.0              fs=float(samplingRate), output='sos')
   923         1        131.0    131.0      0.0          preprocAcc = signal.sosfiltfilt(
   924         1   23789811.0 23789811.0      2.7              sosLP, delsysInterp.loc[:, accCols].to_numpy(), axis=0
   925                                                       )
   926         1    4917606.0 4917606.0      0.6          delsysInterp.loc[:, accCols] = preprocAcc
   927         1        883.0    883.0      0.0      emgCols = [cn for cn in delsysInterp.columns if 'Emg' in cn]
   928         1        126.0    126.0      0.0      if len(emgCols):
   929                                                   # fix for bug affecting the mean of the channel
   930         1        123.0    123.0      0.0          if alignTimeBounds is not None:
   931                                                       keepMaskAsig = pd.Series(False, index=delsysInterp.index)
   932                                                       for atb in alignTimeBounds:
   933                                                           keepMaskAsig = (
   934                                                               keepMaskAsig |
   935                                                               (
   936                                                                   (delsysInterp['t'] >= atb[0]) &
   937                                                                   (delsysInterp['t'] <= atb[1])))
   938                                                   else:
   939         1       7172.0   7172.0      0.0              keepMaskAsig = pd.Series(True, index=delsysInterp.index)
   940         1        103.0    103.0      0.0          sosHP = signal.butter(
   941         1         93.0     93.0      0.0              2, 100, 'high',
   942         1       9684.0   9684.0      0.0              fs=float(samplingRate), output='sos')
   943         1        101.0    101.0      0.0          cornerFrequencyLP = 40
   944         1         99.0     99.0      0.0          sosLP = signal.butter(
   945         1         93.0     93.0      0.0              2, cornerFrequencyLP, 'low',
   946         1       7476.0   7476.0      0.0              fs=float(samplingRate), output='sos')
   947                                                   if False:
   948                                                       t = np.arange(0, .1, samplingRate.magnitude ** (-1))
   949                                                       x = np.zeros_like(t)
   950                                                       x[int(x.size/2)] = 1
   951                                                       y = signal.sosfiltfilt(sosLP, x)
   952                                                       plt.plot(t, y); plt.show()
   953                                                   # weird units hack, TODO check
   954         1    2715659.0 2715659.0      0.3          delsysInterp.loc[:, emgCols] = delsysInterp.loc[:, emgCols] * 1e6
   955         1        121.0    121.0      0.0          preprocEmg = signal.sosfiltfilt(
   956         1         96.0     96.0      0.0              sosHP,
   957                                                       (
   958         1     477860.0 477860.0      0.1                  delsysInterp.loc[:, emgCols] -
   959         1        117.0    117.0      0.0                  delsysInterp
   960         1    3499235.0 3499235.0      0.4                  .loc[keepMaskAsig, emgCols]
   961         1   12312193.0 12312193.0      1.4                  .median(axis=0)).to_numpy(), axis=0
   962                                                       )
   963                                                   # 
   964         1        338.0    338.0      0.0          procNames = [eN.replace('Emg', 'EmgEnv') for eN in emgCols]
   965         1        151.0    151.0      0.0          emgEnvDF = pd.DataFrame(
   966         1        141.0    141.0      0.0              signal.sosfiltfilt(
   967         1    5857158.0 5857158.0      0.7                  sosLP, np.abs(preprocEmg), axis=0),
   968         1       9730.0   9730.0      0.0              columns=procNames
   969                                                       )
   970                                                   # pdb.set_trace()
   971         1    2419341.0 2419341.0      0.3          delsysInterp = pd.concat([delsysInterp, emgEnvDF], axis=1)
   972         1    9798970.0 9798970.0      1.1      tdInterp = pd.concat([delsysInterp.drop(columns='t'), tdInterp], axis=1)
   973                                           
   974                                                   # for cName in emgCols:
   975                                                   #     procName = cName.replace('Emg', 'EmgEnv')
   976                                                   #     # weird units hack, TODO check
   977                                                   #     tdInterp.loc[:, cName] = tdInterp.loc[:, cName] * 1e6
   978                                                   #     preprocEmg = signal.sosfiltfilt(
   979                                                   #         sosHP,
   980                                                   #         (tdInterp[cName] - tdInterp.loc[keepMaskAsig, cName].median()).to_numpy())
   981                                                   #     # 
   982                                                   #     tdInterp[procName] = signal.sosfiltfilt(
   983                                                   #         sosLP, np.abs(preprocEmg))
   984                                                   #     # break
   985                                                   #     # if True:
   986                                                   #     #     plt.plot(tdInterp.loc[keepMaskAsig, cName])
   987                                                   #     #     plt.plot(tdInterp.loc[keepMaskAsig, procName])
   988                                                   #     #     plt.show()
   989                                                   #     tdChanNames.append(procName)
   990                                                   #     #
   991                                               ## moved to cleaning scripts
   992                                               '''
   993                                               if len(allStimTrains):
   994                                                   # fill in blank period
   995                                                   stimMask = (stimRastersDF.drop(columns='t') > 0).any(axis='columns')
   996                                                   # blankingDur = 0.5e-3 + np.round(stAnnotations['totalPW'].max(), decimals=3) - 2 * currentSamplingRate.magnitude ** (-1)
   997                                                   # blankingDur = stAnnotations['totalPW'].max() + 5 * currentSamplingRate.magnitude ** (-1)
   998                                                   blankingDur = stAnnotations['totalPW'].max()
   999                                                   #  TODO: get fixed part from metadata and make robust to
  1000                                                   #  different blanks per stim config stAnnotations['secondPW']
  1001                                                   kernelT = np.arange(
  1002                                                       # -blankingDur,
  1003                                                       -blankingDur + currentSamplingRate.magnitude ** (-1),
  1004                                                       # blankingDur,
  1005                                                       blankingDur + currentSamplingRate.magnitude ** (-1),
  1006                                                       currentSamplingRate.magnitude ** (-1))
  1007                                                   kernel = np.zeros_like(kernelT)
  1008                                                   kernel[kernelT > 0] = 1
  1009                                                   blankMask = (
  1010                                                       np.convolve(kernel, stimMask, 'same') > 0)[:tdInterp.shape[0]]
  1011                                                   checkBlankMask = False
  1012                                                   if checkBlankMask:
  1013                                                       plotIdx = slice(2000000, 2020000)
  1014                                                       fig, ax = plt.subplots()
  1015                                                       twAx = ax.twinx()
  1016                                                       ax.plot(
  1017                                                           tdInterp['t'].iloc[plotIdx],
  1018                                                           tdInterp.iloc[plotIdx, 1], 'b.-', lw=2)
  1019                                                   spinalLfpChans = [
  1020                                                       cN
  1021                                                       for cN in tdInterp.columns
  1022                                                       if 'rostral' in cN or 'caudal' in cN]
  1023                                                   # tdInterp.loc[
  1024                                                   #     blankMask, spinalLfpChans] = np.nan
  1025                                                   # tdInterp.interpolate(axis=0, method='cubic', inplace=True)
  1026                                                   # tdInterp.loc[
  1027                                                   #     blankMask, spinalLfpChans] = 0
  1028                                                   if checkBlankMask:
  1029                                                       ax.plot(
  1030                                                           tdInterp['t'].iloc[plotIdx],
  1031                                                           tdInterp.iloc[plotIdx, 1].interpolate(axis=0, method='cubic'), 'g--', lw=2)
  1032                                                       twAx.plot(
  1033                                                           tdInterp['t'].iloc[plotIdx],
  1034                                                           blankMask[plotIdx], 'r')
  1035                                                       plt.show()
  1036                                               '''
  1037                                               #
  1038         1       7843.0   7843.0      0.0      tdInterp.columns = [i.replace('seg0_', '') for i in tdInterp.columns]
  1039         1    7499369.0 7499369.0      0.9      tdInterp.sort_index(axis='columns', inplace=True)
  1040         1        153.0    153.0      0.0      tdBlockInterp = ns5.dataFrameToAnalogSignals(
  1041         1        116.0    116.0      0.0          tdInterp,
  1042         1        123.0    123.0      0.0          idxT='t', useColNames=True, probeName='',
  1043         1    3247796.0 3247796.0      0.4          dataCol=tdInterp.drop(columns='t').columns,
  1044         1    6351029.0 6351029.0      0.7          samplingRate=samplingRate, verbose=arguments['verbose'])
  1045                                               #
  1046        71      31163.0    438.9      0.0      for aSig in tdBlockInterp.filter(objects=AnalogSignal):
  1047        70       8228.0    117.5      0.0          chName = aSig.channel_index.name
  1048        70    1561577.0  22308.2      0.2          chIdxList = spikesBlock.filter(objects=ChannelIndex, name=chName)
  1049        70       7994.0    114.2      0.0          if not len(chIdxList):
  1050        70       7780.0    111.1      0.0              lastIndex = len(spikesBlock.channel_indexes)
  1051        70       7922.0    113.2      0.0              if len(spikesBlock.channel_indexes[-1].channel_ids):
  1052        69       9061.0    131.3      0.0                  lastID = spikesBlock.channel_indexes[-1].channel_ids[0] + 1
  1053                                                       else:
  1054         1        113.0    113.0      0.0                  lastID = 1
  1055        70       7529.0    107.6      0.0              chIdx = ChannelIndex(
  1056        70       7674.0    109.6      0.0                  index=[lastIndex],
  1057        70       7556.0    107.9      0.0                  channel_names=[chName],
  1058        70       7502.0    107.2      0.0                  channel_ids=[lastID],
  1059        70       7483.0    106.9      0.0                  name=chName,
  1060        70      36865.0    526.6      0.0                  file_origin=spikesBlock.channel_indexes[-1].file_origin
  1061                                                           )
  1062        70      13699.0    195.7      0.0              chIdx.merge_annotations(spikesBlock.channel_indexes[-1])
  1063        70       7725.0    110.4      0.0              spikesBlock.channel_indexes.append(chIdx)
  1064                                                   else:
  1065                                                       chIdx = chIdxList[0]
  1066        70       7721.0    110.3      0.0          chIdx.analogsignals.append(aSig)
  1067        70       7556.0    107.9      0.0          aSig.channel_index = chIdx
  1068        70       7598.0    108.5      0.0          segName = aSig.segment.name
  1069        70    1573418.0  22477.4      0.2          segList = spikesBlock.filter(objects=Segment, name=segName)
  1070        70       8200.0    117.1      0.0          seg=segList[0]
  1071        70       8475.0    121.1      0.0          seg.analogsignals.append(aSig)
  1072        70       8091.0    115.6      0.0          aSig.segment = seg
  1073                                               #
  1074         1      55357.0  55357.0      0.0      spikesBlock = ns5.purgeNixAnn(spikesBlock)
  1075                                               #
  1076         1      24905.0  24905.0      0.0      spikesBlock.create_relationship()
  1077         1        120.0    120.0      0.0      outPathName = analysisDataPath.format(arguments['analysisName'])
  1078         1      72007.0  72007.0      0.0      if os.path.exists(outPathName):
  1079         1       4192.0   4192.0      0.0          os.remove(outPathName)
  1080         1     141688.0 141688.0      0.0      writer = neo.io.NixIO(filename=outPathName)
  1081         1  129468142.0 129468142.0     14.8      writer.write_block(spikesBlock, use_obj_names=True)
  1082         1    1430303.0 1430303.0      0.2      writer.close()
  1083         1        271.0    271.0      0.0      if arguments['commitResults']:
  1084                                                   analysisProcessedSubFolder = os.path.join(
  1085                                                       processedFolder, arguments['analysisName']
  1086                                                       )
  1087                                                   if not os.path.exists(analysisProcessedSubFolder):
  1088                                                       os.makedirs(analysisProcessedSubFolder, exist_ok=True)
  1089                                                   processedOutPath = os.path.join(
  1090                                                       analysisProcessedSubFolder, ns5FileName + '_analyze.nix')
  1091                                                   shutil.copyfile(outPathName, processedOutPath)
  1092                                                   outPathNameBin = outPathName.replace('_analyze.nix', '_binarized.nix')
  1093                                                   processedOutPathBin = os.path.join(
  1094                                                       analysisProcessedSubFolder, ns5FileName + '_binarized.nix')
  1095                                                   shutil.copyfile(outPathNameBin, processedOutPathBin)
  1096                                               # ns5.addBlockToNIX(
  1097                                               #     tdBlockInterp, neoSegIdx=[0],
  1098                                               #     writeSpikes=False, writeEvents=False,
  1099                                               #     purgeNixNames=False,
  1100                                               #     fileName=ns5FileName + '_analyze',
  1101                                               #     folderPath=analysisSubFolder,
  1102                                               #     nixBlockIdx=0, nixSegIdx=[0],
  1103                                               #     )
  1104         1        211.0    211.0      0.0      return

Total time: 0.320219 s
File: C\../../analysis-code/calcISIAnalysisNix.py
Function: parseAutoStimLog at line 86

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    86                                                   @profile
    87                                                   def parseAutoStimLog(jsonPath):
    88         1         18.0     18.0      0.0              try:
    89         1      86000.0  86000.0      2.7                  with open(jsonPath, 'r') as f:
    90         1      37221.0  37221.0      1.2                      stimLog = json.load(f)
    91                                                       except Exception:
    92                                                           with open(jsonPath, 'r') as f:
    93                                                               stimLogText = f.read()
    94                                                               stimLogText = mdt.fixMalformedJson(stimLogText, jsonType='Log')
    95                                                               stimLog = json.loads(stimLogText)
    96                                                       stimResLookup = {
    97         1        724.0    724.0      0.0                  1: 1 * pq.uA,
    98         1        190.0    190.0      0.0                  2: 2 * pq.uA,
    99         1        156.0    156.0      0.0                  3: 5 * pq.uA,
   100         1        152.0    152.0      0.0                  4: 10 * pq.uA,
   101         1        150.0    150.0      0.0                  5: 20 * pq.uA}
   102                                                       stimDict = {
   103         1         18.0     18.0      0.0                  't': [],
   104         1         18.0     18.0      0.0                  'elec': [],
   105                                                           # 'nominalWaveform': [],
   106         1         18.0     18.0      0.0                  'nominalCurrent': [],
   107         1         17.0     17.0      0.0                  'RateInHz': [],
   108         1         18.0     18.0      0.0                  'stimPeriod': [],
   109         1         17.0     17.0      0.0                  'trainDur': [],
   110         1         18.0     18.0      0.0                  'firstPW': [],
   111                                                           # 'interPhase': [],
   112         1         17.0     17.0      0.0                  'secondPW': [],
   113         1         17.0     17.0      0.0                  'totalPW': [],
   114         1         22.0     22.0      0.0                  'stimRes': []
   115                                                           }
   116         1         18.0     18.0      0.0              allNominalWaveforms = []
   117       641      13231.0     20.6      0.4              for idx, entry in enumerate(stimLog):
   118       640      13056.0     20.4      0.4                  t = entry['t']
   119       640      12737.0     19.9      0.4                  if idx == 0:
   120         1         18.0     18.0      0.0                      firstT = t
   121                                                           else:
   122       639      12854.0     20.1      0.4                      if t < firstT:
   123                                                                   continue
   124       640      13231.0     20.7      0.4                  if 'stimRes' in entry:
   125       640      13608.0     21.3      0.4                      ampQuanta = stimResLookup[entry['stimRes']]
   126                                                           else:
   127                                                               ampQuanta = 20 * pq.uA
   128                                                           # print('ampQuanta = {}'.format(ampQuanta))
   129       640      12812.0     20.0      0.4                  if 'stimCmd' in entry:
   130       640      13175.0     20.6      0.4                      allStimCmd = entry['stimCmd']
   131       640      13906.0     21.7      0.4                      if isinstance(allStimCmd, dict):
   132                                                                   # if only one electrode
   133       640      12746.0     19.9      0.4                          allStimCmd = [allStimCmd]
   134      1280      27691.0     21.6      0.9                      for stimCmd in allStimCmd:
   135                                                                   # each stimCmd represents one electrode
   136       640      18797.0     29.4      0.6                          nominalWaveform = []
   137       640      12883.0     20.1      0.4                          lastAmplitude = 0
   138       640      12562.0     19.6      0.4                          totalLen = 0
   139      1920      43734.0     22.8      1.4                          for seqIdx, phase in enumerate(stimCmd['seq']):
   140      1280      27167.0     21.2      0.8                              if phase['enable']:
   141                                                                           phAmp = (
   142                                                                               ampQuanta * phase['ampl'] *
   143      1280     393369.0    307.3     12.3                                      (-1) * ((-1) ** phase['pol'])
   144                                                                               )
   145                                                                           phaseWaveform = [
   146      1280      27911.0     21.8      0.9                                      phAmp
   147      1280     634826.0    496.0     19.8                                      for i in range(31 * phase['length'])]
   148                                                                       else:
   149                                                                           phaseWaveform = [
   150                                                                               0
   151                                                                               for i in range(31 * phase['length'])]
   152                                                                       phaseWaveform[:phase['delay']] = [
   153      1280      41745.0     32.6      1.3                                  lastAmplitude for i in range(phase['delay'])]
   154      1280      27224.0     21.3      0.9                              lastAmplitude = phaseWaveform[-1]
   155      1280      35973.0     28.1      1.1                              nominalWaveform += phaseWaveform
   156      1280      27354.0     21.4      0.9                              totalLen += phase['length']
   157      1280      26774.0     20.9      0.8                              if seqIdx == 0:
   158       640      13651.0     21.3      0.4                                  stimDict['firstPW'].append(
   159       640     112415.0    175.6      3.5                                      (phase['length'] / (3e4)) * pq.s)
   160      1280      26885.0     21.0      0.8                              if seqIdx == 1:
   161       640      13294.0     20.8      0.4                                  stimDict['secondPW'].append(
   162       640     109963.0    171.8      3.4                                      (phase['length'] / (3e4)) * pq.s)
   163       640      14352.0     22.4      0.4                          stimDict['t'].append(t)
   164       640      13889.0     21.7      0.4                          stimDict['stimRes'].append(ampQuanta)
   165       640      13045.0     20.4      0.4                          stimDict['totalPW'].append(
   166       640     104812.0    163.8      3.3                              (totalLen / (3e4)) * pq.s)
   167       640      13522.0     21.1      0.4                          stimDict['elec'].append(
   168       640     120480.0    188.2      3.8                              stimCmd['elec'] * pq.dimensionless)
   169       640      13156.0     20.6      0.4                          allNominalWaveforms.append(
   170       640     238322.0    372.4      7.4                              np.asarray(nominalWaveform))
   171       640      13434.0     21.0      0.4                          nominalIdxMax = np.argmax(
   172       640     290816.0    454.4      9.1                              np.abs(np.asarray(nominalWaveform)))
   173       640      13995.0     21.9      0.4                          stimDict['nominalCurrent'].append(
   174       640      13989.0     21.9      0.4                              nominalWaveform[nominalIdxMax])
   175       640     110803.0    173.1      3.5                          thisStimPeriod = (stimCmd['period'] / (3e4)) * pq.s
   176       640      14686.0     22.9      0.5                          stimDict['stimPeriod'].append(thisStimPeriod)
   177       640      13046.0     20.4      0.4                          stimDict['RateInHz'].append(
   178       640     195628.0    305.7      6.1                              thisStimPeriod ** (-1))
   179       640      13986.0     21.9      0.4                          stimDict['trainDur'].append(
   180       640      86786.0    135.6      2.7                              (stimCmd['repeats'] - 1) * thisStimPeriod)
   181                                                           else:
   182                                                               stimStr = entry['stimString']
   183                                                               stimStrDictRaw = {}
   184                                                               for stimSubStr in stimStr.split(';'):
   185                                                                   if len(stimSubStr):
   186                                                                       splitStr = stimSubStr.split('=')
   187                                                                       stimStrDictRaw[splitStr[0]] = splitStr[1]
   188                                                               stimStrDict = {}
   189                                                               for key, val in stimStrDictRaw.items():
   190                                                                   stimStrDict[key] = [
   191                                                                       float(st)
   192                                                                       for st in val.split(',')
   193                                                                       if len(st)]
   194                                                               stimStrDF = pd.DataFrame(stimStrDict)
   195                                                               stimStrDF['Elect'] = stimStrDF['Elect'].astype(np.int)
   196                                                               stimStrDF.loc[stimStrDF['PL'] == 1, 'Amp'] = (
   197                                                                   stimStrDF.loc[stimStrDF['PL'] == 1, 'Amp'] * (-1))
   198                                                               for rIdx, row in stimStrDF.iterrows():
   199                                                                   stimDict['t'].append(t)
   200                                                                   stimDict['firstPW'].append(
   201                                                                       row['Dur'] * 1e-3 * pq.s)
   202                                                                   stimDict['secondPW'].append(
   203                                                                       row['Dur'] * 1e-3 * pq.s)
   204                                                                   # stimDict['interPhase'].append(
   205                                                                   #     2 * ((3e4) ** -1) * pq.s)  # per page 16 of xippmex manual
   206                                                                   stimDict['totalPW'].append(
   207                                                                       2 * (row['Dur'] * 1e-3 + ((3e4) ** -1)) * pq.s)
   208                                                                   stimDict['nominalCurrent'].append(
   209                                                                       row['Amp'] * ampQuanta)
   210                                                                   stimDict['RateInHz'].append(row['Freq'] * pq.Hz)
   211                                                                   stimDict['stimPeriod'].append(row['Freq'] ** -1)
   212                                                                   stimDict['trainDur'].append(row['TL'] * 1e-3 * pq.s)
   213                                                                   stimDict['elec'].append(
   214                                                                       row['Elect'] * pq.dimensionless)
   215         1         19.0     19.0      0.0              stimDict['labels'] = np.asarray([
   216         1         20.0     20.0      0.0                  'stim update {}'.format(i)
   217         1       3557.0   3557.0      0.1                  for i in range(len(stimDict['elec']))])
   218                                                       # (np.asarray(stimDict['t'])/3e4 <= 1).any()
   219         1        691.0    691.0      0.0              rawStimEventTimes = np.asarray(stimDict.pop('t')) / (30000) * pq.s
   220                                                       # rawStimEventTimes = rawStimEventTimes - rawStimEventTimes[0] + activeTimes.min() * pq.s
   221                                                       # rawStimEventTimes = rawStimEventTimes.magnitude * rawStimEventTimes.units.simplified
   222         1         21.0     21.0      0.0              stimEvents = Event(
   223         1         19.0     19.0      0.0                  name='seg0_stimEvents',
   224         1         19.0     19.0      0.0                  times=rawStimEventTimes,
   225         1       2048.0   2048.0      0.1                  labels=stimDict.pop('labels'))
   226                                                       stimEvents.annotations['arrayAnnNames'] = [
   227         1         23.0     23.0      0.0                  k
   228         1         50.0     50.0      0.0                  for k in stimDict.keys()]
   229         1         21.0     21.0      0.0              stimEvents.annotations['nix_name'] = stimEvents.name
   230                                                       #
   231        10        191.0     19.1      0.0              for k in stimEvents.annotations['arrayAnnNames']:
   232         9       6141.0    682.3      0.2                  stimEvents.array_annotations[k] = stimDict[k]
   233         9        207.0     23.0      0.0                  stimEvents.annotations[k] = stimDict.pop(k)
   234         1         17.0     17.0      0.0              return stimEvents

Total time: 0.906662 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: analogSignalsToDataFrame at line 43

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    43                                           @profile
    44                                           def analogSignalsToDataFrame(
    45                                                   analogsignals, idxT='t', useChanNames=False):
    46         3         22.0      7.3      0.0      asigList = []
    47        71       1519.0     21.4      0.0      for asig in analogsignals:
    48        68       1365.0     20.1      0.0          if asig.shape[1] == 1:
    49        68        672.0      9.9      0.0              if useChanNames:
    50        11        426.0     38.7      0.0                  colNames = [str(asig.channel_index.name)]
    51                                                       else:
    52        57       1426.0     25.0      0.0                  colNames = [str(asig.name)]
    53                                                   else:
    54                                                       colNames = [
    55                                                           asig.name +
    56                                                           '_{}'.format(i) for i in
    57                                                           asig.channel_index.channel_ids
    58                                                           ]
    59        68        728.0     10.7      0.0          asigList.append(
    60        68        910.0     13.4      0.0              pd.DataFrame(
    61        68       3324.0     48.9      0.0                  asig.magnitude, columns=colNames,
    62        68     726451.0  10683.1      8.0                  index=range(asig.shape[0])))
    63         3         22.0      7.3      0.0      asigList.append(
    64         3         38.0     12.7      0.0          pd.DataFrame(
    65         3    5437841.0 1812613.7     60.0              asig.times.magnitude, columns=[idxT],
    66         3      26756.0   8918.7      0.3              index=range(asig.shape[0])))
    67         3    2865121.0 955040.3     31.6      return pd.concat(asigList, axis=1)

Total time: 0.0120799 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: listChanNames at line 69

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    69                                           @profile
    70                                           def listChanNames(
    71                                                   dataBlock, chanQuery,
    72                                                   objType=AnalogSignalProxy, condition=None):
    73                                               allChanList = [
    74         2         18.0      9.0      0.0          i.name
    75         2      23567.0  11783.5     19.5          for i in dataBlock.filter(objects=objType)]
    76         2         17.0      8.5      0.0      if condition == 'hasAsigs':
    77                                                   allChanList = [
    78                                                       i
    79                                                       for i in allChanList
    80                                                       if len(dataBlock.filter(objects=objType, name=i)[0].analogsignals)
    81                                                   ]
    82         2         31.0     15.5      0.0      chansToTrigger = pd.DataFrame(
    83         2       1078.0    539.0      0.9          np.unique(allChanList),
    84         2      16275.0   8137.5     13.5          columns=['chanName'])
    85         2         28.0     14.0      0.0      if chanQuery is not None:
    86         2         23.0     11.5      0.0          chansToTrigger = chansToTrigger.query(
    87         2      79738.0  39869.0     66.0              chanQuery, engine='python')['chanName'].to_list()
    88                                               else:
    89                                                   chansToTrigger = chansToTrigger['chanName'].to_list()
    90         2         24.0     12.0      0.0      return chansToTrigger

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: spikeDictToSpikeTrains at line 92

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    92                                           @profile
    93                                           def spikeDictToSpikeTrains(
    94                                                   spikes, block=None, seg=None,
    95                                                   probeName='insTD', t_stop=None,
    96                                                   waveformUnits=pq.uV,
    97                                                   sampling_rate=3e4 * pq.Hz):
    98                                           
    99                                               if block is None:
   100                                                   assert seg is None
   101                                                   block = Block()
   102                                                   seg = Segment(name=probeName + ' segment')
   103                                                   block.segments.append(seg)
   104                                           
   105                                               if t_stop is None:
   106                                                   t_stop = hf.getLastSpikeTime(spikes) + 1
   107                                           
   108                                               for idx, chanName in enumerate(spikes['ChannelID']):
   109                                                   #  unique units on this channel
   110                                                   unitsOnThisChan = pd.unique(spikes['Classification'][idx])
   111                                                   nixChanName = probeName + '{}'.format(chanName)
   112                                                   chanIdx = ChannelIndex(
   113                                                       name=nixChanName,
   114                                                       index=np.asarray([idx]),
   115                                                       channel_names=np.asarray([nixChanName]))
   116                                                   block.channel_indexes.append(chanIdx)
   117                                                   
   118                                                   for unitIdx, unitName in enumerate(unitsOnThisChan):
   119                                                       unitMask = spikes['Classification'][idx] == unitName
   120                                                       # this unit's spike timestamps
   121                                                       theseTimes = spikes['TimeStamps'][idx][unitMask]
   122                                                       # this unit's waveforms
   123                                                       if len(spikes['Waveforms'][idx].shape) == 3:
   124                                                           theseWaveforms = spikes['Waveforms'][idx][unitMask, :, :]
   125                                                           theseWaveforms = np.swapaxes(theseWaveforms, 1, 2)
   126                                                       elif len(spikes['Waveforms'][idx].shape) == 2:
   127                                                           theseWaveforms = (
   128                                                               spikes['Waveforms'][idx][unitMask, np.newaxis, :])
   129                                                       else:
   130                                                           raise(Exception('spikes[Waveforms] has bad shape'))
   131                                           
   132                                                       unitName = '{}#{}'.format(nixChanName, unitIdx)
   133                                                       unit = Unit(name=unitName)
   134                                                       unit.channel_index = chanIdx
   135                                                       chanIdx.units.append(unit)
   136                                           
   137                                                       train = SpikeTrain(
   138                                                           times=theseTimes, t_stop=t_stop, units='sec',
   139                                                           name=unitName, sampling_rate=sampling_rate,
   140                                                           waveforms=theseWaveforms*waveformUnits,
   141                                                           left_sweep=0, dtype=np.float32)
   142                                                       unit.spiketrains.append(train)
   143                                                       seg.spiketrains.append(train)
   144                                           
   145                                                       unit.create_relationship()
   146                                                   chanIdx.create_relationship()
   147                                               seg.create_relationship()
   148                                               block.create_relationship()
   149                                               return block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: spikeTrainsToSpikeDict at line 151

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   151                                           @profile
   152                                           def spikeTrainsToSpikeDict(
   153                                                   spiketrains):
   154                                               nCh = len(spiketrains)
   155                                               spikes = {
   156                                                   'ChannelID': [i for i in range(nCh)],
   157                                                   'Classification': [np.asarray([]) for i in range(nCh)],
   158                                                   'NEUEVWAV_HeaderIndices': [None for i in range(nCh)],
   159                                                   'TimeStamps': [np.asarray([]) for i in range(nCh)],
   160                                                   'Units': 'uV',
   161                                                   'Waveforms': [np.asarray([]) for i in range(nCh)],
   162                                                   'basic_headers': {'TimeStampResolution': 3e4},
   163                                                   'extended_headers': []
   164                                                   }
   165                                               for idx, st in enumerate(spiketrains):
   166                                                   spikes['ChannelID'][idx] = st.name
   167                                                   if len(spikes['TimeStamps'][idx]):
   168                                                       spikes['TimeStamps'][idx] = np.stack((
   169                                                           spikes['TimeStamps'][idx],
   170                                                           st.times.magnitude), axis=-1)
   171                                                   else:
   172                                                       spikes['TimeStamps'][idx] = st.times.magnitude
   173                                                   
   174                                                   theseWaveforms = np.swapaxes(
   175                                                       st.waveforms, 1, 2)
   176                                                   theseWaveforms = np.atleast_2d(np.squeeze(
   177                                                       theseWaveforms))
   178                                                       
   179                                                   if len(spikes['Waveforms'][idx]):
   180                                                       spikes['Waveforms'][idx] = np.stack((
   181                                                           spikes['Waveforms'][idx],
   182                                                           theseWaveforms.magnitude), axis=-1)
   183                                                   else:
   184                                                       spikes['Waveforms'][idx] = theseWaveforms.magnitude
   185                                                   
   186                                                   classVals = st.times.magnitude ** 0 * idx
   187                                                   if len(spikes['Classification'][idx]):
   188                                                       spikes['Classification'][idx] = np.stack((
   189                                                           spikes['Classification'][idx],
   190                                                           classVals), axis=-1)
   191                                                   else:
   192                                                       spikes['Classification'][idx] = classVals
   193                                               return spikes

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: channelIndexesToSpikeDict at line 195

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   195                                           @profile
   196                                           def channelIndexesToSpikeDict(
   197                                                   channel_indexes):
   198                                               nCh = len(channel_indexes)
   199                                               spikes = {
   200                                                   'ChannelID': [i for i in range(nCh)],
   201                                                   'Classification': [np.asarray([]) for i in range(nCh)],
   202                                                   'NEUEVWAV_HeaderIndices': [None for i in range(nCh)],
   203                                                   'TimeStamps': [np.asarray([]) for i in range(nCh)],
   204                                                   'Units': 'uV',
   205                                                   'Waveforms': [np.asarray([]) for i in range(nCh)],
   206                                                   'basic_headers': {'TimeStampResolution': 3e4},
   207                                                   'extended_headers': []
   208                                                   }
   209                                               #  allocate fields for annotations
   210                                               for dummyCh in channel_indexes:
   211                                                   if len(dummyCh.units):
   212                                                       dummyUnit = dummyCh.units[0]
   213                                                       if len(dummyUnit.spiketrains):
   214                                                           if len(dummyUnit.spiketrains[0].times):
   215                                                               break
   216                                               dummySt = [
   217                                                   st
   218                                                   for st in dummyUnit.spiketrains
   219                                                   if len(st.times)][0]
   220                                               #  allocate fields for array annotations (per spike)
   221                                               if dummySt.array_annotations:
   222                                                   for key in dummySt.array_annotations.keys():
   223                                                       spikes.update({key: [np.asarray([]) for i in range(nCh)]})
   224                                                   
   225                                               maxUnitIdx = 0
   226                                               for idx, chIdx in enumerate(channel_indexes):
   227                                                   spikes['ChannelID'][idx] = chIdx.name
   228                                                   for unitIdx, thisUnit in enumerate(chIdx.units):
   229                                                       for stIdx, st in enumerate(thisUnit.spiketrains):
   230                                                           if not len(st.times):
   231                                                               continue
   232                                                           #  print(
   233                                                           #      'unit {} has {} spiketrains'.format(
   234                                                           #          thisUnit.name,
   235                                                           #          len(thisUnit.spiketrains)))
   236                                                           if len(spikes['TimeStamps'][idx]):
   237                                                               spikes['TimeStamps'][idx] = np.concatenate((
   238                                                                   spikes['TimeStamps'][idx],
   239                                                                   st.times.magnitude), axis=0)
   240                                                           else:
   241                                                               spikes['TimeStamps'][idx] = st.times.magnitude
   242                                                           #  reshape waveforms to comply with BRM convention
   243                                                           theseWaveforms = np.swapaxes(
   244                                                               st.waveforms, 1, 2)
   245                                                           theseWaveforms = np.atleast_2d(np.squeeze(
   246                                                               theseWaveforms))
   247                                                           #  append waveforms
   248                                                           if len(spikes['Waveforms'][idx]):
   249                                                               try:
   250                                                                   spikes['Waveforms'][idx] = np.concatenate((
   251                                                                       spikes['Waveforms'][idx],
   252                                                                       theseWaveforms.magnitude), axis=0)
   253                                                               except Exception:
   254                                                                   traceback.print_exc()
   255                                                           else:
   256                                                               spikes['Waveforms'][idx] = theseWaveforms.magnitude
   257                                                           #  give each unit a global index
   258                                                           classVals = st.times.magnitude ** 0 * maxUnitIdx
   259                                                           st.array_annotations.update({'Classification': classVals})
   260                                                           #  expand array_annotations into spikes dict
   261                                                           for key, value in st.array_annotations.items():
   262                                                               if len(spikes[key][idx]):
   263                                                                   spikes[key][idx] = np.concatenate((
   264                                                                       spikes[key][idx],
   265                                                                       value), axis=0)
   266                                                               else:
   267                                                                   spikes[key][idx] = value
   268                                                           for key, value in st.annotations.items():
   269                                                               if key not in spikes['basic_headers']:
   270                                                                   spikes['basic_headers'].update({key: {}})
   271                                                               try:
   272                                                                   spikes['basic_headers'][key].update({maxUnitIdx: value})
   273                                                               except Exception:
   274                                                                   pass
   275                                                           maxUnitIdx += 1
   276                                               return spikes

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: unitSpikeTrainArrayAnnToDF at line 278

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   278                                           @profile
   279                                           def unitSpikeTrainArrayAnnToDF(
   280                                                   spikeTrainContainer):
   281                                               #  list contains different segments
   282                                               if isinstance(spikeTrainContainer, ChannelIndex):
   283                                                   assert len(spikeTrainContainer.units) == 0
   284                                                   spiketrains = spikeTrainContainer.units[0].spiketrains
   285                                               elif isinstance(spikeTrainContainer, Unit):
   286                                                   spiketrains = spikeTrainContainer.spiketrains
   287                                               elif isinstance(spikeTrainContainer, list):
   288                                                   spiketrains = spikeTrainContainer
   289                                               fullAnnotationsDict = {}
   290                                               for segIdx, st in enumerate(spiketrains):
   291                                                   theseAnnDF = pd.DataFrame(st.array_annotations)
   292                                                   theseAnnDF['t'] = st.times.magnitude
   293                                                   fullAnnotationsDict.update({segIdx: theseAnnDF})
   294                                               annotationsDF = pd.concat(
   295                                                   fullAnnotationsDict, names=['segment', 'index'], sort=True)
   296                                               return annotationsDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getSpikeDFMetadata at line 298

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   298                                           @profile
   299                                           def getSpikeDFMetadata(spikeDF, metaDataCols):
   300                                               spikeDF.reset_index(inplace=True)
   301                                               metaDataCols = np.atleast_1d(metaDataCols)
   302                                               spikeDF.index.name = 'metaDataIdx'
   303                                               metaDataDF = spikeDF.loc[:, metaDataCols].copy()
   304                                               newSpikeDF = spikeDF.drop(columns=metaDataCols).reset_index()
   305                                               return newSpikeDF, metaDataDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: transposeSpikeDF at line 307

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   307                                           @profile
   308                                           def transposeSpikeDF(
   309                                                   spikeDF, transposeToColumns,
   310                                                   fastTranspose=False):
   311                                               newColumnNames = np.atleast_1d(transposeToColumns).tolist()
   312                                               originalColumnNames = np.atleast_1d(spikeDF.columns.names)
   313                                               metaDataCols = np.setdiff1d(spikeDF.index.names, newColumnNames).tolist()
   314                                               if fastTranspose:
   315                                                   #  fast but memory inefficient
   316                                                   return spikeDF.stack().unstack(transposeToColumns)
   317                                               else:
   318                                                   raise(Warning('Caution! transposeSpikeDF might not be working, needs testing RD 06252019'))
   319                                                   #  stash annotations, transpose, recover annotations
   320                                                   newSpikeDF, metaDataDF = getSpikeDFMetadata(spikeDF, metaDataCols)
   321                                                   del spikeDF
   322                                                   gc.collect()
   323                                                   #
   324                                                   newSpikeDF = newSpikeDF.stack().unstack(newColumnNames)
   325                                                   newSpikeDF.reset_index(inplace=True)
   326                                                   #  set the index
   327                                                   newIdxLabels = np.concatenate(
   328                                                       [originalColumnNames, metaDataCols]).tolist()
   329                                                   newSpikeDF.loc[:, metaDataCols] = (
   330                                                       metaDataDF
   331                                                       .loc[newSpikeDF['metaDataIdx'].to_list(), metaDataCols]
   332                                                       .to_numpy())
   333                                                   newSpikeDF = (
   334                                                       newSpikeDF
   335                                                       .drop(columns=['metaDataIdx'])
   336                                                       .set_index(newIdxLabels))
   337                                                   return newSpikeDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: concatenateBlocks at line 339

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   339                                           @profile
   340                                           def concatenateBlocks(
   341                                                   asigBlocks, spikeBlocks, eventBlocks, chunkingMetadata,
   342                                                   samplingRate, chanQuery, lazy, trackMemory, verbose
   343                                                   ):
   344                                               # Scan ahead through all files and ensure that
   345                                               # spikeTrains and units are present across all assembled files
   346                                               channelIndexCache = {}
   347                                               unitCache = {}
   348                                               asigCache = []
   349                                               asigAnnCache = {}
   350                                               spiketrainCache = {}
   351                                               eventCache = {}
   352                                               # get list of channels and units
   353                                               for idx, (chunkIdxStr, chunkMeta) in enumerate(chunkingMetadata.items()):
   354                                                   gc.collect()
   355                                                   chunkIdx = int(chunkIdxStr)
   356                                                   asigBlock = asigBlocks[chunkIdx]
   357                                                   asigSeg = asigBlock.segments[0]
   358                                                   spikeBlock = spikeBlocks[chunkIdx]
   359                                                   eventBlock = eventBlocks[chunkIdx]
   360                                                   eventSeg = eventBlock.segments[0]
   361                                                   for chIdx in asigBlock.filter(objects=ChannelIndex):
   362                                                       chAlreadyThere = (chIdx.name in channelIndexCache.keys())
   363                                                       if not chAlreadyThere:
   364                                                           newChIdx = copy(chIdx)
   365                                                           newChIdx.analogsignals = []
   366                                                           newChIdx.units = []
   367                                                           channelIndexCache[chIdx.name] = newChIdx
   368                                                   for unit in (spikeBlock.filter(objects=Unit)):
   369                                                       if lazy:
   370                                                           theseSpiketrains = []
   371                                                           for stP in unit.spiketrains:
   372                                                               st = loadStProxy(stP)
   373                                                               if len(st.times) > 0:
   374                                                                   theseSpiketrains.append(st)
   375                                                       else:
   376                                                           theseSpiketrains = [
   377                                                               st
   378                                                               for st in unit.spiketrains
   379                                                               if len(st.times)
   380                                                               ]
   381                                                       for st in theseSpiketrains:
   382                                                           st = loadObjArrayAnn(st)
   383                                                           if len(st.times):
   384                                                               st.magnitude[:] = st.times.magnitude + spikeBlock.annotations['chunkTStart']
   385                                                               st.t_start = min(0 * pq.s, st.times[0] * 0.999)
   386                                                               st.t_stop = max(
   387                                                                   st.t_stop + spikeBlock.annotations['chunkTStart'] * pq.s,
   388                                                                   st.times[-1] * 1.001)
   389                                                           else:
   390                                                               st.t_start += spikeBlock.annotations['chunkTStart'] * pq.s
   391                                                               st.t_stop += spikeBlock.annotations['chunkTStart'] * pq.s
   392                                                       uAlreadyThere = (unit.name in unitCache.keys())
   393                                                       if not uAlreadyThere:
   394                                                           newUnit = copy(unit)
   395                                                           newUnit.spiketrains = []
   396                                                           newUnit.annotations['parentChanName'] = unit.channel_index.name
   397                                                           unitCache[unit.name] = newUnit
   398                                                           spiketrainCache[unit.name] = theseSpiketrains
   399                                                       else:
   400                                                           spiketrainCache[unit.name] = spiketrainCache[unit.name] + theseSpiketrains
   401                                                   #
   402                                                   if lazy:
   403                                                       evList = [
   404                                                           evP.load()
   405                                                           for evP in eventSeg.events]
   406                                                   else:
   407                                                       evList = eventSeg.events
   408                                                   for event in evList:
   409                                                       event.magnitude[:] = event.magnitude + eventBlock.annotations['chunkTStart']
   410                                                       if event.name in eventCache.keys():
   411                                                           eventCache[event.name].append(event)
   412                                                       else:
   413                                                           eventCache[event.name] = [event]
   414                                                   # take the requested analog signal channels
   415                                                   if lazy:
   416                                                       tdChanNames = listChanNames(
   417                                                           asigBlock, chanQuery, objType=AnalogSignalProxy)
   418                                                       #############
   419                                                       # tdChanNames = ['seg0_utah1', 'seg0_utah10']
   420                                                       ##############
   421                                                       asigList = []
   422                                                       for asigP in asigSeg.analogsignals:
   423                                                           if asigP.name in tdChanNames:
   424                                                               asig = asigP.load()
   425                                                               asig.channel_index = asigP.channel_index
   426                                                               asigList.append(asig)
   427                                                               if trackMemory:
   428                                                                   print('loading {} from proxy object. memory usage: {:.1f} MB'.format(
   429                                                                       asigP.name, prf.memory_usage_psutil()))
   430                                                   else:
   431                                                       tdChanNames = listChanNames(
   432                                                           asigBlock, chanQuery, objType=AnalogSignal)
   433                                                       asigList = [
   434                                                           asig
   435                                                           for asig in asigSeg.analogsignals
   436                                                           if asig.name in tdChanNames
   437                                                           ]
   438                                                   for asig in asigList:
   439                                                       if asig.size > 0:
   440                                                           dummyAsig = asig
   441                                                   if idx == 0:
   442                                                       outputBlock = Block(
   443                                                           name=asigBlock.name,
   444                                                           file_origin=asigBlock.file_origin,
   445                                                           file_datetime=asigBlock.file_datetime,
   446                                                           rec_datetime=asigBlock.rec_datetime,
   447                                                           **asigBlock.annotations
   448                                                       )
   449                                                       newSeg = Segment(
   450                                                           index=0, name=asigSeg.name,
   451                                                           description=asigSeg.description,
   452                                                           file_origin=asigSeg.file_origin,
   453                                                           file_datetime=asigSeg.file_datetime,
   454                                                           rec_datetime=asigSeg.rec_datetime,
   455                                                           **asigSeg.annotations
   456                                                       )
   457                                                       outputBlock.segments = [newSeg]
   458                                                       for asig in asigList:
   459                                                           asigAnnCache[asig.name] = asig.annotations
   460                                                           asigAnnCache[asig.name]['parentChanName'] = asig.channel_index.name
   461                                                       asigUnits = dummyAsig.units
   462                                                   tdDF = analogSignalsToDataFrame(asigList)
   463                                                   del asigList  # asigs saved to dataframe, no longer needed
   464                                                   tdDF.loc[:, 't'] += asigBlock.annotations['chunkTStart']
   465                                                   tdDF.set_index('t', inplace=True)
   466                                                   if samplingRate != dummyAsig.sampling_rate:
   467                                                       lowPassOpts = {
   468                                                           'low': {
   469                                                               'Wn': float(samplingRate / 2),
   470                                                               'N': 4,
   471                                                               'btype': 'low',
   472                                                               'ftype': 'bessel'
   473                                                           }
   474                                                       }
   475                                                       newT = pd.Series(
   476                                                           np.arange(
   477                                                               dummyAsig.t_start + asigBlock.annotations['chunkTStart'] * pq.s,
   478                                                               dummyAsig.t_stop + asigBlock.annotations['chunkTStart'] * pq.s,
   479                                                               1/samplingRate))
   480                                                       if samplingRate < dummyAsig.sampling_rate:
   481                                                           filterCoeffs = hf.makeFilterCoeffsSOS(
   482                                                               lowPassOpts, float(dummyAsig.sampling_rate))
   483                                                           if trackMemory:
   484                                                               print('Filtering analog data before downsampling. memory usage: {:.1f} MB'.format(
   485                                                                   prf.memory_usage_psutil()))
   486                                                           '''
   487                                                           ### check that axis=0 is the correct option
   488                                                           dummyDF = tdDF.iloc[:, :4].copy()
   489                                                           filteredAsigs0 = signal.sosfiltfilt( filterCoeffs, dummyDF.to_numpy(), axis=0)
   490                                                           filteredAsigs1 = signal.sosfiltfilt( filterCoeffs, dummyDF.to_numpy(), axis=1)
   491                                                           ###
   492                                                           '''
   493                                                           filteredAsigs = signal.sosfiltfilt(
   494                                                               filterCoeffs, tdDF.to_numpy(),
   495                                                               axis=0)
   496                                                           tdDF = pd.DataFrame(
   497                                                               filteredAsigs,
   498                                                               index=tdDF.index,
   499                                                               columns=tdDF.columns)
   500                                                           if trackMemory:
   501                                                               print('Just finished analog data filtering before downsampling. memory usage: {:.1f} MB'.format(
   502                                                                   prf.memory_usage_psutil()))
   503                                                       tdInterp = hf.interpolateDF(
   504                                                           tdDF, newT,
   505                                                           kind='linear', fill_value='extrapolate',
   506                                                           verbose=verbose)
   507                                                       # free up memory used by full resolution asigs
   508                                                       del tdDF
   509                                                   else:
   510                                                       tdInterp = tdDF
   511                                                   #
   512                                                   asigCache.append(tdInterp)
   513                                                   #
   514                                                   print('Finished chunk {}'.format(chunkIdxStr))
   515                                               allTdDF = pd.concat(asigCache)
   516                                               # TODO: check for nans, if, for example a signal is partially missing
   517                                               allTdDF.fillna(method='bfill', inplace=True)
   518                                               allTdDF.fillna(method='ffill', inplace=True)
   519                                               for asigName in allTdDF.columns:
   520                                                   newAsig = AnalogSignal(
   521                                                       allTdDF[asigName].to_numpy() * asigUnits,
   522                                                       name=asigName,
   523                                                       sampling_rate=samplingRate,
   524                                                       dtype=np.float32,
   525                                                       **asigAnnCache[asigName])
   526                                                   chIdxName = asigAnnCache[asigName]['parentChanName']
   527                                                   chIdx = channelIndexCache[chIdxName]
   528                                                   # cross-assign ownership to containers
   529                                                   chIdx.analogsignals.append(newAsig)
   530                                                   newSeg.analogsignals.append(newAsig)
   531                                                   newAsig.channel_index = chIdx
   532                                                   newAsig.segment = newSeg
   533                                               #
   534                                               for uName, unit in unitCache.items():
   535                                                   # concatenate spike times, waveforms, etc.
   536                                                   if len(spiketrainCache[unit.name]):
   537                                                       consolidatedTimes = np.concatenate([
   538                                                               st.times.magnitude
   539                                                               for st in spiketrainCache[unit.name]
   540                                                           ])
   541                                                       # TODO:   decide whether to include this step
   542                                                       #         which snaps the spike times to the nearest
   543                                                       #         *sampled* data point
   544                                                       #
   545                                                       # consolidatedTimes, timesIndex = hf.closestSeries(
   546                                                       #     takeFrom=pd.Series(consolidatedTimes),
   547                                                       #     compareTo=pd.Series(allTdDF.index))
   548                                                       #
   549                                                       # find an example spiketrain with array_annotations
   550                                                       for st in spiketrainCache[unit.name]:
   551                                                           if len(st.times):
   552                                                               dummySt = st
   553                                                               break
   554                                                       consolidatedAnn = {
   555                                                           key: np.array([])
   556                                                           for key, value in dummySt.array_annotations.items()
   557                                                           }
   558                                                       for key, value in consolidatedAnn.items():
   559                                                           consolidatedAnn[key] = np.concatenate([
   560                                                               st.annotations[key]
   561                                                               for st in spiketrainCache[unit.name]
   562                                                           ])
   563                                                       consolidatedWaveforms = np.concatenate([
   564                                                           st.waveforms
   565                                                           for st in spiketrainCache[unit.name]
   566                                                           ])
   567                                                       spikeTStop = max([
   568                                                           st.t_stop
   569                                                           for st in spiketrainCache[unit.name]
   570                                                           ])
   571                                                       spikeTStart = max([
   572                                                           st.t_start
   573                                                           for st in spiketrainCache[unit.name]
   574                                                           ])
   575                                                       spikeAnnotations = {
   576                                                           key: value
   577                                                           for key, value in dummySt.annotations.items()
   578                                                           if key not in dummySt.annotations['arrayAnnNames']
   579                                                       }
   580                                                       newSt = SpikeTrain(
   581                                                           name=dummySt.name,
   582                                                           times=consolidatedTimes, units='sec', t_stop=spikeTStop,
   583                                                           waveforms=consolidatedWaveforms * dummySt.waveforms.units,
   584                                                           left_sweep=dummySt.left_sweep,
   585                                                           sampling_rate=dummySt.sampling_rate,
   586                                                           t_start=spikeTStart, **spikeAnnotations,
   587                                                           array_annotations=consolidatedAnn)
   588                                                       # cross-assign ownership to containers
   589                                                       unit.spiketrains.append(newSt)
   590                                                       newSt.unit = unit
   591                                                       newSeg.spiketrains.append(newSt)
   592                                                       newSt.segment = newSeg
   593                                                       # link chIdxes and Units
   594                                                       if unit.annotations['parentChanName'] in channelIndexCache:
   595                                                           chIdx = channelIndexCache[unit.annotations['parentChanName']]
   596                                                           if unit not in chIdx.units:
   597                                                               chIdx.units.append(unit)
   598                                                               unit.channel_index = chIdx
   599                                                       else:
   600                                                           newChIdx = ChannelIndex(
   601                                                               name=unit.annotations['parentChanName'], index=0)
   602                                                           channelIndexCache[unit.annotations['parentChanName']] = newChIdx
   603                                                           if unit not in newChIdx.units:
   604                                                               newChIdx.units.append(unit)
   605                                                               unit.channel_index = newChIdx
   606                                               #
   607                                               for evName, eventList in eventCache.items():
   608                                                   consolidatedTimes = np.concatenate([
   609                                                       ev.times.magnitude
   610                                                       for ev in eventList
   611                                                       ])
   612                                                   consolidatedLabels = np.concatenate([
   613                                                       ev.labels
   614                                                       for ev in eventList
   615                                                       ])
   616                                                   newEvent = Event(
   617                                                       name=evName,
   618                                                       times=consolidatedTimes * pq.s,
   619                                                       labels=consolidatedLabels
   620                                                       )
   621                                                   # if len(newEvent):
   622                                                   newEvent.segment = newSeg
   623                                                   newSeg.events.append(newEvent)
   624                                               for chIdxName, chIdx in channelIndexCache.items():
   625                                                   if len(chIdx.analogsignals) or len(chIdx.units):
   626                                                       outputBlock.channel_indexes.append(chIdx)
   627                                                       chIdx.block = outputBlock
   628                                               #
   629                                               outputBlock = purgeNixAnn(outputBlock)
   630                                               createRelationship = False
   631                                               if createRelationship:
   632                                                   outputBlock.create_relationship()
   633                                               return outputBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: concatenateEventsContainer at line 660

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   660                                           @profile
   661                                           def concatenateEventsContainer(eventContainer, linkParents=True):
   662                                               if isinstance(eventContainer, dict):
   663                                                   listOfEvents = list(eventContainer.values())
   664                                               else:
   665                                                   listOfEvents = eventContainer
   666                                               nonEmptyEvents = [ev for ev in listOfEvents if len(ev.times)]
   667                                               if not len(nonEmptyEvents) > 0:
   668                                                   return listOfEvents[0]
   669                                               masterEvent = listOfEvents[0]
   670                                               for evIdx, ev in enumerate(listOfEvents[1:]):
   671                                                   try:
   672                                                       masterEvent = masterEvent.merge(ev)
   673                                                   except Exception:
   674                                                       traceback.print_exc()
   675                                                       pdb.set_trace()
   676                                               if masterEvent.array_annotations is not None:
   677                                                   arrayAnnNames = list(masterEvent.array_annotations.keys())
   678                                                   masterEvent.annotations.update(masterEvent.array_annotations)
   679                                                   masterEvent.annotations['arrayAnnNames'] = arrayAnnNames
   680                                               if linkParents:
   681                                                   masterEvent.segment = listOfEvents[0].segment
   682                                                   if isinstance(masterEvent, SpikeTrain):
   683                                                       masterEvent.unit = listOfEvents[0].unit
   684                                               return masterEvent

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: unitSpikeTrainWaveformsToDF at line 743

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   743                                           @profile
   744                                           def unitSpikeTrainWaveformsToDF(
   745                                                   spikeTrainContainer,
   746                                                   dataQuery=None,
   747                                                   transposeToColumns='bin', fastTranspose=True,
   748                                                   lags=None, decimate=1, rollingWindow=None,
   749                                                   getMetaData=True, verbose=False,
   750                                                   whichSegments=None, windowSize=None, procFun=None):
   751                                               #  list contains different segments from *one* unit
   752                                               if isinstance(spikeTrainContainer, ChannelIndex):
   753                                                   assert len(spikeTrainContainer.units) == 0
   754                                                   spiketrains = spikeTrainContainer.units[0].spiketrains
   755                                               elif isinstance(spikeTrainContainer, Unit):
   756                                                   spiketrains = spikeTrainContainer.spiketrains
   757                                               else:
   758                                                   raise(Exception('not a valid container'))
   759                                               # TODO check if really need to assert uniqueness?
   760                                               uniqueSpiketrains = []
   761                                               for st in spiketrains:
   762                                                   if not np.any([st is i for i in uniqueSpiketrains]):
   763                                                       uniqueSpiketrains.append(st)
   764                                               #  subsampling options
   765                                               decimate = int(decimate)
   766                                               if whichSegments is not None:
   767                                                   uniqueSpiketrains = [
   768                                                       uniqueSpiketrains[i]
   769                                                       for i in whichSegments
   770                                                   ]
   771                                               #
   772                                               waveformsList = []
   773                                               #
   774                                               for segIdx, stIn in enumerate(uniqueSpiketrains):
   775                                                   if verbose:
   776                                                       print('extracting spiketrain from {}'.format(stIn.segment))
   777                                                   #  make sure is not a proxyObj
   778                                                   if isinstance(stIn, SpikeTrainProxy):
   779                                                       st = loadStProxy(stIn)
   780                                                       if (getMetaData) or (dataQuery is not None):
   781                                                           # if there's a query, get metadata temporarily to resolve it
   782                                                           st = loadObjArrayAnn(st)
   783                                                   else:
   784                                                       st = stIn
   785                                                   #  extract bins spaced by decimate argument
   786                                                   if not st.times.any():
   787                                                       continue
   788                                                   if verbose:
   789                                                       print('extracting wf from {}'.format(stIn.segment))
   790                                                   wf = np.asarray(
   791                                                       np.squeeze(st.waveforms),
   792                                                       dtype='float32')
   793                                                   if wf.ndim == 3:
   794                                                       print('Waveforms from more than one channel!')
   795                                                       if wf.shape[1] > 0:
   796                                                           wf = wf[:, 0, :]
   797                                                   wfDF = pd.DataFrame(wf)
   798                                                   samplingRate = st.sampling_rate
   799                                                   bins = (
   800                                                       np.asarray(wfDF.columns) / samplingRate -
   801                                                       st.left_sweep)
   802                                                   wfDF.columns = np.around(bins.magnitude, decimals=6)
   803                                                   if windowSize is not None:
   804                                                       winMask = (
   805                                                           (wfDF.columns >= windowSize[0]) &
   806                                                           (wfDF.columns <= windowSize[1]))
   807                                                       wfDF = wfDF.loc[:, winMask]
   808                                                   if procFun is not None:
   809                                                       wfDF = procFun(wfDF, st)
   810                                                   idxLabels = ['segment', 'originalIndex', 't']
   811                                                   wfDF.loc[:, 't'] = np.asarray(st.times.magnitude)
   812                                                   if (getMetaData) or (dataQuery is not None):
   813                                                       # if there's a query, get metadata temporarily to resolve it
   814                                                       annDict = {}
   815                                                       for k, values in st.array_annotations.items():
   816                                                           if isinstance(getMetaData, Iterable):
   817                                                               # if selecting metadata fields, check that
   818                                                               # the key is in the provided list
   819                                                               if k not in getMetaData:
   820                                                                   continue
   821                                                           if isinstance(values[0], str):
   822                                                               v = np.asarray(values, dtype='str')
   823                                                           else:
   824                                                               v = np.asarray(values)
   825                                                           annDict.update({k: v})
   826                                                       skipAnnNames = (
   827                                                           st.annotations['arrayAnnNames'] +
   828                                                           [
   829                                                               'arrayAnnNames', 'arrayAnnDTypes',
   830                                                               'nix_name', 'neo_name', 'id',
   831                                                               'cell_label', 'cluster_label', 'max_on_channel', 'binWidth']
   832                                                           )
   833                                                       annDF = pd.DataFrame(annDict)
   834                                                       for k, value in st.annotations.items():
   835                                                           if isinstance(getMetaData, Iterable):
   836                                                               # if selecting metadata fields, check that
   837                                                               # the key is in the provided list
   838                                                               if k not in getMetaData:
   839                                                                   continue
   840                                                           if k not in skipAnnNames:
   841                                                               annDF.loc[:, k] = value
   842                                                       #
   843                                                       if isinstance(getMetaData, Iterable):
   844                                                           doNotFillList = idxLabels + ['feature', 'bin']
   845                                                           fieldsNeedFiller = [
   846                                                               mdn
   847                                                               for mdn in getMetaData
   848                                                               if (mdn not in doNotFillList) and (mdn not in annDF.columns)]
   849                                                           for mdName in fieldsNeedFiller:
   850                                                               annDF.loc[:, mdName] = 'NA'
   851                                                       annColumns = annDF.columns.to_list()
   852                                                       if getMetaData:
   853                                                           for annNm in annColumns:
   854                                                               if annNm not in idxLabels:
   855                                                                   idxLabels.append(annNm)
   856                                                           # idxLabels += annColumns
   857                                                       spikeDF = annDF.join(wfDF)
   858                                                   else:
   859                                                       spikeDF = wfDF
   860                                                       del wfDF, st
   861                                                   spikeDF.loc[:, 'segment'] = segIdx
   862                                                   spikeDF.loc[:, 'originalIndex'] = spikeDF.index
   863                                                   spikeDF.columns.name = 'bin'
   864                                                   #
   865                                                   if dataQuery is not None:
   866                                                       spikeDF.query(dataQuery, inplace=True)
   867                                                       if not getMetaData:
   868                                                           spikeDF.drop(columns=annColumns, inplace=True)
   869                                                   waveformsList.append(spikeDF)
   870                                               #
   871                                               zeroLagWaveformsDF = pd.concat(waveformsList, axis='index')
   872                                               if verbose:
   873                                                   prf.print_memory_usage('before transposing waveforms')
   874                                               # TODO implement lags and rolling window addition here
   875                                               metaDF = zeroLagWaveformsDF.loc[:, idxLabels].copy()
   876                                               zeroLagWaveformsDF.drop(columns=idxLabels, inplace=True)
   877                                               if lags is None:
   878                                                   lags = [0]
   879                                               laggedWaveformsDict = {
   880                                                   (spikeTrainContainer.name, k): None for k in lags}
   881                                               for lag in lags:
   882                                                   if isinstance(lag, int):
   883                                                       shiftedWaveform = zeroLagWaveformsDF.shift(
   884                                                           lag, axis='columns')
   885                                                       if rollingWindow is not None:
   886                                                           halfRollingWin = int(np.ceil(rollingWindow/2))
   887                                                           seekIdx = slice(
   888                                                               halfRollingWin, -halfRollingWin+1, decimate)
   889                                                           # seekIdx = slice(None, None, decimate)
   890                                                           #shiftedWaveform = (
   891                                                           #    shiftedWaveform
   892                                                           #    .rolling(
   893                                                           #        window=rollingWindow, win_type='gaussian',
   894                                                           #        axis='columns', center=True)
   895                                                           #    .mean(std=halfRollingWin))
   896                                                           shiftedWaveform = (
   897                                                               shiftedWaveform
   898                                                               .rolling(
   899                                                                   window=rollingWindow, 
   900                                                                   axis='columns', center=True)
   901                                                               .mean())
   902                                                       else:
   903                                                           halfRollingWin = 0
   904                                                           seekIdx = slice(None, None, decimate)
   905                                                           if False:
   906                                                               oldShiftedWaveform = zeroLagWaveformsDF.shift(
   907                                                                   lag, axis='columns')
   908                                                               plt.plot(oldShiftedWaveform.iloc[0, :])
   909                                                               plt.plot(shiftedWaveform.iloc[0, :])
   910                                                               plt.show()
   911                                                       laggedWaveformsDict[
   912                                                           (spikeTrainContainer.name, lag)] = (
   913                                                               shiftedWaveform.iloc[:, seekIdx].copy())
   914                                                   if isinstance(lag, tuple):
   915                                                       halfRollingWin = int(np.ceil(lag[1]/2))
   916                                                       seekIdx = slice(
   917                                                           halfRollingWin, -halfRollingWin+1, decimate)
   918                                                       # seekIdx = slice(None, None, decimate)
   919                                                       shiftedWaveform = (
   920                                                           zeroLagWaveformsDF
   921                                                           .shift(lag[0], axis='columns')
   922                                                           .rolling(
   923                                                               window=lag[1], win_type='gaussian',
   924                                                               axis='columns', center=True)
   925                                                           .mean(std=halfRollingWin))
   926                                                       laggedWaveformsDict[
   927                                                           (spikeTrainContainer.name, lag)] = (
   928                                                               shiftedWaveform.iloc[:, seekIdx].copy())
   929                                               #
   930                                               if transposeToColumns == 'feature':
   931                                                   # stack the bin, name the feature column
   932                                                   # 
   933                                                   for idx, (key, value) in enumerate(laggedWaveformsDict.items()):
   934                                                       if idx == 0:
   935                                                           stackedIndexDF = pd.concat(
   936                                                               [metaDF, value], axis='columns')
   937                                                           stackedIndexDF.set_index(idxLabels, inplace=True)
   938                                                           # don't drop nans for now - might need to keep track of them
   939                                                           # if we need to equalize to another array later
   940                                                           newIndex = stackedIndexDF.stack(dropna=False).index
   941                                                           idxLabels.append('bin')
   942                                                       laggedWaveformsDict[key] = value.stack(dropna=False).to_frame(name=key).reset_index(drop=True)
   943                                                   waveformsDF = pd.concat(
   944                                                       laggedWaveformsDict.values(),
   945                                                       axis='columns')
   946                                                   waveformsDF.columns.names = ['feature', 'lag']
   947                                                   waveformsDF.index = newIndex
   948                                                   waveformsDF.columns.name = 'feature'
   949                                               elif transposeToColumns == 'bin':
   950                                                   # add the feature column
   951                                                   waveformsDF = pd.concat(
   952                                                       laggedWaveformsDict,
   953                                                       names=['feature', 'lag', 'originalDummy']).reset_index()
   954                                                   waveformsDF = pd.concat(
   955                                                       [
   956                                                           metaDF.reset_index(drop=True),
   957                                                           waveformsDF.drop(columns='originalDummy')],
   958                                                       axis='columns')
   959                                                   idxLabels += ['feature', 'lag']
   960                                                   waveformsDF.columns.name = 'bin'
   961                                                   waveformsDF.set_index(idxLabels, inplace=True)
   962                                               #
   963                                               if transposeToColumns != waveformsDF.columns.name:
   964                                                   waveformsDF = transposeSpikeDF(
   965                                                       waveformsDF, transposeToColumns,
   966                                                       fastTranspose=fastTranspose)
   967                                               return waveformsDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: concatenateUnitSpikeTrainWaveformsDF at line 969

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   969                                           @profile
   970                                           def concatenateUnitSpikeTrainWaveformsDF(
   971                                                   units, dataQuery=None,
   972                                                   transposeToColumns='bin', concatOn='index',
   973                                                   fastTranspose=True, getMetaData=True, verbose=False,
   974                                                   addLags=None, decimate=1, rollingWindow=None,
   975                                                   metaDataToCategories=False, windowSize=None,
   976                                                   whichSegments=None, procFun=None):
   977                                               allUnits = []
   978                                               for thisUnit in units:
   979                                                   hasAnySpikes = []
   980                                                   for stIn in thisUnit.spiketrains:
   981                                                       if isinstance(stIn, SpikeTrainProxy):
   982                                                           st = stIn.load(
   983                                                               magnitude_mode='rescaled',
   984                                                               load_waveforms=False)
   985                                                       else:
   986                                                           st = stIn
   987                                                       hasAnySpikes.append(st.times.any())
   988                                                   if np.any(hasAnySpikes):
   989                                                       allUnits.append(thisUnit)
   990                                               waveformsList = []
   991                                               for idx, thisUnit in enumerate(allUnits):
   992                                                   if verbose:
   993                                                       print('concatenating unitDF {}'.format(thisUnit.name))
   994                                                   lags = None
   995                                                   if addLags is not None:
   996                                                       if thisUnit.name in addLags:
   997                                                           lags = addLags[thisUnit.name]
   998                                                   unitWaveforms = unitSpikeTrainWaveformsToDF(
   999                                                       thisUnit, dataQuery=dataQuery,
  1000                                                       transposeToColumns=transposeToColumns,
  1001                                                       fastTranspose=fastTranspose, getMetaData=getMetaData,
  1002                                                       lags=lags, decimate=decimate, rollingWindow=rollingWindow,
  1003                                                       verbose=verbose, windowSize=windowSize,
  1004                                                       whichSegments=whichSegments, procFun=procFun)
  1005                                                   if idx == 0:
  1006                                                       idxLabels = unitWaveforms.index.names
  1007                                                   if (concatOn == 'columns') and (idx > 0):
  1008                                                       # other than first time, we already have the metadata
  1009                                                       unitWaveforms.reset_index(drop=True, inplace=True)
  1010                                                   else:
  1011                                                       # first time, or if concatenating indices,
  1012                                                       # keep the the metadata
  1013                                                       unitWaveforms.reset_index(inplace=True)
  1014                                                       if metaDataToCategories:
  1015                                                           # convert metadata to categoricals to free memory
  1016                                                           #
  1017                                                           unitWaveforms[idxLabels] = (
  1018                                                               unitWaveforms[idxLabels]
  1019                                                               .astype('category')
  1020                                                               )
  1021                                                   waveformsList.append(unitWaveforms)
  1022                                                   del unitWaveforms
  1023                                                   if verbose:
  1024                                                       print('memory usage: {:.1f} MB'.format(prf.memory_usage_psutil()))
  1025                                               if verbose:
  1026                                                   print(
  1027                                                       'about to join all, memory usage: {:.1f} MB'
  1028                                                       .format(prf.memory_usage_psutil()))
  1029                                               #  if concatenating indexes, reset the index of the result
  1030                                               #  ignoreIndex = (concatOn == 'index')
  1031                                               allWaveforms = pd.concat(
  1032                                                   waveformsList, axis=concatOn,
  1033                                                   # ignore_index=ignoreIndex
  1034                                                   )
  1035                                               del waveformsList
  1036                                               if verbose:
  1037                                                   print(
  1038                                                       'finished concatenating, memory usage: {:.1f} MB'
  1039                                                       .format(prf.memory_usage_psutil()))
  1040                                               try:
  1041                                                   allWaveforms.set_index(idxLabels, inplace=True)
  1042                                                   allWaveforms.sort_index(
  1043                                                       level=['segment', 'originalIndex', 't'],
  1044                                                       axis='index', inplace=True, kind='mergesort')
  1045                                                   allWaveforms.sort_index(
  1046                                                       axis='columns', inplace=True, kind='mergesort')
  1047                                               except Exception:
  1048                                                   pdb.set_trace()
  1049                                               return allWaveforms

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: alignedAsigsToDF at line 1051

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1051                                           @profile
  1052                                           def alignedAsigsToDF(
  1053                                                   dataBlock, unitNames=None,
  1054                                                   unitQuery=None, dataQuery=None,
  1055                                                   collapseSizes=False, verbose=False,
  1056                                                   duplicateControlsByProgram=False,
  1057                                                   amplitudeColumn='amplitude',
  1058                                                   programColumn='program',
  1059                                                   electrodeColumn='electrode',
  1060                                                   transposeToColumns='bin', concatOn='index', fastTranspose=True,
  1061                                                   addLags=None, decimate=1, rollingWindow=None,
  1062                                                   whichSegments=None, windowSize=None,
  1063                                                   getMetaData=True, metaDataToCategories=True,
  1064                                                   outlierTrials=None, invertOutlierMask=False,
  1065                                                   makeControlProgram=False, removeFuzzyName=False, procFun=None):
  1066                                               #  channels to trigger
  1067                                               if unitNames is None:
  1068                                                   unitNames = listChanNames(dataBlock, unitQuery, objType=Unit)
  1069                                               allUnits = []
  1070                                               for uName in unitNames:
  1071                                                   allUnits += dataBlock.filter(objects=Unit, name=uName)
  1072                                               allWaveforms = concatenateUnitSpikeTrainWaveformsDF(
  1073                                                   allUnits, dataQuery=dataQuery,
  1074                                                   transposeToColumns=transposeToColumns, concatOn=concatOn,
  1075                                                   fastTranspose=fastTranspose,
  1076                                                   addLags=addLags, decimate=decimate, rollingWindow=rollingWindow,
  1077                                                   verbose=verbose, whichSegments=whichSegments,
  1078                                                   windowSize=windowSize, procFun=procFun,
  1079                                                   getMetaData=getMetaData, metaDataToCategories=metaDataToCategories)
  1080                                               #
  1081                                               manipulateIndex = np.any(
  1082                                                   [
  1083                                                       collapseSizes, duplicateControlsByProgram,
  1084                                                       makeControlProgram, removeFuzzyName
  1085                                                       ])
  1086                                               if outlierTrials is not None:
  1087                                                   def rejectionLookup(entry):
  1088                                                       key = []
  1089                                                       for subKey in outlierTrials.index.names:
  1090                                                           keyIdx = allWaveforms.index.names.index(subKey)
  1091                                                           key.append(entry[keyIdx])
  1092                                                       # print(key)
  1093                                                       # outlierTrials.iloc[1, :]
  1094                                                       # allWaveforms.iloc[1, :]
  1095                                                       return outlierTrials[tuple(key)]
  1096                                                   #
  1097                                                   outlierMask = np.asarray(
  1098                                                       allWaveforms.index.map(rejectionLookup),
  1099                                                       dtype=np.bool)
  1100                                                   if invertOutlierMask:
  1101                                                       outlierMask = ~outlierMask
  1102                                                   allWaveforms = allWaveforms.loc[~outlierMask, :]
  1103                                               if manipulateIndex and getMetaData:
  1104                                                   idxLabels = allWaveforms.index.names
  1105                                                   allWaveforms.reset_index(inplace=True)
  1106                                                   # 
  1107                                                   if collapseSizes:
  1108                                                       try:
  1109                                                           allWaveforms.loc[allWaveforms['pedalSizeCat'] == 'XL', 'pedalSizeCat'] = 'L'
  1110                                                           allWaveforms.loc[allWaveforms['pedalSizeCat'] == 'XS', 'pedalSizeCat'] = 'S'
  1111                                                       except Exception:
  1112                                                           traceback.print_exc()
  1113                                                   if makeControlProgram:
  1114                                                       try:
  1115                                                           allWaveforms.loc[allWaveforms[amplitudeColumn] == 0, programColumn] = 999
  1116                                                           allWaveforms.loc[allWaveforms[amplitudeColumn] == 0, electrodeColumn] = 'control'
  1117                                                       except Exception:
  1118                                                           traceback.print_exc()
  1119                                                   if duplicateControlsByProgram:
  1120                                                       #
  1121                                                       noStimWaveforms = (
  1122                                                           allWaveforms
  1123                                                           .loc[allWaveforms[amplitudeColumn] == 0, :]
  1124                                                           )
  1125                                                       stimWaveforms = (
  1126                                                           allWaveforms
  1127                                                           .loc[allWaveforms[amplitudeColumn] != 0, :]
  1128                                                           .copy()
  1129                                                           )
  1130                                                       uniqProgs = stimWaveforms[programColumn].unique()
  1131                                                       progElecLookup = {}
  1132                                                       #pdb.set_trace()
  1133                                                       for progIdx in uniqProgs:
  1134                                                           theseStimDF = stimWaveforms.loc[
  1135                                                               stimWaveforms[programColumn] == progIdx,
  1136                                                               electrodeColumn]
  1137                                                           elecIdx = theseStimDF.iloc[0]
  1138                                                           progElecLookup.update({progIdx: elecIdx})
  1139                                                       #
  1140                                                       if makeControlProgram:
  1141                                                           uniqProgs = np.append(uniqProgs, 999)
  1142                                                           progElecLookup.update({999: 'control'})
  1143                                                       #
  1144                                                       for progIdx in uniqProgs:
  1145                                                           dummyWaveforms = noStimWaveforms.copy()
  1146                                                           dummyWaveforms.loc[:, programColumn] = progIdx
  1147                                                           dummyWaveforms.loc[:, electrodeColumn] = progElecLookup[progIdx]
  1148                                                           stimWaveforms = pd.concat([stimWaveforms, dummyWaveforms])
  1149                                                       stimWaveforms.reset_index(drop=True, inplace=True)
  1150                                                       allWaveforms = stimWaveforms
  1151                                                   #
  1152                                                   if removeFuzzyName:
  1153                                                       fuzzyNamesBase = [
  1154                                                           i.replace('Fuzzy', '')
  1155                                                           for i in idxLabels
  1156                                                           if 'Fuzzy' in i]
  1157                                                       colRenamer = {n + 'Fuzzy': n for n in fuzzyNamesBase}
  1158                                                       fuzzyNamesBasePresent = [
  1159                                                           i
  1160                                                           for i in fuzzyNamesBase
  1161                                                           if i in allWaveforms.columns]
  1162                                                       allWaveforms.drop(columns=fuzzyNamesBasePresent, inplace=True)
  1163                                                       allWaveforms.rename(columns=colRenamer, inplace=True)
  1164                                                       idxLabels = np.unique(
  1165                                                           [i.replace('Fuzzy', '') for i in idxLabels])
  1166                                                   #
  1167                                                   allWaveforms.set_index(
  1168                                                       list(idxLabels),
  1169                                                       inplace=True)
  1170                                                   if isinstance(allWaveforms.columns, pd.MultiIndex):
  1171                                                       allWaveforms.columns = allWaveforms.columns.remove_unused_levels()
  1172                                               #
  1173                                               if transposeToColumns == 'feature':
  1174                                                   zipNames = zip(pd.unique(allWaveforms.columns.get_level_values('feature')).tolist(), unitNames)
  1175                                                   try:
  1176                                                       assert np.all([i == j for i, j in zipNames]), 'columns out of requested order!'
  1177                                                   except Exception:
  1178                                                       traceback.print_exc()
  1179                                                       allWaveforms.reindex(columns=unitNames)
  1180                                               if isinstance(allWaveforms.columns, pd.MultiIndex):
  1181                                                   allWaveforms.columns = allWaveforms.columns.remove_unused_levels()
  1182                                               allWaveforms.sort_index(
  1183                                                   axis='columns', inplace=True, kind='mergesort')
  1184                                               return allWaveforms

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getAsigsAlignedToEvents at line 1186

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1186                                           @profile
  1187                                           def getAsigsAlignedToEvents(
  1188                                                   eventBlock=None, signalBlock=None,
  1189                                                   chansToTrigger=None, chanQuery=None,
  1190                                                   eventName=None, windowSize=None,
  1191                                                   minNReps=None,
  1192                                                   appendToExisting=False,
  1193                                                   checkReferences=True, verbose=False,
  1194                                                   fileName=None, folderPath=None, chunkSize=None
  1195                                                   ):
  1196                                               #  get signals from same block as events?
  1197                                               if signalBlock is None:
  1198                                                   signalBlock = eventBlock
  1199                                               #  channels to trigger
  1200                                               if chansToTrigger is None:
  1201                                                   chansToTrigger = listChanNames(
  1202                                                       signalBlock, chanQuery, objType=ChannelIndex, condition='hasAsigs')
  1203                                               #  allocate block for spiketrains
  1204                                               masterBlock = Block()
  1205                                               try:
  1206                                                   masterBlock.name = signalBlock.annotations['neo_name']
  1207                                                   masterBlock.annotate(nix_name=signalBlock.annotations['neo_name'])
  1208                                               except Exception:
  1209                                                   masterBlock.name = signalBlock.name
  1210                                                   masterBlock.annotate(neo_name=signalBlock.name)
  1211                                                   masterBlock.annotate(nix_name=signalBlock.name)
  1212                                               #  make channels and units for triggered time series
  1213                                               for chanName in chansToTrigger:
  1214                                                   chanIdx = ChannelIndex(name=chanName + '#0', index=[0])
  1215                                                   chanIdx.annotate(nix_name=chanIdx.name)
  1216                                                   thisUnit = Unit(name=chanIdx.name)
  1217                                                   thisUnit.annotate(nix_name=chanIdx.name)
  1218                                                   chanIdx.units.append(thisUnit)
  1219                                                   thisUnit.channel_index = chanIdx
  1220                                                   masterBlock.channel_indexes.append(chanIdx)
  1221                                                   sigChanIdxList = signalBlock.filter(
  1222                                                       objects=ChannelIndex, name=chanName)
  1223                                                   if len(sigChanIdxList):
  1224                                                       sigChanIdx = sigChanIdxList[0]
  1225                                                       if sigChanIdx.coordinates is not None:
  1226                                                           coordUnits = sigChanIdx.coordinates[0][0].units
  1227                                                           chanIdx.coordinates = np.asarray(sigChanIdx.coordinates) * coordUnits
  1228                                                           thisUnit.annotations['parentChanXCoords'] = float(chanIdx.coordinates[:, 0].magnitude)
  1229                                                           thisUnit.annotations['parentChanYCoords'] = float(chanIdx.coordinates[:, 1].magnitude)
  1230                                                           thisUnit.annotations['parentChanCoordinateUnits'] = '{}'.format(coordUnits)
  1231                                               #
  1232                                               totalNSegs = 0
  1233                                               #  print([evSeg.events[3].name for evSeg in eventBlock.segments])
  1234                                               allAlignEventsList = []
  1235                                               for segIdx, eventSeg in enumerate(eventBlock.segments):
  1236                                                   thisEventName = 'seg{}_{}'.format(segIdx, eventName)
  1237                                                   try:
  1238                                                       assert len(eventSeg.filter(name=thisEventName)) == 1
  1239                                                   except Exception:
  1240                                                       traceback.print_exc()
  1241                                                   allEvIn = eventSeg.filter(name=thisEventName)[0]
  1242                                                   if isinstance(allEvIn, EventProxy):
  1243                                                       allAlignEvents = loadObjArrayAnn(allEvIn.load())
  1244                                                   elif isinstance(allEvIn, Event):
  1245                                                       allAlignEvents = allEvIn
  1246                                                   else:
  1247                                                       raise(Exception(
  1248                                                           '{} must be an Event or EventProxy!'
  1249                                                           .format(eventName)))
  1250                                                   allAlignEventsList.append(allAlignEvents)
  1251                                               allAlignEventsDF = unitSpikeTrainArrayAnnToDF(allAlignEventsList)
  1252                                               #
  1253                                               breakDownData = (
  1254                                                   allAlignEventsDF
  1255                                                   .groupby(minNReps['categories'])
  1256                                                   .agg('count')
  1257                                                   .iloc[:, 0]
  1258                                                   )
  1259                                               try:
  1260                                                   breakDownData[breakDownData > minNReps['n']].to_csv(
  1261                                                       os.path.join(
  1262                                                           folderPath, 'numRepetitionsEachCondition.csv'
  1263                                                       ), header=True
  1264                                                   )
  1265                                               except Exception:
  1266                                                   traceback.print_exc()
  1267                                               allAlignEventsDF.loc[:, 'keepMask'] = False
  1268                                               for name, group in allAlignEventsDF.groupby(minNReps['categories']):
  1269                                                   allAlignEventsDF.loc[group.index, 'keepMask'] = (
  1270                                                       breakDownData[name] > minNReps['n'])
  1271                                               for segIdx, group in allAlignEventsDF.groupby('segment'):
  1272                                                   allAlignEventsList[segIdx].array_annotations['keepMask'] = group['keepMask'].to_numpy()
  1273                                               #
  1274                                               for segIdx, eventSeg in enumerate(eventBlock.segments):
  1275                                                   if verbose:
  1276                                                       print(
  1277                                                           'getAsigsAlignedToEvents on segment {} of {}'
  1278                                                           .format(segIdx + 1, len(eventBlock.segments)))
  1279                                                   allAlignEvents = allAlignEventsList[segIdx]
  1280                                                   if chunkSize is None:
  1281                                                       alignEventGroups = [allAlignEvents]
  1282                                                   else:
  1283                                                       nChunks = max(
  1284                                                           int(np.floor(allAlignEvents.shape[0] / chunkSize)),
  1285                                                           1)
  1286                                                       alignEventGroups = []
  1287                                                       for i in range(nChunks):
  1288                                                           if not (i == (nChunks - 1)):
  1289                                                               # not last one
  1290                                                               alignEventGroups.append(
  1291                                                                   allAlignEvents[i * chunkSize: (i + 1) * chunkSize])
  1292                                                           else:
  1293                                                               alignEventGroups.append(
  1294                                                                   allAlignEvents[i * chunkSize:])
  1295                                                   signalSeg = signalBlock.segments[segIdx]
  1296                                                   for subSegIdx, alignEvents in enumerate(alignEventGroups):
  1297                                                       # seg to contain triggered time series
  1298                                                       if verbose:
  1299                                                           print(
  1300                                                               'getAsigsAlignedToEvents on subSegment {} of {}'
  1301                                                               .format(subSegIdx + 1, len(alignEventGroups)))
  1302                                                       if not alignEvents.shape[0] > 0:
  1303                                                           continue
  1304                                                       newSeg = Segment(name='seg{}_'.format(int(totalNSegs)))
  1305                                                       newSeg.annotate(nix_name=newSeg.name)
  1306                                                       masterBlock.segments.append(newSeg)
  1307                                                       for chanName in chansToTrigger:
  1308                                                           asigName = 'seg{}_{}'.format(segIdx, chanName)
  1309                                                           if verbose:
  1310                                                               print(
  1311                                                                   'getAsigsAlignedToEvents on channel {}'
  1312                                                                   .format(chanName))
  1313                                                           assert len(signalSeg.filter(name=asigName)) == 1
  1314                                                           asig = signalSeg.filter(name=asigName)[0]
  1315                                                           nominalWinLen = int(
  1316                                                               (windowSize[1] - windowSize[0]) *
  1317                                                               asig.sampling_rate - 1)
  1318                                                           validMask = (
  1319                                                               ((
  1320                                                                   alignEvents + windowSize[1] +
  1321                                                                   asig.sampling_rate ** (-1)) < asig.t_stop) &
  1322                                                               ((
  1323                                                                   alignEvents + windowSize[0] -
  1324                                                                   asig.sampling_rate ** (-1)) > asig.t_start)
  1325                                                               )
  1326                                                           thisKeepMask = alignEvents.array_annotations['keepMask']
  1327                                                           fullMask = (validMask & thisKeepMask)
  1328                                                           alignEvents = alignEvents[fullMask]
  1329                                                           # array_annotations get sliced with the event, but regular anns do not
  1330                                                           for annName in alignEvents.annotations['arrayAnnNames']:
  1331                                                               alignEvents.annotations[annName] = (
  1332                                                                   alignEvents.array_annotations[annName])
  1333                                                           if isinstance(asig, AnalogSignalProxy):
  1334                                                               if checkReferences:
  1335                                                                   da = (
  1336                                                                       asig
  1337                                                                       ._rawio
  1338                                                                       .da_list['blocks'][0]['segments'][segIdx]['data'])
  1339                                                                   print('segIdx {}, asig.name {}'.format(
  1340                                                                       segIdx, asig.name))
  1341                                                                   print('asig._global_channel_indexes = {}'.format(
  1342                                                                       asig._global_channel_indexes))
  1343                                                                   print('asig references {}'.format(
  1344                                                                       da[asig._global_channel_indexes[0]]))
  1345                                                                   try:
  1346                                                                       assert (
  1347                                                                           asig.name
  1348                                                                           in da[asig._global_channel_indexes[0]].name)
  1349                                                                   except Exception:
  1350                                                                       traceback.print_exc()
  1351                                                               rawWaveforms = [
  1352                                                                   asig.load(
  1353                                                                       time_slice=(t + windowSize[0], t + windowSize[1]))
  1354                                                                   for t in alignEvents]
  1355                                                               if any([rW.shape[0] < nominalWinLen for rW in rawWaveforms]):
  1356                                                                   rawWaveforms = [
  1357                                                                       asig.load(
  1358                                                                           time_slice=(t + windowSize[0], t + windowSize[1] + asig.sampling_period))
  1359                                                                       for t in alignEvents]
  1360                                                           elif isinstance(asig, AnalogSignal):
  1361                                                               rawWaveforms = []
  1362                                                               for t in alignEvents:
  1363                                                                   asigMask = (asig.times > t + windowSize[0]) & (asig.times < t + windowSize[1])
  1364                                                                   rawWaveforms.append(asig[asigMask[:, np.newaxis]])
  1365                                                           else:
  1366                                                               raise(Exception('{} must be an AnalogSignal or AnalogSignalProxy!'.format(asigName)))
  1367                                                           #
  1368                                                           samplingRate = asig.sampling_rate
  1369                                                           waveformUnits = rawWaveforms[0].units
  1370                                                           #  fix length if roundoff error
  1371                                                           #  minLen = min([rW.shape[0] for rW in rawWaveforms])
  1372                                                           rawWaveforms = [rW[:nominalWinLen] for rW in rawWaveforms]
  1373                                                           #
  1374                                                           spikeWaveforms = (
  1375                                                               np.hstack([rW.magnitude for rW in rawWaveforms])
  1376                                                               .transpose()[:, np.newaxis, :] * waveformUnits
  1377                                                               )
  1378                                                           #
  1379                                                           thisUnit = masterBlock.filter(
  1380                                                               objects=Unit, name=chanName + '#0')[0]
  1381                                                           skipEventAnnNames = (
  1382                                                               ['nix_name', 'neo_name']
  1383                                                               )
  1384                                                           stAnn = {
  1385                                                               k: v
  1386                                                               for k, v in alignEvents.annotations.items()
  1387                                                               if k not in skipEventAnnNames
  1388                                                               }
  1389                                                           skipAsigAnnNames = (
  1390                                                               ['channel_id', 'nix_name', 'neo_name']
  1391                                                               )
  1392                                                           stAnn.update({
  1393                                                               k: v
  1394                                                               for k, v in asig.annotations.items()
  1395                                                               if k not in skipAsigAnnNames
  1396                                                           })
  1397                                                           st = SpikeTrain(
  1398                                                               name='seg{}_{}'.format(int(totalNSegs), thisUnit.name),
  1399                                                               times=alignEvents.times,
  1400                                                               waveforms=spikeWaveforms,
  1401                                                               t_start=asig.t_start, t_stop=asig.t_stop,
  1402                                                               left_sweep=windowSize[0] * (-1),
  1403                                                               sampling_rate=samplingRate,
  1404                                                               **stAnn
  1405                                                               )
  1406                                                           st.annotate(nix_name=st.name)
  1407                                                           st.annotations['unitAnnotations'] = json.dumps(
  1408                                                               thisUnit.annotations.copy())
  1409                                                           thisUnit.spiketrains.append(st)
  1410                                                           newSeg.spiketrains.append(st)
  1411                                                           st.unit = thisUnit
  1412                                                       totalNSegs += 1
  1413                                               try:
  1414                                                   eventBlock.filter(
  1415                                                       objects=EventProxy)[0]._rawio.file.close()
  1416                                               except Exception:
  1417                                                   traceback.print_exc()
  1418                                               if signalBlock is not eventBlock:
  1419                                                   try:
  1420                                                       signalBlock.filter(
  1421                                                           objects=AnalogSignalProxy)[0]._rawio.file.close()
  1422                                                   except Exception:
  1423                                                       traceback.print_exc()
  1424                                               triggeredPath = os.path.join(
  1425                                                   folderPath, fileName + '.nix')
  1426                                               if not os.path.exists(triggeredPath):
  1427                                                   appendToExisting = False
  1428                                           
  1429                                               if appendToExisting:
  1430                                                   allSegs = list(range(len(masterBlock.segments)))
  1431                                                   addBlockToNIX(
  1432                                                       masterBlock, neoSegIdx=allSegs,
  1433                                                       writeSpikes=True,
  1434                                                       fileName=fileName,
  1435                                                       folderPath=folderPath,
  1436                                                       purgeNixNames=False,
  1437                                                       nixBlockIdx=0, nixSegIdx=allSegs)
  1438                                               else:
  1439                                                   if os.path.exists(triggeredPath):
  1440                                                       os.remove(triggeredPath)
  1441                                                   masterBlock = purgeNixAnn(masterBlock)
  1442                                                   writer = NixIO(filename=triggeredPath)
  1443                                                   writer.write_block(masterBlock, use_obj_names=True)
  1444                                                   writer.close()
  1445                                               return masterBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: alignedAsigDFtoSpikeTrain at line 1447

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1447                                           @profile
  1448                                           def alignedAsigDFtoSpikeTrain(
  1449                                                   allWaveforms, dataBlock=None, matchSamplingRate=True):
  1450                                               masterBlock = Block()
  1451                                               masterBlock.name = dataBlock.annotations['neo_name']
  1452                                               masterBlock.annotate(nix_name=dataBlock.annotations['neo_name'])
  1453                                               for segIdx, group in allWaveforms.groupby('segment'):
  1454                                                   print('Saving trajectoriess for segment {}'.format(segIdx))
  1455                                                   dataSeg = dataBlock.segments[segIdx]
  1456                                                   exSt = dataSeg.spiketrains[0]
  1457                                                   if isinstance(exSt, SpikeTrainProxy):
  1458                                                       print(
  1459                                                           'alignedAsigDFtoSpikeTrain basing seg {} on {}'
  1460                                                           .format(segIdx, exSt.name))
  1461                                                       stProxy = exSt
  1462                                                       exSt = loadStProxy(stProxy)
  1463                                                       exSt = loadObjArrayAnn(exSt)
  1464                                                   print('exSt.left_sweep is {}'.format(exSt.left_sweep))
  1465                                                   wfBins = ((np.arange(exSt.waveforms.shape[2]) / (exSt.sampling_rate)) - exSt.left_sweep).magnitude
  1466                                                   # seg to contain triggered time series
  1467                                                   newSeg = Segment(name=dataSeg.annotations['neo_name'])
  1468                                                   newSeg.annotate(nix_name=dataSeg.annotations['neo_name'])
  1469                                                   masterBlock.segments.append(newSeg)
  1470                                                   #
  1471                                                   if group.columns.name == 'bin':
  1472                                                       grouper = group.groupby('feature')
  1473                                                       colsAre = 'bin'
  1474                                                   elif group.columns.name == 'feature':
  1475                                                       grouper = group.iteritems()
  1476                                                       colsAre = 'feature'
  1477                                                   for featName, featGroup in grouper:
  1478                                                       print('Saving {}...'.format(featName))
  1479                                                       if featName[-2:] == '#0':
  1480                                                           cleanFeatName = featName
  1481                                                       else:
  1482                                                           cleanFeatName = featName + '#0'
  1483                                                       if segIdx == 0:
  1484                                                           #  allocate units
  1485                                                           chanIdx = ChannelIndex(
  1486                                                               name=cleanFeatName, index=[0])
  1487                                                           chanIdx.annotate(nix_name=chanIdx.name)
  1488                                                           thisUnit = Unit(name=chanIdx.name)
  1489                                                           thisUnit.annotate(nix_name=chanIdx.name)
  1490                                                           chanIdx.units.append(thisUnit)
  1491                                                           thisUnit.channel_index = chanIdx
  1492                                                           masterBlock.channel_indexes.append(chanIdx)
  1493                                                       else:
  1494                                                           thisUnit = masterBlock.filter(
  1495                                                               objects=Unit, name=cleanFeatName)[0]
  1496                                                       if colsAre == 'bin':
  1497                                                           spikeWaveformsDF = featGroup
  1498                                                       elif colsAre == 'feature':
  1499                                                           if isinstance(featGroup, pd.Series):
  1500                                                               featGroup = featGroup.to_frame(name=featName)
  1501                                                               featGroup.columns.name = 'feature'
  1502                                                           spikeWaveformsDF = transposeSpikeDF(
  1503                                                               featGroup,
  1504                                                               'bin', fastTranspose=True)
  1505                                                       if matchSamplingRate:
  1506                                                           if len(spikeWaveformsDF.columns) != len(wfBins):
  1507                                                               wfDF = spikeWaveformsDF.reset_index(drop=True).T
  1508                                                               wfDF = hf.interpolateDF(wfDF, wfBins)
  1509                                                               spikeWaveformsDF = wfDF.T.set_index(spikeWaveformsDF.index)
  1510                                                       spikeWaveforms = spikeWaveformsDF.to_numpy()[:, np.newaxis, :]
  1511                                                       arrAnnDF = spikeWaveformsDF.index.to_frame()
  1512                                                       spikeTimes = arrAnnDF['t']
  1513                                                       arrAnnDF.drop(columns='t', inplace=True)
  1514                                                       arrAnn = {}
  1515                                                       colsToKeep = arrAnnDF.columns.drop(['originalIndex', 'feature', 'segment', 'lag'])
  1516                                                       for cName in colsToKeep:
  1517                                                           values = arrAnnDF[cName].to_numpy()
  1518                                                           if isinstance(values[0], str):
  1519                                                               values = values.astype('U')
  1520                                                           arrAnn.update({str(cName): values.flatten()})
  1521                                                       arrayAnnNames = {
  1522                                                           'arrayAnnNames': list(arrAnn.keys())}
  1523                                                       st = SpikeTrain(
  1524                                                           name='seg{}_{}'.format(int(segIdx), thisUnit.name),
  1525                                                           times=spikeTimes.to_numpy() * exSt.units,
  1526                                                           waveforms=spikeWaveforms * pq.dimensionless,
  1527                                                           t_start=exSt.t_start, t_stop=exSt.t_stop,
  1528                                                           left_sweep=exSt.left_sweep,
  1529                                                           sampling_rate=exSt.sampling_rate,
  1530                                                           **arrAnn, **arrayAnnNames
  1531                                                           )
  1532                                                       st.annotate(nix_name=st.name)
  1533                                                       thisUnit.spiketrains.append(st)
  1534                                                       newSeg.spiketrains.append(st)
  1535                                                       st.unit = thisUnit
  1536                                               return masterBlock

Total time: 0.633277 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: dataFrameToAnalogSignals at line 1538

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1538                                           @profile
  1539                                           def dataFrameToAnalogSignals(
  1540                                                   df,
  1541                                                   block=None, seg=None,
  1542                                                   idxT='NSPTime',
  1543                                                   probeName='insTD', samplingRate=500*pq.Hz,
  1544                                                   timeUnits=pq.s, measureUnits=pq.mV,
  1545                                                   dataCol=['channel_0', 'channel_1'],
  1546                                                   useColNames=False, forceColNames=None,
  1547                                                   namePrefix='', nameSuffix='', verbose=False):
  1548         1         17.0     17.0      0.0      if block is None:
  1549         1         14.0     14.0      0.0          assert seg is None
  1550         1        798.0    798.0      0.0          block = Block(name=probeName)
  1551         1        634.0    634.0      0.0          seg = Segment(name='seg0_' + probeName)
  1552         1         22.0     22.0      0.0          block.segments.append(seg)
  1553         1         13.0     13.0      0.0      if verbose:
  1554         1        845.0    845.0      0.0          print('in dataFrameToAnalogSignals...')
  1555        71       3040.0     42.8      0.0      for idx, colName in enumerate(dataCol):
  1556        70       1590.0     22.7      0.0          if verbose:
  1557        70      46686.0    666.9      0.7              print('    {}'.format(colName))
  1558        70        729.0     10.4      0.0          if forceColNames is not None:
  1559                                                       chanName = forceColNames[idx]
  1560        70        558.0      8.0      0.0          elif useColNames:
  1561        70        784.0     11.2      0.0              chanName = namePrefix + colName + nameSuffix
  1562                                                   else:
  1563                                                       chanName = namePrefix + (probeName.lower() + '{}'.format(idx)) + nameSuffix
  1564                                                   #
  1565        70        672.0      9.6      0.0          chanIdx = ChannelIndex(
  1566        70        511.0      7.3      0.0              name=chanName,
  1567                                                       # index=None,
  1568        70      49590.0    708.4      0.8              index=np.asarray([idx]),
  1569                                                       # channel_names=np.asarray([chanName])
  1570                                                       )
  1571        70       1317.0     18.8      0.0          block.channel_indexes.append(chanIdx)
  1572        70        684.0      9.8      0.0          asig = AnalogSignal(
  1573        70    2602427.0  37177.5     41.1              df[colName].to_numpy() * measureUnits,
  1574        70       2196.0     31.4      0.0              name='seg0_' + chanName,
  1575        70        647.0      9.2      0.0              sampling_rate=samplingRate,
  1576        70    3448243.0  49260.6     54.5              dtype=np.float32,
  1577                                                       # **ann
  1578                                                       )
  1579        70       1957.0     28.0      0.0          if idxT is not None:
  1580        70     103359.0   1476.6      1.6              asig.t_start = df[idxT].iloc[0] * timeUnits
  1581                                                   else:
  1582                                                       asig.t_start = df.index[0] * timeUnits
  1583        70        912.0     13.0      0.0          asig.channel_index = chanIdx
  1584                                                   # assign ownership to containers
  1585        70       1997.0     28.5      0.0          chanIdx.analogsignals.append(asig)
  1586        70       1259.0     18.0      0.0          seg.analogsignals.append(asig)
  1587        70      40025.0    571.8      0.6          chanIdx.create_relationship()
  1588                                               # assign parent to children
  1589         1      20447.0  20447.0      0.3      block.create_relationship()
  1590         1        792.0    792.0      0.0      seg.create_relationship()
  1591         1          7.0      7.0      0.0      return block

Total time: 0.0049957 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: eventDataFrameToEvents at line 1593

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1593                                           @profile
  1594                                           def eventDataFrameToEvents(
  1595                                                   eventDF, idxT=None,
  1596                                                   annCol=None,
  1597                                                   eventName='', tUnits=pq.s,
  1598                                                   makeList=True
  1599                                                   ):
  1600         1          6.0      6.0      0.0      if makeList:
  1601                                                   eventList = []
  1602                                                   for colName in annCol:
  1603                                                       originalDType = type(eventDF[colName].to_numpy()[0]).__name__
  1604                                                       event = Event(
  1605                                                           name=eventName + colName,
  1606                                                           times=eventDF[idxT].to_numpy() * tUnits,
  1607                                                           labels=eventDF[colName].astype(originalDType).to_numpy()
  1608                                                           )
  1609                                                       event.annotate(originalDType=originalDType)
  1610                                                       eventList.append(event)
  1611                                                   return eventList
  1612                                               else:
  1613         1          6.0      6.0      0.0          if annCol is None:
  1614         1       9858.0   9858.0     19.7              annCol = eventDF.drop(columns=idxT).columns
  1615         1         12.0     12.0      0.0          event = Event(
  1616         1          5.0      5.0      0.0              name=eventName,
  1617         1       1295.0   1295.0      2.6              times=eventDF[idxT].to_numpy() * tUnits,
  1618         1       1936.0   1936.0      3.9              labels=np.asarray(eventDF.index)
  1619                                                       )
  1620         1          8.0      8.0      0.0          event.annotations.update(
  1621                                                       {
  1622         1          5.0      5.0      0.0                  'arrayAnnNames': [],
  1623         1         10.0     10.0      0.0                  'arrayAnnDTypes': []
  1624                                                           })
  1625        13        372.0     28.6      0.7          for colName in annCol:
  1626        12       8616.0    718.0     17.2              originalDType = type(eventDF[colName].to_numpy()[0]).__name__
  1627        12      25186.0   2098.8     50.4              arrayAnn = eventDF[colName].astype(originalDType).to_numpy()
  1628        12        107.0      8.9      0.2              event.array_annotations.update(
  1629        12       2196.0    183.0      4.4                  {colName: arrayAnn})
  1630        12         96.0      8.0      0.2              event.annotations['arrayAnnNames'].append(colName)
  1631        12         71.0      5.9      0.1              event.annotations['arrayAnnDTypes'].append(originalDType)
  1632        12         64.0      5.3      0.1              event.annotations.update(
  1633        12        104.0      8.7      0.2                  {colName: arrayAnn})
  1634         1          4.0      4.0      0.0          return event

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: eventsToDataFrame at line 1636

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1636                                           @profile
  1637                                           def eventsToDataFrame(
  1638                                                   events, idxT='t', names=None
  1639                                                   ):
  1640                                               eventDict = {}
  1641                                               calculatedT = False
  1642                                               for event in events:
  1643                                                   if names is not None:
  1644                                                       if event.name not in names:
  1645                                                           continue
  1646                                                   if len(event.times):
  1647                                                       if not calculatedT:
  1648                                                           t = pd.Series(event.times.magnitude)
  1649                                                           calculatedT = True
  1650                                                       try:
  1651                                                           values = event.array_annotations['labels']
  1652                                                       except Exception:
  1653                                                           values = event.labels
  1654                                                       if isinstance(values[0], bytes):
  1655                                                           #  event came from hdf, need to recover dtype
  1656                                                           if 'originalDType' in event.annotations:
  1657                                                               dtypeStr = event.annotations['originalDType'].split(';')[-1]
  1658                                                               if 'np.' not in dtypeStr:
  1659                                                                   dtypeStr = 'np.' + dtypeStr
  1660                                                               originalDType = eval(dtypeStr)
  1661                                                               values = np.asarray(values, dtype=originalDType)
  1662                                                           else:
  1663                                                               values = np.asarray(values, dtype=np.str)
  1664                                                       #  print(values.dtype)
  1665                                                       eventDict.update({
  1666                                                           event.name: pd.Series(values)})
  1667                                               eventDict.update({idxT: t})
  1668                                               return pd.concat(eventDict, axis=1)

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadSpikeMats at line 1670

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1670                                           @profile
  1671                                           def loadSpikeMats(
  1672                                                   dataPath, rasterOpts,
  1673                                                   alignTimes=None, chans=None, loadAll=False,
  1674                                                   absoluteBins=False, transposeSpikeMat=False,
  1675                                                   checkReferences=False,
  1676                                                   aggregateFun=None):
  1677                                           
  1678                                               reader = nixio_fr.NixIO(filename=dataPath)
  1679                                               chanNames = reader.header['signal_channels']['name']
  1680                                               
  1681                                               if chans is not None:
  1682                                                   sigMask = np.isin(chanNames, chans)
  1683                                                   chanNames = chanNames[sigMask]
  1684                                                   
  1685                                               chanIdx = reader.channel_name_to_index(chanNames)
  1686                                               
  1687                                               if not loadAll:
  1688                                                   assert alignTimes is not None
  1689                                                   spikeMats = {i: None for i in alignTimes.index}
  1690                                                   validTrials = pd.Series(True, index=alignTimes.index)
  1691                                               else:
  1692                                                   spikeMats = {
  1693                                                       i: None for i in range(reader.segment_count(block_index=0))}
  1694                                                   validTrials = None
  1695                                               
  1696                                               for segIdx in range(reader.segment_count(block_index=0)):
  1697                                                   if checkReferences:
  1698                                                       for i, cIdx in enumerate(chanIdx):
  1699                                                           da = reader.da_list['blocks'][0]['segments'][segIdx]['data'][cIdx]
  1700                                                           print('name {}, da.name {}'.format(chanNames[i], da.name))
  1701                                                           try:
  1702                                                               assert chanNames[i] in da.name, 'reference problem!!'
  1703                                                           except Exception:
  1704                                                               traceback.print_exc()
  1705                                                   tStart = reader.get_signal_t_start(
  1706                                                       block_index=0, seg_index=segIdx)
  1707                                                   fs = reader.get_signal_sampling_rate(
  1708                                                       channel_indexes=chanIdx
  1709                                                       )
  1710                                                   sigSize = reader.get_signal_size(
  1711                                                       block_index=0, seg_index=segIdx
  1712                                                       )
  1713                                                   tStop = sigSize / fs + tStart
  1714                                                   #  convert to indices early to avoid floating point problems
  1715                                                   
  1716                                                   intervalIdx = int(round(rasterOpts['binInterval'] * fs))
  1717                                                   #  halfIntervalIdx = int(round(intervalIdx / 2))
  1718                                                   
  1719                                                   widthIdx = int(round(rasterOpts['binWidth'] * fs))
  1720                                                   halfWidthIdx = int(round(widthIdx / 2))
  1721                                                   
  1722                                                   if rasterOpts['smoothKernelWidth'] is not None:
  1723                                                       kernWidthIdx = int(round(rasterOpts['smoothKernelWidth'] * fs))
  1724                                                   
  1725                                                   theBins = None
  1726                                           
  1727                                                   if not loadAll:
  1728                                                       winStartIdx = int(round(rasterOpts['windowSize'][0] * fs))
  1729                                                       winStopIdx = int(round(rasterOpts['windowSize'][1] * fs))
  1730                                                       timeMask = (alignTimes > tStart) & (alignTimes < tStop)
  1731                                                       maskedTimes = alignTimes[timeMask]
  1732                                                   else:
  1733                                                       #  irrelevant, will load all
  1734                                                       maskedTimes = pd.Series(np.nan)
  1735                                           
  1736                                                   for idx, tOnset in maskedTimes.iteritems():
  1737                                                       if not loadAll:
  1738                                                           idxOnset = int(round((tOnset - tStart) * fs))
  1739                                                           #  can't not be ints
  1740                                                           iStart = idxOnset + winStartIdx - int(3 * halfWidthIdx)
  1741                                                           iStop = idxOnset + winStopIdx + int(3 * halfWidthIdx)
  1742                                                       else:
  1743                                                           winStartIdx = 0
  1744                                                           iStart = 0
  1745                                                           iStop = sigSize
  1746                                           
  1747                                                       if iStart < 0:
  1748                                                           #  near the first edge
  1749                                                           validTrials[idx] = False
  1750                                                       elif (sigSize < iStop):
  1751                                                           #  near the ending edge
  1752                                                           validTrials[idx] = False
  1753                                                       else:
  1754                                                           #  valid slices
  1755                                                           try:
  1756                                                               rawSpikeMat = pd.DataFrame(
  1757                                                                   reader.get_analogsignal_chunk(
  1758                                                                       block_index=0, seg_index=segIdx,
  1759                                                                       i_start=iStart, i_stop=iStop,
  1760                                                                       channel_names=chanNames))
  1761                                                           except Exception:
  1762                                                               traceback.print_exc()
  1763                                                               #
  1764                                                           if aggregateFun is None:
  1765                                                               procSpikeMat = rawSpikeMat.rolling(
  1766                                                                   window=3 * widthIdx, center=True,
  1767                                                                   win_type='gaussian'
  1768                                                                   ).mean(std=halfWidthIdx)
  1769                                                           else:
  1770                                                               procSpikeMat = rawSpikeMat.rolling(
  1771                                                                   window=widthIdx, center=True
  1772                                                                   ).apply(
  1773                                                                       aggregateFun,
  1774                                                                       raw=True,
  1775                                                                       kwargs={'fs': fs, 'nSamp': widthIdx})
  1776                                                           #
  1777                                                           if rasterOpts['smoothKernelWidth'] is not None:
  1778                                                               procSpikeMat = (
  1779                                                                   procSpikeMat
  1780                                                                   .rolling(
  1781                                                                       window=3 * kernWidthIdx, center=True,
  1782                                                                       win_type='gaussian')
  1783                                                                   .mean(std=kernWidthIdx/2)
  1784                                                                   .dropna().iloc[::intervalIdx, :]
  1785                                                               )
  1786                                                           else:
  1787                                                               procSpikeMat = (
  1788                                                                   procSpikeMat
  1789                                                                   .dropna().iloc[::intervalIdx, :]
  1790                                                               )
  1791                                           
  1792                                                           procSpikeMat.columns = chanNames
  1793                                                           procSpikeMat.columns.name = 'unit'
  1794                                                           if theBins is None:
  1795                                                               theBins = np.asarray(
  1796                                                                   procSpikeMat.index + winStartIdx) / fs
  1797                                                           if absoluteBins:
  1798                                                               procSpikeMat.index = theBins + idxOnset / fs
  1799                                                           else:
  1800                                                               procSpikeMat.index = theBins
  1801                                                           procSpikeMat.index.name = 'bin'
  1802                                                           if loadAll:
  1803                                                               smIdx = segIdx
  1804                                                           else:
  1805                                                               smIdx = idx
  1806                                                               
  1807                                                           spikeMats[smIdx] = procSpikeMat
  1808                                                           if transposeSpikeMat:
  1809                                                               spikeMats[smIdx] = spikeMats[smIdx].transpose()
  1810                                                       #  plt.imshow(rawSpikeMat.to_numpy(), aspect='equal'); plt.show()
  1811                                               return spikeMats, validTrials

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: synchronizeINStoNSP at line 1813

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1813                                           @profile
  1814                                           def synchronizeINStoNSP(
  1815                                                   tapTimestampsNSP=None, tapTimestampsINS=None,
  1816                                                   precalculatedFun=None,
  1817                                                   NSPTimeRanges=(None, None),
  1818                                                   td=None, accel=None, insBlock=None, trialSegment=None, degree=1,
  1819                                                   trimSpiketrains=False
  1820                                                   ):
  1821                                               print('Trial Segment {}'.format(trialSegment))
  1822                                               if precalculatedFun is None:
  1823                                                   assert ((tapTimestampsNSP is not None) & (tapTimestampsINS is not None))
  1824                                                   # sanity check that the intervals match
  1825                                                   insDiff = tapTimestampsINS.diff().dropna().values
  1826                                                   nspDiff = tapTimestampsNSP.diff().dropna().values
  1827                                                   print('On the INS, the diff() between taps was\n{}'.format(insDiff))
  1828                                                   print('On the NSP, the diff() between taps was\n{}'.format(nspDiff))
  1829                                                   print('This amounts to a msec difference of\n{}'.format(
  1830                                                       (insDiff - nspDiff) * 1e3))
  1831                                                   if (insDiff - nspDiff > 20e-3).any():
  1832                                                       raise(Exception('Tap trains too different!'))
  1833                                                   #
  1834                                                   if degree > 0:
  1835                                                       synchPolyCoeffsINStoNSP = np.polyfit(
  1836                                                           x=tapTimestampsINS.values, y=tapTimestampsNSP.values,
  1837                                                           deg=degree)
  1838                                                   else:
  1839                                                       timeOffset = tapTimestampsNSP.values - tapTimestampsINS.values
  1840                                                       synchPolyCoeffsINStoNSP = np.array([1, np.mean(timeOffset)])
  1841                                                   timeInterpFunINStoNSP = np.poly1d(synchPolyCoeffsINStoNSP)
  1842                                               else:
  1843                                                   timeInterpFunINStoNSP = precalculatedFun
  1844                                               if td is not None:
  1845                                                   td.loc[:, 'NSPTime'] = pd.Series(
  1846                                                       timeInterpFunINStoNSP(td['t']), index=td['t'].index)
  1847                                                   td.loc[:, 'NSPTime'] = timeInterpFunINStoNSP(td['t'].to_numpy())
  1848                                               if accel is not None:
  1849                                                   accel.loc[:, 'NSPTime'] = pd.Series(
  1850                                                       timeInterpFunINStoNSP(accel['t']), index=accel['t'].index)
  1851                                               if insBlock is not None:
  1852                                                   # allUnits = [st.unit for st in insBlock.segments[0].spiketrains]
  1853                                                   # [un.name for un in insBlock.filter(objects=Unit)]
  1854                                                   for unit in insBlock.filter(objects=Unit):
  1855                                                       tStart = NSPTimeRanges[0]
  1856                                                       tStop = NSPTimeRanges[1]
  1857                                                       uniqueSt = []
  1858                                                       for st in unit.spiketrains:
  1859                                                           if st not in uniqueSt:
  1860                                                               uniqueSt.append(st)
  1861                                                           else:
  1862                                                               continue
  1863                                                           print('Synchronizing {}'.format(st.name))
  1864                                                           if len(st.times):
  1865                                                               segMaskSt = np.array(
  1866                                                                   st.array_annotations['trialSegment'],
  1867                                                                   dtype=np.int) == trialSegment
  1868                                                               st.magnitude[segMaskSt] = (
  1869                                                                   timeInterpFunINStoNSP(st.times[segMaskSt].magnitude))
  1870                                                               if trimSpiketrains:
  1871                                                                   print('Trimming spiketrain')
  1872                                                                   #  kludgey fix for weirdness concerning t_start
  1873                                                                   st.t_start = min(tStart, st.times[0] * 0.999)
  1874                                                                   st.t_stop = min(tStop, st.times[-1] * 1.001)
  1875                                                                   validMask = st < st.t_stop
  1876                                                                   if ~validMask.all():
  1877                                                                       print('Deleted some spikes')
  1878                                                                       st = st[validMask]
  1879                                                                       # delete invalid spikes
  1880                                                                       if 'arrayAnnNames' in st.annotations.keys():
  1881                                                                           for key in st.annotations['arrayAnnNames']:
  1882                                                                               try:
  1883                                                                                   # st.annotations[key] = np.array(st.array_annotations[key])
  1884                                                                                   st.annotations[key] = np.delete(st.annotations[key], ~validMask)
  1885                                                                               except Exception:
  1886                                                                                   traceback.print_exc()
  1887                                                                                   pdb.set_trace()
  1888                                                           else:
  1889                                                               if trimSpiketrains:
  1890                                                                   st.t_start = tStart
  1891                                                                   st.t_stop = tStop
  1892                                                   #
  1893                                                   allEvents = [
  1894                                                       ev
  1895                                                       for ev in insBlock.filter(objects=Event)
  1896                                                       if ('ins' in ev.name) and ('concatenate' not in ev.name)]
  1897                                                   concatEvents = [
  1898                                                       ev
  1899                                                       for ev in insBlock.filter(objects=Event)
  1900                                                       if ('ins' in ev.name) and ('concatenate' in ev.name)]
  1901                                                   eventsDF = eventsToDataFrame(allEvents, idxT='t')
  1902                                                   newNames = {i: childBaseName(i, 'seg') for i in eventsDF.columns}
  1903                                                   eventsDF.rename(columns=newNames, inplace=True)
  1904                                                   segMask = hf.getStimSerialTrialSegMask(eventsDF, trialSegment)
  1905                                                   evTStart = eventsDF.loc[segMask, 't'].min() * pq.s
  1906                                                   evTStop = eventsDF.loc[segMask, 't'].max() * pq.s
  1907                                                   # print('allEvents[0].shape = {}'.format(allEvents[0].shape))
  1908                                                   # print('allEvents[0].magnitude[segMask][0] = {}'.format(allEvents[0].magnitude[segMask][0]))
  1909                                                   for event in (allEvents + concatEvents):
  1910                                                       if trimSpiketrains:
  1911                                                           thisSegMask = (event.times >= evTStart) & (event.times <= evTStop)
  1912                                                       else:
  1913                                                           thisSegMask = (event.times >= evTStart) & (event.times < evTStop)
  1914                                                       event.magnitude[thisSegMask] = (
  1915                                                           timeInterpFunINStoNSP(event.times[thisSegMask].magnitude))
  1916                                                   # print('allEvents[0].magnitude[segMask][0] = {}'.format(allEvents[0].magnitude[segMask][0]))
  1917                                                   # if len(concatEvents) > trialSegment:
  1918                                                   #     concatEvents[trialSegment].magnitude[:] = timeInterpFunINStoNSP(
  1919                                                   #         concatEvents[trialSegment].times[:].magnitude)
  1920                                               return td, accel, insBlock, timeInterpFunINStoNSP

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: findSegsIncluding at line 1922

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1922                                           @profile
  1923                                           def findSegsIncluding(
  1924                                                   block, timeSlice=None):
  1925                                               segBoundsList = []
  1926                                               for segIdx, seg in enumerate(block.segments):
  1927                                                   segBoundsList.append(pd.DataFrame({
  1928                                                       't_start': seg.t_start,
  1929                                                       't_stop': seg.t_stop
  1930                                                       }, index=[segIdx]))
  1931                                           
  1932                                               segBounds = pd.concat(segBoundsList)
  1933                                               if timeSlice[0] is not None:
  1934                                                   segMask = (segBounds['t_start'] * pq.s >= timeSlice[0]) & (
  1935                                                       segBounds['t_stop'] * pq.s <= timeSlice[1])
  1936                                                   requestedSegs = segBounds.loc[segMask, :]
  1937                                               else:
  1938                                                   timeSlice = (None, None)
  1939                                                   requestedSegs = segBounds
  1940                                               return segBounds, requestedSegs

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: findSegsIncluded at line 1942

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1942                                           @profile
  1943                                           def findSegsIncluded(
  1944                                                   block, timeSlice=None):
  1945                                               segBoundsList = []
  1946                                               for segIdx, seg in enumerate(block.segments):
  1947                                                   segBoundsList.append(pd.DataFrame({
  1948                                                       't_start': seg.t_start,
  1949                                                       't_stop': seg.t_stop
  1950                                                       }, index=[segIdx]))
  1951                                           
  1952                                               segBounds = pd.concat(segBoundsList)
  1953                                               if timeSlice[0] is not None:
  1954                                                   segMask = (segBounds['t_start'] * pq.s <= timeSlice[0]) | (
  1955                                                       segBounds['t_stop'] * pq.s >= timeSlice[1])
  1956                                                   requestedSegs = segBounds.loc[segMask, :]
  1957                                               else:
  1958                                                   timeSlice = (None, None)
  1959                                                   requestedSegs = segBounds
  1960                                               return segBounds, requestedSegs

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getElecLookupTable at line 1962

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1962                                           @profile
  1963                                           def getElecLookupTable(
  1964                                                   block, elecIds=None):
  1965                                               lookupTableList = []
  1966                                               for metaIdx, chanIdx in enumerate(block.channel_indexes):
  1967                                                   if chanIdx.analogsignals:
  1968                                                       #  print(chanIdx.name)
  1969                                                       lookupTableList.append(pd.DataFrame({
  1970                                                           'channelNames': np.asarray(chanIdx.channel_names, dtype=np.str),
  1971                                                           'index': chanIdx.index,
  1972                                                           'metaIndex': metaIdx * chanIdx.index**0,
  1973                                                           'localIndex': (
  1974                                                               list(range(chanIdx.analogsignals[0].shape[1])))
  1975                                                           }))
  1976                                               lookupTable = pd.concat(lookupTableList, ignore_index=True)
  1977                                           
  1978                                               if elecIds is None:
  1979                                                   requestedIndices = lookupTable
  1980                                               else:
  1981                                                   if isinstance(elecIds[0], str):
  1982                                                       idxMask = lookupTable['channelNames'].isin(elecIds)
  1983                                                       requestedIndices = lookupTable.loc[idxMask, :]
  1984                                               return lookupTable, requestedIndices

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getNIXData at line 1986

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1986                                           @profile
  1987                                           def getNIXData(
  1988                                                   fileName=None,
  1989                                                   folderPath=None,
  1990                                                   reader=None, blockIdx=0,
  1991                                                   elecIds=None, startTime_s=None,
  1992                                                   dataLength_s=None, downsample=1,
  1993                                                   signal_group_mode='group-by-same-units',
  1994                                                   closeReader=False):
  1995                                               #  Open file and extract headers
  1996                                               if reader is None:
  1997                                                   assert (fileName is not None) and (folderPath is not None)
  1998                                                   filePath = os.path.join(folderPath, fileName) + '.nix'
  1999                                                   reader = nixio_fr.NixIO(filename=filePath)
  2000                                           
  2001                                               block = reader.read_block(
  2002                                                   block_index=blockIdx, lazy=True,
  2003                                                   signal_group_mode=signal_group_mode)
  2004                                           
  2005                                               for segIdx, seg in enumerate(block.segments):
  2006                                                   seg.events = [i.load() for i in seg.events]
  2007                                                   seg.epochs = [i.load() for i in seg.epochs]
  2008                                           
  2009                                               # find elecIds
  2010                                               lookupTable, requestedIndices = getElecLookupTable(
  2011                                                   block, elecIds=elecIds)
  2012                                           
  2013                                               # find segments that contain the requested times
  2014                                               if dataLength_s is not None:
  2015                                                   assert startTime_s is not None
  2016                                                   timeSlice = (
  2017                                                       startTime_s * pq.s,
  2018                                                       (startTime_s + dataLength_s) * pq.s)
  2019                                               else:
  2020                                                   timeSlice = (None, None)
  2021                                               segBounds, requestedSegs = findSegsIncluding(block, timeSlice)
  2022                                               #
  2023                                               data = pd.DataFrame(columns=elecIds + ['t'])
  2024                                               for segIdx in requestedSegs.index:
  2025                                                   seg = block.segments[segIdx]
  2026                                                   if dataLength_s is not None:
  2027                                                       timeSlice = (
  2028                                                           max(timeSlice[0], seg.t_start),
  2029                                                           min(timeSlice[1], seg.t_stop)
  2030                                                           )
  2031                                                   else:
  2032                                                       timeSlice = (seg.t_start, seg.t_stop)
  2033                                                   segData = pd.DataFrame()
  2034                                                   for metaIdx in pd.unique(requestedIndices['metaIndex']):
  2035                                                       metaIdxMatch = requestedIndices['metaIndex'] == metaIdx
  2036                                                       theseRequestedIndices = requestedIndices.loc[
  2037                                                           metaIdxMatch, :]
  2038                                                       theseElecIds = theseRequestedIndices['channelNames']
  2039                                                       asig = seg.analogsignals[metaIdx]
  2040                                                       thisTimeSlice = (
  2041                                                           max(timeSlice[0], asig.t_start),
  2042                                                           min(timeSlice[1], asig.t_stop)
  2043                                                           )
  2044                                                       reqData = asig.load(
  2045                                                           time_slice=thisTimeSlice,
  2046                                                           channel_indexes=theseRequestedIndices['localIndex'].to_numpy())
  2047                                                       segData = pd.concat((
  2048                                                               segData,
  2049                                                               pd.DataFrame(
  2050                                                                   reqData.magnitude, columns=theseElecIds.to_numpy())),
  2051                                                           axis=1)
  2052                                                   segT = reqData.times
  2053                                                   segData['t'] = segT
  2054                                                   data = pd.concat(
  2055                                                       (data, segData),
  2056                                                       axis=0, ignore_index=True)
  2057                                               channelData = {
  2058                                                   'data': data,
  2059                                                   't': data['t']
  2060                                                   }
  2061                                               if closeReader:
  2062                                                   reader.file.close()
  2063                                                   block = None
  2064                                                   # closing the reader breaks its connection to the block
  2065                                               return channelData, block

Total time: 0.0001412 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: childBaseName at line 2067

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2067                                           @profile
  2068                                           def childBaseName(
  2069                                                   childName, searchTerm):
  2070       136        621.0      4.6     44.0      if searchTerm in childName:
  2071                                                   baseName = '_'.join(childName.split('_')[1:])
  2072                                               else:
  2073       136        427.0      3.1     30.2          baseName = childName
  2074       136        364.0      2.7     25.8      return baseName

Total time: 2.87662 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: readBlockFixNames at line 2076

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2076                                           @profile
  2077                                           def readBlockFixNames(
  2078                                                   rawioReader,
  2079                                                   block_index=0, signal_group_mode='split-all',
  2080                                                   lazy=True, mapDF=None, reduceChannelIndexes=False,
  2081                                                   loadList=None, purgeNixNames=False
  2082                                                   ):
  2083         2         42.0     21.0      0.0      headerSignalChan = pd.DataFrame(
  2084         2      92094.0  46047.0      0.3          rawioReader.header['signal_channels']).set_index('id')
  2085         2         61.0     30.5      0.0      headerUnitChan = pd.DataFrame(
  2086         2      82273.0  41136.5      0.3          rawioReader.header['unit_channels']).set_index('id')
  2087         2         69.0     34.5      0.0      dataBlock = rawioReader.read_block(
  2088         2         28.0     14.0      0.0          block_index=block_index, lazy=lazy,
  2089         2   28442653.0 14221326.5     98.9          signal_group_mode=signal_group_mode)
  2090         2         55.0     27.5      0.0      if dataBlock.name is None:
  2091         2         37.0     18.5      0.0          if 'neo_name' in dataBlock.annotations:
  2092         2         32.0     16.0      0.0              dataBlock.name = dataBlock.annotations['neo_name']
  2093                                               #  on first segment, rename the chan_indexes and units
  2094         2         31.0     15.5      0.0      seg0 = dataBlock.segments[0]
  2095                                               asigLikeList = (
  2096         2       1257.0    628.5      0.0          seg0.filter(objects=AnalogSignalProxy) +
  2097         2       1113.0    556.5      0.0          seg0.filter(objects=AnalogSignal))
  2098         2         30.0     15.0      0.0      if mapDF is not None:
  2099                                                   if headerSignalChan.size > 0:
  2100                                                       asigNameChanger = {}
  2101                                                       for nevID in mapDF['nevID']:
  2102                                                           if int(nevID) in headerSignalChan.index:
  2103                                                               labelFromMap = (
  2104                                                                   mapDF
  2105                                                                   .loc[mapDF['nevID'] == nevID, 'label']
  2106                                                                   .iloc[0])
  2107                                                               asigNameChanger[
  2108                                                                   headerSignalChan.loc[int(nevID), 'name']] = labelFromMap
  2109                                                   else:
  2110                                                       asigOrigNames = np.unique(
  2111                                                           [i.split('#')[0] for i in headerUnitChan['name']])
  2112                                                       asigNameChanger = {}
  2113                                                       for origName in asigOrigNames:
  2114                                                           # ripple specific
  2115                                                           formattedName = origName.replace('.', '_').replace(' raw', '')
  2116                                                           if mapDF['label'].str.contains(formattedName).any():
  2117                                                               asigNameChanger[origName] = formattedName
  2118                                               else:
  2119         2         34.0     17.0      0.0          asigNameChanger = dict()
  2120        59        779.0     13.2      0.0      for asig in asigLikeList:
  2121        57       2762.0     48.5      0.0          asigBaseName = childBaseName(asig.name, 'seg')
  2122                                                   asig.name = (
  2123                                                       asigNameChanger[asigBaseName]
  2124        57        807.0     14.2      0.0              if asigBaseName in asigNameChanger
  2125        57        778.0     13.6      0.0              else asigBaseName)
  2126        57        771.0     13.5      0.0          if mapDF is not None:
  2127                                                       if (mapDF['label'] == asig.name).any():
  2128                                                           asig.annotations['xCoords'] = float(mapDF.loc[mapDF['label'] == asig.name, 'xcoords'].iloc[0])
  2129                                                           asig.annotations['yCoords'] = float(mapDF.loc[mapDF['label'] == asig.name, 'ycoords'].iloc[0])
  2130                                                           asig.annotations['zCoords'] = float(mapDF.loc[mapDF['label'] == asig.name, 'zcoords'].iloc[0])
  2131        57        862.0     15.1      0.0          if 'Channel group ' in asig.channel_index.name:
  2132                                                       newChanName = (
  2133                                                           asigNameChanger[asigBaseName]
  2134        57        771.0     13.5      0.0                  if asigBaseName in asigNameChanger
  2135        57        760.0     13.3      0.0                  else asigBaseName)
  2136        57        801.0     14.1      0.0              asig.channel_index.name = newChanName
  2137        57        915.0     16.1      0.0              if 'neo_name' in asig.channel_index.annotations:
  2138        57        844.0     14.8      0.0                  asig.channel_index.annotations['neo_name'] = newChanName
  2139        57        846.0     14.8      0.0              if 'nix_name' in asig.channel_index.annotations:
  2140        57        841.0     14.8      0.0                  asig.channel_index.annotations['nix_name'] = newChanName
  2141        57        748.0     13.1      0.0              if mapDF is not None:
  2142                                                           try:
  2143                                                               asig.channel_index.coordinates = np.asarray([
  2144                                                                   asig.annotations['xCoords'], asig.annotations['yCoords'], asig.annotations['zCoords']
  2145                                                               ])[np.newaxis, :] * pq.um
  2146                                                           except Exception:
  2147                                                               pass
  2148                                               spikeTrainLikeList = (
  2149         2       1072.0    536.0      0.0          seg0.filter(objects=SpikeTrainProxy) +
  2150         2       1073.0    536.5      0.0          seg0.filter(objects=SpikeTrain))
  2151                                               # add channels for channelIndex that has no asigs but has spikes
  2152         2         27.0     13.5      0.0      nExtraChans = 0
  2153        13        186.0     14.3      0.0      for stp in spikeTrainLikeList:
  2154        11        647.0     58.8      0.0          stpBaseName = childBaseName(stp.name, 'seg')
  2155        11       5436.0    494.2      0.0          nameParser = re.search(r'ch(\d*)#(\d*)', stpBaseName)
  2156        11        160.0     14.5      0.0          if nameParser is not None:
  2157                                                       # first time at this unit, rename it
  2158                                                       chanId = int(nameParser.group(1))
  2159                                                       unitId = int(nameParser.group(2))
  2160                                                       if chanId >= 5121:
  2161                                                           isRippleStimChan = True
  2162                                                           chanId = chanId - 5120
  2163                                                       else:
  2164                                                           isRippleStimChan = False
  2165                                                       ####################
  2166                                                       # asigBaseName = headerSignalChan.loc[chanId, 'name']
  2167                                                       # if mapDF is not None:
  2168                                                       #     if asigBaseName in asigNameChanger:
  2169                                                       #         chanIdLabel = (
  2170                                                       #             asigNameChanger[asigBaseName]
  2171                                                       #             if asigBaseName in asigNameChanger
  2172                                                       #             else asigBaseName)
  2173                                                       #     else:
  2174                                                       #         chanIdLabel = asigBaseName
  2175                                                       # else:
  2176                                                       #     chanIdLabel = asigBaseName
  2177                                                       ###################
  2178                                                       # if swapMaps is not None:
  2179                                                       #     nameCandidates = (swapMaps['to'].loc[swapMaps['to']['nevID'] == chanId, 'label']).to_list()
  2180                                                       # elif mapDF is not None:
  2181                                                       #     nameCandidates = (mapDF.loc[mapDF['nevID'] == chanId, 'label']).to_list()
  2182                                                       # else:
  2183                                                       #     nameCandidates = []
  2184                                                       ##############################
  2185                                                       if mapDF is not None:
  2186                                                           nameCandidates = (
  2187                                                               mapDF
  2188                                                               .loc[mapDF['nevID'] == chanId, 'label']
  2189                                                               .to_list())
  2190                                                       else:
  2191                                                           nameCandidates = []
  2192                                                       if len(nameCandidates) == 1:
  2193                                                           chanIdLabel = nameCandidates[0]
  2194                                                       elif chanId in headerSignalChan:
  2195                                                           chanIdLabel = headerSignalChan.loc[chanId, 'name']
  2196                                                       else:
  2197                                                           chanIdLabel = 'ch{}'.format(chanId)
  2198                                                       #
  2199                                                       if isRippleStimChan:
  2200                                                           stp.name = '{}_stim#{}'.format(chanIdLabel, unitId)
  2201                                                       else:
  2202                                                           stp.name = '{}#{}'.format(chanIdLabel, unitId)
  2203                                                       stp.unit.name = stp.name
  2204                                                   ########################################
  2205                                                   # sanitize ripple names ####
  2206        11        217.0     19.7      0.0          stp.name = stp.name.replace('.', '_').replace(' raw', '')
  2207        11        205.0     18.6      0.0          stp.unit.name = stp.unit.name.replace('.', '_').replace(' raw', '')
  2208                                                   ###########################################
  2209        11        181.0     16.5      0.0          if 'ChannelIndex for ' in stp.unit.channel_index.name:
  2210        11        191.0     17.4      0.0              newChanName = stp.name.replace('_stim#0', '')
  2211                                                       # remove unit #
  2212        11       1652.0    150.2      0.0              newChanName = re.sub(r'#\d', '', newChanName)
  2213        11        167.0     15.2      0.0              stp.unit.channel_index.name = newChanName
  2214                                                       # units and analogsignals have different channel_indexes when loaded by nix
  2215                                                       # add them to each other's parent list
  2216        11        164.0     14.9      0.0              allMatchingChIdx = dataBlock.filter(
  2217        11      75567.0   6869.7      0.3                  objects=ChannelIndex, name=newChanName)
  2218        11        200.0     18.2      0.0              if (len(allMatchingChIdx) > 1) and reduceChannelIndexes:
  2219                                                           assert len(allMatchingChIdx) == 2
  2220                                                           targetChIdx = [
  2221                                                               ch
  2222                                                               for ch in allMatchingChIdx
  2223                                                               if ch is not stp.unit.channel_index][0]
  2224                                                           oldChIdx = stp.unit.channel_index
  2225                                                           targetChIdx.units.append(stp.unit)
  2226                                                           stp.unit.channel_index = targetChIdx
  2227                                                           oldChIdx.units.remove(stp.unit)
  2228                                                           if not (len(oldChIdx.units) or len(oldChIdx.analogsignals)):
  2229                                                               dataBlock.channel_indexes.remove(oldChIdx)
  2230                                                           del oldChIdx
  2231                                                           targetChIdx.create_relationship()
  2232        11        160.0     14.5      0.0              elif reduceChannelIndexes:
  2233        11       3260.0    296.4      0.0                  if newChanName not in headerSignalChan['name']:
  2234        11        175.0     15.9      0.0                      stp.unit.channel_index.index = np.asarray(
  2235        11       1461.0    132.8      0.0                          [headerSignalChan['name'].size + nExtraChans])
  2236        11        164.0     14.9      0.0                      stp.unit.channel_index.channel_ids = np.asarray(
  2237        11       1172.0    106.5      0.0                          [headerSignalChan['name'].size + nExtraChans])
  2238        11        163.0     14.8      0.0                      stp.unit.channel_index.channel_names = np.asarray(
  2239        11        451.0     41.0      0.0                          [newChanName])
  2240        11        164.0     14.9      0.0                      nExtraChans += 1
  2241        11        179.0     16.3      0.0                  if 'neo_name' not in allMatchingChIdx[0].annotations:
  2242        11        175.0     15.9      0.0                      allMatchingChIdx[0].annotations['neo_name'] = allMatchingChIdx[0].name
  2243        11        165.0     15.0      0.0                  if 'nix_name' not in allMatchingChIdx[0].annotations:
  2244        11        168.0     15.3      0.0                      allMatchingChIdx[0].annotations['nix_name'] = allMatchingChIdx[0].name
  2245        11        255.0     23.2      0.0          stp.unit.channel_index.name = stp.unit.channel_index.name.replace('.', '_').replace(' raw', '')
  2246                                               #  rename the children
  2247                                               typesNeedRenaming = [
  2248         2         33.0     16.5      0.0          SpikeTrainProxy, AnalogSignalProxy, EventProxy,
  2249         2         34.0     17.0      0.0          SpikeTrain, AnalogSignal, Event]
  2250         4         75.0     18.8      0.0      for segIdx, seg in enumerate(dataBlock.segments):
  2251         2         31.0     15.5      0.0          if seg.name is None:
  2252         2         68.0     34.0      0.0              seg.name = 'seg{}_'.format(segIdx)
  2253                                                   else:
  2254                                                       if 'seg{}_'.format(segIdx) not in seg.name:
  2255                                                           seg.name = (
  2256                                                               'seg{}_{}'
  2257                                                               .format(
  2258                                                                   segIdx,
  2259                                                                   childBaseName(seg.name, 'seg')))
  2260        14        208.0     14.9      0.0          for objType in typesNeedRenaming:
  2261        88       7493.0     85.1      0.0              for child in seg.filter(objects=objType):
  2262        76       1390.0     18.3      0.0                  if 'seg{}_'.format(segIdx) not in child.name:
  2263                                                               child.name = (
  2264        68        957.0     14.1      0.0                          'seg{}_{}'
  2265                                                                   .format(
  2266        68       3585.0     52.7      0.0                              segIdx, childBaseName(child.name, 'seg')))
  2267                                                           #  todo: decide if below is needed
  2268                                                           #  elif 'seg' in child.name:
  2269                                                           #      childBaseName = '_'.join(child.name.split('_')[1:])
  2270                                                           #      child.name = 'seg{}_{}'.format(segIdx, childBaseName)
  2271                                               # [i.name for i in dataBlock.filter(objects=Unit)]
  2272                                               # [i.name for i in dataBlock.filter(objects=ChannelIndex)]
  2273                                               # [i.name for i in dataBlock.filter(objects=SpikeTrain)]
  2274                                               # [i.name for i in dataBlock.filter(objects=SpikeTrainProxy)]
  2275         2         28.0     14.0      0.0      if lazy:
  2276        13      23639.0   1818.4      0.1          for stP in dataBlock.filter(objects=SpikeTrainProxy):
  2277        11        170.0     15.5      0.0              if 'unitAnnotations' in stP.annotations:
  2278                                                           unAnnStr = stP.annotations['unitAnnotations']
  2279                                                           stP.unit.annotations.update(json.loads(unAnnStr))
  2280         2         33.0     16.5      0.0      if (loadList is not None) and lazy:
  2281                                                   if 'asigs' in loadList:
  2282                                                       loadAsigList(
  2283                                                           dataBlock, listOfAsigProxyNames=loadList['asigs'],
  2284                                                           replaceInParents=True)
  2285                                                   if 'events' in loadList:
  2286                                                       loadEventList(
  2287                                                           dataBlock,
  2288                                                           listOfEventNames=loadList['events'],
  2289                                                           replaceInParents=True)
  2290                                                   if 'spiketrains' in loadList:
  2291                                                       loadSpikeTrainList(
  2292                                                           dataBlock,
  2293                                                           listOfSpikeTrainNames=loadList['spiketrains'],
  2294                                                           replaceInParents=True)
  2295         2         30.0     15.0      0.0      if purgeNixNames:
  2296                                                   dataBlock = purgeNixAnn(dataBlock)
  2297         2         26.0     13.0      0.0      return dataBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadSpikeTrainList at line 2299

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2299                                           @profile
  2300                                           def loadSpikeTrainList(
  2301                                                   dataBlock, listOfSpikeTrainNames=None,
  2302                                                   replaceInParents=True):
  2303                                               listOfSpikeTrains = []
  2304                                               if listOfSpikeTrainNames is None:
  2305                                                   listOfSpikeTrainNames = [
  2306                                                       stp.name
  2307                                                       for stp in dataBlock.filter(objects=SpikeTrainProxy)]
  2308                                               for stP in dataBlock.filter(objects=SpikeTrainProxy):
  2309                                                   if stP.name in listOfSpikeTrainNames:
  2310                                                       st = loadObjArrayAnn(stP.load())
  2311                                                       listOfSpikeTrains.append(st)
  2312                                                       if replaceInParents:
  2313                                                           seg = stP.segment
  2314                                                           segStNames = [s.name for s in seg.spiketrains]
  2315                                                           idxInSeg = segStNames.index(stP.name)
  2316                                                           seg.spiketrains[idxInSeg] = st
  2317                                                           #
  2318                                                           unit = stP.unit
  2319                                                           unitStNames = [s.name for s in unit.spiketrains]
  2320                                                           st.unit = unit
  2321                                                           idxInUnit = unitStNames.index(stP.name)
  2322                                                           unit.spiketrains[idxInUnit] = st
  2323                                               return listOfSpikeTrains

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadEventList at line 2325

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2325                                           @profile
  2326                                           def loadEventList(
  2327                                                   dataBlock,
  2328                                                   listOfEventNames=None, replaceInParents=True):
  2329                                               listOfEvents = []
  2330                                               if listOfEventNames is None:
  2331                                                   listOfEventNames = [
  2332                                                       evp.name
  2333                                                       for evp in dataBlock.filter(objects=EventProxy)]
  2334                                               for evP in dataBlock.filter(objects=EventProxy):
  2335                                                   if evP.name in listOfEventNames:
  2336                                                       ev = loadObjArrayAnn(evP.load())
  2337                                                       listOfEvents.append(ev)
  2338                                                       if replaceInParents:
  2339                                                           seg = evP.segment
  2340                                                           segEvNames = [e.name for e in seg.events]
  2341                                                           idxInSeg = segEvNames.index(evP.name)
  2342                                                           seg.events[idxInSeg] = ev
  2343                                               return listOfEvents

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadAsigList at line 2345

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2345                                           @profile
  2346                                           def loadAsigList(
  2347                                                   dataBlock, listOfAsigProxyNames=None, replaceInParents=True):
  2348                                               listOfAsigs = []
  2349                                               if listOfAsigProxyNames is None:
  2350                                                   listOfAsigProxyNames = [
  2351                                                       asigp.name
  2352                                                       for asigp in dataBlock.filter(objects=AnalogSignalProxy)]
  2353                                               for asigP in dataBlock.filter(objects=AnalogSignalProxy):
  2354                                                   if asigP.name in listOfAsigProxyNames:
  2355                                                       asig = asigP.load()
  2356                                                       asig.annotations = asigP.annotations.copy()
  2357                                                       listOfAsigs.append(asig)
  2358                                                       #
  2359                                                       if replaceInParents:
  2360                                                           seg = asigP.segment
  2361                                                           segAsigNames = [ag.name for ag in seg.analogsignals]
  2362                                                           asig.segment = seg
  2363                                                           idxInSeg = segAsigNames.index(asigP.name)
  2364                                                           seg.analogsignals[idxInSeg] = asig
  2365                                                           #
  2366                                                           chIdx = asigP.channel_index
  2367                                                           chIdxAsigNames = [ag.name for ag in chIdx.analogsignals]
  2368                                                           asig.channel_index = chIdx
  2369                                                           idxInChIdx = chIdxAsigNames.index(asigP.name)
  2370                                                           chIdx.analogsignals[idxInChIdx] = asig
  2371                                               return listOfAsigs

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: addBlockToNIX at line 2373

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2373                                           @profile
  2374                                           def addBlockToNIX(
  2375                                                   newBlock, neoSegIdx=[0],
  2376                                                   writeAsigs=True, writeSpikes=True, writeEvents=True,
  2377                                                   asigNameList=None,
  2378                                                   purgeNixNames=False,
  2379                                                   fileName=None,
  2380                                                   folderPath=None,
  2381                                                   nixBlockIdx=0, nixSegIdx=[0],
  2382                                                   ):
  2383                                               #  base file name
  2384                                               trialBasePath = os.path.join(folderPath, fileName)
  2385                                               if writeAsigs:
  2386                                                   # peek at file to ensure compatibility
  2387                                                   reader = nixio_fr.NixIO(filename=trialBasePath + '.nix')
  2388                                                   tempBlock = reader.read_block(
  2389                                                       block_index=nixBlockIdx,
  2390                                                       lazy=True, signal_group_mode='split-all')
  2391                                                   checkCompatible = {i: False for i in nixSegIdx}
  2392                                                   forceShape = {i: None for i in nixSegIdx}
  2393                                                   forceType = {i: None for i in nixSegIdx}
  2394                                                   forceFS = {i: None for i in nixSegIdx}
  2395                                                   for nixIdx in nixSegIdx:
  2396                                                       tempAsigList = tempBlock.segments[nixIdx].filter(
  2397                                                           objects=AnalogSignalProxy)
  2398                                                       if len(tempAsigList) > 0:
  2399                                                           tempAsig = tempAsigList[0]
  2400                                                           checkCompatible[nixIdx] = True
  2401                                                           forceType[nixIdx] = tempAsig.dtype
  2402                                                           forceShape[nixIdx] = tempAsig.shape[0]  # ? docs say shape[1], but that's confusing
  2403                                                           forceFS[nixIdx] = tempAsig.sampling_rate
  2404                                                   reader.file.close()
  2405                                               #  if newBlock was loaded from a nix file, strip the old nix_names away:
  2406                                               #  todo: replace with function from this module
  2407                                               if purgeNixNames:
  2408                                                   newBlock = purgeNixAnn(newBlock)
  2409                                               #
  2410                                               writer = NixIO(filename=trialBasePath + '.nix')
  2411                                               nixblock = writer.nix_file.blocks[nixBlockIdx]
  2412                                               nixblockName = nixblock.name
  2413                                               if 'nix_name' in newBlock.annotations.keys():
  2414                                                   try:
  2415                                                       assert newBlock.annotations['nix_name'] == nixblockName
  2416                                                   except Exception:
  2417                                                       newBlock.annotations['nix_name'] = nixblockName
  2418                                               else:
  2419                                                   newBlock.annotate(nix_name=nixblockName)
  2420                                               #
  2421                                               for idx, segIdx in enumerate(neoSegIdx):
  2422                                                   nixIdx = nixSegIdx[idx]
  2423                                                   newSeg = newBlock.segments[segIdx]
  2424                                                   nixgroup = nixblock.groups[nixIdx]
  2425                                                   nixSegName = nixgroup.name
  2426                                                   if 'nix_name' in newSeg.annotations.keys():
  2427                                                       try:
  2428                                                           assert newSeg.annotations['nix_name'] == nixSegName
  2429                                                       except Exception:
  2430                                                           newSeg.annotations['nix_name'] = nixSegName
  2431                                                   else:
  2432                                                       newSeg.annotate(nix_name=nixSegName)
  2433                                                   #
  2434                                                   if writeEvents:
  2435                                                       eventList = newSeg.events
  2436                                                       eventOrder = np.argsort([i.name for i in eventList])
  2437                                                       for event in [eventList[i] for i in eventOrder]:
  2438                                                           event = writer._write_event(event, nixblock, nixgroup)
  2439                                                   #
  2440                                                   if writeAsigs:
  2441                                                       asigList = newSeg.filter(objects=AnalogSignal)
  2442                                                       asigOrder = np.argsort([i.name for i in asigList])
  2443                                                       for asig in [asigList[i] for i in asigOrder]:
  2444                                                           if checkCompatible[nixIdx]:
  2445                                                               assert asig.dtype == forceType[nixIdx]
  2446                                                               assert asig.sampling_rate == forceFS[nixIdx]
  2447                                                               #  print('asig.shape[0] = {}'.format(asig.shape[0]))
  2448                                                               #  print('forceShape[nixIdx] = {}'.format(forceShape[nixIdx]))
  2449                                                               assert asig.shape[0] == forceShape[nixIdx]
  2450                                                           asig = writer._write_analogsignal(asig, nixblock, nixgroup)
  2451                                                       #  for isig in newSeg.filter(objects=IrregularlySampledSignal):
  2452                                                       #      isig = writer._write_irregularlysampledsignal(
  2453                                                       #          isig, nixblock, nixgroup)
  2454                                                   #
  2455                                                   if writeSpikes:
  2456                                                       stList = newSeg.filter(objects=SpikeTrain)
  2457                                                       stOrder = np.argsort([i.name for i in stList])
  2458                                                       for st in [stList[i] for i in stOrder]:
  2459                                                           st = writer._write_spiketrain(st, nixblock, nixgroup)
  2460                                               #
  2461                                               for chanIdx in newBlock.filter(objects=ChannelIndex):
  2462                                                   chanIdx = writer._write_channelindex(chanIdx, nixblock)
  2463                                                   #  auto descends into units inside of _write_channelindex
  2464                                               writer._create_source_links(newBlock, nixblock)
  2465                                               writer.close()
  2466                                               print('Done adding block to Nix.')
  2467                                               return newBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadStProxy at line 2469

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2469                                           @profile
  2470                                           def loadStProxy(stProxy):
  2471                                               try:
  2472                                                   st = stProxy.load(
  2473                                                       magnitude_mode='rescaled',
  2474                                                       load_waveforms=True)
  2475                                               except Exception:
  2476                                                   st = stProxy.load(
  2477                                                       magnitude_mode='rescaled',
  2478                                                       load_waveforms=False)
  2479                                                   st.waveforms = np.asarray([]).reshape((0, 0, 0))*pq.mV
  2480                                               return st

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: preproc at line 2482

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2482                                           @profile
  2483                                           def preproc(
  2484                                                   fileName='Trial001',
  2485                                                   rawFolderPath='./',
  2486                                                   outputFolderPath='./', mapDF=None,
  2487                                                   # swapMaps=None,
  2488                                                   electrodeArrayName='utah',
  2489                                                   fillOverflow=True, removeJumps=True,
  2490                                                   removeMeanAcross=False,
  2491                                                   linearDetrend=False,
  2492                                                   interpolateOutliers=False, calcOutliers=False,
  2493                                                   outlierMaskFilterOpts=None,
  2494                                                   outlierThreshold=1,
  2495                                                   calcArtifactTrace=False,
  2496                                                   motorEncoderMask=None,
  2497                                                   calcAverageLFP=False,
  2498                                                   eventInfo=None,
  2499                                                   spikeSourceType='', spikePath=None,
  2500                                                   chunkSize=1800, equalChunks=True, chunkList=None, chunkOffset=0,
  2501                                                   writeMode='rw',
  2502                                                   signal_group_mode='split-all', trialInfo=None,
  2503                                                   asigNameList=None, ainpNameList=None, nameSuffix='',
  2504                                                   saveFromAsigNameList=True,
  2505                                                   calcRigEvents=True, normalizeByImpedance=False,
  2506                                                   LFPFilterOpts=None, encoderCountPerDegree=180e2,
  2507                                                   outlierRemovalDebugFlag=False, impedanceFilePath=None
  2508                                                   ):
  2509                                               #  base file name
  2510                                               rawBasePath = os.path.join(rawFolderPath, fileName)
  2511                                               outputFilePath = os.path.join(
  2512                                                   outputFolderPath,
  2513                                                   fileName + nameSuffix + '.nix')
  2514                                               if os.path.exists(outputFilePath):
  2515                                                   os.remove(outputFilePath)
  2516                                               #  instantiate reader, get metadata
  2517                                               print('Loading\n{}\n'.format(rawBasePath))
  2518                                               reader = BlackrockIO(
  2519                                                   filename=rawBasePath, nsx_to_load=5)
  2520                                               reader.parse_header()
  2521                                               # metadata = reader.header
  2522                                               #  absolute section index
  2523                                               dummyBlock = readBlockFixNames(
  2524                                                   reader,
  2525                                                   block_index=0, lazy=True,
  2526                                                   signal_group_mode=signal_group_mode,
  2527                                                   mapDF=mapDF, reduceChannelIndexes=True,
  2528                                                   # swapMaps=swapMaps
  2529                                                   )
  2530                                               segLen = dummyBlock.segments[0].analogsignals[0].shape[0] / (
  2531                                                   dummyBlock.segments[0].analogsignals[0].sampling_rate)
  2532                                               nChunks = math.ceil(segLen / chunkSize)
  2533                                               #
  2534                                               if equalChunks:
  2535                                                   actualChunkSize = (segLen / nChunks).magnitude
  2536                                               else:
  2537                                                   actualChunkSize = chunkSize
  2538                                               if chunkList is None:
  2539                                                   chunkList = range(nChunks)
  2540                                               chunkingMetadata = {}
  2541                                               for chunkIdx in chunkList:
  2542                                                   print('preproc on chunk {}'.format(chunkIdx))
  2543                                                   #  instantiate spike reader if requested
  2544                                                   if spikeSourceType == 'tdc':
  2545                                                       if spikePath is None:
  2546                                                           spikePath = os.path.join(
  2547                                                               outputFolderPath, 'tdc_' + fileName,
  2548                                                               'tdc_' + fileName + '.nix')
  2549                                                       print('loading {}'.format(spikePath))
  2550                                                       spikeReader = nixio_fr.NixIO(filename=spikePath)
  2551                                                   else:
  2552                                                       spikeReader = None
  2553                                                   #  absolute section index
  2554                                                   block = readBlockFixNames(
  2555                                                       reader,
  2556                                                       block_index=0, lazy=True,
  2557                                                       signal_group_mode=signal_group_mode,
  2558                                                       mapDF=mapDF, reduceChannelIndexes=True,
  2559                                                       # swapMaps=swapMaps
  2560                                                       )
  2561                                                   if spikeReader is not None:
  2562                                                       spikeBlock = readBlockFixNames(
  2563                                                           spikeReader, block_index=0, lazy=True,
  2564                                                           signal_group_mode=signal_group_mode,
  2565                                                           mapDF=mapDF, reduceChannelIndexes=True,
  2566                                                           # swapMaps=swapMaps
  2567                                                           )
  2568                                                       spikeBlock = purgeNixAnn(spikeBlock)
  2569                                                   else:
  2570                                                       spikeBlock = None
  2571                                                   #
  2572                                                   #  instantiate writer
  2573                                                   if (nChunks == 1) or (len(chunkList) == 1):
  2574                                                       partNameSuffix = ""
  2575                                                       thisChunkOutFilePath = outputFilePath
  2576                                                   else:
  2577                                                       partNameSuffix = '_pt{:0>3}'.format(chunkIdx)
  2578                                                       thisChunkOutFilePath = (
  2579                                                           outputFilePath
  2580                                                           .replace('.nix', partNameSuffix + '.nix'))
  2581                                                   #
  2582                                                   if os.path.exists(thisChunkOutFilePath):
  2583                                                       os.remove(thisChunkOutFilePath)
  2584                                                   writer = NixIO(
  2585                                                       filename=thisChunkOutFilePath, mode=writeMode)
  2586                                                   chunkTStart = chunkIdx * actualChunkSize + chunkOffset
  2587                                                   chunkTStop = (chunkIdx + 1) * actualChunkSize + chunkOffset
  2588                                                   chunkingMetadata[chunkIdx] = {
  2589                                                       'filename': thisChunkOutFilePath,
  2590                                                       'partNameSuffix': partNameSuffix,
  2591                                                       'chunkTStart': chunkTStart,
  2592                                                       'chunkTStop': chunkTStop}
  2593                                                   block.annotate(chunkTStart=chunkTStart)
  2594                                                   block.annotate(chunkTStop=chunkTStop)
  2595                                                   block.annotate(
  2596                                                       recDatetimeStr=(
  2597                                                           block
  2598                                                           .rec_datetime
  2599                                                           .replace(tzinfo=timezone.utc)
  2600                                                           .isoformat())
  2601                                                       )
  2602                                                   #
  2603                                                   preprocBlockToNix(
  2604                                                       block, writer,
  2605                                                       chunkTStart=chunkTStart,
  2606                                                       chunkTStop=chunkTStop,
  2607                                                       fillOverflow=fillOverflow,
  2608                                                       removeJumps=removeJumps,
  2609                                                       interpolateOutliers=interpolateOutliers,
  2610                                                       calcOutliers=calcOutliers,
  2611                                                       outlierThreshold=outlierThreshold,
  2612                                                       outlierMaskFilterOpts=outlierMaskFilterOpts,
  2613                                                       calcArtifactTrace=calcArtifactTrace,
  2614                                                       linearDetrend=linearDetrend,
  2615                                                       motorEncoderMask=motorEncoderMask,
  2616                                                       electrodeArrayName=electrodeArrayName,
  2617                                                       calcAverageLFP=calcAverageLFP,
  2618                                                       eventInfo=eventInfo,
  2619                                                       asigNameList=asigNameList, ainpNameList=ainpNameList,
  2620                                                       saveFromAsigNameList=saveFromAsigNameList,
  2621                                                       spikeSourceType=spikeSourceType,
  2622                                                       spikeBlock=spikeBlock,
  2623                                                       calcRigEvents=calcRigEvents,
  2624                                                       normalizeByImpedance=normalizeByImpedance,
  2625                                                       removeMeanAcross=removeMeanAcross,
  2626                                                       LFPFilterOpts=LFPFilterOpts,
  2627                                                       encoderCountPerDegree=encoderCountPerDegree,
  2628                                                       outlierRemovalDebugFlag=outlierRemovalDebugFlag,
  2629                                                       impedanceFilePath=impedanceFilePath,
  2630                                                       )
  2631                                                   #### diagnostics
  2632                                                   diagnosticFolder = os.path.join(
  2633                                                       outputFolderPath,
  2634                                                       'preprocDiagnostics',
  2635                                                       # fileName + nameSuffix + partNameSuffix
  2636                                                       )
  2637                                                   if not os.path.exists(diagnosticFolder):
  2638                                                       os.mkdir(diagnosticFolder)
  2639                                                   asigDiagnostics = {}
  2640                                                   outlierDiagnostics = {}
  2641                                                   diagnosticText = ''
  2642                                                   for asig in block.filter(objects=AnalogSignal):
  2643                                                       annNames = ['mean_removal_r2', 'mean_removal_group']
  2644                                                       for annName in annNames:
  2645                                                           if annName in asig.annotations:
  2646                                                               if asig.name not in asigDiagnostics:
  2647                                                                   asigDiagnostics[asig.name] = {}
  2648                                                               asigDiagnostics[asig.name].update({
  2649                                                                   annName: asig.annotations[annName]})
  2650                                                       annNames = [
  2651                                                           'outlierProportion', 'nDim',
  2652                                                           'noveltyThreshold', 'outlierThreshold'
  2653                                                           ]
  2654                                                       for annName in annNames:
  2655                                                           if annName in asig.annotations:
  2656                                                               if asig.name not in outlierDiagnostics:
  2657                                                                   outlierDiagnostics[asig.name] = {}
  2658                                                               outlierDiagnostics[asig.name].update({
  2659                                                                   annName: '{}'.format(asig.annotations[annName])
  2660                                                               })
  2661                                                   if removeMeanAcross:
  2662                                                       asigDiagnosticsDF = pd.DataFrame(asigDiagnostics).T
  2663                                                       asigDiagnosticsDF.sort_values(by='mean_removal_r2', inplace=True)
  2664                                                       diagnosticText += '<h2>LFP Diagnostics</h2>\n'
  2665                                                       diagnosticText += asigDiagnosticsDF.to_html()
  2666                                                       fig, ax = plt.subplots()
  2667                                                       sns.distplot(asigDiagnosticsDF['mean_removal_r2'], ax=ax)
  2668                                                       ax.set_ylabel('Count of analog signals')
  2669                                                       ax.set_xlabel('R^2 of regressing mean against signal')
  2670                                                       fig.savefig(os.path.join(
  2671                                                               diagnosticFolder,
  2672                                                               fileName + nameSuffix + partNameSuffix + '_meanRemovalR2.png'
  2673                                                           ))
  2674                                                   if interpolateOutliers:
  2675                                                       outlierDiagnosticsDF = pd.DataFrame(outlierDiagnostics).T
  2676                                                       diagnosticText += '<h2>Outlier Diagnostics</h2>\n'
  2677                                                       diagnosticText += outlierDiagnosticsDF.to_html()
  2678                                                   diagnosticTextPath = os.path.join(
  2679                                                       diagnosticFolder,
  2680                                                       fileName + nameSuffix + partNameSuffix + '_asigDiagnostics.html'
  2681                                                       )
  2682                                                   with open(diagnosticTextPath, 'w') as _f:
  2683                                                       _f.write(diagnosticText)
  2684                                                   writer.close()
  2685                                               chunkingInfoPath = os.path.join(
  2686                                                   outputFolderPath,
  2687                                                   fileName + nameSuffix +
  2688                                                   '_chunkingInfo.json'
  2689                                                   )
  2690                                               if os.path.exists(chunkingInfoPath):
  2691                                                   os.remove(chunkingInfoPath)
  2692                                               with open(chunkingInfoPath, 'w') as f:
  2693                                                   json.dump(chunkingMetadata, f)
  2694                                               return

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: preprocBlockToNix at line 2696

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2696                                           @profile
  2697                                           def preprocBlockToNix(
  2698                                                   block, writer,
  2699                                                   chunkTStart=None,
  2700                                                   chunkTStop=None,
  2701                                                   eventInfo=None,
  2702                                                   fillOverflow=False, calcAverageLFP=False,
  2703                                                   interpolateOutliers=False, calcOutliers=False,
  2704                                                   calcArtifactTrace=False,
  2705                                                   outlierMaskFilterOpts=None,
  2706                                                   useMeanToCenter=False,   # mean center? median center?
  2707                                                   linearDetrend=False,
  2708                                                   zScoreEachTrace=False,
  2709                                                   outlierThreshold=1,
  2710                                                   motorEncoderMask=None,
  2711                                                   electrodeArrayName='utah',
  2712                                                   removeJumps=False, trackMemory=True,
  2713                                                   asigNameList=None, ainpNameList=None,
  2714                                                   saveFromAsigNameList=True,
  2715                                                   spikeSourceType='', spikeBlock=None,
  2716                                                   calcRigEvents=True,
  2717                                                   normalizeByImpedance=True,
  2718                                                   impedanceFilePath=None,
  2719                                                   removeMeanAcross=False,
  2720                                                   LFPFilterOpts=None, encoderCountPerDegree=180e2,
  2721                                                   outlierRemovalDebugFlag=False,
  2722                                                   ):
  2723                                               #  prune out nev spike placeholders
  2724                                               #  (will get added back on a chunk by chunk basis,
  2725                                               #  if not pruning units)
  2726                                               if spikeSourceType == 'nev':
  2727                                                   pruneOutUnits = False
  2728                                               else:
  2729                                                   pruneOutUnits = True
  2730                                               #
  2731                                               for chanIdx in block.channel_indexes:
  2732                                                   if chanIdx.units:
  2733                                                       for unit in chanIdx.units:
  2734                                                           if unit.spiketrains:
  2735                                                               unit.spiketrains = []
  2736                                                       if pruneOutUnits:
  2737                                                           chanIdx.units = []
  2738                                               #
  2739                                               if spikeBlock is not None:
  2740                                                   for chanIdx in spikeBlock.channel_indexes:
  2741                                                       if chanIdx.units:
  2742                                                           for unit in chanIdx.units:
  2743                                                               if unit.spiketrains:
  2744                                                                   unit.spiketrains = []
  2745                                               #  precalculate new segment
  2746                                               seg = block.segments[0]
  2747                                               #  remove chanIndexes assigned to units; makes more sense to
  2748                                               #  only use chanIdx for asigs and spikes on that asig
  2749                                               #  block.channel_indexes = (
  2750                                               #      [chanIdx for chanIdx in block.channel_indexes if (
  2751                                               #          chanIdx.analogsignals)])
  2752                                               if calcAverageLFP:
  2753                                                   lastIndex = len(block.channel_indexes)
  2754                                                   lastID = block.channel_indexes[-1].channel_ids[0] + 1
  2755                                                   if asigNameList is None:
  2756                                                       asigNameList = [
  2757                                                           [
  2758                                                               childBaseName(a.name, 'seg')
  2759                                                               for a in seg.analogsignals
  2760                                                               if not (('ainp' in a.name) or ('analog' in a.name))]
  2761                                                           ]
  2762                                                   nMeanChans = len(asigNameList)
  2763                                                   #
  2764                                                   meanChIdxList = []
  2765                                                   for meanChIdx in range(nMeanChans):
  2766                                                       tempChIdx = ChannelIndex(
  2767                                                           index=[lastIndex + meanChIdx],
  2768                                                           channel_names=['{}_rawAverage_{}'.format(electrodeArrayName, meanChIdx)],
  2769                                                           channel_ids=[lastID + meanChIdx],
  2770                                                           name='{}_rawAverage_{}'.format(electrodeArrayName, meanChIdx),
  2771                                                           file_origin=block.channel_indexes[-1].file_origin
  2772                                                           )
  2773                                                       tempChIdx.merge_annotations(block.channel_indexes[-1])
  2774                                                       block.channel_indexes.append(tempChIdx)
  2775                                                       meanChIdxList.append(tempChIdx)
  2776                                                       lastIndex += 1
  2777                                                       lastID += 1
  2778                                                   lastIndex = len(block.channel_indexes)
  2779                                                   lastID = block.channel_indexes[-1].channel_ids[0] + 1
  2780                                                   # if calcArtifactTrace:
  2781                                                   if True:
  2782                                                       artChIdxList = []
  2783                                                       for artChIdx in range(nMeanChans):
  2784                                                           tempChIdx = ChannelIndex(
  2785                                                               index=[lastIndex + artChIdx],
  2786                                                               channel_names=['{}_artifact_{}'.format(electrodeArrayName, artChIdx)],
  2787                                                               channel_ids=[lastID + artChIdx],
  2788                                                               name='{}_artifact_{}'.format(electrodeArrayName, artChIdx),
  2789                                                               file_origin=block.channel_indexes[-1].file_origin
  2790                                                               )
  2791                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2792                                                           block.channel_indexes.append(tempChIdx)
  2793                                                           artChIdxList.append(tempChIdx)
  2794                                                           lastIndex += 1
  2795                                                           lastID += 1
  2796                                                   # if calcOutliers:
  2797                                                   if True:
  2798                                                       devChIdxList = []
  2799                                                       for devChIdx in range(nMeanChans):
  2800                                                           tempChIdx = ChannelIndex(
  2801                                                               index=[lastIndex + devChIdx],
  2802                                                               channel_names=['{}_deviation_{}'.format(electrodeArrayName, devChIdx)],
  2803                                                               channel_ids=[lastID + devChIdx],
  2804                                                               name='{}_deviation_{}'.format(electrodeArrayName, devChIdx),
  2805                                                               file_origin=block.channel_indexes[-1].file_origin
  2806                                                               )
  2807                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2808                                                           block.channel_indexes.append(tempChIdx)
  2809                                                           devChIdxList.append(tempChIdx)
  2810                                                           lastIndex += 1
  2811                                                           lastID += 1
  2812                                                       smDevChIdxList = []
  2813                                                       for devChIdx in range(nMeanChans):
  2814                                                           tempChIdx = ChannelIndex(
  2815                                                               index=[lastIndex + devChIdx],
  2816                                                               channel_names=['{}_smoothed_deviation_{}'.format(electrodeArrayName, devChIdx)],
  2817                                                               channel_ids=[lastID + devChIdx],
  2818                                                               name='{}_smoothed_deviation_{}'.format(electrodeArrayName, devChIdx),
  2819                                                               file_origin=block.channel_indexes[-1].file_origin
  2820                                                               )
  2821                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2822                                                           block.channel_indexes.append(tempChIdx)
  2823                                                           smDevChIdxList.append(tempChIdx)
  2824                                                           lastIndex += 1
  2825                                                           lastID += 1
  2826                                                       outMaskChIdxList = []
  2827                                                       for outMaskChIdx in range(nMeanChans):
  2828                                                           tempChIdx = ChannelIndex(
  2829                                                               index=[lastIndex + outMaskChIdx],
  2830                                                               channel_names=['{}_outlierMask_{}'.format(
  2831                                                                   electrodeArrayName, outMaskChIdx)],
  2832                                                               channel_ids=[lastID + outMaskChIdx],
  2833                                                               name='{}_outlierMask_{}'.format(
  2834                                                                   electrodeArrayName, outMaskChIdx),
  2835                                                               file_origin=block.channel_indexes[-1].file_origin
  2836                                                               )
  2837                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2838                                                           block.channel_indexes.append(tempChIdx)
  2839                                                           outMaskChIdxList.append(tempChIdx)
  2840                                                           lastIndex += 1
  2841                                                           lastID += 1
  2842                                               #  delete asig and irsig proxies from channel index list
  2843                                               for metaIdx, chanIdx in enumerate(block.channel_indexes):
  2844                                                   if chanIdx.analogsignals:
  2845                                                       chanIdx.analogsignals = []
  2846                                                   if chanIdx.irregularlysampledsignals:
  2847                                                       chanIdx.irregularlysampledsignals = []
  2848                                               newSeg = Segment(
  2849                                                       index=0, name=seg.name,
  2850                                                       description=seg.description,
  2851                                                       file_origin=seg.file_origin,
  2852                                                       file_datetime=seg.file_datetime,
  2853                                                       rec_datetime=seg.rec_datetime,
  2854                                                       **seg.annotations
  2855                                                   )
  2856                                               block.segments = [newSeg]
  2857                                               block, nixblock = writer.write_block_meta(block)
  2858                                               # descend into Segments
  2859                                               if impedanceFilePath is not None:
  2860                                                   try:
  2861                                                       impedances = prb_meta.getLatestImpedance(
  2862                                                           block=block, impedanceFilePath=impedanceFilePath)
  2863                                                       averageImpedance = impedances['impedance'].median()
  2864                                                   except Exception:
  2865                                                       traceback.print_exc()
  2866                                               # for segIdx, seg in enumerate(oldSegList):
  2867                                               if spikeBlock is not None:
  2868                                                   spikeSeg = spikeBlock.segments[0]
  2869                                               else:
  2870                                                   spikeSeg = seg
  2871                                               #
  2872                                               if trackMemory:
  2873                                                   print('memory usage: {:.1f} MB'.format(
  2874                                                       prf.memory_usage_psutil()))
  2875                                               newSeg, nixgroup = writer._write_segment_meta(newSeg, nixblock)
  2876                                               #  trim down list of analog signals if necessary
  2877                                               asigNameListSeg = []
  2878                                               if (removeMeanAcross or calcAverageLFP):
  2879                                                   meanGroups = {}
  2880                                               for subListIdx, subList in enumerate(asigNameList):
  2881                                                   subListSeg = [
  2882                                                       'seg{}_{}'.format(0, a)
  2883                                                       for a in subList]
  2884                                                   asigNameListSeg += subListSeg
  2885                                                   if (removeMeanAcross or calcAverageLFP):
  2886                                                       meanGroups[subListIdx] = subListSeg
  2887                                               aSigList = []
  2888                                               # [asig.name for asig in seg.analogsignals]
  2889                                               for a in seg.analogsignals:
  2890                                                   # if np.any([n in a.name for n in asigNameListSeg]):
  2891                                                   if a.name in asigNameListSeg:
  2892                                                       aSigList.append(a)
  2893                                               if ainpNameList is not None:
  2894                                                   ainpNameListSeg = [
  2895                                                       'seg{}_{}'.format(0, a)
  2896                                                       for a in ainpNameList]
  2897                                                   ainpList = []
  2898                                                   for a in seg.analogsignals:
  2899                                                       if np.any([n == a.name for n in ainpNameListSeg]):
  2900                                                           ainpList.append(a)
  2901                                               else:
  2902                                                   ainpList = [
  2903                                                       a
  2904                                                       for a in seg.analogsignals
  2905                                                       if (('ainp' in a.name) or ('analog' in a.name))]
  2906                                                   ainpNameListSeg = [a.name for a in aSigList]
  2907                                               nAsigs = len(aSigList)
  2908                                               if LFPFilterOpts is not None:
  2909                                                   def filterFun(sig, filterCoeffs=None):
  2910                                                       # sig[:] = signal.sosfiltfilt(
  2911                                                       sig[:] = signal.sosfilt(
  2912                                                           filterCoeffs, sig.magnitude.flatten())[:, np.newaxis] * sig.units
  2913                                                       return sig
  2914                                                   filterCoeffs = hf.makeFilterCoeffsSOS(
  2915                                                       LFPFilterOpts, float(seg.analogsignals[0].sampling_rate))
  2916                                                   if False:
  2917                                                       fig, ax1, ax2 = hf.plotFilterResponse(
  2918                                                           filterCoeffs,
  2919                                                           float(seg.analogsignals[0].sampling_rate))
  2920                                                       fig2, ax3, ax4 = hf.plotFilterImpulseResponse(
  2921                                                           LFPFilterOpts,
  2922                                                           float(seg.analogsignals[0].sampling_rate))
  2923                                                       plt.show()
  2924                                               # first pass through asigs, if removing mean across channels
  2925                                               if (removeMeanAcross or calcAverageLFP):
  2926                                                   for aSigIdx, aSigProxy in enumerate(seg.analogsignals):
  2927                                                       if aSigIdx == 0:
  2928                                                           # check bounds
  2929                                                           tStart = max(chunkTStart * pq.s, aSigProxy.t_start)
  2930                                                           tStop = min(chunkTStop * pq.s, aSigProxy.t_stop)
  2931                                                       loadThisOne = (aSigProxy in aSigList)
  2932                                                       if loadThisOne:
  2933                                                           if trackMemory:
  2934                                                               print(
  2935                                                                   'Extracting asig for mean, memory usage: {:.1f} MB'.format(
  2936                                                                       prf.memory_usage_psutil()))
  2937                                                           chanIdx = aSigProxy.channel_index
  2938                                                           asig = aSigProxy.load(
  2939                                                               time_slice=(tStart, tStop),
  2940                                                               magnitude_mode='rescaled')
  2941                                                           if 'tempLFPStore' not in locals():
  2942                                                               tempLFPStore = pd.DataFrame(
  2943                                                                   np.zeros(
  2944                                                                       (asig.shape[0], nAsigs),
  2945                                                                       dtype=np.float32),
  2946                                                                   columns=asigNameListSeg)
  2947                                                           if 'dummyAsig' not in locals():
  2948                                                               dummyAsig = asig.copy()
  2949                                                           #  perform requested preproc operations
  2950                                                           #  if LFPFilterOpts is not None:
  2951                                                           #      asig[:] = filterFun(
  2952                                                           #          asig, filterCoeffs=filterCoeffs)
  2953                                                           if normalizeByImpedance:
  2954                                                               elNmMatchMsk = impedances['elec'] == chanIdx.name
  2955                                                               '''
  2956                                                               asig.magnitude[:] = (
  2957                                                                   (asig.magnitude - np.median(asig.magnitude)) /
  2958                                                                   np.min(
  2959                                                                       impedances.loc[elNmMatchMsk, 'impedance']
  2960                                                                       ))
  2961                                                               '''
  2962                                                               asig.magnitude[:] = (
  2963                                                                   (asig.magnitude) * averageImpedance /
  2964                                                                   np.min(
  2965                                                                       impedances.loc[elNmMatchMsk, 'impedance']
  2966                                                                       ))
  2967                                                           # if fillOverflow:
  2968                                                           #     # fill in overflow:
  2969                                                           #     '''
  2970                                                           #     timeSection['data'], overflowMask = hf.fillInOverflow(
  2971                                                           #         timeSection['data'], fillMethod = 'average')
  2972                                                           #     badData.update({'overflow': overflowMask})
  2973                                                           #     '''
  2974                                                           #     pass
  2975                                                           # if removeJumps:
  2976                                                           #     # find unusual jumps in derivative or amplitude
  2977                                                           #     '''
  2978                                                           #     timeSection['data'], newBadData = hf.fillInJumps(timeSection['data'],
  2979                                                           #     timeSection['samp_per_s'], smoothing_ms = 0.5, nStdDiff = 50,
  2980                                                           #     nStdAmp = 100)
  2981                                                           #     badData.update(newBadData)
  2982                                                           #     '''
  2983                                                           #     pass
  2984                                                           tempLFPStore.loc[:, aSigProxy.name] = asig.magnitude.flatten()
  2985                                                           del asig
  2986                                                           gc.collect()
  2987                                                   # end of first pass
  2988                                                   if (removeMeanAcross or calcAverageLFP):
  2989                                                       centerLFP = np.zeros(
  2990                                                           (tempLFPStore.shape[0], len(asigNameList)),
  2991                                                           dtype=np.float32)
  2992                                                       spreadLFP = np.zeros(
  2993                                                           (tempLFPStore.shape[0], len(asigNameList)),
  2994                                                           dtype=np.float32)
  2995                                                       # if calcOutliers:
  2996                                                       if True:
  2997                                                           if outlierMaskFilterOpts is not None:
  2998                                                               filterCoeffsOutlierMask = hf.makeFilterCoeffsSOS(
  2999                                                                   outlierMaskFilterOpts, float(dummyAsig.sampling_rate))
  3000                                                           lfpDeviation = np.zeros(
  3001                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3002                                                               dtype=np.float32)
  3003                                                           smoothedDeviation = np.zeros(
  3004                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3005                                                               dtype=np.float32)
  3006                                                           outlierMask = np.zeros(
  3007                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3008                                                               dtype=np.bool)
  3009                                                           outlierMetadata = {}
  3010                                                       # if calcArtifactTrace:
  3011                                                       if True:
  3012                                                           artifactSignal = np.zeros(
  3013                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3014                                                               dtype=np.float32)
  3015                                                       ###############
  3016                                                       # tempLFPStore.iloc[:, 0] = np.nan  # for debugging axes
  3017                                                       #############
  3018                                                       plotDevFilterDebug = False
  3019                                                       if plotDevFilterDebug:
  3020                                                           try:
  3021                                                               devFiltDebugMask = (dummyAsig.times > 90 * pq.s) & (dummyAsig.times < 92 * pq.s)
  3022                                                           except Exception:
  3023                                                               pdb.set_trace()
  3024                                                           plotColIdx = 1
  3025                                                           ddfFig, ddfAx = plt.subplots(len(asigNameList), 1)
  3026                                                           ddfFig2, ddfAx2 = plt.subplots()
  3027                                                           ddfFig3, ddfAx3 = plt.subplots(
  3028                                                               1, len(asigNameList),
  3029                                                               sharey=True)
  3030                                                           if len(asigNameList) == 1:
  3031                                                               ddfAx = np.asarray([ddfAx])
  3032                                                               ddfAx3 = np.asarray([ddfAx3])
  3033                                                       for subListIdx, subList in enumerate(asigNameList):
  3034                                                           columnsForThisGroup = meanGroups[subListIdx]
  3035                                                           if trackMemory:
  3036                                                               print(
  3037                                                                   'asig group {}: calculating mean, memory usage: {:.1f} MB'.format(
  3038                                                                       subListIdx, prf.memory_usage_psutil()))
  3039                                                               print('this group contains\n{}'.format(columnsForThisGroup))
  3040                                                           if plotDevFilterDebug:
  3041                                                               ddfAx3[subListIdx].plot(
  3042                                                                   dummyAsig.times[devFiltDebugMask],
  3043                                                                   tempLFPStore.loc[:, columnsForThisGroup].iloc[devFiltDebugMask, plotColIdx],
  3044                                                                   label='original ch'
  3045                                                                   )
  3046                                                           if fillOverflow:
  3047                                                               print('Filling overflow...')
  3048                                                               # fill in overflow:
  3049                                                               tempLFPStore.loc[:, columnsForThisGroup], pltHandles = hf.fillInOverflow2(
  3050                                                                   tempLFPStore.loc[:, columnsForThisGroup].to_numpy(),
  3051                                                                   overFlowFillType='average',
  3052                                                                   overFlowThreshold=8000,
  3053                                                                   debuggingPlots=plotDevFilterDebug
  3054                                                                   )
  3055                                                               if plotDevFilterDebug:
  3056                                                                   pltHandles['ax'].set_title('ch grp {}'.format(subListIdx))
  3057                                                                   ddfAx3[subListIdx].plot(
  3058                                                                       dummyAsig.times[devFiltDebugMask],
  3059                                                                       tempLFPStore.loc[:, columnsForThisGroup].iloc[devFiltDebugMask, plotColIdx],
  3060                                                                       label='filled ch'
  3061                                                                       )
  3062                                                           # zscore of each trace
  3063                                                           if zScoreEachTrace:
  3064                                                               print('About to calculate zscore of each trace (along columns) for prelim outlier detection')
  3065                                                               columnZScore = pd.DataFrame(
  3066                                                                   stats.zscore(
  3067                                                                       tempLFPStore.loc[:, columnsForThisGroup],
  3068                                                                       axis=1),
  3069                                                                   index=tempLFPStore.index,
  3070                                                                   columns=columnsForThisGroup
  3071                                                                   )
  3072                                                               excludeFromMeanMask = columnZScore.abs() > 6
  3073                                                               if useMeanToCenter:
  3074                                                                   centerLFP[:, subListIdx] = (
  3075                                                                       tempLFPStore
  3076                                                                       .loc[:, columnsForThisGroup]
  3077                                                                       .mask(excludeFromMeanMask)
  3078                                                                       .mean(axis=1).to_numpy()
  3079                                                                       )
  3080                                                               else:
  3081                                                                   centerLFP[:, subListIdx] = (
  3082                                                                       tempLFPStore
  3083                                                                       .loc[:, columnsForThisGroup]
  3084                                                                       .mask(excludeFromMeanMask)
  3085                                                                       .median(axis=1).to_numpy()
  3086                                                                       )
  3087                                                           else:
  3088                                                               if useMeanToCenter:
  3089                                                                   centerLFP[:, subListIdx] = (
  3090                                                                       tempLFPStore
  3091                                                                       .loc[:, columnsForThisGroup]
  3092                                                                       .mean(axis=1).to_numpy()
  3093                                                                       )
  3094                                                               else:
  3095                                                                   centerLFP[:, subListIdx] = (
  3096                                                                       tempLFPStore
  3097                                                                       .loc[:, columnsForThisGroup]
  3098                                                                       .median(axis=1).to_numpy()
  3099                                                                       )
  3100                                                           if calcArtifactTrace:
  3101                                                               if LFPFilterOpts is not None:
  3102                                                                   print('applying LFPFilterOpts to cached asigs for artifact ID')
  3103                                                                   # tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfilt(
  3104                                                                   tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfiltfilt(
  3105                                                                       filterCoeffs, tempLFPStore.loc[:, columnsForThisGroup],
  3106                                                                       axis=0)
  3107                                                                   if useMeanToCenter:
  3108                                                                       tempCenter = (
  3109                                                                           tempLFPStore
  3110                                                                           .loc[:, columnsForThisGroup]
  3111                                                                           .mean(axis=1).diff().fillna(0)
  3112                                                                           )
  3113                                                                   else:
  3114                                                                       tempCenter = (
  3115                                                                           tempLFPStore
  3116                                                                           .loc[:, columnsForThisGroup]
  3117                                                                           .median(axis=1).diff().fillna(0)
  3118                                                                           )
  3119                                                               artifactSignal[:, subListIdx] = np.abs(stats.zscore(tempCenter.to_numpy()))
  3120                                                           if calcOutliers:
  3121                                                               if plotDevFilterDebug:
  3122                                                                   ddfAx[subListIdx].plot(
  3123                                                                       dummyAsig.times[devFiltDebugMask],
  3124                                                                       centerLFP[devFiltDebugMask, subListIdx],
  3125                                                                       label='mean of ch group'
  3126                                                                       )
  3127                                                               # filter the traces, if needed
  3128                                                               if LFPFilterOpts is not None:
  3129                                                                   print('applying LFPFilterOpts to cached asigs before outlier detection')
  3130                                                                   # tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfiltfilt(
  3131                                                                   tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfilt(
  3132                                                                       filterCoeffs, tempLFPStore.loc[:, columnsForThisGroup],
  3133                                                                       axis=0)
  3134                                                                   if plotDevFilterDebug:
  3135                                                                       ddfAx3[subListIdx].plot(
  3136                                                                           dummyAsig.times[devFiltDebugMask],
  3137                                                                           tempLFPStore.loc[:, columnsForThisGroup].iloc[devFiltDebugMask, plotColIdx],
  3138                                                                           label='filtered ch'
  3139                                                                           )
  3140                                                               ##################################
  3141                                                               print('Whitening cached traces before outlier detection')
  3142                                                               whitenByPCA = True
  3143                                                               if whitenByPCA:
  3144                                                                   projector = PCA(
  3145                                                                       n_components=None, whiten=True)
  3146                                                                   pcs = projector.fit_transform(
  3147                                                                       tempLFPStore.loc[:, columnsForThisGroup])
  3148                                                                   explVarMask = (
  3149                                                                       np.cumsum(projector.explained_variance_ratio_) < 1 - 1e-2)
  3150                                                                   explVarMask[0] = True  # (keep at least 1)
  3151                                                                   pcs = pcs[:, explVarMask]
  3152                                                                   nDim = pcs.shape[1]
  3153                                                                   lfpDeviation[:, subListIdx] = (pcs ** 2).sum(axis=1)
  3154                                                               else:  # whiten by mahalanobis distance
  3155                                                                   est = EmpiricalCovariance()
  3156                                                                   est.fit(tempLFPStore.loc[:, columnsForThisGroup].to_numpy())
  3157                                                                   lfpDeviation[:, subListIdx] = est.mahalanobis(
  3158                                                                       tempLFPStore.loc[:, columnsForThisGroup].to_numpy())
  3159                                                                   nDim = tempLFPStore.loc[:, columnsForThisGroup].shape[1]
  3160                                                               #
  3161                                                               transformedDeviation = stats.norm.isf(stats.chi2.sf(lfpDeviation[:, subListIdx], nDim))
  3162                                                               infMask = np.isinf(transformedDeviation)
  3163                                                               if infMask.any():
  3164                                                                   transformedDeviation[infMask] = transformedDeviation[~infMask].max()
  3165                                                               debugProbaTrans = False
  3166                                                               if debugProbaTrans:
  3167                                                                   fig, ax = plt.subplots()
  3168                                                                   tAx = ax.twinx()
  3169                                                                   plotMask = (dummyAsig.times >= 60 * pq.s) & (dummyAsig.times < 95 * pq.s)
  3170                                                                   ax.plot(dummyAsig.times[plotMask], transformedDeviation[plotMask], c='b', label='transformed deviation')
  3171                                                                   tAx.plot(dummyAsig.times[plotMask], lfpDeviation[plotMask, subListIdx], c='r', label='original deviation')
  3172                                                                   ax.legend(loc='upper left')
  3173                                                                   tAx.legend(loc='upper right')
  3174                                                                   plt.show()
  3175                                                               lfpDeviation[:, subListIdx] = transformedDeviation
  3176                                                               noveltyThreshold = stats.norm.interval(outlierThreshold)[1]
  3177                                                               # chi2Bounds = stats.chi2.interval(outlierThreshold, nDim)
  3178                                                               # lfpDeviation[:, subListIdx] = lfpDeviation[:, subListIdx] / chi2Bounds[1]
  3179                                                               # print('nDim = {}, chi2Lim = {}'.format(nDim, chi2Bounds))
  3180                                                               # noveltyThreshold = 1
  3181                                                               #
  3182                                                               outlierMetadata[subListIdx] = {
  3183                                                                   'nDim': nDim,
  3184                                                                   'noveltyThreshold': noveltyThreshold,
  3185                                                                   'outlierThreshold': outlierThreshold
  3186                                                                   }
  3187                                                               # smoothedDeviation = signal.sosfilt(
  3188                                                               print('Smoothing deviation')
  3189                                                               tempSmDev = signal.sosfiltfilt(
  3190                                                                   filterCoeffsOutlierMask, lfpDeviation[:, subListIdx])
  3191                                                               smoothedDeviation[:, subListIdx] = tempSmDev
  3192                                                               if plotDevFilterDebug:
  3193                                                                   ddfAx[subListIdx].plot(
  3194                                                                       dummyAsig.times[devFiltDebugMask],
  3195                                                                       lfpDeviation[devFiltDebugMask, subListIdx],
  3196                                                                       label='original deviation (ch grp {})'.format(subListIdx))
  3197                                                                   ddfAx[subListIdx].plot(
  3198                                                                       dummyAsig.times[devFiltDebugMask],
  3199                                                                       smoothedDeviation[devFiltDebugMask, subListIdx],
  3200                                                                       label='filtered deviation (ch grp {})'.format(subListIdx))
  3201                                                               ##
  3202                                                               print('Calculating outlier mask')
  3203                                                               outlierMask[:, subListIdx] = (
  3204                                                                   smoothedDeviation[:, subListIdx] > noveltyThreshold)
  3205                                                               if plotDevFilterDebug:
  3206                                                                   ddfAx[subListIdx].axhline(noveltyThreshold, c='r')
  3207                                                       if plotDevFilterDebug and calcOutliers:
  3208                                                           for subListIdx, subList in enumerate(asigNameList):
  3209                                                               ddfAx[subListIdx].legend(loc='upper right')
  3210                                                               ddfAx[subListIdx].set_title('Deviation')
  3211                                                               ddfAx3[subListIdx].legend(loc='upper right')
  3212                                                               ddfAx3[subListIdx].set_title('Example channel')
  3213                                                               ddfAx2.plot(
  3214                                                                   dummyAsig.times[devFiltDebugMask],
  3215                                                                   smoothedDeviation[devFiltDebugMask, subListIdx],
  3216                                                                   label='ch grp {}'.format(subListIdx))
  3217                                                               ddfAx2.set_title('Smoothed Deviation')
  3218                                                           ddfAx2.legend(loc='upper right')
  3219                                                           plt.show()
  3220                                                       #############
  3221                                                       del tempLFPStore
  3222                                                       gc.collect()
  3223                                               if (removeMeanAcross or calcAverageLFP):
  3224                                                   for mIdx, meanChIdx in enumerate(meanChIdxList):
  3225                                                       meanAsig = AnalogSignal(
  3226                                                           centerLFP[:, mIdx],
  3227                                                           units=dummyAsig.units,
  3228                                                           sampling_rate=dummyAsig.sampling_rate,
  3229                                                           # name='seg{}_{}'.format(idx, meanChIdx.name)
  3230                                                           name='seg{}_{}'.format(0, meanChIdx.name),
  3231                                                           t_start=tStart
  3232                                                       )
  3233                                                       # assign ownership to containers
  3234                                                       meanChIdx.analogsignals.append(meanAsig)
  3235                                                       newSeg.analogsignals.append(meanAsig)
  3236                                                       # assign parent to children
  3237                                                       meanChIdx.create_relationship()
  3238                                                       newSeg.create_relationship()
  3239                                                       # write out to file
  3240                                                       if LFPFilterOpts is not None:
  3241                                                           meanAsig[:] = filterFun(
  3242                                                               meanAsig, filterCoeffs=filterCoeffs)
  3243                                                       meanAsig = writer._write_analogsignal(
  3244                                                           meanAsig, nixblock, nixgroup)
  3245                                                   # if calcArtifactTrace:
  3246                                                   if True:
  3247                                                       for mIdx, artChIdx in enumerate(artChIdxList):
  3248                                                           artAsig = AnalogSignal(
  3249                                                               artifactSignal[:, mIdx],
  3250                                                               units=dummyAsig.units,
  3251                                                               sampling_rate=dummyAsig.sampling_rate,
  3252                                                               # name='seg{}_{}'.format(idx, devChIdx.name)
  3253                                                               name='seg{}_{}'.format(0, artChIdx.name),
  3254                                                               t_start=tStart
  3255                                                               )
  3256                                                           # assign ownership to containers
  3257                                                           artChIdx.analogsignals.append(artAsig)
  3258                                                           newSeg.analogsignals.append(artAsig)
  3259                                                           # assign parent to children
  3260                                                           artChIdx.create_relationship()
  3261                                                           newSeg.create_relationship()
  3262                                                           # write out to file
  3263                                                           artAsig = writer._write_analogsignal(
  3264                                                               artAsig, nixblock, nixgroup)
  3265                                                           #########################################################
  3266                                                   # if calcOutliers:
  3267                                                   if True:
  3268                                                       for mIdx, devChIdx in enumerate(devChIdxList):
  3269                                                           devAsig = AnalogSignal(
  3270                                                               lfpDeviation[:, mIdx],
  3271                                                               units=dummyAsig.units,
  3272                                                               sampling_rate=dummyAsig.sampling_rate,
  3273                                                               # name='seg{}_{}'.format(idx, devChIdx.name)
  3274                                                               name='seg{}_{}'.format(0, devChIdx.name),
  3275                                                               t_start=tStart
  3276                                                               )
  3277                                                           # assign ownership to containers
  3278                                                           devChIdx.analogsignals.append(devAsig)
  3279                                                           newSeg.analogsignals.append(devAsig)
  3280                                                           # assign parent to children
  3281                                                           devChIdx.create_relationship()
  3282                                                           newSeg.create_relationship()
  3283                                                           # write out to file
  3284                                                           devAsig = writer._write_analogsignal(
  3285                                                               devAsig, nixblock, nixgroup)
  3286                                                           #########################################################
  3287                                                       for mIdx, smDevChIdx in enumerate(smDevChIdxList):
  3288                                                           smDevAsig = AnalogSignal(
  3289                                                               smoothedDeviation[:, mIdx],
  3290                                                               units=dummyAsig.units,
  3291                                                               sampling_rate=dummyAsig.sampling_rate,
  3292                                                               # name='seg{}_{}'.format(idx, devChIdx.name)
  3293                                                               name='seg{}_{}'.format(0, smDevChIdx.name),
  3294                                                               t_start=tStart
  3295                                                               )
  3296                                                           # assign ownership to containers
  3297                                                           smDevChIdx.analogsignals.append(smDevAsig)
  3298                                                           newSeg.analogsignals.append(smDevAsig)
  3299                                                           # assign parent to children
  3300                                                           smDevChIdx.create_relationship()
  3301                                                           newSeg.create_relationship()
  3302                                                           # write out to file
  3303                                                           smDevAsig = writer._write_analogsignal(
  3304                                                               smDevAsig, nixblock, nixgroup)
  3305                                                           #########################################################
  3306                                                       for mIdx, outMaskChIdx in enumerate(outMaskChIdxList):
  3307                                                           outMaskAsig = AnalogSignal(
  3308                                                               outlierMask[:, mIdx],
  3309                                                               units=dummyAsig.units,
  3310                                                               sampling_rate=dummyAsig.sampling_rate,
  3311                                                               # name='seg{}_{}'.format(idx, outMaskChIdx.name)
  3312                                                               name='seg{}_{}'.format(0, outMaskChIdx.name),
  3313                                                               t_start=tStart, dtype=np.float32
  3314                                                               )
  3315                                                           outMaskAsig.annotations['outlierProportion'] = np.mean(outlierMask[:, mIdx])
  3316                                                           if calcOutliers:
  3317                                                               outMaskAsig.annotations.update(outlierMetadata[mIdx])
  3318                                                           # assign ownership to containers
  3319                                                           outMaskChIdx.analogsignals.append(outMaskAsig)
  3320                                                           newSeg.analogsignals.append(outMaskAsig)
  3321                                                           # assign parent to children
  3322                                                           outMaskChIdx.create_relationship()
  3323                                                           newSeg.create_relationship()
  3324                                                           # write out to file
  3325                                                           outMaskAsig = writer._write_analogsignal(
  3326                                                               outMaskAsig, nixblock, nixgroup)
  3327                                                   #
  3328                                                   w0 = 60
  3329                                                   bandQ = 20
  3330                                                   bw = w0/bandQ
  3331                                                   noiseSos = signal.iirfilter(
  3332                                                       N=8, Wn=[w0 - bw/2, w0 + bw/2],
  3333                                                       btype='band', ftype='butter',
  3334                                                       analog=False, fs=float(dummyAsig.sampling_rate),
  3335                                                       output='sos')
  3336                                                   # signal.hilbert does not have an option to zero pad
  3337                                                   nextLen = fftpack.helper.next_fast_len(dummyAsig.shape[0])
  3338                                                   deficit = int(nextLen - dummyAsig.shape[0])
  3339                                                   lDef = int(np.floor(deficit / 2))
  3340                                                   rDef = int(np.ceil(deficit / 2)) + 1
  3341                                                   temp = np.pad(
  3342                                                       dummyAsig.magnitude.flatten(),
  3343                                                       (lDef, rDef), mode='constant')
  3344                                                   # lineNoise = signal.sosfiltfilt(
  3345                                                   lineNoise = signal.sosfilt(
  3346                                                       noiseSos, temp, axis=0)
  3347                                                   lineNoiseH = signal.hilbert(lineNoise)
  3348                                                   lineNoise = lineNoise[lDef:-rDef]
  3349                                                   lineNoiseH = lineNoiseH[lDef:-rDef]
  3350                                                   lineNoisePhase = np.angle(lineNoiseH)
  3351                                                   lineNoisePhaseDF = pd.DataFrame(
  3352                                                       lineNoisePhase,
  3353                                                       index=dummyAsig.times,
  3354                                                       columns=['phase']
  3355                                                       )
  3356                                                   plotHilbert = False
  3357                                                   if plotHilbert:
  3358                                                       lineNoiseFreq = (
  3359                                                           np.diff(np.unwrap(lineNoisePhase)) /
  3360                                                           (2.0*np.pi) * float(dummyAsig.sampling_rate))
  3361                                                       lineNoiseEnvelope = np.abs(lineNoiseH)
  3362                                                       i1 = 300000; i2 = 330000
  3363                                                       fig, ax = plt.subplots(2, 1, sharex=True)
  3364                                                       ax[0].plot(dummyAsig.times[devFiltDebugMask], dummyAsig.magnitude[devFiltDebugMask, :])
  3365                                                       ax[0].plot(dummyAsig.times[devFiltDebugMask], lineNoise[devFiltDebugMask])
  3366                                                       ax[0].plot(dummyAsig.times[devFiltDebugMask], lineNoiseEnvelope[devFiltDebugMask])
  3367                                                       axFr = ax[1].twinx()
  3368                                                       ax[1].plot(
  3369                                                           dummyAsig.times[devFiltDebugMask], lineNoisePhase[devFiltDebugMask],
  3370                                                           c='r', label='phase')
  3371                                                       ax[1].legend()
  3372                                                       axFr.plot(
  3373                                                           dummyAsig.times[devFiltDebugMask], lineNoiseFreq[devFiltDebugMask],
  3374                                                           label='freq')
  3375                                                       axFr.set_ylim([59, 61])
  3376                                                       axFr.legend()
  3377                                                       plt.show()
  3378                                               # second pass through asigs, to save
  3379                                               for aSigIdx, aSigProxy in enumerate(seg.analogsignals):
  3380                                                   if aSigIdx == 0:
  3381                                                       # check bounds
  3382                                                       tStart = max(chunkTStart * pq.s, aSigProxy.t_start)
  3383                                                       tStop = min(chunkTStop * pq.s, aSigProxy.t_stop)
  3384                                                   loadThisOne = (
  3385                                                       (saveFromAsigNameList and (aSigProxy in aSigList)) or
  3386                                                       (aSigProxy in ainpList)
  3387                                                       )
  3388                                                   if loadThisOne:
  3389                                                       if trackMemory:
  3390                                                           print('writing asig {} ({}) memory usage: {:.1f} MB'.format(
  3391                                                               aSigIdx, aSigProxy.name, prf.memory_usage_psutil()))
  3392                                                       chanIdx = aSigProxy.channel_index
  3393                                                       asig = aSigProxy.load(
  3394                                                           time_slice=(tStart, tStop),
  3395                                                           magnitude_mode='rescaled')
  3396                                                       #  link AnalogSignal and ID providing channel_index
  3397                                                       asig.channel_index = chanIdx
  3398                                                       #  perform requested preproc operations
  3399                                                       if 'impedances' in locals():
  3400                                                           elNmMatchMsk = impedances['elec'] == chanIdx.name
  3401                                                           if elNmMatchMsk.any():
  3402                                                               originalImpedance = np.min(
  3403                                                                   impedances.loc[elNmMatchMsk, 'impedance']
  3404                                                                   )
  3405                                                               asig.annotations['originalImpedance'] = originalImpedance
  3406                                                               if normalizeByImpedance and (aSigProxy not in ainpList):
  3407                                                                   '''
  3408                                                                   asig.magnitude[:] = (
  3409                                                                       (asig.magnitude - np.median(asig.magnitude)) /
  3410                                                                       np.min(
  3411                                                                           impedances.loc[elNmMatchMsk, 'impedance']
  3412                                                                           )
  3413                                                                       )
  3414                                                                   '''
  3415                                                                   print('Normalizing {} by {} kOhms'.format(asig.name, originalImpedance))
  3416                                                                   asig.magnitude[:] = (
  3417                                                                       (asig.magnitude * averageImpedance) / originalImpedance
  3418                                                                       )
  3419                                                       if fillOverflow:
  3420                                                           # fill in overflow:
  3421                                                           asig.magnitude[:], _ = hf.fillInOverflow2(
  3422                                                               asig.magnitude[:],
  3423                                                               overFlowFillType='average',
  3424                                                               overFlowThreshold=8000,
  3425                                                               debuggingPlots=False
  3426                                                               )
  3427                                                       if removeJumps:
  3428                                                           # find unusual jumps in derivative or amplitude
  3429                                                           '''
  3430                                                           timeSection['data'], newBadData = hf.fillInJumps(timeSection['data'],
  3431                                                           timeSection['samp_per_s'], smoothing_ms = 0.5, nStdDiff = 50,
  3432                                                           nStdAmp = 100)
  3433                                                           badData.update(newBadData)
  3434                                                           '''
  3435                                                           pass
  3436                                                       if calcAverageLFP and (aSigProxy not in ainpList):
  3437                                                           for k, cols in meanGroups.items():
  3438                                                               if asig.name in cols:
  3439                                                                   whichColumnToSubtract = k
  3440                                                           noiseModel = np.polyfit(
  3441                                                               centerLFP[:, whichColumnToSubtract],
  3442                                                               asig.magnitude.flatten(), 1, full=True)
  3443                                                           rSq = 1 - noiseModel[1][0] / np.sum(asig.magnitude.flatten() ** 2)
  3444                                                           asig.annotations['mean_removal_r2'] = rSq
  3445                                                           asig.annotations['mean_removal_group'] = whichColumnToSubtract
  3446                                                           if linearDetrend:
  3447                                                               noiseTerm = np.polyval(
  3448                                                                   noiseModel[0],
  3449                                                                   centerLFP[:, whichColumnToSubtract])
  3450                                                           else:
  3451                                                               noiseTerm = centerLFP[:, whichColumnToSubtract]
  3452                                                           ###
  3453                                                           plotMeanSubtraction = False
  3454                                                           if plotMeanSubtraction:
  3455                                                               i1 = 300000; i2 = 330000
  3456                                                               fig, ax = plt.subplots(1, 1)
  3457                                                               ax.plot(asig.times[devFiltDebugMask], asig.magnitude[devFiltDebugMask, :], label='channel')
  3458                                                               ax.plot(asig.times[devFiltDebugMask], centerLFP[devFiltDebugMask, whichColumnToSubtract], label='mean')
  3459                                                               ax.plot(asig.times[devFiltDebugMask], noiseTerm[devFiltDebugMask], label='adjusted mean')
  3460                                                               ax.legend()
  3461                                                               plt.show()
  3462                                                           ###
  3463                                                           if removeMeanAcross:
  3464                                                               asig.magnitude[:] = np.atleast_2d(
  3465                                                                   asig.magnitude.flatten() - noiseTerm).transpose()
  3466                                                               # asig.magnitude[:] = (
  3467                                                               #     asig.magnitude - np.median(asig.magnitude))
  3468                                                       if (LFPFilterOpts is not None) and (aSigProxy not in ainpList):
  3469                                                           asig.magnitude[:] = filterFun(asig, filterCoeffs=filterCoeffs)
  3470                                                       if (interpolateOutliers) and (aSigProxy not in ainpList) and (not outlierRemovalDebugFlag):
  3471                                                           for k, cols in meanGroups.items():
  3472                                                               if asig.name in cols:
  3473                                                                   whichColumnToSubtract = k
  3474                                                           tempSer = pd.Series(asig.magnitude.flatten())
  3475                                                           tempSer.loc[outlierMask[:, whichColumnToSubtract]] = np.nan
  3476                                                           tempSer = (
  3477                                                               tempSer
  3478                                                               .interpolate(method='linear', limit_area='inside')
  3479                                                               .fillna(method='ffill')
  3480                                                               .fillna(method='bfill')
  3481                                                               )
  3482                                                           asig.magnitude[:, 0] = tempSer.to_numpy()
  3483                                                       # pdb.set_trace()
  3484                                                       if (aSigProxy in aSigList) or (aSigProxy in ainpList):
  3485                                                           # assign ownership to containers
  3486                                                           chanIdx.analogsignals.append(asig)
  3487                                                           newSeg.analogsignals.append(asig)
  3488                                                           # assign parent to children
  3489                                                           chanIdx.create_relationship()
  3490                                                           newSeg.create_relationship()
  3491                                                           # write out to file
  3492                                                           asig = writer._write_analogsignal(
  3493                                                               asig, nixblock, nixgroup)
  3494                                                       del asig
  3495                                                       gc.collect()
  3496                                               for irSigIdx, irSigProxy in enumerate(
  3497                                                       seg.irregularlysampledsignals):
  3498                                                   chanIdx = irSigProxy.channel_index
  3499                                                   #
  3500                                                   isig = irSigProxy.load(
  3501                                                       time_slice=(tStart, tStop),
  3502                                                       magnitude_mode='rescaled')
  3503                                                   #  link irregularlysampledSignal
  3504                                                   #  and ID providing channel_index
  3505                                                   isig.channel_index = chanIdx
  3506                                                   # assign ownership to containers
  3507                                                   chanIdx.irregularlysampledsignals.append(isig)
  3508                                                   newSeg.irregularlysampledsignals.append(isig)
  3509                                                   # assign parent to children
  3510                                                   chanIdx.create_relationship()
  3511                                                   newSeg.create_relationship()
  3512                                                   # write out to file
  3513                                                   isig = writer._write_irregularlysampledsignal(
  3514                                                       isig, nixblock, nixgroup)
  3515                                                   del isig
  3516                                                   gc.collect()
  3517                                               #
  3518                                               if len(spikeSourceType):
  3519                                                   for stIdx, stProxy in enumerate(spikeSeg.spiketrains):
  3520                                                       if trackMemory:
  3521                                                           print('writing spiketrains mem usage: {}'.format(
  3522                                                               prf.memory_usage_psutil()))
  3523                                                       unit = stProxy.unit
  3524                                                       st = loadStProxy(stProxy)
  3525                                                       #  have to manually slice tStop and tStart because
  3526                                                       #  array annotations are not saved natively in the nix file
  3527                                                       #  (we're getting them as plain annotations)
  3528                                                       timeMask = np.asarray(
  3529                                                           (st.times >= tStart) & (st.times < tStop),
  3530                                                           dtype=np.bool)
  3531                                                       try:
  3532                                                           if 'arrayAnnNames' in st.annotations:
  3533                                                               for key in st.annotations['arrayAnnNames']:
  3534                                                                   st.annotations[key] = np.asarray(
  3535                                                                       st.annotations[key])[timeMask]
  3536                                                           st = st[timeMask]
  3537                                                           st.t_start = tStart
  3538                                                           st.t_stop = tStop
  3539                                                       except Exception:
  3540                                                           traceback.print_exc()
  3541                                                       #  tdc may or may not have the same channel ids, but
  3542                                                       #  it will have consistent channel names
  3543                                                       nameParser = re.search(
  3544                                                           r'([a-zA-Z0-9]*)#(\d*)', unit.name)
  3545                                                       chanLabel = nameParser.group(1)
  3546                                                       unitId = nameParser.group(2)
  3547                                                       #
  3548                                                       chIdxName = unit.name.replace('_stim', '').split('#')[0]
  3549                                                       chanIdx = block.filter(objects=ChannelIndex, name=chIdxName)[0]
  3550                                                       # [i.name for i in block.filter(objects=ChannelIndex)]
  3551                                                       # [i.name for i in spikeBlock.filter(objects=Unit)]
  3552                                                       #  print(unit.name)
  3553                                                       if not (unit in chanIdx.units):
  3554                                                           # first time at this unit, add to its chanIdx
  3555                                                           unit.channel_index = chanIdx
  3556                                                           chanIdx.units.append(unit)
  3557                                                       #  except Exception:
  3558                                                       #      traceback.print_exc()
  3559                                                       st.name = 'seg{}_{}'.format(0, unit.name)
  3560                                                       # st.name = 'seg{}_{}'.format(idx, unit.name)
  3561                                                       #  link SpikeTrain and ID providing unit
  3562                                                       if calcAverageLFP:
  3563                                                           if 'arrayAnnNames' in st.annotations:
  3564                                                               st.annotations['arrayAnnNames'] = list(st.annotations['arrayAnnNames'])
  3565                                                           else:
  3566                                                               st.annotations['arrayAnnNames'] = []
  3567                                                           st.annotations['arrayAnnNames'].append('phase60hz')
  3568                                                           phase60hz = hf.interpolateDF(
  3569                                                               lineNoisePhaseDF,
  3570                                                               newX=st.times, columns=['phase']).to_numpy().flatten()
  3571                                                           st.annotations.update({'phase60hz': phase60hz})
  3572                                                           plotPhaseDist = False
  3573                                                           if plotPhaseDist:
  3574                                                               sns.distplot(phase60hz)
  3575                                                               plt.show()
  3576                                                       st.unit = unit
  3577                                                       # assign ownership to containers
  3578                                                       unit.spiketrains.append(st)
  3579                                                       newSeg.spiketrains.append(st)
  3580                                                       # assign parent to children
  3581                                                       unit.create_relationship()
  3582                                                       newSeg.create_relationship()
  3583                                                       # write out to file
  3584                                                       st = writer._write_spiketrain(st, nixblock, nixgroup)
  3585                                                       del st
  3586                                               #  process proprio trial related events
  3587                                               if calcRigEvents:
  3588                                                   print('Processing rig events...')
  3589                                                   analogData = []
  3590                                                   for key, value in eventInfo['inputIDs'].items():
  3591                                                       searchName = 'seg{}_'.format(0) + value
  3592                                                       ainpAsig = seg.filter(
  3593                                                           objects=AnalogSignalProxy,
  3594                                                           name=searchName)[0]
  3595                                                       ainpData = ainpAsig.load(
  3596                                                           time_slice=(tStart, tStop),
  3597                                                           magnitude_mode='rescaled')
  3598                                                       analogData.append(
  3599                                                           pd.DataFrame(ainpData.magnitude, columns=[key]))
  3600                                                       del ainpData
  3601                                                       gc.collect()
  3602                                                   motorData = pd.concat(analogData, axis=1)
  3603                                                   del analogData
  3604                                                   gc.collect()
  3605                                                   if motorEncoderMask is not None:
  3606                                                       ainpData = ainpAsig.load(
  3607                                                           time_slice=(tStart, tStop),
  3608                                                           magnitude_mode='rescaled')
  3609                                                       ainpTime = ainpData.times.magnitude
  3610                                                       meTimeMask = np.zeros_like(ainpTime, dtype=np.bool)
  3611                                                       for meTimeBounds in motorEncoderMask:
  3612                                                           meTimeMask = (
  3613                                                               meTimeMask |
  3614                                                               (
  3615                                                                   (ainpTime > meTimeBounds[0]) &
  3616                                                                   (ainpTime < meTimeBounds[1])
  3617                                                                   )
  3618                                                               )
  3619                                                       columnsToOverride = ['A-', 'A+', 'B-', 'B+', 'Z-', 'Z+']
  3620                                                       for colName in columnsToOverride:
  3621                                                           motorData.loc[~meTimeMask, colName] = motorData.loc[:, colName].quantile(q=0.05)
  3622                                                       del ainpData, ainpTime
  3623                                                       gc.collect()
  3624                                                   motorData = mea.processMotorData(
  3625                                                       motorData, ainpAsig.sampling_rate.magnitude,
  3626                                                       encoderCountPerDegree=encoderCountPerDegree
  3627                                                       )
  3628                                                   keepCols = [
  3629                                                       'position', 'velocity', 'velocityCat',
  3630                                                       'rightBut_int', 'leftBut_int',
  3631                                                       'rightLED_int', 'leftLED_int', 'simiTrigs_int']
  3632                                                   for colName in keepCols:
  3633                                                       if trackMemory:
  3634                                                           print('writing motorData memory usage: {:.1f} MB'.format(
  3635                                                               prf.memory_usage_psutil()))
  3636                                                       chanIdx = ChannelIndex(
  3637                                                           name=colName,
  3638                                                           index=np.asarray([0]),
  3639                                                           channel_names=np.asarray([0]))
  3640                                                       block.channel_indexes.append(chanIdx)
  3641                                                       motorAsig = AnalogSignal(
  3642                                                           motorData[colName].to_numpy() * pq.mV,
  3643                                                           name=colName,
  3644                                                           sampling_rate=ainpAsig.sampling_rate,
  3645                                                           dtype=np.float32)
  3646                                                       motorAsig.t_start = ainpAsig.t_start
  3647                                                       motorAsig.channel_index = chanIdx
  3648                                                       # assign ownership to containers
  3649                                                       chanIdx.analogsignals.append(motorAsig)
  3650                                                       newSeg.analogsignals.append(motorAsig)
  3651                                                       chanIdx.create_relationship()
  3652                                                       newSeg.create_relationship()
  3653                                                       # write out to file
  3654                                                       motorAsig = writer._write_analogsignal(
  3655                                                           motorAsig, nixblock, nixgroup)
  3656                                                       del motorAsig
  3657                                                       gc.collect()
  3658                                                   _, trialEvents = mea.getTrials(
  3659                                                       motorData, ainpAsig.sampling_rate.magnitude,
  3660                                                       float(tStart.magnitude), trialType=None)
  3661                                                   trialEvents.fillna(0)
  3662                                                   trialEvents.rename(
  3663                                                       columns={
  3664                                                           'Label': 'rig_property',
  3665                                                           'Details': 'rig_value'},
  3666                                                       inplace=True)
  3667                                                   del motorData
  3668                                                   gc.collect()
  3669                                                   eventList = eventDataFrameToEvents(
  3670                                                       trialEvents,
  3671                                                       idxT='Time',
  3672                                                       annCol=['rig_property', 'rig_value'])
  3673                                                   for event in eventList:
  3674                                                       if trackMemory:
  3675                                                           print(
  3676                                                               'writing motor events memory usage: {:.1f} MB'
  3677                                                               .format(prf.memory_usage_psutil()))
  3678                                                       event.segment = newSeg
  3679                                                       newSeg.events.append(event)
  3680                                                       newSeg.create_relationship()
  3681                                                       # write out to file
  3682                                                       event = writer._write_event(event, nixblock, nixgroup)
  3683                                                       del event
  3684                                                       gc.collect()
  3685                                                   del trialEvents, eventList
  3686                                               #
  3687                                               for eventProxy in seg.events:
  3688                                                   event = eventProxy.load(
  3689                                                       time_slice=(tStart, tStop))
  3690                                                   event.t_start = tStart
  3691                                                   event.t_stop = tStop
  3692                                                   event.segment = newSeg
  3693                                                   newSeg.events.append(event)
  3694                                                   newSeg.create_relationship()
  3695                                                   # write out to file
  3696                                                   event = writer._write_event(event, nixblock, nixgroup)
  3697                                                   del event
  3698                                                   gc.collect()
  3699                                               #
  3700                                               for epochProxy in seg.epochs:
  3701                                                   epoch = epochProxy.load(
  3702                                                       time_slice=(tStart, tStop))
  3703                                                   epoch.t_start = tStart
  3704                                                   epoch.t_stop = tStop
  3705                                                   epoch.segment = newSeg
  3706                                                   newSeg.events.append(epoch)
  3707                                                   newSeg.create_relationship()
  3708                                                   # write out to file
  3709                                                   epoch = writer._write_epoch(epoch, nixblock, nixgroup)
  3710                                                   del epoch
  3711                                                   gc.collect()
  3712                                               #
  3713                                               chanIdxDiscardNames = []
  3714                                               # descend into ChannelIndexes
  3715                                               for chanIdx in block.channel_indexes:
  3716                                                   if chanIdx.analogsignals or chanIdx.units:
  3717                                                       chanIdx = writer._write_channelindex(chanIdx, nixblock)
  3718                                                   else:
  3719                                                       chanIdxDiscardNames.append(chanIdx.name)
  3720                                               block.channel_indexes = [
  3721                                                   i
  3722                                                   for i in block.channel_indexes
  3723                                                   if i.name not in chanIdxDiscardNames
  3724                                                   ]
  3725                                               writer._create_source_links(block, nixblock)
  3726                                               return

Total time: 0.0062158 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: purgeNixAnn at line 3728

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3728                                           @profile
  3729                                           def purgeNixAnn(
  3730                                                   block, annNames=['nix_name', 'neo_name']):
  3731         6         38.0      6.3      0.1      for annName in annNames:
  3732         4         66.0     16.5      0.1          block.annotations.pop(annName, None)
  3733       232      35085.0    151.2     56.4      for child in block.children_recur:
  3734       230       1233.0      5.4      2.0          if child.annotations:
  3735       137        761.0      5.6      1.2              child.annotations = {
  3736                                                           k: v
  3737       137       3892.0     28.4      6.3                  for k, v in child.annotations.items()
  3738                                                           if k not in annNames}
  3739       116      18714.0    161.3     30.1      for child in block.data_children_recur:
  3740       114        596.0      5.2      1.0          if child.annotations:
  3741        43        229.0      5.3      0.4              child.annotations = {
  3742                                                           k: v
  3743        43       1534.0     35.7      2.5                  for k, v in child.annotations.items()
  3744                                                           if k not in annNames}
  3745         2         10.0      5.0      0.0      return block

Total time: 9.98e-05 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadContainerArrayAnn at line 3747

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3747                                           @profile
  3748                                           def loadContainerArrayAnn(
  3749                                                   container=None, trainList=None
  3750                                                   ):
  3751         1          6.0      6.0      0.6      assert (container is not None) or (trainList is not None)
  3752                                               #
  3753         1          4.0      4.0      0.4      spikesAndEvents = []
  3754         1          3.0      3.0      0.3      returnObj = []
  3755         1          3.0      3.0      0.3      if container is not None:
  3756                                                   #  need the line below! (RD: don't remember why, consider removing)
  3757                                                   container.create_relationship()
  3758                                                   #
  3759                                                   spikesAndEvents += (
  3760                                                       container.filter(objects=SpikeTrain) +
  3761                                                       container.filter(objects=Event)
  3762                                                       )
  3763                                                   returnObj.append(container)
  3764         1          4.0      4.0      0.4      if trainList is not None:
  3765         1          6.0      6.0      0.6          spikesAndEvents += trainList
  3766         1          6.0      6.0      0.6          returnObj.append(trainList)
  3767                                               #
  3768         1          6.0      6.0      0.6      if len(returnObj) == 1:
  3769         1          4.0      4.0      0.4          returnObj = returnObj[0]
  3770                                               else:
  3771                                                   returnObj = tuple(returnObj)
  3772                                               #
  3773        12         39.0      3.2      3.9      for st in spikesAndEvents:
  3774        11        914.0     83.1     91.6          st = loadObjArrayAnn(st)
  3775         1          3.0      3.0      0.3      return returnObj

Total time: 3.63e-05 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadObjArrayAnn at line 3777

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3777                                           @profile
  3778                                           def loadObjArrayAnn(st):
  3779        11         91.0      8.3     25.1      if 'arrayAnnNames' in st.annotations.keys():
  3780                                                   if isinstance(st.annotations['arrayAnnNames'], str):
  3781                                                       st.annotations['arrayAnnNames'] = [st.annotations['arrayAnnNames']]
  3782                                                   elif isinstance(st.annotations['arrayAnnNames'], tuple):
  3783                                                       st.annotations['arrayAnnNames'] = [i for i in st.annotations['arrayAnnNames']]
  3784                                                   #
  3785                                                   for key in st.annotations['arrayAnnNames']:
  3786                                                       #  fromRaw, the ann come back as tuple, need to recast
  3787                                                       try:
  3788                                                           if len(st.times) == 1:
  3789                                                               st.annotations[key] = np.atleast_1d(st.annotations[key]).flatten()
  3790                                                           st.array_annotations.update(
  3791                                                               {key: np.asarray(st.annotations[key])})
  3792                                                           st.annotations[key] = np.asarray(st.annotations[key])
  3793                                                       except Exception:
  3794                                                           print('Error with {}'.format(st.name))
  3795                                                           traceback.print_exc()
  3796                                                           pdb.set_trace()
  3797        11         79.0      7.2     21.8      if hasattr(st, 'waveforms'):
  3798        11         59.0      5.4     16.3          if st.waveforms is None:
  3799                                                       st.waveforms = np.asarray([]).reshape((0, 0, 0)) * pq.mV
  3800        11         80.0      7.3     22.0          elif not len(st.waveforms):
  3801                                                       st.waveforms = np.asarray([]).reshape((0, 0, 0)) * pq.mV
  3802        11         54.0      4.9     14.9      return st

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadWithArrayAnn at line 3804

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3804                                           @profile
  3805                                           def loadWithArrayAnn(
  3806                                                   dataPath, fromRaw=False,
  3807                                                   mapDF=None, reduceChannelIndexes=False):
  3808                                               if fromRaw:
  3809                                                   reader = nixio_fr.NixIO(filename=dataPath)
  3810                                                   block = readBlockFixNames(
  3811                                                       reader, lazy=False,
  3812                                                       mapDF=mapDF,
  3813                                                       reduceChannelIndexes=reduceChannelIndexes)
  3814                                               else:
  3815                                                   reader = NixIO(filename=dataPath)
  3816                                                   block = reader.read_block()
  3817                                                   # [un.name for un in block.filter(objects=Unit)]
  3818                                                   # [len(un.spiketrains) for un in block.filter(objects=Unit)]
  3819                                               
  3820                                               block = loadContainerArrayAnn(container=block)
  3821                                               
  3822                                               if fromRaw:
  3823                                                   reader.file.close()
  3824                                               else:
  3825                                                   reader.close()
  3826                                               return block

Total time: 2.34478 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: blockFromPath at line 3828

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3828                                           @profile
  3829                                           def blockFromPath(
  3830                                                   dataPath, lazy=False, mapDF=None,
  3831                                                   reduceChannelIndexes=False, loadList=None,
  3832                                                   purgeNixNames=False, chunkingInfoPath=None):
  3833         1          8.0      8.0      0.0      chunkingMetadata = None
  3834         1          8.0      8.0      0.0      if chunkingInfoPath is not None:
  3835                                                   if os.path.exists(chunkingInfoPath):
  3836                                                       with open(chunkingInfoPath, 'r') as f:
  3837                                                           chunkingMetadata = json.load(f)
  3838         1          6.0      6.0      0.0      if chunkingMetadata is None:
  3839                                                   chunkingMetadata = {
  3840         1          7.0      7.0      0.0              '0': {
  3841         1          7.0      7.0      0.0                  'filename': dataPath,
  3842         1          6.0      6.0      0.0                  'partNameSuffix': '',
  3843         1          6.0      6.0      0.0                  'chunkTStart': 0,
  3844         1         10.0     10.0      0.0                  'chunkTStop': 'NaN'
  3845                                                       }}
  3846         2         32.0     16.0      0.0      for idx, (chunkIdxStr, chunkMeta) in enumerate(chunkingMetadata.items()):   
  3847         1          7.0      7.0      0.0          thisDataPath = chunkMeta['filename']
  3848         1        730.0    730.0      0.0          assert os.path.exists(thisDataPath)
  3849         1          9.0      9.0      0.0          if idx == 0:
  3850         1          6.0      6.0      0.0              if lazy:
  3851         1         15.0     15.0      0.0                  dataReader = nixio_fr.NixIO(
  3852         1    5425241.0 5425241.0     23.1                      filename=thisDataPath)
  3853         1         24.0     24.0      0.0                  dataBlock = readBlockFixNames(
  3854         1          8.0      8.0      0.0                      dataReader, lazy=lazy, mapDF=mapDF,
  3855         1          7.0      7.0      0.0                      reduceChannelIndexes=reduceChannelIndexes,
  3856         1   18021688.0 18021688.0     76.9                      purgeNixNames=purgeNixNames, loadList=loadList)
  3857                                                       else:
  3858                                                           dataReader = None
  3859                                                           dataBlock = loadWithArrayAnn(thisDataPath)
  3860                                                   else:
  3861                                                       if lazy:
  3862                                                           dataReader2 = nixio_fr.NixIO(
  3863                                                               filename=thisDataPath)
  3864                                                           dataBlock2 = readBlockFixNames(
  3865                                                               dataReader2, lazy=lazy, mapDF=mapDF,
  3866                                                               reduceChannelIndexes=reduceChannelIndexes, loadList=loadList)
  3867                                                       else:
  3868                                                           dataReader2 = None
  3869                                                           dataBlock2 = loadWithArrayAnn(thisDataPath)
  3870                                                       maxSegIdx = len(dataBlock.segments)
  3871                                                       typesNeedRenaming = [
  3872                                                           SpikeTrainProxy, AnalogSignalProxy, EventProxy,
  3873                                                           SpikeTrain, AnalogSignal, Event]
  3874                                                       for segIdx, seg in enumerate(dataBlock2.segments):
  3875                                                           if seg.name is None:
  3876                                                               seg.name = 'seg{}_'.format(maxSegIdx + segIdx)
  3877                                                           else:
  3878                                                               if 'seg{}_'.format(maxSegIdx + segIdx) not in seg.name:
  3879                                                                   seg.name = (
  3880                                                                       'seg{}_{}'
  3881                                                                       .format(
  3882                                                                           maxSegIdx + segIdx,
  3883                                                                           childBaseName(seg.name, 'seg')))
  3884                                                           for objType in typesNeedRenaming:
  3885                                                               for child in seg.filter(objects=objType):
  3886                                                                   if 'seg{}_'.format(maxSegIdx + segIdx) not in child.name:
  3887                                                                       child.name = (
  3888                                                                           'seg{}_{}'
  3889                                                                           .format(
  3890                                                                               maxSegIdx + segIdx, childBaseName(child.name, 'seg')))
  3891                                                       dataBlock.merge(dataBlock2)
  3892         1          7.0      7.0      0.0      return dataReader, dataBlock

Total time: 2.18629 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: calcBinarizedArray at line 3894

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3894                                           @profile
  3895                                           def calcBinarizedArray(
  3896                                                   dataBlock, samplingRate,
  3897                                                   binnedSpikePath=None,
  3898                                                   saveToFile=True, matchT=None):
  3899                                               #
  3900         1        223.0    223.0      0.0      spikeMatBlock = Block(name=dataBlock.name + '_binarized')
  3901         1        214.0    214.0      0.0      spikeMatBlock.merge_annotations(dataBlock)
  3902                                               #
  3903                                               allSpikeTrains = [
  3904         1       6263.0   6263.0      0.0          i for i in dataBlock.filter(objects=SpikeTrain)]
  3905                                               #
  3906        12        130.0     10.8      0.0      for st in allSpikeTrains:
  3907        11        121.0     11.0      0.0          chanList = spikeMatBlock.filter(
  3908        11      31978.0   2907.1      0.1              objects=ChannelIndex, name=st.unit.name)
  3909        11        151.0     13.7      0.0          if not len(chanList):
  3910        11       3417.0    310.6      0.0              chanIdx = ChannelIndex(name=st.unit.name, index=np.asarray([0]))
  3911                                                       #  print(chanIdx.name)
  3912        11        166.0     15.1      0.0              spikeMatBlock.channel_indexes.append(chanIdx)
  3913        11       1805.0    164.1      0.0              thisUnit = Unit(name=st.unit.name)
  3914        11        138.0     12.5      0.0              chanIdx.units.append(thisUnit)
  3915        11        125.0     11.4      0.0              thisUnit.channel_index = chanIdx
  3916                                               #
  3917         2         43.0     21.5      0.0      for segIdx, seg in enumerate(dataBlock.segments):
  3918         1        269.0    269.0      0.0          newSeg = Segment(name='seg{}_{}'.format(segIdx, spikeMatBlock.name))
  3919         1         90.0     90.0      0.0          newSeg.merge_annotations(seg)
  3920         1         11.0     11.0      0.0          spikeMatBlock.segments.append(newSeg)
  3921                                                   #  tStart = dataBlock.segments[0].t_start
  3922                                                   #  tStop = dataBlock.segments[0].t_stop
  3923         1       5527.0   5527.0      0.0          tStart = seg.t_start
  3924         1       5395.0   5395.0      0.0          tStop = seg.t_stop
  3925                                                   # make dummy binary spike train, in case ths chan didn't fire
  3926                                                   segSpikeTrains = [
  3927         1        575.0    575.0      0.0              i for i in seg.filter(objects=SpikeTrain) if '#' in i.name]
  3928         1         16.0     16.0      0.0          dummyBin = binarize(
  3929         1         13.0     13.0      0.0              segSpikeTrains[0],
  3930         1         12.0     12.0      0.0              sampling_rate=samplingRate,
  3931         1         11.0     11.0      0.0              t_start=tStart,
  3932         1     391361.0 391361.0      1.8              t_stop=tStop + samplingRate ** -1) * 0
  3933        12        336.0     28.0      0.0          for chanIdx in spikeMatBlock.channel_indexes:
  3934                                                       #  print(chanIdx.name)
  3935        11        247.0     22.5      0.0              stList = seg.filter(
  3936        11        277.0     25.2      0.0                  objects=SpikeTrain,
  3937        11      19176.0   1743.3      0.1                  name='seg{}_{}'.format(segIdx, chanIdx.name)
  3938                                                           )
  3939        11        346.0     31.5      0.0              if len(stList):
  3940        11        233.0     21.2      0.0                  st = stList[0]
  3941        11       9285.0    844.1      0.0                  print('binarizing {}'.format(st.name))
  3942        11        261.0     23.7      0.0                  stBin = binarize(
  3943        11        210.0     19.1      0.0                      st,
  3944        11        230.0     20.9      0.0                      sampling_rate=samplingRate,
  3945        11        231.0     21.0      0.0                      t_start=tStart,
  3946        11    4485138.0 407739.8     20.5                      t_stop=tStop + samplingRate ** -1)
  3947        11       1093.0     99.4      0.0                  spikeMatBlock.segments[segIdx].spiketrains.append(st)
  3948                                                           #  to do: link st to spikematblock's chidx and units
  3949        11      18499.0   1681.7      0.1                  assert len(chanIdx.filter(objects=Unit)) == 1
  3950        11      11347.0   1031.5      0.1                  thisUnit = chanIdx.filter(objects=Unit)[0]
  3951        11        312.0     28.4      0.0                  thisUnit.spiketrains.append(st)
  3952        11        358.0     32.5      0.0                  st.unit = thisUnit
  3953        11        326.0     29.6      0.0                  st.segment = spikeMatBlock.segments[segIdx]
  3954                                                       else:
  3955                                                           print('{} has no spikes'.format(st.name))
  3956                                                           stBin = dummyBin
  3957                                                       skipStAnnNames = [
  3958        11        281.0     25.5      0.0                  'nix_name', 'neo_name', 'arrayAnnNames']
  3959        11        445.0     40.5      0.0              if 'arrayAnnNames' in st.annotations:
  3960                                                           skipStAnnNames += list(st.annotations['arrayAnnNames'])
  3961        11        275.0     25.0      0.0              asigAnn = {
  3962                                                           k: v
  3963        11       1186.0    107.8      0.0                  for k, v in st.annotations.items()
  3964                                                           if k not in skipStAnnNames
  3965                                                           }
  3966        11        256.0     23.3      0.0              asig = AnalogSignal(
  3967        11     462619.0  42056.3      2.1                  stBin * samplingRate,
  3968        11       1248.0    113.5      0.0                  name='seg{}_{}_raster'.format(segIdx, st.unit.name),
  3969        11        218.0     19.8      0.0                  sampling_rate=samplingRate,
  3970        11        526.0     47.8      0.0                  dtype=np.int,
  3971        11     436060.0  39641.8      2.0                  **asigAnn)
  3972        11        426.0     38.7      0.0              if matchT is not None:
  3973                                                           asig = asig[:matchT.shape[0], :]
  3974        11        600.0     54.5      0.0              asig.t_start = tStart
  3975        11       3505.0    318.6      0.0              asig.annotate(binWidth=1 / samplingRate.magnitude)
  3976        11        503.0     45.7      0.0              chanIdx.analogsignals.append(asig)
  3977        11        228.0     20.7      0.0              asig.channel_index = chanIdx
  3978        11        541.0     49.2      0.0              spikeMatBlock.segments[segIdx].analogsignals.append(asig)
  3979                                               #
  3980        12        148.0     12.3      0.0      for chanIdx in spikeMatBlock.channel_indexes:
  3981        11        184.0     16.7      0.0          chanIdx.name = chanIdx.name + '_raster'
  3982                                               #
  3983         1       5896.0   5896.0      0.0      spikeMatBlock.create_relationship()
  3984         1      12824.0  12824.0      0.1      spikeMatBlock = purgeNixAnn(spikeMatBlock)
  3985         1         13.0     13.0      0.0      if saveToFile:
  3986         1        941.0    941.0      0.0          if os.path.exists(binnedSpikePath):
  3987                                                       os.remove(binnedSpikePath)
  3988         1      61040.0  61040.0      0.3          writer = NixIO(filename=binnedSpikePath)
  3989         1   15094937.0 15094937.0     69.0          writer.write_block(spikeMatBlock, use_obj_names=True)
  3990         1     782004.0 782004.0      3.6          writer.close()
  3991         1         25.0     25.0      0.0      return spikeMatBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: calcFR at line 3993

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3993                                           @profile
  3994                                           def calcFR(
  3995                                                   binnedPath, dataPath,
  3996                                                   suffix='fr', aggregateFun=None,
  3997                                                   chanNames=None, rasterOpts=None, verbose=False
  3998                                                   ):
  3999                                               print('Loading rasters...')
  4000                                               masterSpikeMats, _ = loadSpikeMats(
  4001                                                   binnedPath, rasterOpts,
  4002                                                   aggregateFun=aggregateFun,
  4003                                                   chans=chanNames,
  4004                                                   loadAll=True, checkReferences=False)
  4005                                               print('Loading data file...')
  4006                                               dataReader = nixio_fr.NixIO(
  4007                                                   filename=dataPath)
  4008                                               dataBlock = dataReader.read_block(
  4009                                                   block_index=0, lazy=True,
  4010                                                   signal_group_mode='split-all')
  4011                                               masterBlock = Block()
  4012                                               masterBlock.name = dataBlock.annotations['neo_name']
  4013                                               #
  4014                                               for segIdx, segSpikeMat in masterSpikeMats.items():
  4015                                                   print('Calculating FR for segment {}'.format(segIdx))
  4016                                                   spikeMatDF = segSpikeMat.reset_index().rename(
  4017                                                       columns={'bin': 't'})
  4018                                           
  4019                                                   dataSeg = dataBlock.segments[segIdx]
  4020                                                   dummyAsig = dataSeg.filter(
  4021                                                       objects=AnalogSignalProxy)[0].load(channel_indexes=[0])
  4022                                                   samplingRate = dummyAsig.sampling_rate
  4023                                                   newT = dummyAsig.times.magnitude
  4024                                                   spikeMatDF['t'] = spikeMatDF['t'] + newT[0]
  4025                                           
  4026                                                   segSpikeMatInterp = hf.interpolateDF(
  4027                                                       spikeMatDF, pd.Series(newT),
  4028                                                       kind='linear', fill_value=(0, 0),
  4029                                                       x='t')
  4030                                                   spikeMatBlockInterp = dataFrameToAnalogSignals(
  4031                                                       segSpikeMatInterp,
  4032                                                       idxT='t', useColNames=True,
  4033                                                       dataCol=segSpikeMatInterp.drop(columns='t').columns,
  4034                                                       samplingRate=samplingRate)
  4035                                                   spikeMatBlockInterp.name = dataBlock.annotations['neo_name']
  4036                                                   spikeMatBlockInterp.annotate(
  4037                                                       nix_name=dataBlock.annotations['neo_name'])
  4038                                                   spikeMatBlockInterp.segments[0].name = dataSeg.annotations['neo_name']
  4039                                                   spikeMatBlockInterp.segments[0].annotate(
  4040                                                       nix_name=dataSeg.annotations['neo_name'])
  4041                                                   asigList = spikeMatBlockInterp.filter(objects=AnalogSignal)
  4042                                                   for asig in asigList:
  4043                                                       asig.annotate(binWidth=rasterOpts['binWidth'])
  4044                                                       if '_raster' in asig.name:
  4045                                                           asig.name = asig.name.replace('_raster', '_' + suffix)
  4046                                                       asig.name = 'seg{}_{}'.format(segIdx, childBaseName(asig.name, 'seg'))
  4047                                                       asig.annotate(nix_name=asig.name)
  4048                                                   chanIdxList = spikeMatBlockInterp.filter(objects=ChannelIndex)
  4049                                                   for chanIdx in chanIdxList:
  4050                                                       if '_raster' in chanIdx.name:
  4051                                                           chanIdx.name = chanIdx.name.replace('_raster', '_' + suffix)
  4052                                                       chanIdx.annotate(nix_name=chanIdx.name)
  4053                                           
  4054                                                   # masterBlock.merge(spikeMatBlockInterp)
  4055                                                   frBlockPath = dataPath.replace('_analyze.nix', '_fr.nix')
  4056                                                   writer = NixIO(filename=frBlockPath)
  4057                                                   writer.write_block(spikeMatBlockInterp, use_obj_names=True)
  4058                                                   writer.close()
  4059                                               #
  4060                                               dataReader.file.close()
  4061                                               return masterBlock

