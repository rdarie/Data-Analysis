Timer unit: 1e-07 s

Total time: 42.1463 s
File: C\../../analysis-code/assembleExperimentData.py
Function: assembleExperimentDataWrapper at line 33

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    33                                           @profile
    34                                           def assembleExperimentDataWrapper():
    35         1       9720.0   9720.0      0.0      from currentExperiment import parseAnalysisOptions
    36         1      95138.0  95138.0      0.0      from docopt import docopt
    37         1      27284.0  27284.0      0.0      arguments = {arg.lstrip('-'): value for arg, value in docopt(__doc__).items()}
    38         1         19.0     19.0      0.0      expOpts, allOpts = parseAnalysisOptions(
    39         1         22.0     22.0      0.0          int(arguments['blockIdx']),
    40         1   14951812.0 14951812.0      3.5          arguments['exp'])
    41         1         66.0     66.0      0.0      globals().update(expOpts)
    42         1         63.0     63.0      0.0      globals().update(allOpts)
    43                                           
    44         1         16.0     16.0      0.0      applyTimeOffset = False
    45         1         16.0     16.0      0.0      suffixList = []
    46         1         21.0     21.0      0.0      if arguments['processAsigs']:
    47         1         17.0     17.0      0.0          suffixList.append('_analyze')
    48         1         18.0     18.0      0.0      if arguments['processRasters']:
    49         1         16.0     16.0      0.0          suffixList.append('_binarized')
    50         1         16.0     16.0      0.0      suffixList.append('_fr')
    51                                           
    52         1         17.0     17.0      0.0      for suffix in suffixList:
    53         1        523.0    523.0      0.0          print('assembling {}'.format(suffix))
    54         1         19.0     19.0      0.0          experimentDataPath = os.path.join(
    55         1         18.0     18.0      0.0              scratchFolder, arguments['analysisName'],
    56                                                       assembledName +
    57         1        194.0    194.0      0.0              suffix + '.nix')
    58                                                   # Scan ahead through all files and ensure that
    59                                                   # spikeTrains and units are present across all assembled files
    60         1         19.0     19.0      0.0          masterChanDF = pd.DataFrame([], columns=[
    61         1         15.0     15.0      0.0              'index', 'channel_names', 'channel_ids',
    62         1      45434.0  45434.0      0.0              'hasUnits', 'hasAsigs'
    63                                                       ])
    64         1      34930.0  34930.0      0.0          masterUnitDF = pd.DataFrame([], columns=['parentChanName'])
    65         1         22.0     22.0      0.0          blocksCache = {}
    66         4         99.0     24.8      0.0          for idx, trialBasePath in enumerate(trialsToAssemble):
    67                                                       trialDataPath = (
    68         3         57.0     19.0      0.0                  trialBasePath
    69         3        112.0     37.3      0.0                  .format(arguments['analysisName'])
    70         3        113.0     37.7      0.0                  .replace('.nix', '{}.nix'.format(suffix))
    71                                                           )
    72                                                       # dataReader, dataBlock = preproc.blockFromPath(
    73                                                       #     trialDataPath, lazy=True, reduceChannelIndexes=True)
    74         3         93.0     31.0      0.0              dataBlock = preproc.loadWithArrayAnn(
    75         3         56.0     18.7      0.0                  trialDataPath, fromRaw=False,
    76         3  211221087.0 70407029.0     50.1                  reduceChannelIndexes=True)
    77                                                       # [cI.name for cI in dataBlock.channel_indexes]
    78                                                       #pdb.set_trace()
    79                                                       #
    80         3         71.0     23.7      0.0              blocksCache[trialDataPath] = dataBlock
    81         3         56.0     18.7      0.0              if idx == 0:
    82         1         16.0     16.0      0.0                  masterDataPath = trialDataPath
    83       226      78194.0    346.0      0.0              for chIdx in dataBlock.filter(objects=ChannelIndex):
    84       223     196857.0    882.8      0.0                  chAlreadyThere = masterChanDF.index == chIdx.name
    85       223      16303.0     73.1      0.0                  if not chAlreadyThere.any():
    86        83    1257585.0  15151.6      0.3                      masterChanDF.loc[chIdx.name, 'hasUnits'] = len(chIdx.units) > 0
    87        83     360071.0   4338.2      0.1                      masterChanDF.loc[chIdx.name, 'hasAsigs'] = len(chIdx.analogsignals) > 0
    88        83       2120.0     25.5      0.0                      try:
    89        83       2439.0     29.4      0.0                          chIdxNames = chIdx.channel_names
    90        83       1816.0     21.9      0.0                          chIdxIDS = chIdx.channel_ids
    91        83     165677.0   1996.1      0.0                          print('chIdx index = {}'.format(chIdx.index))
    92        83       2335.0     28.1      0.0                          if not len(chIdxIDS):
    93        13        321.0     24.7      0.0                              chIdxIDS = [int(chIdx.index)]
    94        83       1799.0     21.7      0.0                          if not len(chIdxNames):
    95        13        256.0     19.7      0.0                              chIdxNames = [chIdx.name]
    96        83     381637.0   4598.0      0.1                          masterChanDF.loc[chIdx.name, 'index'] = int(chIdx.index)
    97        83     367065.0   4422.5      0.1                          masterChanDF.loc[chIdx.name, 'channel_names'] = chIdxNames
    98        83     366866.0   4420.1      0.1                          masterChanDF.loc[chIdx.name, 'channel_ids'] = chIdxIDS
    99                                                               except Exception:
   100                                                                   traceback.print_exc()
   101       249       7947.0     31.9      0.0                      for annName,  annVal in chIdx.annotations.items():
   102       166     775799.0   4673.5      0.2                          masterChanDF.loc[chIdx.name, annName] = annVal
   103                                                           else:
   104       140       3907.0     27.9      0.0                      if len(chIdx.units) > 0:
   105                                                                   masterChanDF.loc[chAlreadyThere, 'hasUnits'] = True
   106       140       3387.0     24.2      0.0                      if len(chIdx.analogsignals):
   107       140    1615414.0  11538.7      0.4                          masterChanDF.loc[chAlreadyThere, 'hasAsigs'] = True
   108        16     108484.0   6780.2      0.0              for unit in (dataBlock.filter(objects=Unit)):
   109        13      11339.0    872.2      0.0                  uAlreadyThere = masterUnitDF.index == unit.name
   110        13        961.0     73.9      0.0                  if not uAlreadyThere.any():
   111        52       1673.0     32.2      0.0                      for annName, annVal in unit.annotations.items():
   112        39     375894.0   9638.3      0.1                          masterUnitDF.loc[unit.name, annName] = annVal
   113        13        346.0     26.6      0.0                      unitParentChanName = unit.channel_index.name
   114        13      54923.0   4224.8      0.0                      masterUnitDF.loc[unit.name, 'parentChanName'] = unitParentChanName
   115                                                               # chAlreadyThere = masterChanDF.index == unitParentChanName
   116                                                       # dataReader.file.close()
   117                                                   # now merge the blocks
   118         2         55.0     27.5      0.0          for idx, trialBasePath in enumerate(trialsToAssemble):
   119                                                       trialDataPath = (
   120         2         37.0     18.5      0.0                  trialBasePath
   121         2         72.0     36.0      0.0                  .format(arguments['analysisName'])
   122         2         80.0     40.0      0.0                  .replace('.nix', '{}.nix'.format(suffix))
   123                                                           )
   124         2       1190.0    595.0      0.0              print('loading trial {}'.format(trialDataPath))
   125         2         36.0     18.0      0.0              if idx == 0:
   126         1         35.0     35.0      0.0                  blocksCache[trialDataPath].name = experimentName + suffix
   127         1         17.0     17.0      0.0                  if applyTimeOffset:
   128                                                               masterTStart = blocksCache[trialDataPath].filter(objects=AnalogSignal)[0].t_start
   129                                                               oldTStop = blocksCache[trialDataPath].filter(objects=AnalogSignal)[0].t_stop
   130                                                       else:
   131         1         38.0     38.0      0.0                  blocksCache[trialDataPath].name = blocksCache[masterDataPath].name
   132         1         17.0     17.0      0.0                  if applyTimeOffset:
   133                                                               tStart = blocksCache[trialDataPath].filter(objects=AnalogSignal)[0].t_start
   134                                                               timeOffset = oldTStop - tStart
   135                                                               blocksCache[trialDataPath] = hf.timeOffsetBlock(
   136                                                                   blocksCache[trialDataPath], timeOffset, masterTStart)
   137                                                               #  [i.times for i in dataBlock.filter(objects=SpikeTrain)]
   138                                                               #  [i.unit.channel_index.name for i in masterBlock.filter(objects=SpikeTrain)]
   139                                                               tStop = dataBlock.filter(objects=AnalogSignal)[0].t_stop
   140                                                       # if suffix == '_binarized':
   141                                                       #     for seg in blocksCache[trialDataPath].segments:
   142                                                       #         seg.spiketrains = []
   143       168     327816.0   1951.3      0.1              for rowIdx, row in masterChanDF.iterrows():
   144       166       4052.0     24.4      0.0                  matchingCh = blocksCache[trialDataPath].filter(
   145       166    4723683.0  28455.9      1.1                      objects=ChannelIndex, name=rowIdx)
   146       166       4337.0     26.1      0.0                  if not len(matchingCh):
   147                                                               '''
   148                                                                   # [ch.index for ch in blocksCache[trialDataPath].filter(objects=ChannelIndex)]
   149                                                                   # if row['index'] is None:
   150                                                                   #     pdb.set_trace()
   151                                                                   #     chIdx = ChannelIndex(
   152                                                                   #         name=rowIdx,
   153                                                                   #         index=np.asarray([0]),
   154                                                                   #         channel_ids=np.asarray([0]),
   155                                                                   #         channel_names=np.asarray([rowIdx]),
   156                                                                   #         file_origin=blocksCache[trialDataPath].channel_indexes[-1].file_origin
   157                                                                   #         )
   158                                                                   # else:
   159                                                               '''
   160                                                               # create it
   161        16       8051.0    503.2      0.0                      print('ch {} not found; creating now'.format(rowIdx))
   162        16        317.0     19.8      0.0                      chIdx = ChannelIndex(
   163        16        283.0     17.7      0.0                          name=rowIdx,
   164        16       7289.0    455.6      0.0                          index=np.asarray([row['index']]).flatten(),
   165        16       6924.0    432.8      0.0                          channel_ids=np.asarray([row['channel_ids']]).flatten(),
   166        16       5604.0    350.2      0.0                          channel_names=np.asarray([row['channel_names']]).flatten(),
   167        16       6660.0    416.2      0.0                          file_origin=blocksCache[trialDataPath].channel_indexes[-1].file_origin
   168                                                                   )
   169        80     127369.0   1592.1      0.0                      for aN in row.drop(['index', 'channel_names', 'channel_ids']).index:
   170        64      15874.0    248.0      0.0                          chIdx.annotations[aN] = row[aN]
   171        16        474.0     29.6      0.0                      blocksCache[trialDataPath].channel_indexes.append(chIdx)
   172        16        376.0     23.5      0.0                      chIdx.block = blocksCache[trialDataPath]
   173                                                               # create blank asigs
   174        16       3390.0    211.9      0.0                      if row['hasAsigs']:
   175                                                                   dummyAsig = blocksCache[trialDataPath].filter(objects=AnalogSignal)[0].copy()
   176                                                                   dummyAsig.name = 'seg0_' + chIdx.name
   177                                                                   dummyAsig.annotations['neo_name'] = dummyAsig.name
   178                                                                   dummyAsig.magnitude[:] = 0
   179                                                                   dummyAsig.channel_index = chIdx
   180                                                                   chIdx.analogsignals.append(dummyAsig)
   181                                                                   blocksCache[trialDataPath].segments[0].analogsignals.append(dummyAsig)
   182                                                                   dummyAsig.segment = blocksCache[trialDataPath].segments[0]
   183                                                                   # pdb.set_trace()
   184         2     106319.0  53159.5      0.0              anySpikeTrains = blocksCache[trialDataPath].filter(objects=SpikeTrain)
   185         2        102.0     51.0      0.0              if len(anySpikeTrains):
   186         2        759.0    379.5      0.0                  wvfUnits = anySpikeTrains[0].waveforms.units
   187         2        379.0    189.5      0.0                  stTimeUnits = anySpikeTrains[0].units
   188                                                       else:
   189                                                           stTimeUnits = pq.s
   190                                                           wvfUnits = pq.uV
   191        15      51026.0   3401.7      0.0              for rowIdx, row in masterUnitDF.iterrows():
   192        14        452.0     32.3      0.0                  matchingUnit = blocksCache[trialDataPath].filter(
   193        14     524330.0  37452.1      0.1                      objects=Unit, name=rowIdx)
   194        14        440.0     31.4      0.0                  if not len(matchingUnit):
   195         1        999.0    999.0      0.0                      parentChanName = row['parentChanName']
   196                                                               # parentChanName = rowIdx
   197                                                               # if parentChanName.endswith('_stim#0'):
   198                                                               #     parentChanName.replace('_stim#0', '')
   199                                                               # if parentChanName.endswith('#0'):
   200                                                               #     parentChanName.replace('#0', '')
   201         1         59.0     59.0      0.0                      matchingCh = blocksCache[trialDataPath].filter(
   202         1      84547.0  84547.0      0.0                          objects=ChannelIndex, name=rowIdx)
   203         1         80.0     80.0      0.0                      if not len(matchingCh):
   204                                                                   '''
   205                                                                       # [ch.index for ch in blocksCache[trialDataPath].filter(objects=ChannelIndex)]
   206                                                                       # if row['index'] is None:
   207                                                                       #     pdb.set_trace()
   208                                                                       #     chIdx = ChannelIndex(
   209                                                                       #         name=rowIdx,
   210                                                                       #         index=np.asarray([0]),
   211                                                                       #         channel_ids=np.asarray([0]),
   212                                                                       #         channel_names=np.asarray([rowIdx]),
   213                                                                       #         file_origin=blocksCache[trialDataPath].channel_indexes[-1].file_origin
   214                                                                       #         )
   215                                                                       # else:
   216                                                                   '''
   217                                                                   # create it
   218         1       1063.0   1063.0      0.0                          print('ch {} not found; creating now'.format(rowIdx))
   219         1         54.0     54.0      0.0                          chIdx = ChannelIndex(
   220         1         50.0     50.0      0.0                              name=rowIdx,
   221         1       2096.0   2096.0      0.0                              index=np.asarray([row['index']]).flatten(),
   222                                                                       channel_ids=np.asarray([row['channel_ids']]).flatten(),
   223                                                                       channel_names=np.asarray([row['channel_names']]).flatten(),
   224                                                                       file_origin=blocksCache[trialDataPath].channel_indexes[-1].file_origin
   225                                                                       )
   226                                                                   for aN in row.drop(['index', 'channel_names', 'channel_ids']).index:
   227                                                                       chIdx.annotations[aN] = row[aN]
   228                                                                   blocksCache[trialDataPath].channel_indexes.append(chIdx)
   229                                                                   chIdx.block = blocksCache[trialDataPath]
   230                                                                   # create blank asigs
   231                                                                   if row['hasAsigs']:
   232                                                                       dummyAsig = blocksCache[trialDataPath].filter(objects=AnalogSignal)[0].copy()
   233                                                                       dummyAsig.name = 'seg0_' + chIdx.name
   234                                                                       dummyAsig.annotations['neo_name'] = dummyAsig.name
   235                                                                       dummyAsig.magnitude[:] = 0
   236                                                                       dummyAsig.channel_index = chIdx
   237                                                                       chIdx.analogsignals.append(dummyAsig)
   238                                                                       blocksCache[trialDataPath].segments[0].analogsignals.append(dummyAsig)
   239                                                                       dummyAsig.segment = blocksCache[trialDataPath].segments[0]
   240                                                                       # pdb.set_trace()
   241        13     452419.0  34801.5      0.1                  anySpikeTrains = blocksCache[trialDataPath].filter(objects=SpikeTrain)
   242        13        501.0     38.5      0.0                  if len(anySpikeTrains):
   243        13       5962.0    458.6      0.0                      wvfUnits = anySpikeTrains[0].waveforms.units
   244        13       2471.0    190.1      0.0                      stTimeUnits = anySpikeTrains[0].units
   245                                                           else:
   246                                                               stTimeUnits = pq.s
   247                                                               wvfUnits = pq.uV
   248       182     452363.0   2485.5      0.1                  for rowIdx, row in masterUnitDF.iterrows():
   249       169       5179.0     30.6      0.0                      matchingUnit = blocksCache[trialDataPath].filter(
   250       169    6426354.0  38025.8      1.5                          objects=Unit, name=rowIdx)
   251       169       5750.0     34.0      0.0                      if not len(matchingUnit):
   252         8       6935.0    866.9      0.0                          parentChanName = row['parentChanName']
   253                                                                   # parentChanName = rowIdx
   254                                                                   # if parentChanName.endswith('_stim#0'):
   255                                                                   #     parentChanName.replace('_stim#0', '')
   256                                                                   # if parentChanName.endswith('#0'):
   257                                                                   #     parentChanName.replace('#0', '')
   258         8        436.0     54.5      0.0                          matchingCh = blocksCache[trialDataPath].filter(
   259         8     402705.0  50338.1      0.1                              objects=ChannelIndex, name=parentChanName)
   260                                                                   '''
   261                                                                       if not len(matchingCh):
   262                                                                           masterListEntry = masterChanDF.loc[parentChanName, :]
   263                                                                           parentChIdx = ChannelIndex(
   264                                                                               name=parentChanName,
   265                                                                               index=masterListEntry['index'],
   266                                                                               channel_ids=masterListEntry['channel_ids'],
   267                                                                               channel_names=masterListEntry['channel_names'],
   268                                                                               file_origin=blocksCache[trialDataPath].channel_indexes[-1].file_origin
   269                                                                               )
   270                                                                           blocksCache[trialDataPath].channel_indexes.append(parentChIdx)
   271                                                                           parentChIdx.block = blocksCache[trialDataPath]
   272                                                                       else:
   273                                                                           parentChIdx = matchingCh[0]
   274                                                                   '''
   275         8        375.0     46.9      0.0                          parentChIdx = matchingCh[0]
   276         8       6901.0    862.6      0.0                          print('unit {} not found; creating now'.format(rowIdx))
   277         8       3831.0    478.9      0.0                          newUnit = Unit(name=rowIdx)
   278        40       6233.0    155.8      0.0                          for annName in row.index:
   279        32      11105.0    347.0      0.0                              newUnit.annotations[annName] = row[annName]
   280         8        219.0     27.4      0.0                          newUnit.channel_index = parentChIdx
   281         8        296.0     37.0      0.0                          parentChIdx.units.append(newUnit)
   282        16        945.0     59.1      0.0                          for seg in blocksCache[trialDataPath].segments:
   283         8        390.0     48.8      0.0                              dummyST = SpikeTrain(
   284         8        381.0     47.6      0.0                                  times=[], units=stTimeUnits,
   285         8      85737.0  10717.1      0.0                                  t_stop=seg.filter(objects=AnalogSignal)[0].t_stop,
   286         8       3168.0    396.0      0.0                                  waveforms=np.array([]).reshape((0, 0, 0)) * wvfUnits,
   287         8      19483.0   2435.4      0.0                                  name=seg.name + newUnit.name)
   288         8        310.0     38.8      0.0                              dummyST.unit = newUnit
   289         8        248.0     31.0      0.0                              dummyST.segment = seg
   290         8        293.0     36.6      0.0                              newUnit.spiketrains.append(dummyST)
   291         8        312.0     39.0      0.0                              seg.spiketrains.append(dummyST)
   292        13        357.0     27.5      0.0                  typesNeedRenaming = [SpikeTrain, AnalogSignal, Event]
   293        13        296.0     22.8      0.0                  blocksCache[trialDataPath].segments[0].name = 'seg{}_{}'.format(
   294        13        722.0     55.5      0.0                      idx, blocksCache[trialDataPath].name)
   295        52       1072.0     20.6      0.0                  for objType in typesNeedRenaming:
   296        39    1344706.0  34479.6      0.3                      listOfChildren = blocksCache[trialDataPath].filter(objects=objType)
   297        39       1118.0     28.7      0.0                      print('{}\n{} objects of type {}'.format(
   298        39      22624.0    580.1      0.0                          trialDataPath, len(listOfChildren), objType
   299                                                               ))
   300      1261      26198.0     20.8      0.0                      for child in listOfChildren:
   301      1222      86455.0     70.7      0.0                          childBaseName = preproc.childBaseName(child.name, 'seg')
   302      1222      31909.0     26.1      0.0                          child.name = 'seg{}_{}'.format(idx, childBaseName)
   303        13     347372.0  26720.9      0.1                  blocksCache[trialDataPath].create_relationship()
   304        13     783735.0  60287.3      0.2                  blocksCache[trialDataPath] = preproc.purgeNixAnn(blocksCache[trialDataPath])
   305                                                           ########
   306        13        309.0     23.8      0.0                  sanityCheck = False
   307        13        504.0     38.8      0.0                  if sanityCheck and idx == 2:
   308                                                               doublePath = trialDataPath.replace(suffix, suffix + '_backup')
   309                                                               if os.path.exists(doublePath):
   310                                                                   os.remove(doublePath)
   311                                                               print('writing {} ...'.format(doublePath))
   312                                                               for idx, chIdx in enumerate(blocksCache[trialDataPath].channel_indexes):
   313                                                                   print('{}: {}, chan_id = {}'.format(
   314                                                                       chIdx.name, chIdx.index, chIdx.channel_ids))
   315                                                               writer = neo.io.NixIO(filename=doublePath)
   316                                                               writer.write_block(blocksCache[trialDataPath], use_obj_names=True)
   317                                                               writer.close()
   318                                                           ############
   319        13        316.0     24.3      0.0                  if idx > 0:
   320                                                               blocksCache[masterDataPath].merge(blocksCache[trialDataPath])
   321                                                               if applyTimeOffset:
   322                                                                   oldTStop = tStop
   323                                                       '''
   324                                                           print([evSeg.events[0].name for evSeg in masterBlock.segments])
   325                                                           print([asig.name for asig in masterBlock.filter(objects=AnalogSignal)])
   326                                                           print([st.name for st in masterBlock.filter(objects=SpikeTrain)])
   327                                                           print([ev.name for ev in masterBlock.filter(objects=Event)])
   328                                                           print([chIdx.name for chIdx in blocksCache[trialDataPath].filter(objects=ChannelIndex)])
   329                                                           print([un.name for un in masterBlock.filter(objects=Unit)])
   330                                                       '''
   331                                                       # blocksCache[masterDataPath].create_relationship()
   332         1       1242.0   1242.0      0.0              if os.path.exists(experimentDataPath):
   333         1     845632.0 845632.0      0.2                  os.remove(experimentDataPath)
   334         1      65770.0  65770.0      0.0              writer = neo.io.NixIO(filename=experimentDataPath)
   335         1       1336.0   1336.0      0.0              print('writing {} ...'.format(experimentDataPath))
   336         1  170390617.0 170390617.0     40.4              writer.write_block(blocksCache[masterDataPath], use_obj_names=True)
   337         1    1043858.0 1043858.0      0.2              writer.close()
   338         1         44.0     44.0      0.0              if arguments['commitResults']:
   339                                                           analysisProcessedSubFolder = os.path.join(
   340                                                               processedFolder, arguments['analysisName']
   341                                                               )
   342                                                           if not os.path.exists(analysisProcessedSubFolder):
   343                                                               os.makedirs(analysisProcessedSubFolder, exist_ok=True)
   344                                                           for suffix in suffixList:
   345                                                               experimentDataPath = os.path.join(
   346                                                                   scratchFolder, arguments['analysisName'],
   347                                                                   assembledName +
   348                                                                   suffix + '.nix')
   349                                                               processedOutPath = os.path.join(
   350                                                                   analysisProcessedSubFolder, arguments['analysisName'],
   351                                                                   assembledName +
   352                                                                   suffix + '.nix')
   353                                                               print('copying from:\n{}\ninto\n{}'.format(experimentDataPath, processedOutPath))
   354                                                               shutil.copyfile(experimentDataPath, processedOutPath)
   355                                               return

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: analogSignalsToDataFrame at line 43

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    43                                           @profile
    44                                           def analogSignalsToDataFrame(
    45                                                   analogsignals, idxT='t', useChanNames=False):
    46                                               asigList = []
    47                                               for asig in analogsignals:
    48                                                   if asig.shape[1] == 1:
    49                                                       if useChanNames:
    50                                                           colNames = [str(asig.channel_index.name)]
    51                                                       else:
    52                                                           colNames = [str(asig.name)]
    53                                                   else:
    54                                                       colNames = [
    55                                                           asig.name +
    56                                                           '_{}'.format(i) for i in
    57                                                           asig.channel_index.channel_ids
    58                                                           ]
    59                                                   asigList.append(
    60                                                       pd.DataFrame(
    61                                                           asig.magnitude, columns=colNames,
    62                                                           index=range(asig.shape[0])))
    63                                               asigList.append(
    64                                                   pd.DataFrame(
    65                                                       asig.times.magnitude, columns=[idxT],
    66                                                       index=range(asig.shape[0])))
    67                                               return pd.concat(asigList, axis=1)

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: listChanNames at line 69

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    69                                           @profile
    70                                           def listChanNames(
    71                                                   dataBlock, chanQuery,
    72                                                   objType=AnalogSignalProxy, condition=None):
    73                                               allChanList = [
    74                                                   i.name
    75                                                   for i in dataBlock.filter(objects=objType)]
    76                                               if condition == 'hasAsigs':
    77                                                   allChanList = [
    78                                                       i
    79                                                       for i in allChanList
    80                                                       if len(dataBlock.filter(objects=objType, name=i)[0].analogsignals)
    81                                                   ]
    82                                               chansToTrigger = pd.DataFrame(
    83                                                   np.unique(allChanList),
    84                                                   columns=['chanName'])
    85                                               if chanQuery is not None:
    86                                                   chansToTrigger = chansToTrigger.query(
    87                                                       chanQuery, engine='python')['chanName'].to_list()
    88                                               else:
    89                                                   chansToTrigger = chansToTrigger['chanName'].to_list()
    90                                               return chansToTrigger

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: spikeDictToSpikeTrains at line 92

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    92                                           @profile
    93                                           def spikeDictToSpikeTrains(
    94                                                   spikes, block=None, seg=None,
    95                                                   probeName='insTD', t_stop=None,
    96                                                   waveformUnits=pq.uV,
    97                                                   sampling_rate=3e4 * pq.Hz):
    98                                           
    99                                               if block is None:
   100                                                   assert seg is None
   101                                                   block = Block()
   102                                                   seg = Segment(name=probeName + ' segment')
   103                                                   block.segments.append(seg)
   104                                           
   105                                               if t_stop is None:
   106                                                   t_stop = hf.getLastSpikeTime(spikes) + 1
   107                                           
   108                                               for idx, chanName in enumerate(spikes['ChannelID']):
   109                                                   #  unique units on this channel
   110                                                   unitsOnThisChan = pd.unique(spikes['Classification'][idx])
   111                                                   nixChanName = probeName + '{}'.format(chanName)
   112                                                   chanIdx = ChannelIndex(
   113                                                       name=nixChanName,
   114                                                       index=np.asarray([idx]),
   115                                                       channel_names=np.asarray([nixChanName]))
   116                                                   block.channel_indexes.append(chanIdx)
   117                                                   
   118                                                   for unitIdx, unitName in enumerate(unitsOnThisChan):
   119                                                       unitMask = spikes['Classification'][idx] == unitName
   120                                                       # this unit's spike timestamps
   121                                                       theseTimes = spikes['TimeStamps'][idx][unitMask]
   122                                                       # this unit's waveforms
   123                                                       if len(spikes['Waveforms'][idx].shape) == 3:
   124                                                           theseWaveforms = spikes['Waveforms'][idx][unitMask, :, :]
   125                                                           theseWaveforms = np.swapaxes(theseWaveforms, 1, 2)
   126                                                       elif len(spikes['Waveforms'][idx].shape) == 2:
   127                                                           theseWaveforms = (
   128                                                               spikes['Waveforms'][idx][unitMask, np.newaxis, :])
   129                                                       else:
   130                                                           raise(Exception('spikes[Waveforms] has bad shape'))
   131                                           
   132                                                       unitName = '{}#{}'.format(nixChanName, unitIdx)
   133                                                       unit = Unit(name=unitName)
   134                                                       unit.channel_index = chanIdx
   135                                                       chanIdx.units.append(unit)
   136                                           
   137                                                       train = SpikeTrain(
   138                                                           times=theseTimes, t_stop=t_stop, units='sec',
   139                                                           name=unitName, sampling_rate=sampling_rate,
   140                                                           waveforms=theseWaveforms*waveformUnits,
   141                                                           left_sweep=0, dtype=np.float32)
   142                                                       unit.spiketrains.append(train)
   143                                                       seg.spiketrains.append(train)
   144                                           
   145                                                       unit.create_relationship()
   146                                                   chanIdx.create_relationship()
   147                                               seg.create_relationship()
   148                                               block.create_relationship()
   149                                               return block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: spikeTrainsToSpikeDict at line 151

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   151                                           @profile
   152                                           def spikeTrainsToSpikeDict(
   153                                                   spiketrains):
   154                                               nCh = len(spiketrains)
   155                                               spikes = {
   156                                                   'ChannelID': [i for i in range(nCh)],
   157                                                   'Classification': [np.asarray([]) for i in range(nCh)],
   158                                                   'NEUEVWAV_HeaderIndices': [None for i in range(nCh)],
   159                                                   'TimeStamps': [np.asarray([]) for i in range(nCh)],
   160                                                   'Units': 'uV',
   161                                                   'Waveforms': [np.asarray([]) for i in range(nCh)],
   162                                                   'basic_headers': {'TimeStampResolution': 3e4},
   163                                                   'extended_headers': []
   164                                                   }
   165                                               for idx, st in enumerate(spiketrains):
   166                                                   spikes['ChannelID'][idx] = st.name
   167                                                   if len(spikes['TimeStamps'][idx]):
   168                                                       spikes['TimeStamps'][idx] = np.stack((
   169                                                           spikes['TimeStamps'][idx],
   170                                                           st.times.magnitude), axis=-1)
   171                                                   else:
   172                                                       spikes['TimeStamps'][idx] = st.times.magnitude
   173                                                   
   174                                                   theseWaveforms = np.swapaxes(
   175                                                       st.waveforms, 1, 2)
   176                                                   theseWaveforms = np.atleast_2d(np.squeeze(
   177                                                       theseWaveforms))
   178                                                       
   179                                                   if len(spikes['Waveforms'][idx]):
   180                                                       spikes['Waveforms'][idx] = np.stack((
   181                                                           spikes['Waveforms'][idx],
   182                                                           theseWaveforms.magnitude), axis=-1)
   183                                                   else:
   184                                                       spikes['Waveforms'][idx] = theseWaveforms.magnitude
   185                                                   
   186                                                   classVals = st.times.magnitude ** 0 * idx
   187                                                   if len(spikes['Classification'][idx]):
   188                                                       spikes['Classification'][idx] = np.stack((
   189                                                           spikes['Classification'][idx],
   190                                                           classVals), axis=-1)
   191                                                   else:
   192                                                       spikes['Classification'][idx] = classVals
   193                                               return spikes

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: channelIndexesToSpikeDict at line 195

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   195                                           @profile
   196                                           def channelIndexesToSpikeDict(
   197                                                   channel_indexes):
   198                                               nCh = len(channel_indexes)
   199                                               spikes = {
   200                                                   'ChannelID': [i for i in range(nCh)],
   201                                                   'Classification': [np.asarray([]) for i in range(nCh)],
   202                                                   'NEUEVWAV_HeaderIndices': [None for i in range(nCh)],
   203                                                   'TimeStamps': [np.asarray([]) for i in range(nCh)],
   204                                                   'Units': 'uV',
   205                                                   'Waveforms': [np.asarray([]) for i in range(nCh)],
   206                                                   'basic_headers': {'TimeStampResolution': 3e4},
   207                                                   'extended_headers': []
   208                                                   }
   209                                               #  allocate fields for annotations
   210                                               for dummyCh in channel_indexes:
   211                                                   if len(dummyCh.units):
   212                                                       dummyUnit = dummyCh.units[0]
   213                                                       if len(dummyUnit.spiketrains):
   214                                                           if len(dummyUnit.spiketrains[0].times):
   215                                                               break
   216                                               dummySt = [
   217                                                   st
   218                                                   for st in dummyUnit.spiketrains
   219                                                   if len(st.times)][0]
   220                                               #  allocate fields for array annotations (per spike)
   221                                               if dummySt.array_annotations:
   222                                                   for key in dummySt.array_annotations.keys():
   223                                                       spikes.update({key: [np.asarray([]) for i in range(nCh)]})
   224                                                   
   225                                               maxUnitIdx = 0
   226                                               for idx, chIdx in enumerate(channel_indexes):
   227                                                   spikes['ChannelID'][idx] = chIdx.name
   228                                                   for unitIdx, thisUnit in enumerate(chIdx.units):
   229                                                       for stIdx, st in enumerate(thisUnit.spiketrains):
   230                                                           if not len(st.times):
   231                                                               continue
   232                                                           #  print(
   233                                                           #      'unit {} has {} spiketrains'.format(
   234                                                           #          thisUnit.name,
   235                                                           #          len(thisUnit.spiketrains)))
   236                                                           if len(spikes['TimeStamps'][idx]):
   237                                                               spikes['TimeStamps'][idx] = np.concatenate((
   238                                                                   spikes['TimeStamps'][idx],
   239                                                                   st.times.magnitude), axis=0)
   240                                                           else:
   241                                                               spikes['TimeStamps'][idx] = st.times.magnitude
   242                                                           #  reshape waveforms to comply with BRM convention
   243                                                           theseWaveforms = np.swapaxes(
   244                                                               st.waveforms, 1, 2)
   245                                                           theseWaveforms = np.atleast_2d(np.squeeze(
   246                                                               theseWaveforms))
   247                                                           #  append waveforms
   248                                                           if len(spikes['Waveforms'][idx]):
   249                                                               try:
   250                                                                   spikes['Waveforms'][idx] = np.concatenate((
   251                                                                       spikes['Waveforms'][idx],
   252                                                                       theseWaveforms.magnitude), axis=0)
   253                                                               except Exception:
   254                                                                   traceback.print_exc()
   255                                                           else:
   256                                                               spikes['Waveforms'][idx] = theseWaveforms.magnitude
   257                                                           #  give each unit a global index
   258                                                           classVals = st.times.magnitude ** 0 * maxUnitIdx
   259                                                           st.array_annotations.update({'Classification': classVals})
   260                                                           #  expand array_annotations into spikes dict
   261                                                           for key, value in st.array_annotations.items():
   262                                                               if len(spikes[key][idx]):
   263                                                                   spikes[key][idx] = np.concatenate((
   264                                                                       spikes[key][idx],
   265                                                                       value), axis=0)
   266                                                               else:
   267                                                                   spikes[key][idx] = value
   268                                                           for key, value in st.annotations.items():
   269                                                               if key not in spikes['basic_headers']:
   270                                                                   spikes['basic_headers'].update({key: {}})
   271                                                               try:
   272                                                                   spikes['basic_headers'][key].update({maxUnitIdx: value})
   273                                                               except Exception:
   274                                                                   pass
   275                                                           maxUnitIdx += 1
   276                                               return spikes

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: unitSpikeTrainArrayAnnToDF at line 278

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   278                                           @profile
   279                                           def unitSpikeTrainArrayAnnToDF(
   280                                                   spikeTrainContainer):
   281                                               #  list contains different segments
   282                                               if isinstance(spikeTrainContainer, ChannelIndex):
   283                                                   assert len(spikeTrainContainer.units) == 0
   284                                                   spiketrains = spikeTrainContainer.units[0].spiketrains
   285                                               elif isinstance(spikeTrainContainer, Unit):
   286                                                   spiketrains = spikeTrainContainer.spiketrains
   287                                               elif isinstance(spikeTrainContainer, list):
   288                                                   spiketrains = spikeTrainContainer
   289                                               fullAnnotationsDict = {}
   290                                               for segIdx, st in enumerate(spiketrains):
   291                                                   theseAnnDF = pd.DataFrame(st.array_annotations)
   292                                                   theseAnnDF['t'] = st.times.magnitude
   293                                                   fullAnnotationsDict.update({segIdx: theseAnnDF})
   294                                               annotationsDF = pd.concat(
   295                                                   fullAnnotationsDict, names=['segment', 'index'], sort=True)
   296                                               return annotationsDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getSpikeDFMetadata at line 298

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   298                                           @profile
   299                                           def getSpikeDFMetadata(spikeDF, metaDataCols):
   300                                               spikeDF.reset_index(inplace=True)
   301                                               metaDataCols = np.atleast_1d(metaDataCols)
   302                                               spikeDF.index.name = 'metaDataIdx'
   303                                               metaDataDF = spikeDF.loc[:, metaDataCols].copy()
   304                                               newSpikeDF = spikeDF.drop(columns=metaDataCols).reset_index()
   305                                               return newSpikeDF, metaDataDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: transposeSpikeDF at line 307

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   307                                           @profile
   308                                           def transposeSpikeDF(
   309                                                   spikeDF, transposeToColumns,
   310                                                   fastTranspose=False):
   311                                               newColumnNames = np.atleast_1d(transposeToColumns).tolist()
   312                                               originalColumnNames = np.atleast_1d(spikeDF.columns.names)
   313                                               metaDataCols = np.setdiff1d(spikeDF.index.names, newColumnNames).tolist()
   314                                               if fastTranspose:
   315                                                   #  fast but memory inefficient
   316                                                   return spikeDF.stack().unstack(transposeToColumns)
   317                                               else:
   318                                                   raise(Warning('Caution! transposeSpikeDF might not be working, needs testing RD 06252019'))
   319                                                   #  stash annotations, transpose, recover annotations
   320                                                   newSpikeDF, metaDataDF = getSpikeDFMetadata(spikeDF, metaDataCols)
   321                                                   del spikeDF
   322                                                   gc.collect()
   323                                                   #
   324                                                   newSpikeDF = newSpikeDF.stack().unstack(newColumnNames)
   325                                                   newSpikeDF.reset_index(inplace=True)
   326                                                   #  set the index
   327                                                   newIdxLabels = np.concatenate(
   328                                                       [originalColumnNames, metaDataCols]).tolist()
   329                                                   newSpikeDF.loc[:, metaDataCols] = (
   330                                                       metaDataDF
   331                                                       .loc[newSpikeDF['metaDataIdx'].to_list(), metaDataCols]
   332                                                       .to_numpy())
   333                                                   newSpikeDF = (
   334                                                       newSpikeDF
   335                                                       .drop(columns=['metaDataIdx'])
   336                                                       .set_index(newIdxLabels))
   337                                                   return newSpikeDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: concatenateBlocks at line 339

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   339                                           @profile
   340                                           def concatenateBlocks(
   341                                                   asigBlocks, spikeBlocks, eventBlocks, chunkingMetadata,
   342                                                   samplingRate, chanQuery, lazy, trackMemory, verbose
   343                                                   ):
   344                                               # Scan ahead through all files and ensure that
   345                                               # spikeTrains and units are present across all assembled files
   346                                               channelIndexCache = {}
   347                                               unitCache = {}
   348                                               asigCache = []
   349                                               asigAnnCache = {}
   350                                               spiketrainCache = {}
   351                                               eventCache = {}
   352                                               # get list of channels and units
   353                                               for idx, (chunkIdxStr, chunkMeta) in enumerate(chunkingMetadata.items()):
   354                                                   gc.collect()
   355                                                   chunkIdx = int(chunkIdxStr)
   356                                                   asigBlock = asigBlocks[chunkIdx]
   357                                                   asigSeg = asigBlock.segments[0]
   358                                                   spikeBlock = spikeBlocks[chunkIdx]
   359                                                   eventBlock = eventBlocks[chunkIdx]
   360                                                   eventSeg = eventBlock.segments[0]
   361                                                   for chIdx in asigBlock.filter(objects=ChannelIndex):
   362                                                       chAlreadyThere = (chIdx.name in channelIndexCache.keys())
   363                                                       if not chAlreadyThere:
   364                                                           newChIdx = copy(chIdx)
   365                                                           newChIdx.analogsignals = []
   366                                                           newChIdx.units = []
   367                                                           channelIndexCache[chIdx.name] = newChIdx
   368                                                   for unit in (spikeBlock.filter(objects=Unit)):
   369                                                       if lazy:
   370                                                           theseSpiketrains = []
   371                                                           for stP in unit.spiketrains:
   372                                                               st = loadStProxy(stP)
   373                                                               if len(st.times) > 0:
   374                                                                   theseSpiketrains.append(st)
   375                                                       else:
   376                                                           theseSpiketrains = [
   377                                                               st
   378                                                               for st in unit.spiketrains
   379                                                               if len(st.times)
   380                                                               ]
   381                                                       for st in theseSpiketrains:
   382                                                           st = loadObjArrayAnn(st)
   383                                                           if len(st.times):
   384                                                               st.magnitude[:] = st.times.magnitude + spikeBlock.annotations['chunkTStart']
   385                                                               st.t_start = min(0 * pq.s, st.times[0] * 0.999)
   386                                                               st.t_stop = max(
   387                                                                   st.t_stop + spikeBlock.annotations['chunkTStart'] * pq.s,
   388                                                                   st.times[-1] * 1.001)
   389                                                           else:
   390                                                               st.t_start += spikeBlock.annotations['chunkTStart'] * pq.s
   391                                                               st.t_stop += spikeBlock.annotations['chunkTStart'] * pq.s
   392                                                       uAlreadyThere = (unit.name in unitCache.keys())
   393                                                       if not uAlreadyThere:
   394                                                           newUnit = copy(unit)
   395                                                           newUnit.spiketrains = []
   396                                                           newUnit.annotations['parentChanName'] = unit.channel_index.name
   397                                                           unitCache[unit.name] = newUnit
   398                                                           spiketrainCache[unit.name] = theseSpiketrains
   399                                                       else:
   400                                                           spiketrainCache[unit.name] = spiketrainCache[unit.name] + theseSpiketrains
   401                                                   #
   402                                                   if lazy:
   403                                                       evList = [
   404                                                           evP.load()
   405                                                           for evP in eventSeg.events]
   406                                                   else:
   407                                                       evList = eventSeg.events
   408                                                   for event in evList:
   409                                                       event.magnitude[:] = event.magnitude + eventBlock.annotations['chunkTStart']
   410                                                       if event.name in eventCache.keys():
   411                                                           eventCache[event.name].append(event)
   412                                                       else:
   413                                                           eventCache[event.name] = [event]
   414                                                   # take the requested analog signal channels
   415                                                   if lazy:
   416                                                       tdChanNames = listChanNames(
   417                                                           asigBlock, chanQuery, objType=AnalogSignalProxy)
   418                                                       #############
   419                                                       # tdChanNames = ['seg0_utah1', 'seg0_utah10']
   420                                                       ##############
   421                                                       asigList = []
   422                                                       for asigP in asigSeg.analogsignals:
   423                                                           if asigP.name in tdChanNames:
   424                                                               asig = asigP.load()
   425                                                               asig.channel_index = asigP.channel_index
   426                                                               asigList.append(asig)
   427                                                               if trackMemory:
   428                                                                   print('loading {} from proxy object. memory usage: {:.1f} MB'.format(
   429                                                                       asigP.name, prf.memory_usage_psutil()))
   430                                                   else:
   431                                                       tdChanNames = listChanNames(
   432                                                           asigBlock, chanQuery, objType=AnalogSignal)
   433                                                       asigList = [
   434                                                           asig
   435                                                           for asig in asigSeg.analogsignals
   436                                                           if asig.name in tdChanNames
   437                                                           ]
   438                                                   for asig in asigList:
   439                                                       if asig.size > 0:
   440                                                           dummyAsig = asig
   441                                                   if idx == 0:
   442                                                       outputBlock = Block(
   443                                                           name=asigBlock.name,
   444                                                           file_origin=asigBlock.file_origin,
   445                                                           file_datetime=asigBlock.file_datetime,
   446                                                           rec_datetime=asigBlock.rec_datetime,
   447                                                           **asigBlock.annotations
   448                                                       )
   449                                                       newSeg = Segment(
   450                                                           index=0, name=asigSeg.name,
   451                                                           description=asigSeg.description,
   452                                                           file_origin=asigSeg.file_origin,
   453                                                           file_datetime=asigSeg.file_datetime,
   454                                                           rec_datetime=asigSeg.rec_datetime,
   455                                                           **asigSeg.annotations
   456                                                       )
   457                                                       outputBlock.segments = [newSeg]
   458                                                       for asig in asigList:
   459                                                           asigAnnCache[asig.name] = asig.annotations
   460                                                           asigAnnCache[asig.name]['parentChanName'] = asig.channel_index.name
   461                                                       asigUnits = dummyAsig.units
   462                                                   tdDF = analogSignalsToDataFrame(asigList)
   463                                                   del asigList  # asigs saved to dataframe, no longer needed
   464                                                   tdDF.loc[:, 't'] += asigBlock.annotations['chunkTStart']
   465                                                   tdDF.set_index('t', inplace=True)
   466                                                   if samplingRate != dummyAsig.sampling_rate:
   467                                                       lowPassOpts = {
   468                                                           'low': {
   469                                                               'Wn': float(samplingRate / 2),
   470                                                               'N': 4,
   471                                                               'btype': 'low',
   472                                                               'ftype': 'bessel'
   473                                                           }
   474                                                       }
   475                                                       newT = pd.Series(
   476                                                           np.arange(
   477                                                               dummyAsig.t_start + asigBlock.annotations['chunkTStart'] * pq.s,
   478                                                               dummyAsig.t_stop + asigBlock.annotations['chunkTStart'] * pq.s,
   479                                                               1/samplingRate))
   480                                                       if samplingRate < dummyAsig.sampling_rate:
   481                                                           filterCoeffs = hf.makeFilterCoeffsSOS(
   482                                                               lowPassOpts, float(dummyAsig.sampling_rate))
   483                                                           if trackMemory:
   484                                                               print('Filtering analog data before downsampling. memory usage: {:.1f} MB'.format(
   485                                                                   prf.memory_usage_psutil()))
   486                                                           '''
   487                                                           ### check that axis=0 is the correct option
   488                                                           dummyDF = tdDF.iloc[:, :4].copy()
   489                                                           filteredAsigs0 = signal.sosfiltfilt( filterCoeffs, dummyDF.to_numpy(), axis=0)
   490                                                           filteredAsigs1 = signal.sosfiltfilt( filterCoeffs, dummyDF.to_numpy(), axis=1)
   491                                                           ###
   492                                                           '''
   493                                                           filteredAsigs = signal.sosfiltfilt(
   494                                                               filterCoeffs, tdDF.to_numpy(),
   495                                                               axis=0)
   496                                                           tdDF = pd.DataFrame(
   497                                                               filteredAsigs,
   498                                                               index=tdDF.index,
   499                                                               columns=tdDF.columns)
   500                                                           if trackMemory:
   501                                                               print('Just finished analog data filtering before downsampling. memory usage: {:.1f} MB'.format(
   502                                                                   prf.memory_usage_psutil()))
   503                                                       tdInterp = hf.interpolateDF(
   504                                                           tdDF, newT,
   505                                                           kind='linear', fill_value='extrapolate',
   506                                                           verbose=verbose)
   507                                                       # free up memory used by full resolution asigs
   508                                                       del tdDF
   509                                                   else:
   510                                                       tdInterp = tdDF
   511                                                   #
   512                                                   asigCache.append(tdInterp)
   513                                                   #
   514                                                   print('Finished chunk {}'.format(chunkIdxStr))
   515                                               allTdDF = pd.concat(asigCache)
   516                                               # TODO: check for nans, if, for example a signal is partially missing
   517                                               allTdDF.fillna(method='bfill', inplace=True)
   518                                               allTdDF.fillna(method='ffill', inplace=True)
   519                                               for asigName in allTdDF.columns:
   520                                                   newAsig = AnalogSignal(
   521                                                       allTdDF[asigName].to_numpy() * asigUnits,
   522                                                       name=asigName,
   523                                                       sampling_rate=samplingRate,
   524                                                       dtype=np.float32,
   525                                                       **asigAnnCache[asigName])
   526                                                   chIdxName = asigAnnCache[asigName]['parentChanName']
   527                                                   chIdx = channelIndexCache[chIdxName]
   528                                                   # cross-assign ownership to containers
   529                                                   chIdx.analogsignals.append(newAsig)
   530                                                   newSeg.analogsignals.append(newAsig)
   531                                                   newAsig.channel_index = chIdx
   532                                                   newAsig.segment = newSeg
   533                                               #
   534                                               for uName, unit in unitCache.items():
   535                                                   # concatenate spike times, waveforms, etc.
   536                                                   if len(spiketrainCache[unit.name]):
   537                                                       consolidatedTimes = np.concatenate([
   538                                                               st.times.magnitude
   539                                                               for st in spiketrainCache[unit.name]
   540                                                           ])
   541                                                       # TODO:   decide whether to include this step
   542                                                       #         which snaps the spike times to the nearest
   543                                                       #         *sampled* data point
   544                                                       #
   545                                                       # consolidatedTimes, timesIndex = hf.closestSeries(
   546                                                       #     takeFrom=pd.Series(consolidatedTimes),
   547                                                       #     compareTo=pd.Series(allTdDF.index))
   548                                                       #
   549                                                       # find an example spiketrain with array_annotations
   550                                                       for st in spiketrainCache[unit.name]:
   551                                                           if len(st.times):
   552                                                               dummySt = st
   553                                                               break
   554                                                       consolidatedAnn = {
   555                                                           key: np.array([])
   556                                                           for key, value in dummySt.array_annotations.items()
   557                                                           }
   558                                                       for key, value in consolidatedAnn.items():
   559                                                           consolidatedAnn[key] = np.concatenate([
   560                                                               st.annotations[key]
   561                                                               for st in spiketrainCache[unit.name]
   562                                                           ])
   563                                                       consolidatedWaveforms = np.concatenate([
   564                                                           st.waveforms
   565                                                           for st in spiketrainCache[unit.name]
   566                                                           ])
   567                                                       spikeTStop = max([
   568                                                           st.t_stop
   569                                                           for st in spiketrainCache[unit.name]
   570                                                           ])
   571                                                       spikeTStart = max([
   572                                                           st.t_start
   573                                                           for st in spiketrainCache[unit.name]
   574                                                           ])
   575                                                       spikeAnnotations = {
   576                                                           key: value
   577                                                           for key, value in dummySt.annotations.items()
   578                                                           if key not in dummySt.annotations['arrayAnnNames']
   579                                                       }
   580                                                       newSt = SpikeTrain(
   581                                                           name=dummySt.name,
   582                                                           times=consolidatedTimes, units='sec', t_stop=spikeTStop,
   583                                                           waveforms=consolidatedWaveforms * dummySt.waveforms.units,
   584                                                           left_sweep=dummySt.left_sweep,
   585                                                           sampling_rate=dummySt.sampling_rate,
   586                                                           t_start=spikeTStart, **spikeAnnotations,
   587                                                           array_annotations=consolidatedAnn)
   588                                                       # cross-assign ownership to containers
   589                                                       unit.spiketrains.append(newSt)
   590                                                       newSt.unit = unit
   591                                                       newSeg.spiketrains.append(newSt)
   592                                                       newSt.segment = newSeg
   593                                                       # link chIdxes and Units
   594                                                       if unit.annotations['parentChanName'] in channelIndexCache:
   595                                                           chIdx = channelIndexCache[unit.annotations['parentChanName']]
   596                                                           if unit not in chIdx.units:
   597                                                               chIdx.units.append(unit)
   598                                                               unit.channel_index = chIdx
   599                                                       else:
   600                                                           newChIdx = ChannelIndex(
   601                                                               name=unit.annotations['parentChanName'], index=0)
   602                                                           channelIndexCache[unit.annotations['parentChanName']] = newChIdx
   603                                                           if unit not in newChIdx.units:
   604                                                               newChIdx.units.append(unit)
   605                                                               unit.channel_index = newChIdx
   606                                               #
   607                                               for evName, eventList in eventCache.items():
   608                                                   consolidatedTimes = np.concatenate([
   609                                                       ev.times.magnitude
   610                                                       for ev in eventList
   611                                                       ])
   612                                                   consolidatedLabels = np.concatenate([
   613                                                       ev.labels
   614                                                       for ev in eventList
   615                                                       ])
   616                                                   newEvent = Event(
   617                                                       name=evName,
   618                                                       times=consolidatedTimes * pq.s,
   619                                                       labels=consolidatedLabels
   620                                                       )
   621                                                   # if len(newEvent):
   622                                                   newEvent.segment = newSeg
   623                                                   newSeg.events.append(newEvent)
   624                                               for chIdxName, chIdx in channelIndexCache.items():
   625                                                   if len(chIdx.analogsignals) or len(chIdx.units):
   626                                                       outputBlock.channel_indexes.append(chIdx)
   627                                                       chIdx.block = outputBlock
   628                                               #
   629                                               outputBlock = purgeNixAnn(outputBlock)
   630                                               createRelationship = False
   631                                               if createRelationship:
   632                                                   outputBlock.create_relationship()
   633                                               return outputBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: concatenateEventsContainer at line 660

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   660                                           @profile
   661                                           def concatenateEventsContainer(eventContainer, linkParents=True):
   662                                               if isinstance(eventContainer, dict):
   663                                                   listOfEvents = list(eventContainer.values())
   664                                               else:
   665                                                   listOfEvents = eventContainer
   666                                               nonEmptyEvents = [ev for ev in listOfEvents if len(ev.times)]
   667                                               if not len(nonEmptyEvents) > 0:
   668                                                   return listOfEvents[0]
   669                                               masterEvent = listOfEvents[0]
   670                                               for evIdx, ev in enumerate(listOfEvents[1:]):
   671                                                   try:
   672                                                       masterEvent = masterEvent.merge(ev)
   673                                                   except Exception:
   674                                                       traceback.print_exc()
   675                                                       pdb.set_trace()
   676                                               if masterEvent.array_annotations is not None:
   677                                                   arrayAnnNames = list(masterEvent.array_annotations.keys())
   678                                                   masterEvent.annotations.update(masterEvent.array_annotations)
   679                                                   masterEvent.annotations['arrayAnnNames'] = arrayAnnNames
   680                                               if linkParents:
   681                                                   masterEvent.segment = listOfEvents[0].segment
   682                                                   if isinstance(masterEvent, SpikeTrain):
   683                                                       masterEvent.unit = listOfEvents[0].unit
   684                                               return masterEvent

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: unitSpikeTrainWaveformsToDF at line 743

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   743                                           @profile
   744                                           def unitSpikeTrainWaveformsToDF(
   745                                                   spikeTrainContainer,
   746                                                   dataQuery=None,
   747                                                   transposeToColumns='bin', fastTranspose=True,
   748                                                   lags=None, decimate=1, rollingWindow=None,
   749                                                   getMetaData=True, verbose=False,
   750                                                   whichSegments=None, windowSize=None, procFun=None):
   751                                               #  list contains different segments from *one* unit
   752                                               if isinstance(spikeTrainContainer, ChannelIndex):
   753                                                   assert len(spikeTrainContainer.units) == 0
   754                                                   spiketrains = spikeTrainContainer.units[0].spiketrains
   755                                               elif isinstance(spikeTrainContainer, Unit):
   756                                                   spiketrains = spikeTrainContainer.spiketrains
   757                                               else:
   758                                                   raise(Exception('not a valid container'))
   759                                               # TODO check if really need to assert uniqueness?
   760                                               uniqueSpiketrains = []
   761                                               for st in spiketrains:
   762                                                   if not np.any([st is i for i in uniqueSpiketrains]):
   763                                                       uniqueSpiketrains.append(st)
   764                                               #  subsampling options
   765                                               decimate = int(decimate)
   766                                               if whichSegments is not None:
   767                                                   uniqueSpiketrains = [
   768                                                       uniqueSpiketrains[i]
   769                                                       for i in whichSegments
   770                                                   ]
   771                                               #
   772                                               waveformsList = []
   773                                               #
   774                                               for segIdx, stIn in enumerate(uniqueSpiketrains):
   775                                                   if verbose:
   776                                                       print('extracting spiketrain from {}'.format(stIn.segment))
   777                                                   #  make sure is not a proxyObj
   778                                                   if isinstance(stIn, SpikeTrainProxy):
   779                                                       st = loadStProxy(stIn)
   780                                                       if (getMetaData) or (dataQuery is not None):
   781                                                           # if there's a query, get metadata temporarily to resolve it
   782                                                           st = loadObjArrayAnn(st)
   783                                                   else:
   784                                                       st = stIn
   785                                                   #  extract bins spaced by decimate argument
   786                                                   if not st.times.any():
   787                                                       continue
   788                                                   if verbose:
   789                                                       print('extracting wf from {}'.format(stIn.segment))
   790                                                   wf = np.asarray(
   791                                                       np.squeeze(st.waveforms),
   792                                                       dtype='float32')
   793                                                   if wf.ndim == 3:
   794                                                       print('Waveforms from more than one channel!')
   795                                                       if wf.shape[1] > 0:
   796                                                           wf = wf[:, 0, :]
   797                                                   wfDF = pd.DataFrame(wf)
   798                                                   samplingRate = st.sampling_rate
   799                                                   bins = (
   800                                                       np.asarray(wfDF.columns) / samplingRate -
   801                                                       st.left_sweep)
   802                                                   wfDF.columns = np.around(bins.magnitude, decimals=6)
   803                                                   if windowSize is not None:
   804                                                       winMask = (
   805                                                           (wfDF.columns >= windowSize[0]) &
   806                                                           (wfDF.columns <= windowSize[1]))
   807                                                       wfDF = wfDF.loc[:, winMask]
   808                                                   if procFun is not None:
   809                                                       wfDF = procFun(wfDF, st)
   810                                                   idxLabels = ['segment', 'originalIndex', 't']
   811                                                   wfDF.loc[:, 't'] = np.asarray(st.times.magnitude)
   812                                                   if (getMetaData) or (dataQuery is not None):
   813                                                       # if there's a query, get metadata temporarily to resolve it
   814                                                       annDict = {}
   815                                                       for k, values in st.array_annotations.items():
   816                                                           if isinstance(getMetaData, Iterable):
   817                                                               # if selecting metadata fields, check that
   818                                                               # the key is in the provided list
   819                                                               if k not in getMetaData:
   820                                                                   continue
   821                                                           if isinstance(values[0], str):
   822                                                               v = np.asarray(values, dtype='str')
   823                                                           else:
   824                                                               v = np.asarray(values)
   825                                                           annDict.update({k: v})
   826                                                       skipAnnNames = (
   827                                                           st.annotations['arrayAnnNames'] +
   828                                                           [
   829                                                               'arrayAnnNames', 'arrayAnnDTypes',
   830                                                               'nix_name', 'neo_name', 'id',
   831                                                               'cell_label', 'cluster_label', 'max_on_channel', 'binWidth']
   832                                                           )
   833                                                       annDF = pd.DataFrame(annDict)
   834                                                       for k, value in st.annotations.items():
   835                                                           if isinstance(getMetaData, Iterable):
   836                                                               # if selecting metadata fields, check that
   837                                                               # the key is in the provided list
   838                                                               if k not in getMetaData:
   839                                                                   continue
   840                                                           if k not in skipAnnNames:
   841                                                               annDF.loc[:, k] = value
   842                                                       #
   843                                                       if isinstance(getMetaData, Iterable):
   844                                                           doNotFillList = idxLabels + ['feature', 'bin']
   845                                                           fieldsNeedFiller = [
   846                                                               mdn
   847                                                               for mdn in getMetaData
   848                                                               if (mdn not in doNotFillList) and (mdn not in annDF.columns)]
   849                                                           for mdName in fieldsNeedFiller:
   850                                                               annDF.loc[:, mdName] = 'NA'
   851                                                       annColumns = annDF.columns.to_list()
   852                                                       if getMetaData:
   853                                                           for annNm in annColumns:
   854                                                               if annNm not in idxLabels:
   855                                                                   idxLabels.append(annNm)
   856                                                           # idxLabels += annColumns
   857                                                       spikeDF = annDF.join(wfDF)
   858                                                   else:
   859                                                       spikeDF = wfDF
   860                                                       del wfDF, st
   861                                                   spikeDF.loc[:, 'segment'] = segIdx
   862                                                   spikeDF.loc[:, 'originalIndex'] = spikeDF.index
   863                                                   spikeDF.columns.name = 'bin'
   864                                                   #
   865                                                   if dataQuery is not None:
   866                                                       spikeDF.query(dataQuery, inplace=True)
   867                                                       if not getMetaData:
   868                                                           spikeDF.drop(columns=annColumns, inplace=True)
   869                                                   waveformsList.append(spikeDF)
   870                                               #
   871                                               zeroLagWaveformsDF = pd.concat(waveformsList, axis='index')
   872                                               if verbose:
   873                                                   prf.print_memory_usage('before transposing waveforms')
   874                                               # TODO implement lags and rolling window addition here
   875                                               metaDF = zeroLagWaveformsDF.loc[:, idxLabels].copy()
   876                                               zeroLagWaveformsDF.drop(columns=idxLabels, inplace=True)
   877                                               if lags is None:
   878                                                   lags = [0]
   879                                               laggedWaveformsDict = {
   880                                                   (spikeTrainContainer.name, k): None for k in lags}
   881                                               for lag in lags:
   882                                                   if isinstance(lag, int):
   883                                                       shiftedWaveform = zeroLagWaveformsDF.shift(
   884                                                           lag, axis='columns')
   885                                                       if rollingWindow is not None:
   886                                                           halfRollingWin = int(np.ceil(rollingWindow/2))
   887                                                           seekIdx = slice(
   888                                                               halfRollingWin, -halfRollingWin+1, decimate)
   889                                                           # seekIdx = slice(None, None, decimate)
   890                                                           #shiftedWaveform = (
   891                                                           #    shiftedWaveform
   892                                                           #    .rolling(
   893                                                           #        window=rollingWindow, win_type='gaussian',
   894                                                           #        axis='columns', center=True)
   895                                                           #    .mean(std=halfRollingWin))
   896                                                           shiftedWaveform = (
   897                                                               shiftedWaveform
   898                                                               .rolling(
   899                                                                   window=rollingWindow, 
   900                                                                   axis='columns', center=True)
   901                                                               .mean())
   902                                                       else:
   903                                                           halfRollingWin = 0
   904                                                           seekIdx = slice(None, None, decimate)
   905                                                           if False:
   906                                                               oldShiftedWaveform = zeroLagWaveformsDF.shift(
   907                                                                   lag, axis='columns')
   908                                                               plt.plot(oldShiftedWaveform.iloc[0, :])
   909                                                               plt.plot(shiftedWaveform.iloc[0, :])
   910                                                               plt.show()
   911                                                       laggedWaveformsDict[
   912                                                           (spikeTrainContainer.name, lag)] = (
   913                                                               shiftedWaveform.iloc[:, seekIdx].copy())
   914                                                   if isinstance(lag, tuple):
   915                                                       halfRollingWin = int(np.ceil(lag[1]/2))
   916                                                       seekIdx = slice(
   917                                                           halfRollingWin, -halfRollingWin+1, decimate)
   918                                                       # seekIdx = slice(None, None, decimate)
   919                                                       shiftedWaveform = (
   920                                                           zeroLagWaveformsDF
   921                                                           .shift(lag[0], axis='columns')
   922                                                           .rolling(
   923                                                               window=lag[1], win_type='gaussian',
   924                                                               axis='columns', center=True)
   925                                                           .mean(std=halfRollingWin))
   926                                                       laggedWaveformsDict[
   927                                                           (spikeTrainContainer.name, lag)] = (
   928                                                               shiftedWaveform.iloc[:, seekIdx].copy())
   929                                               #
   930                                               if transposeToColumns == 'feature':
   931                                                   # stack the bin, name the feature column
   932                                                   # 
   933                                                   for idx, (key, value) in enumerate(laggedWaveformsDict.items()):
   934                                                       if idx == 0:
   935                                                           stackedIndexDF = pd.concat(
   936                                                               [metaDF, value], axis='columns')
   937                                                           stackedIndexDF.set_index(idxLabels, inplace=True)
   938                                                           # don't drop nans for now - might need to keep track of them
   939                                                           # if we need to equalize to another array later
   940                                                           newIndex = stackedIndexDF.stack(dropna=False).index
   941                                                           idxLabels.append('bin')
   942                                                       laggedWaveformsDict[key] = value.stack(dropna=False).to_frame(name=key).reset_index(drop=True)
   943                                                   waveformsDF = pd.concat(
   944                                                       laggedWaveformsDict.values(),
   945                                                       axis='columns')
   946                                                   waveformsDF.columns.names = ['feature', 'lag']
   947                                                   waveformsDF.index = newIndex
   948                                                   waveformsDF.columns.name = 'feature'
   949                                               elif transposeToColumns == 'bin':
   950                                                   # add the feature column
   951                                                   waveformsDF = pd.concat(
   952                                                       laggedWaveformsDict,
   953                                                       names=['feature', 'lag', 'originalDummy']).reset_index()
   954                                                   waveformsDF = pd.concat(
   955                                                       [
   956                                                           metaDF.reset_index(drop=True),
   957                                                           waveformsDF.drop(columns='originalDummy')],
   958                                                       axis='columns')
   959                                                   idxLabels += ['feature', 'lag']
   960                                                   waveformsDF.columns.name = 'bin'
   961                                                   waveformsDF.set_index(idxLabels, inplace=True)
   962                                               #
   963                                               if transposeToColumns != waveformsDF.columns.name:
   964                                                   waveformsDF = transposeSpikeDF(
   965                                                       waveformsDF, transposeToColumns,
   966                                                       fastTranspose=fastTranspose)
   967                                               return waveformsDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: concatenateUnitSpikeTrainWaveformsDF at line 969

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   969                                           @profile
   970                                           def concatenateUnitSpikeTrainWaveformsDF(
   971                                                   units, dataQuery=None,
   972                                                   transposeToColumns='bin', concatOn='index',
   973                                                   fastTranspose=True, getMetaData=True, verbose=False,
   974                                                   addLags=None, decimate=1, rollingWindow=None,
   975                                                   metaDataToCategories=False, windowSize=None,
   976                                                   whichSegments=None, procFun=None):
   977                                               allUnits = []
   978                                               for thisUnit in units:
   979                                                   hasAnySpikes = []
   980                                                   for stIn in thisUnit.spiketrains:
   981                                                       if isinstance(stIn, SpikeTrainProxy):
   982                                                           st = stIn.load(
   983                                                               magnitude_mode='rescaled',
   984                                                               load_waveforms=False)
   985                                                       else:
   986                                                           st = stIn
   987                                                       hasAnySpikes.append(st.times.any())
   988                                                   if np.any(hasAnySpikes):
   989                                                       allUnits.append(thisUnit)
   990                                               waveformsList = []
   991                                               for idx, thisUnit in enumerate(allUnits):
   992                                                   if verbose:
   993                                                       print('concatenating unitDF {}'.format(thisUnit.name))
   994                                                   lags = None
   995                                                   if addLags is not None:
   996                                                       if thisUnit.name in addLags:
   997                                                           lags = addLags[thisUnit.name]
   998                                                   unitWaveforms = unitSpikeTrainWaveformsToDF(
   999                                                       thisUnit, dataQuery=dataQuery,
  1000                                                       transposeToColumns=transposeToColumns,
  1001                                                       fastTranspose=fastTranspose, getMetaData=getMetaData,
  1002                                                       lags=lags, decimate=decimate, rollingWindow=rollingWindow,
  1003                                                       verbose=verbose, windowSize=windowSize,
  1004                                                       whichSegments=whichSegments, procFun=procFun)
  1005                                                   if idx == 0:
  1006                                                       idxLabels = unitWaveforms.index.names
  1007                                                   if (concatOn == 'columns') and (idx > 0):
  1008                                                       # other than first time, we already have the metadata
  1009                                                       unitWaveforms.reset_index(drop=True, inplace=True)
  1010                                                   else:
  1011                                                       # first time, or if concatenating indices,
  1012                                                       # keep the the metadata
  1013                                                       unitWaveforms.reset_index(inplace=True)
  1014                                                       if metaDataToCategories:
  1015                                                           # convert metadata to categoricals to free memory
  1016                                                           #
  1017                                                           unitWaveforms[idxLabels] = (
  1018                                                               unitWaveforms[idxLabels]
  1019                                                               .astype('category')
  1020                                                               )
  1021                                                   waveformsList.append(unitWaveforms)
  1022                                                   del unitWaveforms
  1023                                                   if verbose:
  1024                                                       print('memory usage: {:.1f} MB'.format(prf.memory_usage_psutil()))
  1025                                               if verbose:
  1026                                                   print(
  1027                                                       'about to join all, memory usage: {:.1f} MB'
  1028                                                       .format(prf.memory_usage_psutil()))
  1029                                               #  if concatenating indexes, reset the index of the result
  1030                                               #  ignoreIndex = (concatOn == 'index')
  1031                                               allWaveforms = pd.concat(
  1032                                                   waveformsList, axis=concatOn,
  1033                                                   # ignore_index=ignoreIndex
  1034                                                   )
  1035                                               del waveformsList
  1036                                               if verbose:
  1037                                                   print(
  1038                                                       'finished concatenating, memory usage: {:.1f} MB'
  1039                                                       .format(prf.memory_usage_psutil()))
  1040                                               try:
  1041                                                   allWaveforms.set_index(idxLabels, inplace=True)
  1042                                                   allWaveforms.sort_index(
  1043                                                       level=['segment', 'originalIndex', 't'],
  1044                                                       axis='index', inplace=True, kind='mergesort')
  1045                                                   allWaveforms.sort_index(
  1046                                                       axis='columns', inplace=True, kind='mergesort')
  1047                                               except Exception:
  1048                                                   pdb.set_trace()
  1049                                               return allWaveforms

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: alignedAsigsToDF at line 1051

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1051                                           @profile
  1052                                           def alignedAsigsToDF(
  1053                                                   dataBlock, unitNames=None,
  1054                                                   unitQuery=None, dataQuery=None,
  1055                                                   collapseSizes=False, verbose=False,
  1056                                                   duplicateControlsByProgram=False,
  1057                                                   amplitudeColumn='amplitude',
  1058                                                   programColumn='program',
  1059                                                   electrodeColumn='electrode',
  1060                                                   transposeToColumns='bin', concatOn='index', fastTranspose=True,
  1061                                                   addLags=None, decimate=1, rollingWindow=None,
  1062                                                   whichSegments=None, windowSize=None,
  1063                                                   getMetaData=True, metaDataToCategories=True,
  1064                                                   outlierTrials=None, invertOutlierMask=False,
  1065                                                   makeControlProgram=False, removeFuzzyName=False, procFun=None):
  1066                                               #  channels to trigger
  1067                                               if unitNames is None:
  1068                                                   unitNames = listChanNames(dataBlock, unitQuery, objType=Unit)
  1069                                               allUnits = []
  1070                                               for uName in unitNames:
  1071                                                   allUnits += dataBlock.filter(objects=Unit, name=uName)
  1072                                               allWaveforms = concatenateUnitSpikeTrainWaveformsDF(
  1073                                                   allUnits, dataQuery=dataQuery,
  1074                                                   transposeToColumns=transposeToColumns, concatOn=concatOn,
  1075                                                   fastTranspose=fastTranspose,
  1076                                                   addLags=addLags, decimate=decimate, rollingWindow=rollingWindow,
  1077                                                   verbose=verbose, whichSegments=whichSegments,
  1078                                                   windowSize=windowSize, procFun=procFun,
  1079                                                   getMetaData=getMetaData, metaDataToCategories=metaDataToCategories)
  1080                                               #
  1081                                               manipulateIndex = np.any(
  1082                                                   [
  1083                                                       collapseSizes, duplicateControlsByProgram,
  1084                                                       makeControlProgram, removeFuzzyName
  1085                                                       ])
  1086                                               if outlierTrials is not None:
  1087                                                   def rejectionLookup(entry):
  1088                                                       key = []
  1089                                                       for subKey in outlierTrials.index.names:
  1090                                                           keyIdx = allWaveforms.index.names.index(subKey)
  1091                                                           key.append(entry[keyIdx])
  1092                                                       # print(key)
  1093                                                       # outlierTrials.iloc[1, :]
  1094                                                       # allWaveforms.iloc[1, :]
  1095                                                       return outlierTrials[tuple(key)]
  1096                                                   #
  1097                                                   outlierMask = np.asarray(
  1098                                                       allWaveforms.index.map(rejectionLookup),
  1099                                                       dtype=np.bool)
  1100                                                   if invertOutlierMask:
  1101                                                       outlierMask = ~outlierMask
  1102                                                   allWaveforms = allWaveforms.loc[~outlierMask, :]
  1103                                               if manipulateIndex and getMetaData:
  1104                                                   idxLabels = allWaveforms.index.names
  1105                                                   allWaveforms.reset_index(inplace=True)
  1106                                                   # 
  1107                                                   if collapseSizes:
  1108                                                       try:
  1109                                                           allWaveforms.loc[allWaveforms['pedalSizeCat'] == 'XL', 'pedalSizeCat'] = 'L'
  1110                                                           allWaveforms.loc[allWaveforms['pedalSizeCat'] == 'XS', 'pedalSizeCat'] = 'S'
  1111                                                       except Exception:
  1112                                                           traceback.print_exc()
  1113                                                   if makeControlProgram:
  1114                                                       try:
  1115                                                           allWaveforms.loc[allWaveforms[amplitudeColumn] == 0, programColumn] = 999
  1116                                                           allWaveforms.loc[allWaveforms[amplitudeColumn] == 0, electrodeColumn] = 'control'
  1117                                                       except Exception:
  1118                                                           traceback.print_exc()
  1119                                                   if duplicateControlsByProgram:
  1120                                                       #
  1121                                                       noStimWaveforms = (
  1122                                                           allWaveforms
  1123                                                           .loc[allWaveforms[amplitudeColumn] == 0, :]
  1124                                                           )
  1125                                                       stimWaveforms = (
  1126                                                           allWaveforms
  1127                                                           .loc[allWaveforms[amplitudeColumn] != 0, :]
  1128                                                           .copy()
  1129                                                           )
  1130                                                       uniqProgs = stimWaveforms[programColumn].unique()
  1131                                                       progElecLookup = {}
  1132                                                       #pdb.set_trace()
  1133                                                       for progIdx in uniqProgs:
  1134                                                           theseStimDF = stimWaveforms.loc[
  1135                                                               stimWaveforms[programColumn] == progIdx,
  1136                                                               electrodeColumn]
  1137                                                           elecIdx = theseStimDF.iloc[0]
  1138                                                           progElecLookup.update({progIdx: elecIdx})
  1139                                                       #
  1140                                                       if makeControlProgram:
  1141                                                           uniqProgs = np.append(uniqProgs, 999)
  1142                                                           progElecLookup.update({999: 'control'})
  1143                                                       #
  1144                                                       for progIdx in uniqProgs:
  1145                                                           dummyWaveforms = noStimWaveforms.copy()
  1146                                                           dummyWaveforms.loc[:, programColumn] = progIdx
  1147                                                           dummyWaveforms.loc[:, electrodeColumn] = progElecLookup[progIdx]
  1148                                                           stimWaveforms = pd.concat([stimWaveforms, dummyWaveforms])
  1149                                                       stimWaveforms.reset_index(drop=True, inplace=True)
  1150                                                       allWaveforms = stimWaveforms
  1151                                                   #
  1152                                                   if removeFuzzyName:
  1153                                                       fuzzyNamesBase = [
  1154                                                           i.replace('Fuzzy', '')
  1155                                                           for i in idxLabels
  1156                                                           if 'Fuzzy' in i]
  1157                                                       colRenamer = {n + 'Fuzzy': n for n in fuzzyNamesBase}
  1158                                                       fuzzyNamesBasePresent = [
  1159                                                           i
  1160                                                           for i in fuzzyNamesBase
  1161                                                           if i in allWaveforms.columns]
  1162                                                       allWaveforms.drop(columns=fuzzyNamesBasePresent, inplace=True)
  1163                                                       allWaveforms.rename(columns=colRenamer, inplace=True)
  1164                                                       idxLabels = np.unique(
  1165                                                           [i.replace('Fuzzy', '') for i in idxLabels])
  1166                                                   #
  1167                                                   allWaveforms.set_index(
  1168                                                       list(idxLabels),
  1169                                                       inplace=True)
  1170                                                   if isinstance(allWaveforms.columns, pd.MultiIndex):
  1171                                                       allWaveforms.columns = allWaveforms.columns.remove_unused_levels()
  1172                                               #
  1173                                               if transposeToColumns == 'feature':
  1174                                                   zipNames = zip(pd.unique(allWaveforms.columns.get_level_values('feature')).tolist(), unitNames)
  1175                                                   try:
  1176                                                       assert np.all([i == j for i, j in zipNames]), 'columns out of requested order!'
  1177                                                   except Exception:
  1178                                                       traceback.print_exc()
  1179                                                       allWaveforms.reindex(columns=unitNames)
  1180                                               if isinstance(allWaveforms.columns, pd.MultiIndex):
  1181                                                   allWaveforms.columns = allWaveforms.columns.remove_unused_levels()
  1182                                               allWaveforms.sort_index(
  1183                                                   axis='columns', inplace=True, kind='mergesort')
  1184                                               return allWaveforms

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getAsigsAlignedToEvents at line 1186

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1186                                           @profile
  1187                                           def getAsigsAlignedToEvents(
  1188                                                   eventBlock=None, signalBlock=None,
  1189                                                   chansToTrigger=None, chanQuery=None,
  1190                                                   eventName=None, windowSize=None,
  1191                                                   minNReps=None,
  1192                                                   appendToExisting=False,
  1193                                                   checkReferences=True, verbose=False,
  1194                                                   fileName=None, folderPath=None, chunkSize=None
  1195                                                   ):
  1196                                               #  get signals from same block as events?
  1197                                               if signalBlock is None:
  1198                                                   signalBlock = eventBlock
  1199                                               #  channels to trigger
  1200                                               if chansToTrigger is None:
  1201                                                   chansToTrigger = listChanNames(
  1202                                                       signalBlock, chanQuery, objType=ChannelIndex, condition='hasAsigs')
  1203                                               #  allocate block for spiketrains
  1204                                               masterBlock = Block()
  1205                                               try:
  1206                                                   masterBlock.name = signalBlock.annotations['neo_name']
  1207                                                   masterBlock.annotate(nix_name=signalBlock.annotations['neo_name'])
  1208                                               except Exception:
  1209                                                   masterBlock.name = signalBlock.name
  1210                                                   masterBlock.annotate(neo_name=signalBlock.name)
  1211                                                   masterBlock.annotate(nix_name=signalBlock.name)
  1212                                               #  make channels and units for triggered time series
  1213                                               for chanName in chansToTrigger:
  1214                                                   chanIdx = ChannelIndex(name=chanName + '#0', index=[0])
  1215                                                   chanIdx.annotate(nix_name=chanIdx.name)
  1216                                                   thisUnit = Unit(name=chanIdx.name)
  1217                                                   thisUnit.annotate(nix_name=chanIdx.name)
  1218                                                   chanIdx.units.append(thisUnit)
  1219                                                   thisUnit.channel_index = chanIdx
  1220                                                   masterBlock.channel_indexes.append(chanIdx)
  1221                                                   sigChanIdxList = signalBlock.filter(
  1222                                                       objects=ChannelIndex, name=chanName)
  1223                                                   if len(sigChanIdxList):
  1224                                                       sigChanIdx = sigChanIdxList[0]
  1225                                                       if sigChanIdx.coordinates is not None:
  1226                                                           coordUnits = sigChanIdx.coordinates[0][0].units
  1227                                                           chanIdx.coordinates = np.asarray(sigChanIdx.coordinates) * coordUnits
  1228                                                           thisUnit.annotations['parentChanXCoords'] = float(chanIdx.coordinates[:, 0].magnitude)
  1229                                                           thisUnit.annotations['parentChanYCoords'] = float(chanIdx.coordinates[:, 1].magnitude)
  1230                                                           thisUnit.annotations['parentChanCoordinateUnits'] = '{}'.format(coordUnits)
  1231                                               #
  1232                                               totalNSegs = 0
  1233                                               #  print([evSeg.events[3].name for evSeg in eventBlock.segments])
  1234                                               allAlignEventsList = []
  1235                                               for segIdx, eventSeg in enumerate(eventBlock.segments):
  1236                                                   thisEventName = 'seg{}_{}'.format(segIdx, eventName)
  1237                                                   try:
  1238                                                       assert len(eventSeg.filter(name=thisEventName)) == 1
  1239                                                   except Exception:
  1240                                                       traceback.print_exc()
  1241                                                   allEvIn = eventSeg.filter(name=thisEventName)[0]
  1242                                                   if isinstance(allEvIn, EventProxy):
  1243                                                       allAlignEvents = loadObjArrayAnn(allEvIn.load())
  1244                                                   elif isinstance(allEvIn, Event):
  1245                                                       allAlignEvents = allEvIn
  1246                                                   else:
  1247                                                       raise(Exception(
  1248                                                           '{} must be an Event or EventProxy!'
  1249                                                           .format(eventName)))
  1250                                                   allAlignEventsList.append(allAlignEvents)
  1251                                               allAlignEventsDF = unitSpikeTrainArrayAnnToDF(allAlignEventsList)
  1252                                               #
  1253                                               breakDownData = (
  1254                                                   allAlignEventsDF
  1255                                                   .groupby(minNReps['categories'])
  1256                                                   .agg('count')
  1257                                                   .iloc[:, 0]
  1258                                                   )
  1259                                               try:
  1260                                                   breakDownData[breakDownData > minNReps['n']].to_csv(
  1261                                                       os.path.join(
  1262                                                           folderPath, 'numRepetitionsEachCondition.csv'
  1263                                                       ), header=True
  1264                                                   )
  1265                                               except Exception:
  1266                                                   traceback.print_exc()
  1267                                               allAlignEventsDF.loc[:, 'keepMask'] = False
  1268                                               for name, group in allAlignEventsDF.groupby(minNReps['categories']):
  1269                                                   allAlignEventsDF.loc[group.index, 'keepMask'] = (
  1270                                                       breakDownData[name] > minNReps['n'])
  1271                                               for segIdx, group in allAlignEventsDF.groupby('segment'):
  1272                                                   allAlignEventsList[segIdx].array_annotations['keepMask'] = group['keepMask'].to_numpy()
  1273                                               #
  1274                                               for segIdx, eventSeg in enumerate(eventBlock.segments):
  1275                                                   if verbose:
  1276                                                       print(
  1277                                                           'getAsigsAlignedToEvents on segment {} of {}'
  1278                                                           .format(segIdx + 1, len(eventBlock.segments)))
  1279                                                   allAlignEvents = allAlignEventsList[segIdx]
  1280                                                   if chunkSize is None:
  1281                                                       alignEventGroups = [allAlignEvents]
  1282                                                   else:
  1283                                                       nChunks = max(
  1284                                                           int(np.floor(allAlignEvents.shape[0] / chunkSize)),
  1285                                                           1)
  1286                                                       alignEventGroups = []
  1287                                                       for i in range(nChunks):
  1288                                                           if not (i == (nChunks - 1)):
  1289                                                               # not last one
  1290                                                               alignEventGroups.append(
  1291                                                                   allAlignEvents[i * chunkSize: (i + 1) * chunkSize])
  1292                                                           else:
  1293                                                               alignEventGroups.append(
  1294                                                                   allAlignEvents[i * chunkSize:])
  1295                                                   signalSeg = signalBlock.segments[segIdx]
  1296                                                   for subSegIdx, alignEvents in enumerate(alignEventGroups):
  1297                                                       # seg to contain triggered time series
  1298                                                       if verbose:
  1299                                                           print(
  1300                                                               'getAsigsAlignedToEvents on subSegment {} of {}'
  1301                                                               .format(subSegIdx + 1, len(alignEventGroups)))
  1302                                                       if not alignEvents.shape[0] > 0:
  1303                                                           continue
  1304                                                       newSeg = Segment(name='seg{}_'.format(int(totalNSegs)))
  1305                                                       newSeg.annotate(nix_name=newSeg.name)
  1306                                                       masterBlock.segments.append(newSeg)
  1307                                                       for chanName in chansToTrigger:
  1308                                                           asigName = 'seg{}_{}'.format(segIdx, chanName)
  1309                                                           if verbose:
  1310                                                               print(
  1311                                                                   'getAsigsAlignedToEvents on channel {}'
  1312                                                                   .format(chanName))
  1313                                                           assert len(signalSeg.filter(name=asigName)) == 1
  1314                                                           asig = signalSeg.filter(name=asigName)[0]
  1315                                                           nominalWinLen = int(
  1316                                                               (windowSize[1] - windowSize[0]) *
  1317                                                               asig.sampling_rate - 1)
  1318                                                           validMask = (
  1319                                                               ((
  1320                                                                   alignEvents + windowSize[1] +
  1321                                                                   asig.sampling_rate ** (-1)) < asig.t_stop) &
  1322                                                               ((
  1323                                                                   alignEvents + windowSize[0] -
  1324                                                                   asig.sampling_rate ** (-1)) > asig.t_start)
  1325                                                               )
  1326                                                           thisKeepMask = alignEvents.array_annotations['keepMask']
  1327                                                           fullMask = (validMask & thisKeepMask)
  1328                                                           alignEvents = alignEvents[fullMask]
  1329                                                           # array_annotations get sliced with the event, but regular anns do not
  1330                                                           for annName in alignEvents.annotations['arrayAnnNames']:
  1331                                                               alignEvents.annotations[annName] = (
  1332                                                                   alignEvents.array_annotations[annName])
  1333                                                           if isinstance(asig, AnalogSignalProxy):
  1334                                                               if checkReferences:
  1335                                                                   da = (
  1336                                                                       asig
  1337                                                                       ._rawio
  1338                                                                       .da_list['blocks'][0]['segments'][segIdx]['data'])
  1339                                                                   print('segIdx {}, asig.name {}'.format(
  1340                                                                       segIdx, asig.name))
  1341                                                                   print('asig._global_channel_indexes = {}'.format(
  1342                                                                       asig._global_channel_indexes))
  1343                                                                   print('asig references {}'.format(
  1344                                                                       da[asig._global_channel_indexes[0]]))
  1345                                                                   try:
  1346                                                                       assert (
  1347                                                                           asig.name
  1348                                                                           in da[asig._global_channel_indexes[0]].name)
  1349                                                                   except Exception:
  1350                                                                       traceback.print_exc()
  1351                                                               rawWaveforms = [
  1352                                                                   asig.load(
  1353                                                                       time_slice=(t + windowSize[0], t + windowSize[1]))
  1354                                                                   for t in alignEvents]
  1355                                                               if any([rW.shape[0] < nominalWinLen for rW in rawWaveforms]):
  1356                                                                   rawWaveforms = [
  1357                                                                       asig.load(
  1358                                                                           time_slice=(t + windowSize[0], t + windowSize[1] + asig.sampling_period))
  1359                                                                       for t in alignEvents]
  1360                                                           elif isinstance(asig, AnalogSignal):
  1361                                                               rawWaveforms = []
  1362                                                               for t in alignEvents:
  1363                                                                   asigMask = (asig.times > t + windowSize[0]) & (asig.times < t + windowSize[1])
  1364                                                                   rawWaveforms.append(asig[asigMask[:, np.newaxis]])
  1365                                                           else:
  1366                                                               raise(Exception('{} must be an AnalogSignal or AnalogSignalProxy!'.format(asigName)))
  1367                                                           #
  1368                                                           samplingRate = asig.sampling_rate
  1369                                                           waveformUnits = rawWaveforms[0].units
  1370                                                           #  fix length if roundoff error
  1371                                                           #  minLen = min([rW.shape[0] for rW in rawWaveforms])
  1372                                                           rawWaveforms = [rW[:nominalWinLen] for rW in rawWaveforms]
  1373                                                           #
  1374                                                           spikeWaveforms = (
  1375                                                               np.hstack([rW.magnitude for rW in rawWaveforms])
  1376                                                               .transpose()[:, np.newaxis, :] * waveformUnits
  1377                                                               )
  1378                                                           #
  1379                                                           thisUnit = masterBlock.filter(
  1380                                                               objects=Unit, name=chanName + '#0')[0]
  1381                                                           skipEventAnnNames = (
  1382                                                               ['nix_name', 'neo_name']
  1383                                                               )
  1384                                                           stAnn = {
  1385                                                               k: v
  1386                                                               for k, v in alignEvents.annotations.items()
  1387                                                               if k not in skipEventAnnNames
  1388                                                               }
  1389                                                           skipAsigAnnNames = (
  1390                                                               ['channel_id', 'nix_name', 'neo_name']
  1391                                                               )
  1392                                                           stAnn.update({
  1393                                                               k: v
  1394                                                               for k, v in asig.annotations.items()
  1395                                                               if k not in skipAsigAnnNames
  1396                                                           })
  1397                                                           st = SpikeTrain(
  1398                                                               name='seg{}_{}'.format(int(totalNSegs), thisUnit.name),
  1399                                                               times=alignEvents.times,
  1400                                                               waveforms=spikeWaveforms,
  1401                                                               t_start=asig.t_start, t_stop=asig.t_stop,
  1402                                                               left_sweep=windowSize[0] * (-1),
  1403                                                               sampling_rate=samplingRate,
  1404                                                               **stAnn
  1405                                                               )
  1406                                                           st.annotate(nix_name=st.name)
  1407                                                           st.annotations['unitAnnotations'] = json.dumps(
  1408                                                               thisUnit.annotations.copy())
  1409                                                           thisUnit.spiketrains.append(st)
  1410                                                           newSeg.spiketrains.append(st)
  1411                                                           st.unit = thisUnit
  1412                                                       totalNSegs += 1
  1413                                               try:
  1414                                                   eventBlock.filter(
  1415                                                       objects=EventProxy)[0]._rawio.file.close()
  1416                                               except Exception:
  1417                                                   traceback.print_exc()
  1418                                               if signalBlock is not eventBlock:
  1419                                                   try:
  1420                                                       signalBlock.filter(
  1421                                                           objects=AnalogSignalProxy)[0]._rawio.file.close()
  1422                                                   except Exception:
  1423                                                       traceback.print_exc()
  1424                                               triggeredPath = os.path.join(
  1425                                                   folderPath, fileName + '.nix')
  1426                                               if not os.path.exists(triggeredPath):
  1427                                                   appendToExisting = False
  1428                                           
  1429                                               if appendToExisting:
  1430                                                   allSegs = list(range(len(masterBlock.segments)))
  1431                                                   addBlockToNIX(
  1432                                                       masterBlock, neoSegIdx=allSegs,
  1433                                                       writeSpikes=True,
  1434                                                       fileName=fileName,
  1435                                                       folderPath=folderPath,
  1436                                                       purgeNixNames=False,
  1437                                                       nixBlockIdx=0, nixSegIdx=allSegs)
  1438                                               else:
  1439                                                   if os.path.exists(triggeredPath):
  1440                                                       os.remove(triggeredPath)
  1441                                                   masterBlock = purgeNixAnn(masterBlock)
  1442                                                   writer = NixIO(filename=triggeredPath)
  1443                                                   writer.write_block(masterBlock, use_obj_names=True)
  1444                                                   writer.close()
  1445                                               return masterBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: alignedAsigDFtoSpikeTrain at line 1447

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1447                                           @profile
  1448                                           def alignedAsigDFtoSpikeTrain(
  1449                                                   allWaveforms, dataBlock=None, matchSamplingRate=True):
  1450                                               masterBlock = Block()
  1451                                               masterBlock.name = dataBlock.annotations['neo_name']
  1452                                               masterBlock.annotate(nix_name=dataBlock.annotations['neo_name'])
  1453                                               for segIdx, group in allWaveforms.groupby('segment'):
  1454                                                   print('Saving trajectoriess for segment {}'.format(segIdx))
  1455                                                   dataSeg = dataBlock.segments[segIdx]
  1456                                                   exSt = dataSeg.spiketrains[0]
  1457                                                   if isinstance(exSt, SpikeTrainProxy):
  1458                                                       print(
  1459                                                           'alignedAsigDFtoSpikeTrain basing seg {} on {}'
  1460                                                           .format(segIdx, exSt.name))
  1461                                                       stProxy = exSt
  1462                                                       exSt = loadStProxy(stProxy)
  1463                                                       exSt = loadObjArrayAnn(exSt)
  1464                                                   print('exSt.left_sweep is {}'.format(exSt.left_sweep))
  1465                                                   wfBins = ((np.arange(exSt.waveforms.shape[2]) / (exSt.sampling_rate)) - exSt.left_sweep).magnitude
  1466                                                   # seg to contain triggered time series
  1467                                                   newSeg = Segment(name=dataSeg.annotations['neo_name'])
  1468                                                   newSeg.annotate(nix_name=dataSeg.annotations['neo_name'])
  1469                                                   masterBlock.segments.append(newSeg)
  1470                                                   #
  1471                                                   if group.columns.name == 'bin':
  1472                                                       grouper = group.groupby('feature')
  1473                                                       colsAre = 'bin'
  1474                                                   elif group.columns.name == 'feature':
  1475                                                       grouper = group.iteritems()
  1476                                                       colsAre = 'feature'
  1477                                                   for featName, featGroup in grouper:
  1478                                                       print('Saving {}...'.format(featName))
  1479                                                       if featName[-2:] == '#0':
  1480                                                           cleanFeatName = featName
  1481                                                       else:
  1482                                                           cleanFeatName = featName + '#0'
  1483                                                       if segIdx == 0:
  1484                                                           #  allocate units
  1485                                                           chanIdx = ChannelIndex(
  1486                                                               name=cleanFeatName, index=[0])
  1487                                                           chanIdx.annotate(nix_name=chanIdx.name)
  1488                                                           thisUnit = Unit(name=chanIdx.name)
  1489                                                           thisUnit.annotate(nix_name=chanIdx.name)
  1490                                                           chanIdx.units.append(thisUnit)
  1491                                                           thisUnit.channel_index = chanIdx
  1492                                                           masterBlock.channel_indexes.append(chanIdx)
  1493                                                       else:
  1494                                                           thisUnit = masterBlock.filter(
  1495                                                               objects=Unit, name=cleanFeatName)[0]
  1496                                                       if colsAre == 'bin':
  1497                                                           spikeWaveformsDF = featGroup
  1498                                                       elif colsAre == 'feature':
  1499                                                           if isinstance(featGroup, pd.Series):
  1500                                                               featGroup = featGroup.to_frame(name=featName)
  1501                                                               featGroup.columns.name = 'feature'
  1502                                                           spikeWaveformsDF = transposeSpikeDF(
  1503                                                               featGroup,
  1504                                                               'bin', fastTranspose=True)
  1505                                                       if matchSamplingRate:
  1506                                                           if len(spikeWaveformsDF.columns) != len(wfBins):
  1507                                                               wfDF = spikeWaveformsDF.reset_index(drop=True).T
  1508                                                               wfDF = hf.interpolateDF(wfDF, wfBins)
  1509                                                               spikeWaveformsDF = wfDF.T.set_index(spikeWaveformsDF.index)
  1510                                                       spikeWaveforms = spikeWaveformsDF.to_numpy()[:, np.newaxis, :]
  1511                                                       arrAnnDF = spikeWaveformsDF.index.to_frame()
  1512                                                       spikeTimes = arrAnnDF['t']
  1513                                                       arrAnnDF.drop(columns='t', inplace=True)
  1514                                                       arrAnn = {}
  1515                                                       colsToKeep = arrAnnDF.columns.drop(['originalIndex', 'feature', 'segment', 'lag'])
  1516                                                       for cName in colsToKeep:
  1517                                                           values = arrAnnDF[cName].to_numpy()
  1518                                                           if isinstance(values[0], str):
  1519                                                               values = values.astype('U')
  1520                                                           arrAnn.update({str(cName): values.flatten()})
  1521                                                       arrayAnnNames = {
  1522                                                           'arrayAnnNames': list(arrAnn.keys())}
  1523                                                       st = SpikeTrain(
  1524                                                           name='seg{}_{}'.format(int(segIdx), thisUnit.name),
  1525                                                           times=spikeTimes.to_numpy() * exSt.units,
  1526                                                           waveforms=spikeWaveforms * pq.dimensionless,
  1527                                                           t_start=exSt.t_start, t_stop=exSt.t_stop,
  1528                                                           left_sweep=exSt.left_sweep,
  1529                                                           sampling_rate=exSt.sampling_rate,
  1530                                                           **arrAnn, **arrayAnnNames
  1531                                                           )
  1532                                                       st.annotate(nix_name=st.name)
  1533                                                       thisUnit.spiketrains.append(st)
  1534                                                       newSeg.spiketrains.append(st)
  1535                                                       st.unit = thisUnit
  1536                                               return masterBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: dataFrameToAnalogSignals at line 1538

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1538                                           @profile
  1539                                           def dataFrameToAnalogSignals(
  1540                                                   df,
  1541                                                   block=None, seg=None,
  1542                                                   idxT='NSPTime',
  1543                                                   probeName='insTD', samplingRate=500*pq.Hz,
  1544                                                   timeUnits=pq.s, measureUnits=pq.mV,
  1545                                                   dataCol=['channel_0', 'channel_1'],
  1546                                                   useColNames=False, forceColNames=None,
  1547                                                   namePrefix='', nameSuffix='', verbose=False):
  1548                                               if block is None:
  1549                                                   assert seg is None
  1550                                                   block = Block(name=probeName)
  1551                                                   seg = Segment(name='seg0_' + probeName)
  1552                                                   block.segments.append(seg)
  1553                                               if verbose:
  1554                                                   print('in dataFrameToAnalogSignals...')
  1555                                               for idx, colName in enumerate(dataCol):
  1556                                                   if verbose:
  1557                                                       print('    {}'.format(colName))
  1558                                                   if forceColNames is not None:
  1559                                                       chanName = forceColNames[idx]
  1560                                                   elif useColNames:
  1561                                                       chanName = namePrefix + colName + nameSuffix
  1562                                                   else:
  1563                                                       chanName = namePrefix + (probeName.lower() + '{}'.format(idx)) + nameSuffix
  1564                                                   #
  1565                                                   chanIdx = ChannelIndex(
  1566                                                       name=chanName,
  1567                                                       # index=None,
  1568                                                       index=np.asarray([idx]),
  1569                                                       # channel_names=np.asarray([chanName])
  1570                                                       )
  1571                                                   block.channel_indexes.append(chanIdx)
  1572                                                   asig = AnalogSignal(
  1573                                                       df[colName].to_numpy() * measureUnits,
  1574                                                       name='seg0_' + chanName,
  1575                                                       sampling_rate=samplingRate,
  1576                                                       dtype=np.float32,
  1577                                                       # **ann
  1578                                                       )
  1579                                                   if idxT is not None:
  1580                                                       asig.t_start = df[idxT].iloc[0] * timeUnits
  1581                                                   else:
  1582                                                       asig.t_start = df.index[0] * timeUnits
  1583                                                   asig.channel_index = chanIdx
  1584                                                   # assign ownership to containers
  1585                                                   chanIdx.analogsignals.append(asig)
  1586                                                   seg.analogsignals.append(asig)
  1587                                                   chanIdx.create_relationship()
  1588                                               # assign parent to children
  1589                                               block.create_relationship()
  1590                                               seg.create_relationship()
  1591                                               return block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: eventDataFrameToEvents at line 1593

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1593                                           @profile
  1594                                           def eventDataFrameToEvents(
  1595                                                   eventDF, idxT=None,
  1596                                                   annCol=None,
  1597                                                   eventName='', tUnits=pq.s,
  1598                                                   makeList=True
  1599                                                   ):
  1600                                               if makeList:
  1601                                                   eventList = []
  1602                                                   for colName in annCol:
  1603                                                       originalDType = type(eventDF[colName].to_numpy()[0]).__name__
  1604                                                       event = Event(
  1605                                                           name=eventName + colName,
  1606                                                           times=eventDF[idxT].to_numpy() * tUnits,
  1607                                                           labels=eventDF[colName].astype(originalDType).to_numpy()
  1608                                                           )
  1609                                                       event.annotate(originalDType=originalDType)
  1610                                                       eventList.append(event)
  1611                                                   return eventList
  1612                                               else:
  1613                                                   if annCol is None:
  1614                                                       annCol = eventDF.drop(columns=idxT).columns
  1615                                                   event = Event(
  1616                                                       name=eventName,
  1617                                                       times=eventDF[idxT].to_numpy() * tUnits,
  1618                                                       labels=np.asarray(eventDF.index)
  1619                                                       )
  1620                                                   event.annotations.update(
  1621                                                       {
  1622                                                           'arrayAnnNames': [],
  1623                                                           'arrayAnnDTypes': []
  1624                                                           })
  1625                                                   for colName in annCol:
  1626                                                       originalDType = type(eventDF[colName].to_numpy()[0]).__name__
  1627                                                       arrayAnn = eventDF[colName].astype(originalDType).to_numpy()
  1628                                                       event.array_annotations.update(
  1629                                                           {colName: arrayAnn})
  1630                                                       event.annotations['arrayAnnNames'].append(colName)
  1631                                                       event.annotations['arrayAnnDTypes'].append(originalDType)
  1632                                                       event.annotations.update(
  1633                                                           {colName: arrayAnn})
  1634                                                   return event

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: eventsToDataFrame at line 1636

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1636                                           @profile
  1637                                           def eventsToDataFrame(
  1638                                                   events, idxT='t', names=None
  1639                                                   ):
  1640                                               eventDict = {}
  1641                                               calculatedT = False
  1642                                               for event in events:
  1643                                                   if names is not None:
  1644                                                       if event.name not in names:
  1645                                                           continue
  1646                                                   if len(event.times):
  1647                                                       if not calculatedT:
  1648                                                           t = pd.Series(event.times.magnitude)
  1649                                                           calculatedT = True
  1650                                                       try:
  1651                                                           values = event.array_annotations['labels']
  1652                                                       except Exception:
  1653                                                           values = event.labels
  1654                                                       if isinstance(values[0], bytes):
  1655                                                           #  event came from hdf, need to recover dtype
  1656                                                           if 'originalDType' in event.annotations:
  1657                                                               dtypeStr = event.annotations['originalDType'].split(';')[-1]
  1658                                                               if 'np.' not in dtypeStr:
  1659                                                                   dtypeStr = 'np.' + dtypeStr
  1660                                                               originalDType = eval(dtypeStr)
  1661                                                               values = np.asarray(values, dtype=originalDType)
  1662                                                           else:
  1663                                                               values = np.asarray(values, dtype=np.str)
  1664                                                       #  print(values.dtype)
  1665                                                       eventDict.update({
  1666                                                           event.name: pd.Series(values)})
  1667                                               eventDict.update({idxT: t})
  1668                                               return pd.concat(eventDict, axis=1)

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadSpikeMats at line 1670

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1670                                           @profile
  1671                                           def loadSpikeMats(
  1672                                                   dataPath, rasterOpts,
  1673                                                   alignTimes=None, chans=None, loadAll=False,
  1674                                                   absoluteBins=False, transposeSpikeMat=False,
  1675                                                   checkReferences=False,
  1676                                                   aggregateFun=None):
  1677                                           
  1678                                               reader = nixio_fr.NixIO(filename=dataPath)
  1679                                               chanNames = reader.header['signal_channels']['name']
  1680                                               
  1681                                               if chans is not None:
  1682                                                   sigMask = np.isin(chanNames, chans)
  1683                                                   chanNames = chanNames[sigMask]
  1684                                                   
  1685                                               chanIdx = reader.channel_name_to_index(chanNames)
  1686                                               
  1687                                               if not loadAll:
  1688                                                   assert alignTimes is not None
  1689                                                   spikeMats = {i: None for i in alignTimes.index}
  1690                                                   validTrials = pd.Series(True, index=alignTimes.index)
  1691                                               else:
  1692                                                   spikeMats = {
  1693                                                       i: None for i in range(reader.segment_count(block_index=0))}
  1694                                                   validTrials = None
  1695                                               
  1696                                               for segIdx in range(reader.segment_count(block_index=0)):
  1697                                                   if checkReferences:
  1698                                                       for i, cIdx in enumerate(chanIdx):
  1699                                                           da = reader.da_list['blocks'][0]['segments'][segIdx]['data'][cIdx]
  1700                                                           print('name {}, da.name {}'.format(chanNames[i], da.name))
  1701                                                           try:
  1702                                                               assert chanNames[i] in da.name, 'reference problem!!'
  1703                                                           except Exception:
  1704                                                               traceback.print_exc()
  1705                                                   tStart = reader.get_signal_t_start(
  1706                                                       block_index=0, seg_index=segIdx)
  1707                                                   fs = reader.get_signal_sampling_rate(
  1708                                                       channel_indexes=chanIdx
  1709                                                       )
  1710                                                   sigSize = reader.get_signal_size(
  1711                                                       block_index=0, seg_index=segIdx
  1712                                                       )
  1713                                                   tStop = sigSize / fs + tStart
  1714                                                   #  convert to indices early to avoid floating point problems
  1715                                                   
  1716                                                   intervalIdx = int(round(rasterOpts['binInterval'] * fs))
  1717                                                   #  halfIntervalIdx = int(round(intervalIdx / 2))
  1718                                                   
  1719                                                   widthIdx = int(round(rasterOpts['binWidth'] * fs))
  1720                                                   halfWidthIdx = int(round(widthIdx / 2))
  1721                                                   
  1722                                                   if rasterOpts['smoothKernelWidth'] is not None:
  1723                                                       kernWidthIdx = int(round(rasterOpts['smoothKernelWidth'] * fs))
  1724                                                   
  1725                                                   theBins = None
  1726                                           
  1727                                                   if not loadAll:
  1728                                                       winStartIdx = int(round(rasterOpts['windowSize'][0] * fs))
  1729                                                       winStopIdx = int(round(rasterOpts['windowSize'][1] * fs))
  1730                                                       timeMask = (alignTimes > tStart) & (alignTimes < tStop)
  1731                                                       maskedTimes = alignTimes[timeMask]
  1732                                                   else:
  1733                                                       #  irrelevant, will load all
  1734                                                       maskedTimes = pd.Series(np.nan)
  1735                                           
  1736                                                   for idx, tOnset in maskedTimes.iteritems():
  1737                                                       if not loadAll:
  1738                                                           idxOnset = int(round((tOnset - tStart) * fs))
  1739                                                           #  can't not be ints
  1740                                                           iStart = idxOnset + winStartIdx - int(3 * halfWidthIdx)
  1741                                                           iStop = idxOnset + winStopIdx + int(3 * halfWidthIdx)
  1742                                                       else:
  1743                                                           winStartIdx = 0
  1744                                                           iStart = 0
  1745                                                           iStop = sigSize
  1746                                           
  1747                                                       if iStart < 0:
  1748                                                           #  near the first edge
  1749                                                           validTrials[idx] = False
  1750                                                       elif (sigSize < iStop):
  1751                                                           #  near the ending edge
  1752                                                           validTrials[idx] = False
  1753                                                       else:
  1754                                                           #  valid slices
  1755                                                           try:
  1756                                                               rawSpikeMat = pd.DataFrame(
  1757                                                                   reader.get_analogsignal_chunk(
  1758                                                                       block_index=0, seg_index=segIdx,
  1759                                                                       i_start=iStart, i_stop=iStop,
  1760                                                                       channel_names=chanNames))
  1761                                                           except Exception:
  1762                                                               traceback.print_exc()
  1763                                                               #
  1764                                                           if aggregateFun is None:
  1765                                                               procSpikeMat = rawSpikeMat.rolling(
  1766                                                                   window=3 * widthIdx, center=True,
  1767                                                                   win_type='gaussian'
  1768                                                                   ).mean(std=halfWidthIdx)
  1769                                                           else:
  1770                                                               procSpikeMat = rawSpikeMat.rolling(
  1771                                                                   window=widthIdx, center=True
  1772                                                                   ).apply(
  1773                                                                       aggregateFun,
  1774                                                                       raw=True,
  1775                                                                       kwargs={'fs': fs, 'nSamp': widthIdx})
  1776                                                           #
  1777                                                           if rasterOpts['smoothKernelWidth'] is not None:
  1778                                                               procSpikeMat = (
  1779                                                                   procSpikeMat
  1780                                                                   .rolling(
  1781                                                                       window=3 * kernWidthIdx, center=True,
  1782                                                                       win_type='gaussian')
  1783                                                                   .mean(std=kernWidthIdx/2)
  1784                                                                   .dropna().iloc[::intervalIdx, :]
  1785                                                               )
  1786                                                           else:
  1787                                                               procSpikeMat = (
  1788                                                                   procSpikeMat
  1789                                                                   .dropna().iloc[::intervalIdx, :]
  1790                                                               )
  1791                                           
  1792                                                           procSpikeMat.columns = chanNames
  1793                                                           procSpikeMat.columns.name = 'unit'
  1794                                                           if theBins is None:
  1795                                                               theBins = np.asarray(
  1796                                                                   procSpikeMat.index + winStartIdx) / fs
  1797                                                           if absoluteBins:
  1798                                                               procSpikeMat.index = theBins + idxOnset / fs
  1799                                                           else:
  1800                                                               procSpikeMat.index = theBins
  1801                                                           procSpikeMat.index.name = 'bin'
  1802                                                           if loadAll:
  1803                                                               smIdx = segIdx
  1804                                                           else:
  1805                                                               smIdx = idx
  1806                                                               
  1807                                                           spikeMats[smIdx] = procSpikeMat
  1808                                                           if transposeSpikeMat:
  1809                                                               spikeMats[smIdx] = spikeMats[smIdx].transpose()
  1810                                                       #  plt.imshow(rawSpikeMat.to_numpy(), aspect='equal'); plt.show()
  1811                                               return spikeMats, validTrials

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: synchronizeINStoNSP at line 1813

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1813                                           @profile
  1814                                           def synchronizeINStoNSP(
  1815                                                   tapTimestampsNSP=None, tapTimestampsINS=None,
  1816                                                   precalculatedFun=None,
  1817                                                   NSPTimeRanges=(None, None),
  1818                                                   td=None, accel=None, insBlock=None, trialSegment=None, degree=1,
  1819                                                   trimSpiketrains=False
  1820                                                   ):
  1821                                               print('Trial Segment {}'.format(trialSegment))
  1822                                               if precalculatedFun is None:
  1823                                                   assert ((tapTimestampsNSP is not None) & (tapTimestampsINS is not None))
  1824                                                   # sanity check that the intervals match
  1825                                                   insDiff = tapTimestampsINS.diff().dropna().values
  1826                                                   nspDiff = tapTimestampsNSP.diff().dropna().values
  1827                                                   print('On the INS, the diff() between taps was\n{}'.format(insDiff))
  1828                                                   print('On the NSP, the diff() between taps was\n{}'.format(nspDiff))
  1829                                                   print('This amounts to a msec difference of\n{}'.format(
  1830                                                       (insDiff - nspDiff) * 1e3))
  1831                                                   if (insDiff - nspDiff > 20e-3).any():
  1832                                                       raise(Exception('Tap trains too different!'))
  1833                                                   #
  1834                                                   if degree > 0:
  1835                                                       synchPolyCoeffsINStoNSP = np.polyfit(
  1836                                                           x=tapTimestampsINS.values, y=tapTimestampsNSP.values,
  1837                                                           deg=degree)
  1838                                                   else:
  1839                                                       timeOffset = tapTimestampsNSP.values - tapTimestampsINS.values
  1840                                                       synchPolyCoeffsINStoNSP = np.array([1, np.mean(timeOffset)])
  1841                                                   timeInterpFunINStoNSP = np.poly1d(synchPolyCoeffsINStoNSP)
  1842                                               else:
  1843                                                   timeInterpFunINStoNSP = precalculatedFun
  1844                                               if td is not None:
  1845                                                   td.loc[:, 'NSPTime'] = pd.Series(
  1846                                                       timeInterpFunINStoNSP(td['t']), index=td['t'].index)
  1847                                                   td.loc[:, 'NSPTime'] = timeInterpFunINStoNSP(td['t'].to_numpy())
  1848                                               if accel is not None:
  1849                                                   accel.loc[:, 'NSPTime'] = pd.Series(
  1850                                                       timeInterpFunINStoNSP(accel['t']), index=accel['t'].index)
  1851                                               if insBlock is not None:
  1852                                                   # allUnits = [st.unit for st in insBlock.segments[0].spiketrains]
  1853                                                   # [un.name for un in insBlock.filter(objects=Unit)]
  1854                                                   for unit in insBlock.filter(objects=Unit):
  1855                                                       tStart = NSPTimeRanges[0]
  1856                                                       tStop = NSPTimeRanges[1]
  1857                                                       uniqueSt = []
  1858                                                       for st in unit.spiketrains:
  1859                                                           if st not in uniqueSt:
  1860                                                               uniqueSt.append(st)
  1861                                                           else:
  1862                                                               continue
  1863                                                           print('Synchronizing {}'.format(st.name))
  1864                                                           if len(st.times):
  1865                                                               segMaskSt = np.array(
  1866                                                                   st.array_annotations['trialSegment'],
  1867                                                                   dtype=np.int) == trialSegment
  1868                                                               st.magnitude[segMaskSt] = (
  1869                                                                   timeInterpFunINStoNSP(st.times[segMaskSt].magnitude))
  1870                                                               if trimSpiketrains:
  1871                                                                   print('Trimming spiketrain')
  1872                                                                   #  kludgey fix for weirdness concerning t_start
  1873                                                                   st.t_start = min(tStart, st.times[0] * 0.999)
  1874                                                                   st.t_stop = min(tStop, st.times[-1] * 1.001)
  1875                                                                   validMask = st < st.t_stop
  1876                                                                   if ~validMask.all():
  1877                                                                       print('Deleted some spikes')
  1878                                                                       st = st[validMask]
  1879                                                                       # delete invalid spikes
  1880                                                                       if 'arrayAnnNames' in st.annotations.keys():
  1881                                                                           for key in st.annotations['arrayAnnNames']:
  1882                                                                               try:
  1883                                                                                   # st.annotations[key] = np.array(st.array_annotations[key])
  1884                                                                                   st.annotations[key] = np.delete(st.annotations[key], ~validMask)
  1885                                                                               except Exception:
  1886                                                                                   traceback.print_exc()
  1887                                                                                   pdb.set_trace()
  1888                                                           else:
  1889                                                               if trimSpiketrains:
  1890                                                                   st.t_start = tStart
  1891                                                                   st.t_stop = tStop
  1892                                                   #
  1893                                                   allEvents = [
  1894                                                       ev
  1895                                                       for ev in insBlock.filter(objects=Event)
  1896                                                       if ('ins' in ev.name) and ('concatenate' not in ev.name)]
  1897                                                   concatEvents = [
  1898                                                       ev
  1899                                                       for ev in insBlock.filter(objects=Event)
  1900                                                       if ('ins' in ev.name) and ('concatenate' in ev.name)]
  1901                                                   eventsDF = eventsToDataFrame(allEvents, idxT='t')
  1902                                                   newNames = {i: childBaseName(i, 'seg') for i in eventsDF.columns}
  1903                                                   eventsDF.rename(columns=newNames, inplace=True)
  1904                                                   segMask = hf.getStimSerialTrialSegMask(eventsDF, trialSegment)
  1905                                                   evTStart = eventsDF.loc[segMask, 't'].min() * pq.s
  1906                                                   evTStop = eventsDF.loc[segMask, 't'].max() * pq.s
  1907                                                   # print('allEvents[0].shape = {}'.format(allEvents[0].shape))
  1908                                                   # print('allEvents[0].magnitude[segMask][0] = {}'.format(allEvents[0].magnitude[segMask][0]))
  1909                                                   for event in (allEvents + concatEvents):
  1910                                                       if trimSpiketrains:
  1911                                                           thisSegMask = (event.times >= evTStart) & (event.times <= evTStop)
  1912                                                       else:
  1913                                                           thisSegMask = (event.times >= evTStart) & (event.times < evTStop)
  1914                                                       event.magnitude[thisSegMask] = (
  1915                                                           timeInterpFunINStoNSP(event.times[thisSegMask].magnitude))
  1916                                                   # print('allEvents[0].magnitude[segMask][0] = {}'.format(allEvents[0].magnitude[segMask][0]))
  1917                                                   # if len(concatEvents) > trialSegment:
  1918                                                   #     concatEvents[trialSegment].magnitude[:] = timeInterpFunINStoNSP(
  1919                                                   #         concatEvents[trialSegment].times[:].magnitude)
  1920                                               return td, accel, insBlock, timeInterpFunINStoNSP

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: findSegsIncluding at line 1922

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1922                                           @profile
  1923                                           def findSegsIncluding(
  1924                                                   block, timeSlice=None):
  1925                                               segBoundsList = []
  1926                                               for segIdx, seg in enumerate(block.segments):
  1927                                                   segBoundsList.append(pd.DataFrame({
  1928                                                       't_start': seg.t_start,
  1929                                                       't_stop': seg.t_stop
  1930                                                       }, index=[segIdx]))
  1931                                           
  1932                                               segBounds = pd.concat(segBoundsList)
  1933                                               if timeSlice[0] is not None:
  1934                                                   segMask = (segBounds['t_start'] * pq.s >= timeSlice[0]) & (
  1935                                                       segBounds['t_stop'] * pq.s <= timeSlice[1])
  1936                                                   requestedSegs = segBounds.loc[segMask, :]
  1937                                               else:
  1938                                                   timeSlice = (None, None)
  1939                                                   requestedSegs = segBounds
  1940                                               return segBounds, requestedSegs

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: findSegsIncluded at line 1942

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1942                                           @profile
  1943                                           def findSegsIncluded(
  1944                                                   block, timeSlice=None):
  1945                                               segBoundsList = []
  1946                                               for segIdx, seg in enumerate(block.segments):
  1947                                                   segBoundsList.append(pd.DataFrame({
  1948                                                       't_start': seg.t_start,
  1949                                                       't_stop': seg.t_stop
  1950                                                       }, index=[segIdx]))
  1951                                           
  1952                                               segBounds = pd.concat(segBoundsList)
  1953                                               if timeSlice[0] is not None:
  1954                                                   segMask = (segBounds['t_start'] * pq.s <= timeSlice[0]) | (
  1955                                                       segBounds['t_stop'] * pq.s >= timeSlice[1])
  1956                                                   requestedSegs = segBounds.loc[segMask, :]
  1957                                               else:
  1958                                                   timeSlice = (None, None)
  1959                                                   requestedSegs = segBounds
  1960                                               return segBounds, requestedSegs

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getElecLookupTable at line 1962

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1962                                           @profile
  1963                                           def getElecLookupTable(
  1964                                                   block, elecIds=None):
  1965                                               lookupTableList = []
  1966                                               for metaIdx, chanIdx in enumerate(block.channel_indexes):
  1967                                                   if chanIdx.analogsignals:
  1968                                                       #  print(chanIdx.name)
  1969                                                       lookupTableList.append(pd.DataFrame({
  1970                                                           'channelNames': np.asarray(chanIdx.channel_names, dtype=np.str),
  1971                                                           'index': chanIdx.index,
  1972                                                           'metaIndex': metaIdx * chanIdx.index**0,
  1973                                                           'localIndex': (
  1974                                                               list(range(chanIdx.analogsignals[0].shape[1])))
  1975                                                           }))
  1976                                               lookupTable = pd.concat(lookupTableList, ignore_index=True)
  1977                                           
  1978                                               if elecIds is None:
  1979                                                   requestedIndices = lookupTable
  1980                                               else:
  1981                                                   if isinstance(elecIds[0], str):
  1982                                                       idxMask = lookupTable['channelNames'].isin(elecIds)
  1983                                                       requestedIndices = lookupTable.loc[idxMask, :]
  1984                                               return lookupTable, requestedIndices

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getNIXData at line 1986

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1986                                           @profile
  1987                                           def getNIXData(
  1988                                                   fileName=None,
  1989                                                   folderPath=None,
  1990                                                   reader=None, blockIdx=0,
  1991                                                   elecIds=None, startTime_s=None,
  1992                                                   dataLength_s=None, downsample=1,
  1993                                                   signal_group_mode='group-by-same-units',
  1994                                                   closeReader=False):
  1995                                               #  Open file and extract headers
  1996                                               if reader is None:
  1997                                                   assert (fileName is not None) and (folderPath is not None)
  1998                                                   filePath = os.path.join(folderPath, fileName) + '.nix'
  1999                                                   reader = nixio_fr.NixIO(filename=filePath)
  2000                                           
  2001                                               block = reader.read_block(
  2002                                                   block_index=blockIdx, lazy=True,
  2003                                                   signal_group_mode=signal_group_mode)
  2004                                           
  2005                                               for segIdx, seg in enumerate(block.segments):
  2006                                                   seg.events = [i.load() for i in seg.events]
  2007                                                   seg.epochs = [i.load() for i in seg.epochs]
  2008                                           
  2009                                               # find elecIds
  2010                                               lookupTable, requestedIndices = getElecLookupTable(
  2011                                                   block, elecIds=elecIds)
  2012                                           
  2013                                               # find segments that contain the requested times
  2014                                               if dataLength_s is not None:
  2015                                                   assert startTime_s is not None
  2016                                                   timeSlice = (
  2017                                                       startTime_s * pq.s,
  2018                                                       (startTime_s + dataLength_s) * pq.s)
  2019                                               else:
  2020                                                   timeSlice = (None, None)
  2021                                               segBounds, requestedSegs = findSegsIncluding(block, timeSlice)
  2022                                               #
  2023                                               data = pd.DataFrame(columns=elecIds + ['t'])
  2024                                               for segIdx in requestedSegs.index:
  2025                                                   seg = block.segments[segIdx]
  2026                                                   if dataLength_s is not None:
  2027                                                       timeSlice = (
  2028                                                           max(timeSlice[0], seg.t_start),
  2029                                                           min(timeSlice[1], seg.t_stop)
  2030                                                           )
  2031                                                   else:
  2032                                                       timeSlice = (seg.t_start, seg.t_stop)
  2033                                                   segData = pd.DataFrame()
  2034                                                   for metaIdx in pd.unique(requestedIndices['metaIndex']):
  2035                                                       metaIdxMatch = requestedIndices['metaIndex'] == metaIdx
  2036                                                       theseRequestedIndices = requestedIndices.loc[
  2037                                                           metaIdxMatch, :]
  2038                                                       theseElecIds = theseRequestedIndices['channelNames']
  2039                                                       asig = seg.analogsignals[metaIdx]
  2040                                                       thisTimeSlice = (
  2041                                                           max(timeSlice[0], asig.t_start),
  2042                                                           min(timeSlice[1], asig.t_stop)
  2043                                                           )
  2044                                                       reqData = asig.load(
  2045                                                           time_slice=thisTimeSlice,
  2046                                                           channel_indexes=theseRequestedIndices['localIndex'].to_numpy())
  2047                                                       segData = pd.concat((
  2048                                                               segData,
  2049                                                               pd.DataFrame(
  2050                                                                   reqData.magnitude, columns=theseElecIds.to_numpy())),
  2051                                                           axis=1)
  2052                                                   segT = reqData.times
  2053                                                   segData['t'] = segT
  2054                                                   data = pd.concat(
  2055                                                       (data, segData),
  2056                                                       axis=0, ignore_index=True)
  2057                                               channelData = {
  2058                                                   'data': data,
  2059                                                   't': data['t']
  2060                                                   }
  2061                                               if closeReader:
  2062                                                   reader.file.close()
  2063                                                   block = None
  2064                                                   # closing the reader breaks its connection to the block
  2065                                               return channelData, block

Total time: 0.0021846 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: childBaseName at line 2067

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2067                                           @profile
  2068                                           def childBaseName(
  2069                                                   childName, searchTerm):
  2070      1222       5232.0      4.3     23.9      if searchTerm in childName:
  2071      1222      12510.0     10.2     57.3          baseName = '_'.join(childName.split('_')[1:])
  2072                                               else:
  2073                                                   baseName = childName
  2074      1222       4104.0      3.4     18.8      return baseName

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: readBlockFixNames at line 2076

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2076                                           @profile
  2077                                           def readBlockFixNames(
  2078                                                   rawioReader,
  2079                                                   block_index=0, signal_group_mode='split-all',
  2080                                                   lazy=True, mapDF=None, reduceChannelIndexes=False,
  2081                                                   loadList=None, purgeNixNames=False
  2082                                                   ):
  2083                                               headerSignalChan = pd.DataFrame(
  2084                                                   rawioReader.header['signal_channels']).set_index('id')
  2085                                               headerUnitChan = pd.DataFrame(
  2086                                                   rawioReader.header['unit_channels']).set_index('id')
  2087                                               dataBlock = rawioReader.read_block(
  2088                                                   block_index=block_index, lazy=lazy,
  2089                                                   signal_group_mode=signal_group_mode)
  2090                                               if dataBlock.name is None:
  2091                                                   if 'neo_name' in dataBlock.annotations:
  2092                                                       dataBlock.name = dataBlock.annotations['neo_name']
  2093                                               #  on first segment, rename the chan_indexes and units
  2094                                               seg0 = dataBlock.segments[0]
  2095                                               asigLikeList = (
  2096                                                   seg0.filter(objects=AnalogSignalProxy) +
  2097                                                   seg0.filter(objects=AnalogSignal))
  2098                                               if mapDF is not None:
  2099                                                   if headerSignalChan.size > 0:
  2100                                                       asigNameChanger = {}
  2101                                                       for nevID in mapDF['nevID']:
  2102                                                           if int(nevID) in headerSignalChan.index:
  2103                                                               labelFromMap = (
  2104                                                                   mapDF
  2105                                                                   .loc[mapDF['nevID'] == nevID, 'label']
  2106                                                                   .iloc[0])
  2107                                                               asigNameChanger[
  2108                                                                   headerSignalChan.loc[int(nevID), 'name']] = labelFromMap
  2109                                                   else:
  2110                                                       asigOrigNames = np.unique(
  2111                                                           [i.split('#')[0] for i in headerUnitChan['name']])
  2112                                                       asigNameChanger = {}
  2113                                                       for origName in asigOrigNames:
  2114                                                           # ripple specific
  2115                                                           formattedName = origName.replace('.', '_').replace(' raw', '')
  2116                                                           if mapDF['label'].str.contains(formattedName).any():
  2117                                                               asigNameChanger[origName] = formattedName
  2118                                               else:
  2119                                                   asigNameChanger = dict()
  2120                                               for asig in asigLikeList:
  2121                                                   asigBaseName = childBaseName(asig.name, 'seg')
  2122                                                   asig.name = (
  2123                                                       asigNameChanger[asigBaseName]
  2124                                                       if asigBaseName in asigNameChanger
  2125                                                       else asigBaseName)
  2126                                                   if mapDF is not None:
  2127                                                       if (mapDF['label'] == asig.name).any():
  2128                                                           asig.annotations['xCoords'] = float(mapDF.loc[mapDF['label'] == asig.name, 'xcoords'].iloc[0])
  2129                                                           asig.annotations['yCoords'] = float(mapDF.loc[mapDF['label'] == asig.name, 'ycoords'].iloc[0])
  2130                                                           asig.annotations['zCoords'] = float(mapDF.loc[mapDF['label'] == asig.name, 'zcoords'].iloc[0])
  2131                                                   if 'Channel group ' in asig.channel_index.name:
  2132                                                       newChanName = (
  2133                                                           asigNameChanger[asigBaseName]
  2134                                                           if asigBaseName in asigNameChanger
  2135                                                           else asigBaseName)
  2136                                                       asig.channel_index.name = newChanName
  2137                                                       if 'neo_name' in asig.channel_index.annotations:
  2138                                                           asig.channel_index.annotations['neo_name'] = newChanName
  2139                                                       if 'nix_name' in asig.channel_index.annotations:
  2140                                                           asig.channel_index.annotations['nix_name'] = newChanName
  2141                                                       if mapDF is not None:
  2142                                                           try:
  2143                                                               asig.channel_index.coordinates = np.asarray([
  2144                                                                   asig.annotations['xCoords'], asig.annotations['yCoords'], asig.annotations['zCoords']
  2145                                                               ])[np.newaxis, :] * pq.um
  2146                                                           except Exception:
  2147                                                               pass
  2148                                               spikeTrainLikeList = (
  2149                                                   seg0.filter(objects=SpikeTrainProxy) +
  2150                                                   seg0.filter(objects=SpikeTrain))
  2151                                               # add channels for channelIndex that has no asigs but has spikes
  2152                                               nExtraChans = 0
  2153                                               for stp in spikeTrainLikeList:
  2154                                                   stpBaseName = childBaseName(stp.name, 'seg')
  2155                                                   nameParser = re.search(r'ch(\d*)#(\d*)', stpBaseName)
  2156                                                   if nameParser is not None:
  2157                                                       # first time at this unit, rename it
  2158                                                       chanId = int(nameParser.group(1))
  2159                                                       unitId = int(nameParser.group(2))
  2160                                                       if chanId >= 5121:
  2161                                                           isRippleStimChan = True
  2162                                                           chanId = chanId - 5120
  2163                                                       else:
  2164                                                           isRippleStimChan = False
  2165                                                       ####################
  2166                                                       # asigBaseName = headerSignalChan.loc[chanId, 'name']
  2167                                                       # if mapDF is not None:
  2168                                                       #     if asigBaseName in asigNameChanger:
  2169                                                       #         chanIdLabel = (
  2170                                                       #             asigNameChanger[asigBaseName]
  2171                                                       #             if asigBaseName in asigNameChanger
  2172                                                       #             else asigBaseName)
  2173                                                       #     else:
  2174                                                       #         chanIdLabel = asigBaseName
  2175                                                       # else:
  2176                                                       #     chanIdLabel = asigBaseName
  2177                                                       ###################
  2178                                                       # if swapMaps is not None:
  2179                                                       #     nameCandidates = (swapMaps['to'].loc[swapMaps['to']['nevID'] == chanId, 'label']).to_list()
  2180                                                       # elif mapDF is not None:
  2181                                                       #     nameCandidates = (mapDF.loc[mapDF['nevID'] == chanId, 'label']).to_list()
  2182                                                       # else:
  2183                                                       #     nameCandidates = []
  2184                                                       ##############################
  2185                                                       if mapDF is not None:
  2186                                                           nameCandidates = (
  2187                                                               mapDF
  2188                                                               .loc[mapDF['nevID'] == chanId, 'label']
  2189                                                               .to_list())
  2190                                                       else:
  2191                                                           nameCandidates = []
  2192                                                       if len(nameCandidates) == 1:
  2193                                                           chanIdLabel = nameCandidates[0]
  2194                                                       elif chanId in headerSignalChan:
  2195                                                           chanIdLabel = headerSignalChan.loc[chanId, 'name']
  2196                                                       else:
  2197                                                           chanIdLabel = 'ch{}'.format(chanId)
  2198                                                       #
  2199                                                       if isRippleStimChan:
  2200                                                           stp.name = '{}_stim#{}'.format(chanIdLabel, unitId)
  2201                                                       else:
  2202                                                           stp.name = '{}#{}'.format(chanIdLabel, unitId)
  2203                                                       stp.unit.name = stp.name
  2204                                                   ########################################
  2205                                                   # sanitize ripple names ####
  2206                                                   stp.name = stp.name.replace('.', '_').replace(' raw', '')
  2207                                                   stp.unit.name = stp.unit.name.replace('.', '_').replace(' raw', '')
  2208                                                   ###########################################
  2209                                                   if 'ChannelIndex for ' in stp.unit.channel_index.name:
  2210                                                       newChanName = stp.name.replace('_stim#0', '')
  2211                                                       # remove unit #
  2212                                                       newChanName = re.sub(r'#\d', '', newChanName)
  2213                                                       stp.unit.channel_index.name = newChanName
  2214                                                       # units and analogsignals have different channel_indexes when loaded by nix
  2215                                                       # add them to each other's parent list
  2216                                                       allMatchingChIdx = dataBlock.filter(
  2217                                                           objects=ChannelIndex, name=newChanName)
  2218                                                       if (len(allMatchingChIdx) > 1) and reduceChannelIndexes:
  2219                                                           assert len(allMatchingChIdx) == 2
  2220                                                           targetChIdx = [
  2221                                                               ch
  2222                                                               for ch in allMatchingChIdx
  2223                                                               if ch is not stp.unit.channel_index][0]
  2224                                                           oldChIdx = stp.unit.channel_index
  2225                                                           targetChIdx.units.append(stp.unit)
  2226                                                           stp.unit.channel_index = targetChIdx
  2227                                                           oldChIdx.units.remove(stp.unit)
  2228                                                           if not (len(oldChIdx.units) or len(oldChIdx.analogsignals)):
  2229                                                               dataBlock.channel_indexes.remove(oldChIdx)
  2230                                                           del oldChIdx
  2231                                                           targetChIdx.create_relationship()
  2232                                                       elif reduceChannelIndexes:
  2233                                                           if newChanName not in headerSignalChan['name']:
  2234                                                               stp.unit.channel_index.index = np.asarray(
  2235                                                                   [headerSignalChan['name'].size + nExtraChans])
  2236                                                               stp.unit.channel_index.channel_ids = np.asarray(
  2237                                                                   [headerSignalChan['name'].size + nExtraChans])
  2238                                                               stp.unit.channel_index.channel_names = np.asarray(
  2239                                                                   [newChanName])
  2240                                                               nExtraChans += 1
  2241                                                           if 'neo_name' not in allMatchingChIdx[0].annotations:
  2242                                                               allMatchingChIdx[0].annotations['neo_name'] = allMatchingChIdx[0].name
  2243                                                           if 'nix_name' not in allMatchingChIdx[0].annotations:
  2244                                                               allMatchingChIdx[0].annotations['nix_name'] = allMatchingChIdx[0].name
  2245                                                   stp.unit.channel_index.name = stp.unit.channel_index.name.replace('.', '_').replace(' raw', '')
  2246                                               #  rename the children
  2247                                               typesNeedRenaming = [
  2248                                                   SpikeTrainProxy, AnalogSignalProxy, EventProxy,
  2249                                                   SpikeTrain, AnalogSignal, Event]
  2250                                               for segIdx, seg in enumerate(dataBlock.segments):
  2251                                                   if seg.name is None:
  2252                                                       seg.name = 'seg{}_'.format(segIdx)
  2253                                                   else:
  2254                                                       if 'seg{}_'.format(segIdx) not in seg.name:
  2255                                                           seg.name = (
  2256                                                               'seg{}_{}'
  2257                                                               .format(
  2258                                                                   segIdx,
  2259                                                                   childBaseName(seg.name, 'seg')))
  2260                                                   for objType in typesNeedRenaming:
  2261                                                       for child in seg.filter(objects=objType):
  2262                                                           if 'seg{}_'.format(segIdx) not in child.name:
  2263                                                               child.name = (
  2264                                                                   'seg{}_{}'
  2265                                                                   .format(
  2266                                                                       segIdx, childBaseName(child.name, 'seg')))
  2267                                                           #  todo: decide if below is needed
  2268                                                           #  elif 'seg' in child.name:
  2269                                                           #      childBaseName = '_'.join(child.name.split('_')[1:])
  2270                                                           #      child.name = 'seg{}_{}'.format(segIdx, childBaseName)
  2271                                               # [i.name for i in dataBlock.filter(objects=Unit)]
  2272                                               # [i.name for i in dataBlock.filter(objects=ChannelIndex)]
  2273                                               # [i.name for i in dataBlock.filter(objects=SpikeTrain)]
  2274                                               # [i.name for i in dataBlock.filter(objects=SpikeTrainProxy)]
  2275                                               if lazy:
  2276                                                   for stP in dataBlock.filter(objects=SpikeTrainProxy):
  2277                                                       if 'unitAnnotations' in stP.annotations:
  2278                                                           unAnnStr = stP.annotations['unitAnnotations']
  2279                                                           stP.unit.annotations.update(json.loads(unAnnStr))
  2280                                               if (loadList is not None) and lazy:
  2281                                                   if 'asigs' in loadList:
  2282                                                       loadAsigList(
  2283                                                           dataBlock, listOfAsigProxyNames=loadList['asigs'],
  2284                                                           replaceInParents=True)
  2285                                                   if 'events' in loadList:
  2286                                                       loadEventList(
  2287                                                           dataBlock,
  2288                                                           listOfEventNames=loadList['events'],
  2289                                                           replaceInParents=True)
  2290                                                   if 'spiketrains' in loadList:
  2291                                                       loadSpikeTrainList(
  2292                                                           dataBlock,
  2293                                                           listOfSpikeTrainNames=loadList['spiketrains'],
  2294                                                           replaceInParents=True)
  2295                                               if purgeNixNames:
  2296                                                   dataBlock = purgeNixAnn(dataBlock)
  2297                                               return dataBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadSpikeTrainList at line 2299

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2299                                           @profile
  2300                                           def loadSpikeTrainList(
  2301                                                   dataBlock, listOfSpikeTrainNames=None,
  2302                                                   replaceInParents=True):
  2303                                               listOfSpikeTrains = []
  2304                                               if listOfSpikeTrainNames is None:
  2305                                                   listOfSpikeTrainNames = [
  2306                                                       stp.name
  2307                                                       for stp in dataBlock.filter(objects=SpikeTrainProxy)]
  2308                                               for stP in dataBlock.filter(objects=SpikeTrainProxy):
  2309                                                   if stP.name in listOfSpikeTrainNames:
  2310                                                       st = loadObjArrayAnn(stP.load())
  2311                                                       listOfSpikeTrains.append(st)
  2312                                                       if replaceInParents:
  2313                                                           seg = stP.segment
  2314                                                           segStNames = [s.name for s in seg.spiketrains]
  2315                                                           idxInSeg = segStNames.index(stP.name)
  2316                                                           seg.spiketrains[idxInSeg] = st
  2317                                                           #
  2318                                                           unit = stP.unit
  2319                                                           unitStNames = [s.name for s in unit.spiketrains]
  2320                                                           st.unit = unit
  2321                                                           idxInUnit = unitStNames.index(stP.name)
  2322                                                           unit.spiketrains[idxInUnit] = st
  2323                                               return listOfSpikeTrains

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadEventList at line 2325

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2325                                           @profile
  2326                                           def loadEventList(
  2327                                                   dataBlock,
  2328                                                   listOfEventNames=None, replaceInParents=True):
  2329                                               listOfEvents = []
  2330                                               if listOfEventNames is None:
  2331                                                   listOfEventNames = [
  2332                                                       evp.name
  2333                                                       for evp in dataBlock.filter(objects=EventProxy)]
  2334                                               for evP in dataBlock.filter(objects=EventProxy):
  2335                                                   if evP.name in listOfEventNames:
  2336                                                       ev = loadObjArrayAnn(evP.load())
  2337                                                       listOfEvents.append(ev)
  2338                                                       if replaceInParents:
  2339                                                           seg = evP.segment
  2340                                                           segEvNames = [e.name for e in seg.events]
  2341                                                           idxInSeg = segEvNames.index(evP.name)
  2342                                                           seg.events[idxInSeg] = ev
  2343                                               return listOfEvents

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadAsigList at line 2345

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2345                                           @profile
  2346                                           def loadAsigList(
  2347                                                   dataBlock, listOfAsigProxyNames=None, replaceInParents=True):
  2348                                               listOfAsigs = []
  2349                                               if listOfAsigProxyNames is None:
  2350                                                   listOfAsigProxyNames = [
  2351                                                       asigp.name
  2352                                                       for asigp in dataBlock.filter(objects=AnalogSignalProxy)]
  2353                                               for asigP in dataBlock.filter(objects=AnalogSignalProxy):
  2354                                                   if asigP.name in listOfAsigProxyNames:
  2355                                                       asig = asigP.load()
  2356                                                       asig.annotations = asigP.annotations.copy()
  2357                                                       listOfAsigs.append(asig)
  2358                                                       #
  2359                                                       if replaceInParents:
  2360                                                           seg = asigP.segment
  2361                                                           segAsigNames = [ag.name for ag in seg.analogsignals]
  2362                                                           asig.segment = seg
  2363                                                           idxInSeg = segAsigNames.index(asigP.name)
  2364                                                           seg.analogsignals[idxInSeg] = asig
  2365                                                           #
  2366                                                           chIdx = asigP.channel_index
  2367                                                           chIdxAsigNames = [ag.name for ag in chIdx.analogsignals]
  2368                                                           asig.channel_index = chIdx
  2369                                                           idxInChIdx = chIdxAsigNames.index(asigP.name)
  2370                                                           chIdx.analogsignals[idxInChIdx] = asig
  2371                                               return listOfAsigs

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: addBlockToNIX at line 2373

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2373                                           @profile
  2374                                           def addBlockToNIX(
  2375                                                   newBlock, neoSegIdx=[0],
  2376                                                   writeAsigs=True, writeSpikes=True, writeEvents=True,
  2377                                                   asigNameList=None,
  2378                                                   purgeNixNames=False,
  2379                                                   fileName=None,
  2380                                                   folderPath=None,
  2381                                                   nixBlockIdx=0, nixSegIdx=[0],
  2382                                                   ):
  2383                                               #  base file name
  2384                                               trialBasePath = os.path.join(folderPath, fileName)
  2385                                               if writeAsigs:
  2386                                                   # peek at file to ensure compatibility
  2387                                                   reader = nixio_fr.NixIO(filename=trialBasePath + '.nix')
  2388                                                   tempBlock = reader.read_block(
  2389                                                       block_index=nixBlockIdx,
  2390                                                       lazy=True, signal_group_mode='split-all')
  2391                                                   checkCompatible = {i: False for i in nixSegIdx}
  2392                                                   forceShape = {i: None for i in nixSegIdx}
  2393                                                   forceType = {i: None for i in nixSegIdx}
  2394                                                   forceFS = {i: None for i in nixSegIdx}
  2395                                                   for nixIdx in nixSegIdx:
  2396                                                       tempAsigList = tempBlock.segments[nixIdx].filter(
  2397                                                           objects=AnalogSignalProxy)
  2398                                                       if len(tempAsigList) > 0:
  2399                                                           tempAsig = tempAsigList[0]
  2400                                                           checkCompatible[nixIdx] = True
  2401                                                           forceType[nixIdx] = tempAsig.dtype
  2402                                                           forceShape[nixIdx] = tempAsig.shape[0]  # ? docs say shape[1], but that's confusing
  2403                                                           forceFS[nixIdx] = tempAsig.sampling_rate
  2404                                                   reader.file.close()
  2405                                               #  if newBlock was loaded from a nix file, strip the old nix_names away:
  2406                                               #  todo: replace with function from this module
  2407                                               if purgeNixNames:
  2408                                                   newBlock = purgeNixAnn(newBlock)
  2409                                               #
  2410                                               writer = NixIO(filename=trialBasePath + '.nix')
  2411                                               nixblock = writer.nix_file.blocks[nixBlockIdx]
  2412                                               nixblockName = nixblock.name
  2413                                               if 'nix_name' in newBlock.annotations.keys():
  2414                                                   try:
  2415                                                       assert newBlock.annotations['nix_name'] == nixblockName
  2416                                                   except Exception:
  2417                                                       newBlock.annotations['nix_name'] = nixblockName
  2418                                               else:
  2419                                                   newBlock.annotate(nix_name=nixblockName)
  2420                                               #
  2421                                               for idx, segIdx in enumerate(neoSegIdx):
  2422                                                   nixIdx = nixSegIdx[idx]
  2423                                                   newSeg = newBlock.segments[segIdx]
  2424                                                   nixgroup = nixblock.groups[nixIdx]
  2425                                                   nixSegName = nixgroup.name
  2426                                                   if 'nix_name' in newSeg.annotations.keys():
  2427                                                       try:
  2428                                                           assert newSeg.annotations['nix_name'] == nixSegName
  2429                                                       except Exception:
  2430                                                           newSeg.annotations['nix_name'] = nixSegName
  2431                                                   else:
  2432                                                       newSeg.annotate(nix_name=nixSegName)
  2433                                                   #
  2434                                                   if writeEvents:
  2435                                                       eventList = newSeg.events
  2436                                                       eventOrder = np.argsort([i.name for i in eventList])
  2437                                                       for event in [eventList[i] for i in eventOrder]:
  2438                                                           event = writer._write_event(event, nixblock, nixgroup)
  2439                                                   #
  2440                                                   if writeAsigs:
  2441                                                       asigList = newSeg.filter(objects=AnalogSignal)
  2442                                                       asigOrder = np.argsort([i.name for i in asigList])
  2443                                                       for asig in [asigList[i] for i in asigOrder]:
  2444                                                           if checkCompatible[nixIdx]:
  2445                                                               assert asig.dtype == forceType[nixIdx]
  2446                                                               assert asig.sampling_rate == forceFS[nixIdx]
  2447                                                               #  print('asig.shape[0] = {}'.format(asig.shape[0]))
  2448                                                               #  print('forceShape[nixIdx] = {}'.format(forceShape[nixIdx]))
  2449                                                               assert asig.shape[0] == forceShape[nixIdx]
  2450                                                           asig = writer._write_analogsignal(asig, nixblock, nixgroup)
  2451                                                       #  for isig in newSeg.filter(objects=IrregularlySampledSignal):
  2452                                                       #      isig = writer._write_irregularlysampledsignal(
  2453                                                       #          isig, nixblock, nixgroup)
  2454                                                   #
  2455                                                   if writeSpikes:
  2456                                                       stList = newSeg.filter(objects=SpikeTrain)
  2457                                                       stOrder = np.argsort([i.name for i in stList])
  2458                                                       for st in [stList[i] for i in stOrder]:
  2459                                                           st = writer._write_spiketrain(st, nixblock, nixgroup)
  2460                                               #
  2461                                               for chanIdx in newBlock.filter(objects=ChannelIndex):
  2462                                                   chanIdx = writer._write_channelindex(chanIdx, nixblock)
  2463                                                   #  auto descends into units inside of _write_channelindex
  2464                                               writer._create_source_links(newBlock, nixblock)
  2465                                               writer.close()
  2466                                               print('Done adding block to Nix.')
  2467                                               return newBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadStProxy at line 2469

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2469                                           @profile
  2470                                           def loadStProxy(stProxy):
  2471                                               try:
  2472                                                   st = stProxy.load(
  2473                                                       magnitude_mode='rescaled',
  2474                                                       load_waveforms=True)
  2475                                               except Exception:
  2476                                                   st = stProxy.load(
  2477                                                       magnitude_mode='rescaled',
  2478                                                       load_waveforms=False)
  2479                                                   st.waveforms = np.asarray([]).reshape((0, 0, 0))*pq.mV
  2480                                               return st

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: preproc at line 2482

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2482                                           @profile
  2483                                           def preproc(
  2484                                                   fileName='Trial001',
  2485                                                   rawFolderPath='./',
  2486                                                   outputFolderPath='./', mapDF=None,
  2487                                                   # swapMaps=None,
  2488                                                   electrodeArrayName='utah',
  2489                                                   fillOverflow=True, removeJumps=True,
  2490                                                   removeMeanAcross=False,
  2491                                                   linearDetrend=False,
  2492                                                   interpolateOutliers=False, calcOutliers=False,
  2493                                                   outlierMaskFilterOpts=None,
  2494                                                   outlierThreshold=1,
  2495                                                   calcArtifactTrace=False,
  2496                                                   motorEncoderMask=None,
  2497                                                   calcAverageLFP=False,
  2498                                                   eventInfo=None,
  2499                                                   spikeSourceType='', spikePath=None,
  2500                                                   chunkSize=1800, equalChunks=True, chunkList=None, chunkOffset=0,
  2501                                                   writeMode='rw',
  2502                                                   signal_group_mode='split-all', trialInfo=None,
  2503                                                   asigNameList=None, ainpNameList=None, nameSuffix='',
  2504                                                   saveFromAsigNameList=True,
  2505                                                   calcRigEvents=True, normalizeByImpedance=False,
  2506                                                   LFPFilterOpts=None, encoderCountPerDegree=180e2,
  2507                                                   outlierRemovalDebugFlag=False, impedanceFilePath=None
  2508                                                   ):
  2509                                               #  base file name
  2510                                               rawBasePath = os.path.join(rawFolderPath, fileName)
  2511                                               outputFilePath = os.path.join(
  2512                                                   outputFolderPath,
  2513                                                   fileName + nameSuffix + '.nix')
  2514                                               if os.path.exists(outputFilePath):
  2515                                                   os.remove(outputFilePath)
  2516                                               #  instantiate reader, get metadata
  2517                                               print('Loading\n{}\n'.format(rawBasePath))
  2518                                               reader = BlackrockIO(
  2519                                                   filename=rawBasePath, nsx_to_load=5)
  2520                                               reader.parse_header()
  2521                                               # metadata = reader.header
  2522                                               #  absolute section index
  2523                                               dummyBlock = readBlockFixNames(
  2524                                                   reader,
  2525                                                   block_index=0, lazy=True,
  2526                                                   signal_group_mode=signal_group_mode,
  2527                                                   mapDF=mapDF, reduceChannelIndexes=True,
  2528                                                   # swapMaps=swapMaps
  2529                                                   )
  2530                                               segLen = dummyBlock.segments[0].analogsignals[0].shape[0] / (
  2531                                                   dummyBlock.segments[0].analogsignals[0].sampling_rate)
  2532                                               nChunks = math.ceil(segLen / chunkSize)
  2533                                               #
  2534                                               if equalChunks:
  2535                                                   actualChunkSize = (segLen / nChunks).magnitude
  2536                                               else:
  2537                                                   actualChunkSize = chunkSize
  2538                                               if chunkList is None:
  2539                                                   chunkList = range(nChunks)
  2540                                               chunkingMetadata = {}
  2541                                               for chunkIdx in chunkList:
  2542                                                   print('preproc on chunk {}'.format(chunkIdx))
  2543                                                   #  instantiate spike reader if requested
  2544                                                   if spikeSourceType == 'tdc':
  2545                                                       if spikePath is None:
  2546                                                           spikePath = os.path.join(
  2547                                                               outputFolderPath, 'tdc_' + fileName,
  2548                                                               'tdc_' + fileName + '.nix')
  2549                                                       print('loading {}'.format(spikePath))
  2550                                                       spikeReader = nixio_fr.NixIO(filename=spikePath)
  2551                                                   else:
  2552                                                       spikeReader = None
  2553                                                   #  absolute section index
  2554                                                   block = readBlockFixNames(
  2555                                                       reader,
  2556                                                       block_index=0, lazy=True,
  2557                                                       signal_group_mode=signal_group_mode,
  2558                                                       mapDF=mapDF, reduceChannelIndexes=True,
  2559                                                       # swapMaps=swapMaps
  2560                                                       )
  2561                                                   if spikeReader is not None:
  2562                                                       spikeBlock = readBlockFixNames(
  2563                                                           spikeReader, block_index=0, lazy=True,
  2564                                                           signal_group_mode=signal_group_mode,
  2565                                                           mapDF=mapDF, reduceChannelIndexes=True,
  2566                                                           # swapMaps=swapMaps
  2567                                                           )
  2568                                                       spikeBlock = purgeNixAnn(spikeBlock)
  2569                                                   else:
  2570                                                       spikeBlock = None
  2571                                                   #
  2572                                                   #  instantiate writer
  2573                                                   if (nChunks == 1) or (len(chunkList) == 1):
  2574                                                       partNameSuffix = ""
  2575                                                       thisChunkOutFilePath = outputFilePath
  2576                                                   else:
  2577                                                       partNameSuffix = '_pt{:0>3}'.format(chunkIdx)
  2578                                                       thisChunkOutFilePath = (
  2579                                                           outputFilePath
  2580                                                           .replace('.nix', partNameSuffix + '.nix'))
  2581                                                   #
  2582                                                   if os.path.exists(thisChunkOutFilePath):
  2583                                                       os.remove(thisChunkOutFilePath)
  2584                                                   writer = NixIO(
  2585                                                       filename=thisChunkOutFilePath, mode=writeMode)
  2586                                                   chunkTStart = chunkIdx * actualChunkSize + chunkOffset
  2587                                                   chunkTStop = (chunkIdx + 1) * actualChunkSize + chunkOffset
  2588                                                   chunkingMetadata[chunkIdx] = {
  2589                                                       'filename': thisChunkOutFilePath,
  2590                                                       'partNameSuffix': partNameSuffix,
  2591                                                       'chunkTStart': chunkTStart,
  2592                                                       'chunkTStop': chunkTStop}
  2593                                                   block.annotate(chunkTStart=chunkTStart)
  2594                                                   block.annotate(chunkTStop=chunkTStop)
  2595                                                   block.annotate(
  2596                                                       recDatetimeStr=(
  2597                                                           block
  2598                                                           .rec_datetime
  2599                                                           .replace(tzinfo=timezone.utc)
  2600                                                           .isoformat())
  2601                                                       )
  2602                                                   #
  2603                                                   preprocBlockToNix(
  2604                                                       block, writer,
  2605                                                       chunkTStart=chunkTStart,
  2606                                                       chunkTStop=chunkTStop,
  2607                                                       fillOverflow=fillOverflow,
  2608                                                       removeJumps=removeJumps,
  2609                                                       interpolateOutliers=interpolateOutliers,
  2610                                                       calcOutliers=calcOutliers,
  2611                                                       outlierThreshold=outlierThreshold,
  2612                                                       outlierMaskFilterOpts=outlierMaskFilterOpts,
  2613                                                       calcArtifactTrace=calcArtifactTrace,
  2614                                                       linearDetrend=linearDetrend,
  2615                                                       motorEncoderMask=motorEncoderMask,
  2616                                                       electrodeArrayName=electrodeArrayName,
  2617                                                       calcAverageLFP=calcAverageLFP,
  2618                                                       eventInfo=eventInfo,
  2619                                                       asigNameList=asigNameList, ainpNameList=ainpNameList,
  2620                                                       saveFromAsigNameList=saveFromAsigNameList,
  2621                                                       spikeSourceType=spikeSourceType,
  2622                                                       spikeBlock=spikeBlock,
  2623                                                       calcRigEvents=calcRigEvents,
  2624                                                       normalizeByImpedance=normalizeByImpedance,
  2625                                                       removeMeanAcross=removeMeanAcross,
  2626                                                       LFPFilterOpts=LFPFilterOpts,
  2627                                                       encoderCountPerDegree=encoderCountPerDegree,
  2628                                                       outlierRemovalDebugFlag=outlierRemovalDebugFlag,
  2629                                                       impedanceFilePath=impedanceFilePath,
  2630                                                       )
  2631                                                   #### diagnostics
  2632                                                   diagnosticFolder = os.path.join(
  2633                                                       outputFolderPath,
  2634                                                       'preprocDiagnostics',
  2635                                                       # fileName + nameSuffix + partNameSuffix
  2636                                                       )
  2637                                                   if not os.path.exists(diagnosticFolder):
  2638                                                       os.mkdir(diagnosticFolder)
  2639                                                   asigDiagnostics = {}
  2640                                                   outlierDiagnostics = {}
  2641                                                   diagnosticText = ''
  2642                                                   for asig in block.filter(objects=AnalogSignal):
  2643                                                       annNames = ['mean_removal_r2', 'mean_removal_group']
  2644                                                       for annName in annNames:
  2645                                                           if annName in asig.annotations:
  2646                                                               if asig.name not in asigDiagnostics:
  2647                                                                   asigDiagnostics[asig.name] = {}
  2648                                                               asigDiagnostics[asig.name].update({
  2649                                                                   annName: asig.annotations[annName]})
  2650                                                       annNames = [
  2651                                                           'outlierProportion', 'nDim',
  2652                                                           'noveltyThreshold', 'outlierThreshold'
  2653                                                           ]
  2654                                                       for annName in annNames:
  2655                                                           if annName in asig.annotations:
  2656                                                               if asig.name not in outlierDiagnostics:
  2657                                                                   outlierDiagnostics[asig.name] = {}
  2658                                                               outlierDiagnostics[asig.name].update({
  2659                                                                   annName: '{}'.format(asig.annotations[annName])
  2660                                                               })
  2661                                                   if removeMeanAcross:
  2662                                                       asigDiagnosticsDF = pd.DataFrame(asigDiagnostics).T
  2663                                                       asigDiagnosticsDF.sort_values(by='mean_removal_r2', inplace=True)
  2664                                                       diagnosticText += '<h2>LFP Diagnostics</h2>\n'
  2665                                                       diagnosticText += asigDiagnosticsDF.to_html()
  2666                                                       fig, ax = plt.subplots()
  2667                                                       sns.distplot(asigDiagnosticsDF['mean_removal_r2'], ax=ax)
  2668                                                       ax.set_ylabel('Count of analog signals')
  2669                                                       ax.set_xlabel('R^2 of regressing mean against signal')
  2670                                                       fig.savefig(os.path.join(
  2671                                                               diagnosticFolder,
  2672                                                               fileName + nameSuffix + partNameSuffix + '_meanRemovalR2.png'
  2673                                                           ))
  2674                                                   if interpolateOutliers:
  2675                                                       outlierDiagnosticsDF = pd.DataFrame(outlierDiagnostics).T
  2676                                                       diagnosticText += '<h2>Outlier Diagnostics</h2>\n'
  2677                                                       diagnosticText += outlierDiagnosticsDF.to_html()
  2678                                                   diagnosticTextPath = os.path.join(
  2679                                                       diagnosticFolder,
  2680                                                       fileName + nameSuffix + partNameSuffix + '_asigDiagnostics.html'
  2681                                                       )
  2682                                                   with open(diagnosticTextPath, 'w') as _f:
  2683                                                       _f.write(diagnosticText)
  2684                                                   writer.close()
  2685                                               chunkingInfoPath = os.path.join(
  2686                                                   outputFolderPath,
  2687                                                   fileName + nameSuffix +
  2688                                                   '_chunkingInfo.json'
  2689                                                   )
  2690                                               if os.path.exists(chunkingInfoPath):
  2691                                                   os.remove(chunkingInfoPath)
  2692                                               with open(chunkingInfoPath, 'w') as f:
  2693                                                   json.dump(chunkingMetadata, f)
  2694                                               return

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: preprocBlockToNix at line 2696

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2696                                           @profile
  2697                                           def preprocBlockToNix(
  2698                                                   block, writer,
  2699                                                   chunkTStart=None,
  2700                                                   chunkTStop=None,
  2701                                                   eventInfo=None,
  2702                                                   fillOverflow=False, calcAverageLFP=False,
  2703                                                   interpolateOutliers=False, calcOutliers=False,
  2704                                                   calcArtifactTrace=False,
  2705                                                   outlierMaskFilterOpts=None,
  2706                                                   useMeanToCenter=False,   # mean center? median center?
  2707                                                   linearDetrend=False,
  2708                                                   zScoreEachTrace=False,
  2709                                                   outlierThreshold=1,
  2710                                                   motorEncoderMask=None,
  2711                                                   electrodeArrayName='utah',
  2712                                                   removeJumps=False, trackMemory=True,
  2713                                                   asigNameList=None, ainpNameList=None,
  2714                                                   saveFromAsigNameList=True,
  2715                                                   spikeSourceType='', spikeBlock=None,
  2716                                                   calcRigEvents=True,
  2717                                                   normalizeByImpedance=True,
  2718                                                   impedanceFilePath=None,
  2719                                                   removeMeanAcross=False,
  2720                                                   LFPFilterOpts=None, encoderCountPerDegree=180e2,
  2721                                                   outlierRemovalDebugFlag=False,
  2722                                                   ):
  2723                                               #  prune out nev spike placeholders
  2724                                               #  (will get added back on a chunk by chunk basis,
  2725                                               #  if not pruning units)
  2726                                               if spikeSourceType == 'nev':
  2727                                                   pruneOutUnits = False
  2728                                               else:
  2729                                                   pruneOutUnits = True
  2730                                               #
  2731                                               for chanIdx in block.channel_indexes:
  2732                                                   if chanIdx.units:
  2733                                                       for unit in chanIdx.units:
  2734                                                           if unit.spiketrains:
  2735                                                               unit.spiketrains = []
  2736                                                       if pruneOutUnits:
  2737                                                           chanIdx.units = []
  2738                                               #
  2739                                               if spikeBlock is not None:
  2740                                                   for chanIdx in spikeBlock.channel_indexes:
  2741                                                       if chanIdx.units:
  2742                                                           for unit in chanIdx.units:
  2743                                                               if unit.spiketrains:
  2744                                                                   unit.spiketrains = []
  2745                                               #  precalculate new segment
  2746                                               seg = block.segments[0]
  2747                                               #  remove chanIndexes assigned to units; makes more sense to
  2748                                               #  only use chanIdx for asigs and spikes on that asig
  2749                                               #  block.channel_indexes = (
  2750                                               #      [chanIdx for chanIdx in block.channel_indexes if (
  2751                                               #          chanIdx.analogsignals)])
  2752                                               if calcAverageLFP:
  2753                                                   lastIndex = len(block.channel_indexes)
  2754                                                   lastID = block.channel_indexes[-1].channel_ids[0] + 1
  2755                                                   if asigNameList is None:
  2756                                                       asigNameList = [
  2757                                                           [
  2758                                                               childBaseName(a.name, 'seg')
  2759                                                               for a in seg.analogsignals
  2760                                                               if not (('ainp' in a.name) or ('analog' in a.name))]
  2761                                                           ]
  2762                                                   nMeanChans = len(asigNameList)
  2763                                                   #
  2764                                                   meanChIdxList = []
  2765                                                   for meanChIdx in range(nMeanChans):
  2766                                                       tempChIdx = ChannelIndex(
  2767                                                           index=[lastIndex + meanChIdx],
  2768                                                           channel_names=['{}_rawAverage_{}'.format(electrodeArrayName, meanChIdx)],
  2769                                                           channel_ids=[lastID + meanChIdx],
  2770                                                           name='{}_rawAverage_{}'.format(electrodeArrayName, meanChIdx),
  2771                                                           file_origin=block.channel_indexes[-1].file_origin
  2772                                                           )
  2773                                                       tempChIdx.merge_annotations(block.channel_indexes[-1])
  2774                                                       block.channel_indexes.append(tempChIdx)
  2775                                                       meanChIdxList.append(tempChIdx)
  2776                                                       lastIndex += 1
  2777                                                       lastID += 1
  2778                                                   lastIndex = len(block.channel_indexes)
  2779                                                   lastID = block.channel_indexes[-1].channel_ids[0] + 1
  2780                                                   # if calcArtifactTrace:
  2781                                                   if True:
  2782                                                       artChIdxList = []
  2783                                                       for artChIdx in range(nMeanChans):
  2784                                                           tempChIdx = ChannelIndex(
  2785                                                               index=[lastIndex + artChIdx],
  2786                                                               channel_names=['{}_artifact_{}'.format(electrodeArrayName, artChIdx)],
  2787                                                               channel_ids=[lastID + artChIdx],
  2788                                                               name='{}_artifact_{}'.format(electrodeArrayName, artChIdx),
  2789                                                               file_origin=block.channel_indexes[-1].file_origin
  2790                                                               )
  2791                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2792                                                           block.channel_indexes.append(tempChIdx)
  2793                                                           artChIdxList.append(tempChIdx)
  2794                                                           lastIndex += 1
  2795                                                           lastID += 1
  2796                                                   # if calcOutliers:
  2797                                                   if True:
  2798                                                       devChIdxList = []
  2799                                                       for devChIdx in range(nMeanChans):
  2800                                                           tempChIdx = ChannelIndex(
  2801                                                               index=[lastIndex + devChIdx],
  2802                                                               channel_names=['{}_deviation_{}'.format(electrodeArrayName, devChIdx)],
  2803                                                               channel_ids=[lastID + devChIdx],
  2804                                                               name='{}_deviation_{}'.format(electrodeArrayName, devChIdx),
  2805                                                               file_origin=block.channel_indexes[-1].file_origin
  2806                                                               )
  2807                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2808                                                           block.channel_indexes.append(tempChIdx)
  2809                                                           devChIdxList.append(tempChIdx)
  2810                                                           lastIndex += 1
  2811                                                           lastID += 1
  2812                                                       smDevChIdxList = []
  2813                                                       for devChIdx in range(nMeanChans):
  2814                                                           tempChIdx = ChannelIndex(
  2815                                                               index=[lastIndex + devChIdx],
  2816                                                               channel_names=['{}_smoothed_deviation_{}'.format(electrodeArrayName, devChIdx)],
  2817                                                               channel_ids=[lastID + devChIdx],
  2818                                                               name='{}_smoothed_deviation_{}'.format(electrodeArrayName, devChIdx),
  2819                                                               file_origin=block.channel_indexes[-1].file_origin
  2820                                                               )
  2821                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2822                                                           block.channel_indexes.append(tempChIdx)
  2823                                                           smDevChIdxList.append(tempChIdx)
  2824                                                           lastIndex += 1
  2825                                                           lastID += 1
  2826                                                       outMaskChIdxList = []
  2827                                                       for outMaskChIdx in range(nMeanChans):
  2828                                                           tempChIdx = ChannelIndex(
  2829                                                               index=[lastIndex + outMaskChIdx],
  2830                                                               channel_names=['{}_outlierMask_{}'.format(
  2831                                                                   electrodeArrayName, outMaskChIdx)],
  2832                                                               channel_ids=[lastID + outMaskChIdx],
  2833                                                               name='{}_outlierMask_{}'.format(
  2834                                                                   electrodeArrayName, outMaskChIdx),
  2835                                                               file_origin=block.channel_indexes[-1].file_origin
  2836                                                               )
  2837                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2838                                                           block.channel_indexes.append(tempChIdx)
  2839                                                           outMaskChIdxList.append(tempChIdx)
  2840                                                           lastIndex += 1
  2841                                                           lastID += 1
  2842                                               #  delete asig and irsig proxies from channel index list
  2843                                               for metaIdx, chanIdx in enumerate(block.channel_indexes):
  2844                                                   if chanIdx.analogsignals:
  2845                                                       chanIdx.analogsignals = []
  2846                                                   if chanIdx.irregularlysampledsignals:
  2847                                                       chanIdx.irregularlysampledsignals = []
  2848                                               newSeg = Segment(
  2849                                                       index=0, name=seg.name,
  2850                                                       description=seg.description,
  2851                                                       file_origin=seg.file_origin,
  2852                                                       file_datetime=seg.file_datetime,
  2853                                                       rec_datetime=seg.rec_datetime,
  2854                                                       **seg.annotations
  2855                                                   )
  2856                                               block.segments = [newSeg]
  2857                                               block, nixblock = writer.write_block_meta(block)
  2858                                               # descend into Segments
  2859                                               if impedanceFilePath is not None:
  2860                                                   try:
  2861                                                       impedances = prb_meta.getLatestImpedance(
  2862                                                           block=block, impedanceFilePath=impedanceFilePath)
  2863                                                       averageImpedance = impedances['impedance'].median()
  2864                                                   except Exception:
  2865                                                       traceback.print_exc()
  2866                                               # for segIdx, seg in enumerate(oldSegList):
  2867                                               if spikeBlock is not None:
  2868                                                   spikeSeg = spikeBlock.segments[0]
  2869                                               else:
  2870                                                   spikeSeg = seg
  2871                                               #
  2872                                               if trackMemory:
  2873                                                   print('memory usage: {:.1f} MB'.format(
  2874                                                       prf.memory_usage_psutil()))
  2875                                               newSeg, nixgroup = writer._write_segment_meta(newSeg, nixblock)
  2876                                               #  trim down list of analog signals if necessary
  2877                                               asigNameListSeg = []
  2878                                               if (removeMeanAcross or calcAverageLFP):
  2879                                                   meanGroups = {}
  2880                                               for subListIdx, subList in enumerate(asigNameList):
  2881                                                   subListSeg = [
  2882                                                       'seg{}_{}'.format(0, a)
  2883                                                       for a in subList]
  2884                                                   asigNameListSeg += subListSeg
  2885                                                   if (removeMeanAcross or calcAverageLFP):
  2886                                                       meanGroups[subListIdx] = subListSeg
  2887                                               aSigList = []
  2888                                               # [asig.name for asig in seg.analogsignals]
  2889                                               for a in seg.analogsignals:
  2890                                                   # if np.any([n in a.name for n in asigNameListSeg]):
  2891                                                   if a.name in asigNameListSeg:
  2892                                                       aSigList.append(a)
  2893                                               if ainpNameList is not None:
  2894                                                   ainpNameListSeg = [
  2895                                                       'seg{}_{}'.format(0, a)
  2896                                                       for a in ainpNameList]
  2897                                                   ainpList = []
  2898                                                   for a in seg.analogsignals:
  2899                                                       if np.any([n == a.name for n in ainpNameListSeg]):
  2900                                                           ainpList.append(a)
  2901                                               else:
  2902                                                   ainpList = [
  2903                                                       a
  2904                                                       for a in seg.analogsignals
  2905                                                       if (('ainp' in a.name) or ('analog' in a.name))]
  2906                                                   ainpNameListSeg = [a.name for a in aSigList]
  2907                                               nAsigs = len(aSigList)
  2908                                               if LFPFilterOpts is not None:
  2909                                                   def filterFun(sig, filterCoeffs=None):
  2910                                                       # sig[:] = signal.sosfiltfilt(
  2911                                                       sig[:] = signal.sosfilt(
  2912                                                           filterCoeffs, sig.magnitude.flatten())[:, np.newaxis] * sig.units
  2913                                                       return sig
  2914                                                   filterCoeffs = hf.makeFilterCoeffsSOS(
  2915                                                       LFPFilterOpts, float(seg.analogsignals[0].sampling_rate))
  2916                                                   if False:
  2917                                                       fig, ax1, ax2 = hf.plotFilterResponse(
  2918                                                           filterCoeffs,
  2919                                                           float(seg.analogsignals[0].sampling_rate))
  2920                                                       fig2, ax3, ax4 = hf.plotFilterImpulseResponse(
  2921                                                           LFPFilterOpts,
  2922                                                           float(seg.analogsignals[0].sampling_rate))
  2923                                                       plt.show()
  2924                                               # first pass through asigs, if removing mean across channels
  2925                                               if (removeMeanAcross or calcAverageLFP):
  2926                                                   for aSigIdx, aSigProxy in enumerate(seg.analogsignals):
  2927                                                       if aSigIdx == 0:
  2928                                                           # check bounds
  2929                                                           tStart = max(chunkTStart * pq.s, aSigProxy.t_start)
  2930                                                           tStop = min(chunkTStop * pq.s, aSigProxy.t_stop)
  2931                                                       loadThisOne = (aSigProxy in aSigList)
  2932                                                       if loadThisOne:
  2933                                                           if trackMemory:
  2934                                                               print(
  2935                                                                   'Extracting asig for mean, memory usage: {:.1f} MB'.format(
  2936                                                                       prf.memory_usage_psutil()))
  2937                                                           chanIdx = aSigProxy.channel_index
  2938                                                           asig = aSigProxy.load(
  2939                                                               time_slice=(tStart, tStop),
  2940                                                               magnitude_mode='rescaled')
  2941                                                           if 'tempLFPStore' not in locals():
  2942                                                               tempLFPStore = pd.DataFrame(
  2943                                                                   np.zeros(
  2944                                                                       (asig.shape[0], nAsigs),
  2945                                                                       dtype=np.float32),
  2946                                                                   columns=asigNameListSeg)
  2947                                                           if 'dummyAsig' not in locals():
  2948                                                               dummyAsig = asig.copy()
  2949                                                           #  perform requested preproc operations
  2950                                                           #  if LFPFilterOpts is not None:
  2951                                                           #      asig[:] = filterFun(
  2952                                                           #          asig, filterCoeffs=filterCoeffs)
  2953                                                           if normalizeByImpedance:
  2954                                                               elNmMatchMsk = impedances['elec'] == chanIdx.name
  2955                                                               '''
  2956                                                               asig.magnitude[:] = (
  2957                                                                   (asig.magnitude - np.median(asig.magnitude)) /
  2958                                                                   np.min(
  2959                                                                       impedances.loc[elNmMatchMsk, 'impedance']
  2960                                                                       ))
  2961                                                               '''
  2962                                                               asig.magnitude[:] = (
  2963                                                                   (asig.magnitude) * averageImpedance /
  2964                                                                   np.min(
  2965                                                                       impedances.loc[elNmMatchMsk, 'impedance']
  2966                                                                       ))
  2967                                                           # if fillOverflow:
  2968                                                           #     # fill in overflow:
  2969                                                           #     '''
  2970                                                           #     timeSection['data'], overflowMask = hf.fillInOverflow(
  2971                                                           #         timeSection['data'], fillMethod = 'average')
  2972                                                           #     badData.update({'overflow': overflowMask})
  2973                                                           #     '''
  2974                                                           #     pass
  2975                                                           # if removeJumps:
  2976                                                           #     # find unusual jumps in derivative or amplitude
  2977                                                           #     '''
  2978                                                           #     timeSection['data'], newBadData = hf.fillInJumps(timeSection['data'],
  2979                                                           #     timeSection['samp_per_s'], smoothing_ms = 0.5, nStdDiff = 50,
  2980                                                           #     nStdAmp = 100)
  2981                                                           #     badData.update(newBadData)
  2982                                                           #     '''
  2983                                                           #     pass
  2984                                                           tempLFPStore.loc[:, aSigProxy.name] = asig.magnitude.flatten()
  2985                                                           del asig
  2986                                                           gc.collect()
  2987                                                   # end of first pass
  2988                                                   if (removeMeanAcross or calcAverageLFP):
  2989                                                       centerLFP = np.zeros(
  2990                                                           (tempLFPStore.shape[0], len(asigNameList)),
  2991                                                           dtype=np.float32)
  2992                                                       spreadLFP = np.zeros(
  2993                                                           (tempLFPStore.shape[0], len(asigNameList)),
  2994                                                           dtype=np.float32)
  2995                                                       # if calcOutliers:
  2996                                                       if True:
  2997                                                           if outlierMaskFilterOpts is not None:
  2998                                                               filterCoeffsOutlierMask = hf.makeFilterCoeffsSOS(
  2999                                                                   outlierMaskFilterOpts, float(dummyAsig.sampling_rate))
  3000                                                           lfpDeviation = np.zeros(
  3001                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3002                                                               dtype=np.float32)
  3003                                                           smoothedDeviation = np.zeros(
  3004                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3005                                                               dtype=np.float32)
  3006                                                           outlierMask = np.zeros(
  3007                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3008                                                               dtype=np.bool)
  3009                                                           outlierMetadata = {}
  3010                                                       # if calcArtifactTrace:
  3011                                                       if True:
  3012                                                           artifactSignal = np.zeros(
  3013                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3014                                                               dtype=np.float32)
  3015                                                       ###############
  3016                                                       # tempLFPStore.iloc[:, 0] = np.nan  # for debugging axes
  3017                                                       #############
  3018                                                       plotDevFilterDebug = False
  3019                                                       if plotDevFilterDebug:
  3020                                                           try:
  3021                                                               devFiltDebugMask = (dummyAsig.times > 90 * pq.s) & (dummyAsig.times < 92 * pq.s)
  3022                                                           except Exception:
  3023                                                               pdb.set_trace()
  3024                                                           plotColIdx = 1
  3025                                                           ddfFig, ddfAx = plt.subplots(len(asigNameList), 1)
  3026                                                           ddfFig2, ddfAx2 = plt.subplots()
  3027                                                           ddfFig3, ddfAx3 = plt.subplots(
  3028                                                               1, len(asigNameList),
  3029                                                               sharey=True)
  3030                                                           if len(asigNameList) == 1:
  3031                                                               ddfAx = np.asarray([ddfAx])
  3032                                                               ddfAx3 = np.asarray([ddfAx3])
  3033                                                       for subListIdx, subList in enumerate(asigNameList):
  3034                                                           columnsForThisGroup = meanGroups[subListIdx]
  3035                                                           if trackMemory:
  3036                                                               print(
  3037                                                                   'asig group {}: calculating mean, memory usage: {:.1f} MB'.format(
  3038                                                                       subListIdx, prf.memory_usage_psutil()))
  3039                                                               print('this group contains\n{}'.format(columnsForThisGroup))
  3040                                                           if plotDevFilterDebug:
  3041                                                               ddfAx3[subListIdx].plot(
  3042                                                                   dummyAsig.times[devFiltDebugMask],
  3043                                                                   tempLFPStore.loc[:, columnsForThisGroup].iloc[devFiltDebugMask, plotColIdx],
  3044                                                                   label='original ch'
  3045                                                                   )
  3046                                                           if fillOverflow:
  3047                                                               print('Filling overflow...')
  3048                                                               # fill in overflow:
  3049                                                               tempLFPStore.loc[:, columnsForThisGroup], pltHandles = hf.fillInOverflow2(
  3050                                                                   tempLFPStore.loc[:, columnsForThisGroup].to_numpy(),
  3051                                                                   overFlowFillType='average',
  3052                                                                   overFlowThreshold=8000,
  3053                                                                   debuggingPlots=plotDevFilterDebug
  3054                                                                   )
  3055                                                               if plotDevFilterDebug:
  3056                                                                   pltHandles['ax'].set_title('ch grp {}'.format(subListIdx))
  3057                                                                   ddfAx3[subListIdx].plot(
  3058                                                                       dummyAsig.times[devFiltDebugMask],
  3059                                                                       tempLFPStore.loc[:, columnsForThisGroup].iloc[devFiltDebugMask, plotColIdx],
  3060                                                                       label='filled ch'
  3061                                                                       )
  3062                                                           # zscore of each trace
  3063                                                           if zScoreEachTrace:
  3064                                                               print('About to calculate zscore of each trace (along columns) for prelim outlier detection')
  3065                                                               columnZScore = pd.DataFrame(
  3066                                                                   stats.zscore(
  3067                                                                       tempLFPStore.loc[:, columnsForThisGroup],
  3068                                                                       axis=1),
  3069                                                                   index=tempLFPStore.index,
  3070                                                                   columns=columnsForThisGroup
  3071                                                                   )
  3072                                                               excludeFromMeanMask = columnZScore.abs() > 6
  3073                                                               if useMeanToCenter:
  3074                                                                   centerLFP[:, subListIdx] = (
  3075                                                                       tempLFPStore
  3076                                                                       .loc[:, columnsForThisGroup]
  3077                                                                       .mask(excludeFromMeanMask)
  3078                                                                       .mean(axis=1).to_numpy()
  3079                                                                       )
  3080                                                               else:
  3081                                                                   centerLFP[:, subListIdx] = (
  3082                                                                       tempLFPStore
  3083                                                                       .loc[:, columnsForThisGroup]
  3084                                                                       .mask(excludeFromMeanMask)
  3085                                                                       .median(axis=1).to_numpy()
  3086                                                                       )
  3087                                                           else:
  3088                                                               if useMeanToCenter:
  3089                                                                   centerLFP[:, subListIdx] = (
  3090                                                                       tempLFPStore
  3091                                                                       .loc[:, columnsForThisGroup]
  3092                                                                       .mean(axis=1).to_numpy()
  3093                                                                       )
  3094                                                               else:
  3095                                                                   centerLFP[:, subListIdx] = (
  3096                                                                       tempLFPStore
  3097                                                                       .loc[:, columnsForThisGroup]
  3098                                                                       .median(axis=1).to_numpy()
  3099                                                                       )
  3100                                                           if calcArtifactTrace:
  3101                                                               if LFPFilterOpts is not None:
  3102                                                                   print('applying LFPFilterOpts to cached asigs for artifact ID')
  3103                                                                   # tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfilt(
  3104                                                                   tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfiltfilt(
  3105                                                                       filterCoeffs, tempLFPStore.loc[:, columnsForThisGroup],
  3106                                                                       axis=0)
  3107                                                                   if useMeanToCenter:
  3108                                                                       tempCenter = (
  3109                                                                           tempLFPStore
  3110                                                                           .loc[:, columnsForThisGroup]
  3111                                                                           .mean(axis=1).diff().fillna(0)
  3112                                                                           )
  3113                                                                   else:
  3114                                                                       tempCenter = (
  3115                                                                           tempLFPStore
  3116                                                                           .loc[:, columnsForThisGroup]
  3117                                                                           .median(axis=1).diff().fillna(0)
  3118                                                                           )
  3119                                                               artifactSignal[:, subListIdx] = np.abs(stats.zscore(tempCenter.to_numpy()))
  3120                                                           if calcOutliers:
  3121                                                               if plotDevFilterDebug:
  3122                                                                   ddfAx[subListIdx].plot(
  3123                                                                       dummyAsig.times[devFiltDebugMask],
  3124                                                                       centerLFP[devFiltDebugMask, subListIdx],
  3125                                                                       label='mean of ch group'
  3126                                                                       )
  3127                                                               # filter the traces, if needed
  3128                                                               if LFPFilterOpts is not None:
  3129                                                                   print('applying LFPFilterOpts to cached asigs before outlier detection')
  3130                                                                   # tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfiltfilt(
  3131                                                                   tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfilt(
  3132                                                                       filterCoeffs, tempLFPStore.loc[:, columnsForThisGroup],
  3133                                                                       axis=0)
  3134                                                                   if plotDevFilterDebug:
  3135                                                                       ddfAx3[subListIdx].plot(
  3136                                                                           dummyAsig.times[devFiltDebugMask],
  3137                                                                           tempLFPStore.loc[:, columnsForThisGroup].iloc[devFiltDebugMask, plotColIdx],
  3138                                                                           label='filtered ch'
  3139                                                                           )
  3140                                                               ##################################
  3141                                                               print('Whitening cached traces before outlier detection')
  3142                                                               whitenByPCA = True
  3143                                                               if whitenByPCA:
  3144                                                                   projector = PCA(
  3145                                                                       n_components=None, whiten=True)
  3146                                                                   pcs = projector.fit_transform(
  3147                                                                       tempLFPStore.loc[:, columnsForThisGroup])
  3148                                                                   explVarMask = (
  3149                                                                       np.cumsum(projector.explained_variance_ratio_) < 1 - 1e-2)
  3150                                                                   explVarMask[0] = True  # (keep at least 1)
  3151                                                                   pcs = pcs[:, explVarMask]
  3152                                                                   nDim = pcs.shape[1]
  3153                                                                   lfpDeviation[:, subListIdx] = (pcs ** 2).sum(axis=1)
  3154                                                               else:  # whiten by mahalanobis distance
  3155                                                                   est = EmpiricalCovariance()
  3156                                                                   est.fit(tempLFPStore.loc[:, columnsForThisGroup].to_numpy())
  3157                                                                   lfpDeviation[:, subListIdx] = est.mahalanobis(
  3158                                                                       tempLFPStore.loc[:, columnsForThisGroup].to_numpy())
  3159                                                                   nDim = tempLFPStore.loc[:, columnsForThisGroup].shape[1]
  3160                                                               #
  3161                                                               transformedDeviation = stats.norm.isf(stats.chi2.sf(lfpDeviation[:, subListIdx], nDim))
  3162                                                               infMask = np.isinf(transformedDeviation)
  3163                                                               if infMask.any():
  3164                                                                   transformedDeviation[infMask] = transformedDeviation[~infMask].max()
  3165                                                               debugProbaTrans = False
  3166                                                               if debugProbaTrans:
  3167                                                                   fig, ax = plt.subplots()
  3168                                                                   tAx = ax.twinx()
  3169                                                                   plotMask = (dummyAsig.times >= 60 * pq.s) & (dummyAsig.times < 95 * pq.s)
  3170                                                                   ax.plot(dummyAsig.times[plotMask], transformedDeviation[plotMask], c='b', label='transformed deviation')
  3171                                                                   tAx.plot(dummyAsig.times[plotMask], lfpDeviation[plotMask, subListIdx], c='r', label='original deviation')
  3172                                                                   ax.legend(loc='upper left')
  3173                                                                   tAx.legend(loc='upper right')
  3174                                                                   plt.show()
  3175                                                               lfpDeviation[:, subListIdx] = transformedDeviation
  3176                                                               noveltyThreshold = stats.norm.interval(outlierThreshold)[1]
  3177                                                               # chi2Bounds = stats.chi2.interval(outlierThreshold, nDim)
  3178                                                               # lfpDeviation[:, subListIdx] = lfpDeviation[:, subListIdx] / chi2Bounds[1]
  3179                                                               # print('nDim = {}, chi2Lim = {}'.format(nDim, chi2Bounds))
  3180                                                               # noveltyThreshold = 1
  3181                                                               #
  3182                                                               outlierMetadata[subListIdx] = {
  3183                                                                   'nDim': nDim,
  3184                                                                   'noveltyThreshold': noveltyThreshold,
  3185                                                                   'outlierThreshold': outlierThreshold
  3186                                                                   }
  3187                                                               # smoothedDeviation = signal.sosfilt(
  3188                                                               print('Smoothing deviation')
  3189                                                               tempSmDev = signal.sosfiltfilt(
  3190                                                                   filterCoeffsOutlierMask, lfpDeviation[:, subListIdx])
  3191                                                               smoothedDeviation[:, subListIdx] = tempSmDev
  3192                                                               if plotDevFilterDebug:
  3193                                                                   ddfAx[subListIdx].plot(
  3194                                                                       dummyAsig.times[devFiltDebugMask],
  3195                                                                       lfpDeviation[devFiltDebugMask, subListIdx],
  3196                                                                       label='original deviation (ch grp {})'.format(subListIdx))
  3197                                                                   ddfAx[subListIdx].plot(
  3198                                                                       dummyAsig.times[devFiltDebugMask],
  3199                                                                       smoothedDeviation[devFiltDebugMask, subListIdx],
  3200                                                                       label='filtered deviation (ch grp {})'.format(subListIdx))
  3201                                                               ##
  3202                                                               print('Calculating outlier mask')
  3203                                                               outlierMask[:, subListIdx] = (
  3204                                                                   smoothedDeviation[:, subListIdx] > noveltyThreshold)
  3205                                                               if plotDevFilterDebug:
  3206                                                                   ddfAx[subListIdx].axhline(noveltyThreshold, c='r')
  3207                                                       if plotDevFilterDebug and calcOutliers:
  3208                                                           for subListIdx, subList in enumerate(asigNameList):
  3209                                                               ddfAx[subListIdx].legend(loc='upper right')
  3210                                                               ddfAx[subListIdx].set_title('Deviation')
  3211                                                               ddfAx3[subListIdx].legend(loc='upper right')
  3212                                                               ddfAx3[subListIdx].set_title('Example channel')
  3213                                                               ddfAx2.plot(
  3214                                                                   dummyAsig.times[devFiltDebugMask],
  3215                                                                   smoothedDeviation[devFiltDebugMask, subListIdx],
  3216                                                                   label='ch grp {}'.format(subListIdx))
  3217                                                               ddfAx2.set_title('Smoothed Deviation')
  3218                                                           ddfAx2.legend(loc='upper right')
  3219                                                           plt.show()
  3220                                                       #############
  3221                                                       del tempLFPStore
  3222                                                       gc.collect()
  3223                                               if (removeMeanAcross or calcAverageLFP):
  3224                                                   for mIdx, meanChIdx in enumerate(meanChIdxList):
  3225                                                       meanAsig = AnalogSignal(
  3226                                                           centerLFP[:, mIdx],
  3227                                                           units=dummyAsig.units,
  3228                                                           sampling_rate=dummyAsig.sampling_rate,
  3229                                                           # name='seg{}_{}'.format(idx, meanChIdx.name)
  3230                                                           name='seg{}_{}'.format(0, meanChIdx.name),
  3231                                                           t_start=tStart
  3232                                                       )
  3233                                                       # assign ownership to containers
  3234                                                       meanChIdx.analogsignals.append(meanAsig)
  3235                                                       newSeg.analogsignals.append(meanAsig)
  3236                                                       # assign parent to children
  3237                                                       meanChIdx.create_relationship()
  3238                                                       newSeg.create_relationship()
  3239                                                       # write out to file
  3240                                                       if LFPFilterOpts is not None:
  3241                                                           meanAsig[:] = filterFun(
  3242                                                               meanAsig, filterCoeffs=filterCoeffs)
  3243                                                       meanAsig = writer._write_analogsignal(
  3244                                                           meanAsig, nixblock, nixgroup)
  3245                                                   # if calcArtifactTrace:
  3246                                                   if True:
  3247                                                       for mIdx, artChIdx in enumerate(artChIdxList):
  3248                                                           artAsig = AnalogSignal(
  3249                                                               artifactSignal[:, mIdx],
  3250                                                               units=dummyAsig.units,
  3251                                                               sampling_rate=dummyAsig.sampling_rate,
  3252                                                               # name='seg{}_{}'.format(idx, devChIdx.name)
  3253                                                               name='seg{}_{}'.format(0, artChIdx.name),
  3254                                                               t_start=tStart
  3255                                                               )
  3256                                                           # assign ownership to containers
  3257                                                           artChIdx.analogsignals.append(artAsig)
  3258                                                           newSeg.analogsignals.append(artAsig)
  3259                                                           # assign parent to children
  3260                                                           artChIdx.create_relationship()
  3261                                                           newSeg.create_relationship()
  3262                                                           # write out to file
  3263                                                           artAsig = writer._write_analogsignal(
  3264                                                               artAsig, nixblock, nixgroup)
  3265                                                           #########################################################
  3266                                                   # if calcOutliers:
  3267                                                   if True:
  3268                                                       for mIdx, devChIdx in enumerate(devChIdxList):
  3269                                                           devAsig = AnalogSignal(
  3270                                                               lfpDeviation[:, mIdx],
  3271                                                               units=dummyAsig.units,
  3272                                                               sampling_rate=dummyAsig.sampling_rate,
  3273                                                               # name='seg{}_{}'.format(idx, devChIdx.name)
  3274                                                               name='seg{}_{}'.format(0, devChIdx.name),
  3275                                                               t_start=tStart
  3276                                                               )
  3277                                                           # assign ownership to containers
  3278                                                           devChIdx.analogsignals.append(devAsig)
  3279                                                           newSeg.analogsignals.append(devAsig)
  3280                                                           # assign parent to children
  3281                                                           devChIdx.create_relationship()
  3282                                                           newSeg.create_relationship()
  3283                                                           # write out to file
  3284                                                           devAsig = writer._write_analogsignal(
  3285                                                               devAsig, nixblock, nixgroup)
  3286                                                           #########################################################
  3287                                                       for mIdx, smDevChIdx in enumerate(smDevChIdxList):
  3288                                                           smDevAsig = AnalogSignal(
  3289                                                               smoothedDeviation[:, mIdx],
  3290                                                               units=dummyAsig.units,
  3291                                                               sampling_rate=dummyAsig.sampling_rate,
  3292                                                               # name='seg{}_{}'.format(idx, devChIdx.name)
  3293                                                               name='seg{}_{}'.format(0, smDevChIdx.name),
  3294                                                               t_start=tStart
  3295                                                               )
  3296                                                           # assign ownership to containers
  3297                                                           smDevChIdx.analogsignals.append(smDevAsig)
  3298                                                           newSeg.analogsignals.append(smDevAsig)
  3299                                                           # assign parent to children
  3300                                                           smDevChIdx.create_relationship()
  3301                                                           newSeg.create_relationship()
  3302                                                           # write out to file
  3303                                                           smDevAsig = writer._write_analogsignal(
  3304                                                               smDevAsig, nixblock, nixgroup)
  3305                                                           #########################################################
  3306                                                       for mIdx, outMaskChIdx in enumerate(outMaskChIdxList):
  3307                                                           outMaskAsig = AnalogSignal(
  3308                                                               outlierMask[:, mIdx],
  3309                                                               units=dummyAsig.units,
  3310                                                               sampling_rate=dummyAsig.sampling_rate,
  3311                                                               # name='seg{}_{}'.format(idx, outMaskChIdx.name)
  3312                                                               name='seg{}_{}'.format(0, outMaskChIdx.name),
  3313                                                               t_start=tStart, dtype=np.float32
  3314                                                               )
  3315                                                           outMaskAsig.annotations['outlierProportion'] = np.mean(outlierMask[:, mIdx])
  3316                                                           if calcOutliers:
  3317                                                               outMaskAsig.annotations.update(outlierMetadata[mIdx])
  3318                                                           # assign ownership to containers
  3319                                                           outMaskChIdx.analogsignals.append(outMaskAsig)
  3320                                                           newSeg.analogsignals.append(outMaskAsig)
  3321                                                           # assign parent to children
  3322                                                           outMaskChIdx.create_relationship()
  3323                                                           newSeg.create_relationship()
  3324                                                           # write out to file
  3325                                                           outMaskAsig = writer._write_analogsignal(
  3326                                                               outMaskAsig, nixblock, nixgroup)
  3327                                                   #
  3328                                                   w0 = 60
  3329                                                   bandQ = 20
  3330                                                   bw = w0/bandQ
  3331                                                   noiseSos = signal.iirfilter(
  3332                                                       N=8, Wn=[w0 - bw/2, w0 + bw/2],
  3333                                                       btype='band', ftype='butter',
  3334                                                       analog=False, fs=float(dummyAsig.sampling_rate),
  3335                                                       output='sos')
  3336                                                   # signal.hilbert does not have an option to zero pad
  3337                                                   nextLen = fftpack.helper.next_fast_len(dummyAsig.shape[0])
  3338                                                   deficit = int(nextLen - dummyAsig.shape[0])
  3339                                                   lDef = int(np.floor(deficit / 2))
  3340                                                   rDef = int(np.ceil(deficit / 2)) + 1
  3341                                                   temp = np.pad(
  3342                                                       dummyAsig.magnitude.flatten(),
  3343                                                       (lDef, rDef), mode='constant')
  3344                                                   # lineNoise = signal.sosfiltfilt(
  3345                                                   lineNoise = signal.sosfilt(
  3346                                                       noiseSos, temp, axis=0)
  3347                                                   lineNoiseH = signal.hilbert(lineNoise)
  3348                                                   lineNoise = lineNoise[lDef:-rDef]
  3349                                                   lineNoiseH = lineNoiseH[lDef:-rDef]
  3350                                                   lineNoisePhase = np.angle(lineNoiseH)
  3351                                                   lineNoisePhaseDF = pd.DataFrame(
  3352                                                       lineNoisePhase,
  3353                                                       index=dummyAsig.times,
  3354                                                       columns=['phase']
  3355                                                       )
  3356                                                   plotHilbert = False
  3357                                                   if plotHilbert:
  3358                                                       lineNoiseFreq = (
  3359                                                           np.diff(np.unwrap(lineNoisePhase)) /
  3360                                                           (2.0*np.pi) * float(dummyAsig.sampling_rate))
  3361                                                       lineNoiseEnvelope = np.abs(lineNoiseH)
  3362                                                       i1 = 300000; i2 = 330000
  3363                                                       fig, ax = plt.subplots(2, 1, sharex=True)
  3364                                                       ax[0].plot(dummyAsig.times[devFiltDebugMask], dummyAsig.magnitude[devFiltDebugMask, :])
  3365                                                       ax[0].plot(dummyAsig.times[devFiltDebugMask], lineNoise[devFiltDebugMask])
  3366                                                       ax[0].plot(dummyAsig.times[devFiltDebugMask], lineNoiseEnvelope[devFiltDebugMask])
  3367                                                       axFr = ax[1].twinx()
  3368                                                       ax[1].plot(
  3369                                                           dummyAsig.times[devFiltDebugMask], lineNoisePhase[devFiltDebugMask],
  3370                                                           c='r', label='phase')
  3371                                                       ax[1].legend()
  3372                                                       axFr.plot(
  3373                                                           dummyAsig.times[devFiltDebugMask], lineNoiseFreq[devFiltDebugMask],
  3374                                                           label='freq')
  3375                                                       axFr.set_ylim([59, 61])
  3376                                                       axFr.legend()
  3377                                                       plt.show()
  3378                                               # second pass through asigs, to save
  3379                                               for aSigIdx, aSigProxy in enumerate(seg.analogsignals):
  3380                                                   if aSigIdx == 0:
  3381                                                       # check bounds
  3382                                                       tStart = max(chunkTStart * pq.s, aSigProxy.t_start)
  3383                                                       tStop = min(chunkTStop * pq.s, aSigProxy.t_stop)
  3384                                                   loadThisOne = (
  3385                                                       (saveFromAsigNameList and (aSigProxy in aSigList)) or
  3386                                                       (aSigProxy in ainpList)
  3387                                                       )
  3388                                                   if loadThisOne:
  3389                                                       if trackMemory:
  3390                                                           print('writing asig {} ({}) memory usage: {:.1f} MB'.format(
  3391                                                               aSigIdx, aSigProxy.name, prf.memory_usage_psutil()))
  3392                                                       chanIdx = aSigProxy.channel_index
  3393                                                       asig = aSigProxy.load(
  3394                                                           time_slice=(tStart, tStop),
  3395                                                           magnitude_mode='rescaled')
  3396                                                       #  link AnalogSignal and ID providing channel_index
  3397                                                       asig.channel_index = chanIdx
  3398                                                       #  perform requested preproc operations
  3399                                                       if 'impedances' in locals():
  3400                                                           elNmMatchMsk = impedances['elec'] == chanIdx.name
  3401                                                           if elNmMatchMsk.any():
  3402                                                               originalImpedance = np.min(
  3403                                                                   impedances.loc[elNmMatchMsk, 'impedance']
  3404                                                                   )
  3405                                                               asig.annotations['originalImpedance'] = originalImpedance
  3406                                                               if normalizeByImpedance and (aSigProxy not in ainpList):
  3407                                                                   '''
  3408                                                                   asig.magnitude[:] = (
  3409                                                                       (asig.magnitude - np.median(asig.magnitude)) /
  3410                                                                       np.min(
  3411                                                                           impedances.loc[elNmMatchMsk, 'impedance']
  3412                                                                           )
  3413                                                                       )
  3414                                                                   '''
  3415                                                                   print('Normalizing {} by {} kOhms'.format(asig.name, originalImpedance))
  3416                                                                   asig.magnitude[:] = (
  3417                                                                       (asig.magnitude * averageImpedance) / originalImpedance
  3418                                                                       )
  3419                                                       if fillOverflow:
  3420                                                           # fill in overflow:
  3421                                                           asig.magnitude[:], _ = hf.fillInOverflow2(
  3422                                                               asig.magnitude[:],
  3423                                                               overFlowFillType='average',
  3424                                                               overFlowThreshold=8000,
  3425                                                               debuggingPlots=False
  3426                                                               )
  3427                                                       if removeJumps:
  3428                                                           # find unusual jumps in derivative or amplitude
  3429                                                           '''
  3430                                                           timeSection['data'], newBadData = hf.fillInJumps(timeSection['data'],
  3431                                                           timeSection['samp_per_s'], smoothing_ms = 0.5, nStdDiff = 50,
  3432                                                           nStdAmp = 100)
  3433                                                           badData.update(newBadData)
  3434                                                           '''
  3435                                                           pass
  3436                                                       if calcAverageLFP and (aSigProxy not in ainpList):
  3437                                                           for k, cols in meanGroups.items():
  3438                                                               if asig.name in cols:
  3439                                                                   whichColumnToSubtract = k
  3440                                                           noiseModel = np.polyfit(
  3441                                                               centerLFP[:, whichColumnToSubtract],
  3442                                                               asig.magnitude.flatten(), 1, full=True)
  3443                                                           rSq = 1 - noiseModel[1][0] / np.sum(asig.magnitude.flatten() ** 2)
  3444                                                           asig.annotations['mean_removal_r2'] = rSq
  3445                                                           asig.annotations['mean_removal_group'] = whichColumnToSubtract
  3446                                                           if linearDetrend:
  3447                                                               noiseTerm = np.polyval(
  3448                                                                   noiseModel[0],
  3449                                                                   centerLFP[:, whichColumnToSubtract])
  3450                                                           else:
  3451                                                               noiseTerm = centerLFP[:, whichColumnToSubtract]
  3452                                                           ###
  3453                                                           plotMeanSubtraction = False
  3454                                                           if plotMeanSubtraction:
  3455                                                               i1 = 300000; i2 = 330000
  3456                                                               fig, ax = plt.subplots(1, 1)
  3457                                                               ax.plot(asig.times[devFiltDebugMask], asig.magnitude[devFiltDebugMask, :], label='channel')
  3458                                                               ax.plot(asig.times[devFiltDebugMask], centerLFP[devFiltDebugMask, whichColumnToSubtract], label='mean')
  3459                                                               ax.plot(asig.times[devFiltDebugMask], noiseTerm[devFiltDebugMask], label='adjusted mean')
  3460                                                               ax.legend()
  3461                                                               plt.show()
  3462                                                           ###
  3463                                                           if removeMeanAcross:
  3464                                                               asig.magnitude[:] = np.atleast_2d(
  3465                                                                   asig.magnitude.flatten() - noiseTerm).transpose()
  3466                                                               # asig.magnitude[:] = (
  3467                                                               #     asig.magnitude - np.median(asig.magnitude))
  3468                                                       if (LFPFilterOpts is not None) and (aSigProxy not in ainpList):
  3469                                                           asig.magnitude[:] = filterFun(asig, filterCoeffs=filterCoeffs)
  3470                                                       if (interpolateOutliers) and (aSigProxy not in ainpList) and (not outlierRemovalDebugFlag):
  3471                                                           for k, cols in meanGroups.items():
  3472                                                               if asig.name in cols:
  3473                                                                   whichColumnToSubtract = k
  3474                                                           tempSer = pd.Series(asig.magnitude.flatten())
  3475                                                           tempSer.loc[outlierMask[:, whichColumnToSubtract]] = np.nan
  3476                                                           tempSer = (
  3477                                                               tempSer
  3478                                                               .interpolate(method='linear', limit_area='inside')
  3479                                                               .fillna(method='ffill')
  3480                                                               .fillna(method='bfill')
  3481                                                               )
  3482                                                           asig.magnitude[:, 0] = tempSer.to_numpy()
  3483                                                       # pdb.set_trace()
  3484                                                       if (aSigProxy in aSigList) or (aSigProxy in ainpList):
  3485                                                           # assign ownership to containers
  3486                                                           chanIdx.analogsignals.append(asig)
  3487                                                           newSeg.analogsignals.append(asig)
  3488                                                           # assign parent to children
  3489                                                           chanIdx.create_relationship()
  3490                                                           newSeg.create_relationship()
  3491                                                           # write out to file
  3492                                                           asig = writer._write_analogsignal(
  3493                                                               asig, nixblock, nixgroup)
  3494                                                       del asig
  3495                                                       gc.collect()
  3496                                               for irSigIdx, irSigProxy in enumerate(
  3497                                                       seg.irregularlysampledsignals):
  3498                                                   chanIdx = irSigProxy.channel_index
  3499                                                   #
  3500                                                   isig = irSigProxy.load(
  3501                                                       time_slice=(tStart, tStop),
  3502                                                       magnitude_mode='rescaled')
  3503                                                   #  link irregularlysampledSignal
  3504                                                   #  and ID providing channel_index
  3505                                                   isig.channel_index = chanIdx
  3506                                                   # assign ownership to containers
  3507                                                   chanIdx.irregularlysampledsignals.append(isig)
  3508                                                   newSeg.irregularlysampledsignals.append(isig)
  3509                                                   # assign parent to children
  3510                                                   chanIdx.create_relationship()
  3511                                                   newSeg.create_relationship()
  3512                                                   # write out to file
  3513                                                   isig = writer._write_irregularlysampledsignal(
  3514                                                       isig, nixblock, nixgroup)
  3515                                                   del isig
  3516                                                   gc.collect()
  3517                                               #
  3518                                               if len(spikeSourceType):
  3519                                                   for stIdx, stProxy in enumerate(spikeSeg.spiketrains):
  3520                                                       if trackMemory:
  3521                                                           print('writing spiketrains mem usage: {}'.format(
  3522                                                               prf.memory_usage_psutil()))
  3523                                                       unit = stProxy.unit
  3524                                                       st = loadStProxy(stProxy)
  3525                                                       #  have to manually slice tStop and tStart because
  3526                                                       #  array annotations are not saved natively in the nix file
  3527                                                       #  (we're getting them as plain annotations)
  3528                                                       timeMask = np.asarray(
  3529                                                           (st.times >= tStart) & (st.times < tStop),
  3530                                                           dtype=np.bool)
  3531                                                       try:
  3532                                                           if 'arrayAnnNames' in st.annotations:
  3533                                                               for key in st.annotations['arrayAnnNames']:
  3534                                                                   st.annotations[key] = np.asarray(
  3535                                                                       st.annotations[key])[timeMask]
  3536                                                           st = st[timeMask]
  3537                                                           st.t_start = tStart
  3538                                                           st.t_stop = tStop
  3539                                                       except Exception:
  3540                                                           traceback.print_exc()
  3541                                                       #  tdc may or may not have the same channel ids, but
  3542                                                       #  it will have consistent channel names
  3543                                                       nameParser = re.search(
  3544                                                           r'([a-zA-Z0-9]*)#(\d*)', unit.name)
  3545                                                       chanLabel = nameParser.group(1)
  3546                                                       unitId = nameParser.group(2)
  3547                                                       #
  3548                                                       chIdxName = unit.name.replace('_stim', '').split('#')[0]
  3549                                                       chanIdx = block.filter(objects=ChannelIndex, name=chIdxName)[0]
  3550                                                       # [i.name for i in block.filter(objects=ChannelIndex)]
  3551                                                       # [i.name for i in spikeBlock.filter(objects=Unit)]
  3552                                                       #  print(unit.name)
  3553                                                       if not (unit in chanIdx.units):
  3554                                                           # first time at this unit, add to its chanIdx
  3555                                                           unit.channel_index = chanIdx
  3556                                                           chanIdx.units.append(unit)
  3557                                                       #  except Exception:
  3558                                                       #      traceback.print_exc()
  3559                                                       st.name = 'seg{}_{}'.format(0, unit.name)
  3560                                                       # st.name = 'seg{}_{}'.format(idx, unit.name)
  3561                                                       #  link SpikeTrain and ID providing unit
  3562                                                       if calcAverageLFP:
  3563                                                           if 'arrayAnnNames' in st.annotations:
  3564                                                               st.annotations['arrayAnnNames'] = list(st.annotations['arrayAnnNames'])
  3565                                                           else:
  3566                                                               st.annotations['arrayAnnNames'] = []
  3567                                                           st.annotations['arrayAnnNames'].append('phase60hz')
  3568                                                           phase60hz = hf.interpolateDF(
  3569                                                               lineNoisePhaseDF,
  3570                                                               newX=st.times, columns=['phase']).to_numpy().flatten()
  3571                                                           st.annotations.update({'phase60hz': phase60hz})
  3572                                                           plotPhaseDist = False
  3573                                                           if plotPhaseDist:
  3574                                                               sns.distplot(phase60hz)
  3575                                                               plt.show()
  3576                                                       st.unit = unit
  3577                                                       # assign ownership to containers
  3578                                                       unit.spiketrains.append(st)
  3579                                                       newSeg.spiketrains.append(st)
  3580                                                       # assign parent to children
  3581                                                       unit.create_relationship()
  3582                                                       newSeg.create_relationship()
  3583                                                       # write out to file
  3584                                                       st = writer._write_spiketrain(st, nixblock, nixgroup)
  3585                                                       del st
  3586                                               #  process proprio trial related events
  3587                                               if calcRigEvents:
  3588                                                   print('Processing rig events...')
  3589                                                   analogData = []
  3590                                                   for key, value in eventInfo['inputIDs'].items():
  3591                                                       searchName = 'seg{}_'.format(0) + value
  3592                                                       ainpAsig = seg.filter(
  3593                                                           objects=AnalogSignalProxy,
  3594                                                           name=searchName)[0]
  3595                                                       ainpData = ainpAsig.load(
  3596                                                           time_slice=(tStart, tStop),
  3597                                                           magnitude_mode='rescaled')
  3598                                                       analogData.append(
  3599                                                           pd.DataFrame(ainpData.magnitude, columns=[key]))
  3600                                                       del ainpData
  3601                                                       gc.collect()
  3602                                                   motorData = pd.concat(analogData, axis=1)
  3603                                                   del analogData
  3604                                                   gc.collect()
  3605                                                   if motorEncoderMask is not None:
  3606                                                       ainpData = ainpAsig.load(
  3607                                                           time_slice=(tStart, tStop),
  3608                                                           magnitude_mode='rescaled')
  3609                                                       ainpTime = ainpData.times.magnitude
  3610                                                       meTimeMask = np.zeros_like(ainpTime, dtype=np.bool)
  3611                                                       for meTimeBounds in motorEncoderMask:
  3612                                                           meTimeMask = (
  3613                                                               meTimeMask |
  3614                                                               (
  3615                                                                   (ainpTime > meTimeBounds[0]) &
  3616                                                                   (ainpTime < meTimeBounds[1])
  3617                                                                   )
  3618                                                               )
  3619                                                       columnsToOverride = ['A-', 'A+', 'B-', 'B+', 'Z-', 'Z+']
  3620                                                       for colName in columnsToOverride:
  3621                                                           motorData.loc[~meTimeMask, colName] = motorData.loc[:, colName].quantile(q=0.05)
  3622                                                       del ainpData, ainpTime
  3623                                                       gc.collect()
  3624                                                   motorData = mea.processMotorData(
  3625                                                       motorData, ainpAsig.sampling_rate.magnitude,
  3626                                                       encoderCountPerDegree=encoderCountPerDegree
  3627                                                       )
  3628                                                   keepCols = [
  3629                                                       'position', 'velocity', 'velocityCat',
  3630                                                       'rightBut_int', 'leftBut_int',
  3631                                                       'rightLED_int', 'leftLED_int', 'simiTrigs_int']
  3632                                                   for colName in keepCols:
  3633                                                       if trackMemory:
  3634                                                           print('writing motorData memory usage: {:.1f} MB'.format(
  3635                                                               prf.memory_usage_psutil()))
  3636                                                       chanIdx = ChannelIndex(
  3637                                                           name=colName,
  3638                                                           index=np.asarray([0]),
  3639                                                           channel_names=np.asarray([0]))
  3640                                                       block.channel_indexes.append(chanIdx)
  3641                                                       motorAsig = AnalogSignal(
  3642                                                           motorData[colName].to_numpy() * pq.mV,
  3643                                                           name=colName,
  3644                                                           sampling_rate=ainpAsig.sampling_rate,
  3645                                                           dtype=np.float32)
  3646                                                       motorAsig.t_start = ainpAsig.t_start
  3647                                                       motorAsig.channel_index = chanIdx
  3648                                                       # assign ownership to containers
  3649                                                       chanIdx.analogsignals.append(motorAsig)
  3650                                                       newSeg.analogsignals.append(motorAsig)
  3651                                                       chanIdx.create_relationship()
  3652                                                       newSeg.create_relationship()
  3653                                                       # write out to file
  3654                                                       motorAsig = writer._write_analogsignal(
  3655                                                           motorAsig, nixblock, nixgroup)
  3656                                                       del motorAsig
  3657                                                       gc.collect()
  3658                                                   _, trialEvents = mea.getTrials(
  3659                                                       motorData, ainpAsig.sampling_rate.magnitude,
  3660                                                       float(tStart.magnitude), trialType=None)
  3661                                                   trialEvents.fillna(0)
  3662                                                   trialEvents.rename(
  3663                                                       columns={
  3664                                                           'Label': 'rig_property',
  3665                                                           'Details': 'rig_value'},
  3666                                                       inplace=True)
  3667                                                   del motorData
  3668                                                   gc.collect()
  3669                                                   eventList = eventDataFrameToEvents(
  3670                                                       trialEvents,
  3671                                                       idxT='Time',
  3672                                                       annCol=['rig_property', 'rig_value'])
  3673                                                   for event in eventList:
  3674                                                       if trackMemory:
  3675                                                           print(
  3676                                                               'writing motor events memory usage: {:.1f} MB'
  3677                                                               .format(prf.memory_usage_psutil()))
  3678                                                       event.segment = newSeg
  3679                                                       newSeg.events.append(event)
  3680                                                       newSeg.create_relationship()
  3681                                                       # write out to file
  3682                                                       event = writer._write_event(event, nixblock, nixgroup)
  3683                                                       del event
  3684                                                       gc.collect()
  3685                                                   del trialEvents, eventList
  3686                                               #
  3687                                               for eventProxy in seg.events:
  3688                                                   event = eventProxy.load(
  3689                                                       time_slice=(tStart, tStop))
  3690                                                   event.t_start = tStart
  3691                                                   event.t_stop = tStop
  3692                                                   event.segment = newSeg
  3693                                                   newSeg.events.append(event)
  3694                                                   newSeg.create_relationship()
  3695                                                   # write out to file
  3696                                                   event = writer._write_event(event, nixblock, nixgroup)
  3697                                                   del event
  3698                                                   gc.collect()
  3699                                               #
  3700                                               for epochProxy in seg.epochs:
  3701                                                   epoch = epochProxy.load(
  3702                                                       time_slice=(tStart, tStop))
  3703                                                   epoch.t_start = tStart
  3704                                                   epoch.t_stop = tStop
  3705                                                   epoch.segment = newSeg
  3706                                                   newSeg.events.append(epoch)
  3707                                                   newSeg.create_relationship()
  3708                                                   # write out to file
  3709                                                   epoch = writer._write_epoch(epoch, nixblock, nixgroup)
  3710                                                   del epoch
  3711                                                   gc.collect()
  3712                                               #
  3713                                               chanIdxDiscardNames = []
  3714                                               # descend into ChannelIndexes
  3715                                               for chanIdx in block.channel_indexes:
  3716                                                   if chanIdx.analogsignals or chanIdx.units:
  3717                                                       chanIdx = writer._write_channelindex(chanIdx, nixblock)
  3718                                                   else:
  3719                                                       chanIdxDiscardNames.append(chanIdx.name)
  3720                                               block.channel_indexes = [
  3721                                                   i
  3722                                                   for i in block.channel_indexes
  3723                                                   if i.name not in chanIdxDiscardNames
  3724                                                   ]
  3725                                               writer._create_source_links(block, nixblock)
  3726                                               return

Total time: 0.0723491 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: purgeNixAnn at line 3728

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3728                                           @profile
  3729                                           def purgeNixAnn(
  3730                                                   block, annNames=['nix_name', 'neo_name']):
  3731        39        248.0      6.4      0.0      for annName in annNames:
  3732        26        293.0     11.3      0.0          block.annotations.pop(annName, None)
  3733      2496     414364.0    166.0     57.3      for child in block.children_recur:
  3734      2483      16521.0      6.7      2.3          if child.annotations:
  3735       615       3666.0      6.0      0.5              child.annotations = {
  3736                                                           k: v
  3737       615      19136.0     31.1      2.6                  for k, v in child.annotations.items()
  3738                                                           if k not in annNames}
  3739      1235     248530.0    201.2     34.4      for child in block.data_children_recur:
  3740      1222       9353.0      7.7      1.3          if child.annotations:
  3741       195       1448.0      7.4      0.2              child.annotations = {
  3742                                                           k: v
  3743       195       9852.0     50.5      1.4                  for k, v in child.annotations.items()
  3744                                                           if k not in annNames}
  3745        13         80.0      6.2      0.0      return block

Total time: 0.624032 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadContainerArrayAnn at line 3747

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3747                                           @profile
  3748                                           def loadContainerArrayAnn(
  3749                                                   container=None, trainList=None
  3750                                                   ):
  3751         3         18.0      6.0      0.0      assert (container is not None) or (trainList is not None)
  3752                                               #
  3753         3         15.0      5.0      0.0      spikesAndEvents = []
  3754         3         11.0      3.7      0.0      returnObj = []
  3755         3         13.0      4.3      0.0      if container is not None:
  3756                                                   #  need the line below! (RD: don't remember why, consider removing)
  3757         3      60761.0  20253.7      1.0          container.create_relationship()
  3758                                                   #
  3759         3         17.0      5.7      0.0          spikesAndEvents += (
  3760         3      69530.0  23176.7      1.1              container.filter(objects=SpikeTrain) +
  3761         3      67985.0  22661.7      1.1              container.filter(objects=Event)
  3762                                                       )
  3763         3         22.0      7.3      0.0          returnObj.append(container)
  3764         3         15.0      5.0      0.0      if trainList is not None:
  3765                                                   spikesAndEvents += trainList
  3766                                                   returnObj.append(trainList)
  3767                                               #
  3768         3         28.0      9.3      0.0      if len(returnObj) == 1:
  3769         3         18.0      6.0      0.0          returnObj = returnObj[0]
  3770                                               else:
  3771                                                   returnObj = tuple(returnObj)
  3772                                               #
  3773        49        342.0      7.0      0.0      for st in spikesAndEvents:
  3774        46    6041529.0 131337.6     96.8          st = loadObjArrayAnn(st)
  3775         3         12.0      4.0      0.0      return returnObj

Total time: 0.60265 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadObjArrayAnn at line 3777

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3777                                           @profile
  3778                                           def loadObjArrayAnn(st):
  3779        46        873.0     19.0      0.0      if 'arrayAnnNames' in st.annotations.keys():
  3780        19        333.0     17.5      0.0          if isinstance(st.annotations['arrayAnnNames'], str):
  3781                                                       st.annotations['arrayAnnNames'] = [st.annotations['arrayAnnNames']]
  3782        19        195.0     10.3      0.0          elif isinstance(st.annotations['arrayAnnNames'], tuple):
  3783                                                       st.annotations['arrayAnnNames'] = [i for i in st.annotations['arrayAnnNames']]
  3784                                                   #
  3785       199       4499.0     22.6      0.1          for key in st.annotations['arrayAnnNames']:
  3786                                                       #  fromRaw, the ann come back as tuple, need to recast
  3787       180       1127.0      6.3      0.0              try:
  3788       180     127602.0    708.9      2.1                  if len(st.times) == 1:
  3789                                                               st.annotations[key] = np.atleast_1d(st.annotations[key]).flatten()
  3790       180       1797.0     10.0      0.0                  st.array_annotations.update(
  3791       180     941557.0   5230.9     15.6                      {key: np.asarray(st.annotations[key])})
  3792       180    4947392.0  27485.5     82.1                  st.annotations[key] = np.asarray(st.annotations[key])
  3793                                                       except Exception:
  3794                                                           print('Error with {}'.format(st.name))
  3795                                                           traceback.print_exc()
  3796                                                           pdb.set_trace()
  3797        46        500.0     10.9      0.0      if hasattr(st, 'waveforms'):
  3798        13        136.0     10.5      0.0          if st.waveforms is None:
  3799                                                       st.waveforms = np.asarray([]).reshape((0, 0, 0)) * pq.mV
  3800        13        213.0     16.4      0.0          elif not len(st.waveforms):
  3801                                                       st.waveforms = np.asarray([]).reshape((0, 0, 0)) * pq.mV
  3802        46        273.0      5.9      0.0      return st

Total time: 21.122 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadWithArrayAnn at line 3804

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3804                                           @profile
  3805                                           def loadWithArrayAnn(
  3806                                                   dataPath, fromRaw=False,
  3807                                                   mapDF=None, reduceChannelIndexes=False):
  3808         3         23.0      7.7      0.0      if fromRaw:
  3809                                                   reader = nixio_fr.NixIO(filename=dataPath)
  3810                                                   block = readBlockFixNames(
  3811                                                       reader, lazy=False,
  3812                                                       mapDF=mapDF,
  3813                                                       reduceChannelIndexes=reduceChannelIndexes)
  3814                                               else:
  3815         3     384054.0 128018.0      0.2          reader = NixIO(filename=dataPath)
  3816         3  202670856.0 67556952.0     96.0          block = reader.read_block()
  3817                                                   # [un.name for un in block.filter(objects=Unit)]
  3818                                                   # [len(un.spiketrains) for un in block.filter(objects=Unit)]
  3819                                               
  3820         3    6241607.0 2080535.7      3.0      block = loadContainerArrayAnn(container=block)
  3821                                               
  3822         3         16.0      5.3      0.0      if fromRaw:
  3823                                                   reader.file.close()
  3824                                               else:
  3825         3    1923471.0 641157.0      0.9          reader.close()
  3826         3         64.0     21.3      0.0      return block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: blockFromPath at line 3828

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3828                                           @profile
  3829                                           def blockFromPath(
  3830                                                   dataPath, lazy=False, mapDF=None,
  3831                                                   reduceChannelIndexes=False, loadList=None,
  3832                                                   purgeNixNames=False, chunkingInfoPath=None):
  3833                                               chunkingMetadata = None
  3834                                               if chunkingInfoPath is not None:
  3835                                                   if os.path.exists(chunkingInfoPath):
  3836                                                       with open(chunkingInfoPath, 'r') as f:
  3837                                                           chunkingMetadata = json.load(f)
  3838                                               if chunkingMetadata is None:
  3839                                                   chunkingMetadata = {
  3840                                                       '0': {
  3841                                                           'filename': dataPath,
  3842                                                           'partNameSuffix': '',
  3843                                                           'chunkTStart': 0,
  3844                                                           'chunkTStop': 'NaN'
  3845                                                       }}
  3846                                               for idx, (chunkIdxStr, chunkMeta) in enumerate(chunkingMetadata.items()):   
  3847                                                   thisDataPath = chunkMeta['filename']
  3848                                                   assert os.path.exists(thisDataPath)
  3849                                                   if idx == 0:
  3850                                                       if lazy:
  3851                                                           dataReader = nixio_fr.NixIO(
  3852                                                               filename=thisDataPath)
  3853                                                           dataBlock = readBlockFixNames(
  3854                                                               dataReader, lazy=lazy, mapDF=mapDF,
  3855                                                               reduceChannelIndexes=reduceChannelIndexes,
  3856                                                               purgeNixNames=purgeNixNames, loadList=loadList)
  3857                                                       else:
  3858                                                           dataReader = None
  3859                                                           dataBlock = loadWithArrayAnn(thisDataPath)
  3860                                                   else:
  3861                                                       if lazy:
  3862                                                           dataReader2 = nixio_fr.NixIO(
  3863                                                               filename=thisDataPath)
  3864                                                           dataBlock2 = readBlockFixNames(
  3865                                                               dataReader2, lazy=lazy, mapDF=mapDF,
  3866                                                               reduceChannelIndexes=reduceChannelIndexes, loadList=loadList)
  3867                                                       else:
  3868                                                           dataReader2 = None
  3869                                                           dataBlock2 = loadWithArrayAnn(thisDataPath)
  3870                                                       maxSegIdx = len(dataBlock.segments)
  3871                                                       typesNeedRenaming = [
  3872                                                           SpikeTrainProxy, AnalogSignalProxy, EventProxy,
  3873                                                           SpikeTrain, AnalogSignal, Event]
  3874                                                       for segIdx, seg in enumerate(dataBlock2.segments):
  3875                                                           if seg.name is None:
  3876                                                               seg.name = 'seg{}_'.format(maxSegIdx + segIdx)
  3877                                                           else:
  3878                                                               if 'seg{}_'.format(maxSegIdx + segIdx) not in seg.name:
  3879                                                                   seg.name = (
  3880                                                                       'seg{}_{}'
  3881                                                                       .format(
  3882                                                                           maxSegIdx + segIdx,
  3883                                                                           childBaseName(seg.name, 'seg')))
  3884                                                           for objType in typesNeedRenaming:
  3885                                                               for child in seg.filter(objects=objType):
  3886                                                                   if 'seg{}_'.format(maxSegIdx + segIdx) not in child.name:
  3887                                                                       child.name = (
  3888                                                                           'seg{}_{}'
  3889                                                                           .format(
  3890                                                                               maxSegIdx + segIdx, childBaseName(child.name, 'seg')))
  3891                                                       dataBlock.merge(dataBlock2)
  3892                                               return dataReader, dataBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: calcBinarizedArray at line 3894

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3894                                           @profile
  3895                                           def calcBinarizedArray(
  3896                                                   dataBlock, samplingRate,
  3897                                                   binnedSpikePath=None,
  3898                                                   saveToFile=True, matchT=None):
  3899                                               #
  3900                                               spikeMatBlock = Block(name=dataBlock.name + '_binarized')
  3901                                               spikeMatBlock.merge_annotations(dataBlock)
  3902                                               #
  3903                                               allSpikeTrains = [
  3904                                                   i for i in dataBlock.filter(objects=SpikeTrain)]
  3905                                               #
  3906                                               for st in allSpikeTrains:
  3907                                                   chanList = spikeMatBlock.filter(
  3908                                                       objects=ChannelIndex, name=st.unit.name)
  3909                                                   if not len(chanList):
  3910                                                       chanIdx = ChannelIndex(name=st.unit.name, index=np.asarray([0]))
  3911                                                       #  print(chanIdx.name)
  3912                                                       spikeMatBlock.channel_indexes.append(chanIdx)
  3913                                                       thisUnit = Unit(name=st.unit.name)
  3914                                                       chanIdx.units.append(thisUnit)
  3915                                                       thisUnit.channel_index = chanIdx
  3916                                               #
  3917                                               for segIdx, seg in enumerate(dataBlock.segments):
  3918                                                   newSeg = Segment(name='seg{}_{}'.format(segIdx, spikeMatBlock.name))
  3919                                                   newSeg.merge_annotations(seg)
  3920                                                   spikeMatBlock.segments.append(newSeg)
  3921                                                   #  tStart = dataBlock.segments[0].t_start
  3922                                                   #  tStop = dataBlock.segments[0].t_stop
  3923                                                   tStart = seg.t_start
  3924                                                   tStop = seg.t_stop
  3925                                                   # make dummy binary spike train, in case ths chan didn't fire
  3926                                                   segSpikeTrains = [
  3927                                                       i for i in seg.filter(objects=SpikeTrain) if '#' in i.name]
  3928                                                   dummyBin = binarize(
  3929                                                       segSpikeTrains[0],
  3930                                                       sampling_rate=samplingRate,
  3931                                                       t_start=tStart,
  3932                                                       t_stop=tStop + samplingRate ** -1) * 0
  3933                                                   for chanIdx in spikeMatBlock.channel_indexes:
  3934                                                       #  print(chanIdx.name)
  3935                                                       stList = seg.filter(
  3936                                                           objects=SpikeTrain,
  3937                                                           name='seg{}_{}'.format(segIdx, chanIdx.name)
  3938                                                           )
  3939                                                       if len(stList):
  3940                                                           st = stList[0]
  3941                                                           print('binarizing {}'.format(st.name))
  3942                                                           stBin = binarize(
  3943                                                               st,
  3944                                                               sampling_rate=samplingRate,
  3945                                                               t_start=tStart,
  3946                                                               t_stop=tStop + samplingRate ** -1)
  3947                                                           spikeMatBlock.segments[segIdx].spiketrains.append(st)
  3948                                                           #  to do: link st to spikematblock's chidx and units
  3949                                                           assert len(chanIdx.filter(objects=Unit)) == 1
  3950                                                           thisUnit = chanIdx.filter(objects=Unit)[0]
  3951                                                           thisUnit.spiketrains.append(st)
  3952                                                           st.unit = thisUnit
  3953                                                           st.segment = spikeMatBlock.segments[segIdx]
  3954                                                       else:
  3955                                                           print('{} has no spikes'.format(st.name))
  3956                                                           stBin = dummyBin
  3957                                                       skipStAnnNames = [
  3958                                                           'nix_name', 'neo_name', 'arrayAnnNames']
  3959                                                       if 'arrayAnnNames' in st.annotations:
  3960                                                           skipStAnnNames += list(st.annotations['arrayAnnNames'])
  3961                                                       asigAnn = {
  3962                                                           k: v
  3963                                                           for k, v in st.annotations.items()
  3964                                                           if k not in skipStAnnNames
  3965                                                           }
  3966                                                       asig = AnalogSignal(
  3967                                                           stBin * samplingRate,
  3968                                                           name='seg{}_{}_raster'.format(segIdx, st.unit.name),
  3969                                                           sampling_rate=samplingRate,
  3970                                                           dtype=np.int,
  3971                                                           **asigAnn)
  3972                                                       if matchT is not None:
  3973                                                           asig = asig[:matchT.shape[0], :]
  3974                                                       asig.t_start = tStart
  3975                                                       asig.annotate(binWidth=1 / samplingRate.magnitude)
  3976                                                       chanIdx.analogsignals.append(asig)
  3977                                                       asig.channel_index = chanIdx
  3978                                                       spikeMatBlock.segments[segIdx].analogsignals.append(asig)
  3979                                               #
  3980                                               for chanIdx in spikeMatBlock.channel_indexes:
  3981                                                   chanIdx.name = chanIdx.name + '_raster'
  3982                                               #
  3983                                               spikeMatBlock.create_relationship()
  3984                                               spikeMatBlock = purgeNixAnn(spikeMatBlock)
  3985                                               if saveToFile:
  3986                                                   if os.path.exists(binnedSpikePath):
  3987                                                       os.remove(binnedSpikePath)
  3988                                                   writer = NixIO(filename=binnedSpikePath)
  3989                                                   writer.write_block(spikeMatBlock, use_obj_names=True)
  3990                                                   writer.close()
  3991                                               return spikeMatBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: calcFR at line 3993

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3993                                           @profile
  3994                                           def calcFR(
  3995                                                   binnedPath, dataPath,
  3996                                                   suffix='fr', aggregateFun=None,
  3997                                                   chanNames=None, rasterOpts=None, verbose=False
  3998                                                   ):
  3999                                               print('Loading rasters...')
  4000                                               masterSpikeMats, _ = loadSpikeMats(
  4001                                                   binnedPath, rasterOpts,
  4002                                                   aggregateFun=aggregateFun,
  4003                                                   chans=chanNames,
  4004                                                   loadAll=True, checkReferences=False)
  4005                                               print('Loading data file...')
  4006                                               dataReader = nixio_fr.NixIO(
  4007                                                   filename=dataPath)
  4008                                               dataBlock = dataReader.read_block(
  4009                                                   block_index=0, lazy=True,
  4010                                                   signal_group_mode='split-all')
  4011                                               masterBlock = Block()
  4012                                               masterBlock.name = dataBlock.annotations['neo_name']
  4013                                               #
  4014                                               for segIdx, segSpikeMat in masterSpikeMats.items():
  4015                                                   print('Calculating FR for segment {}'.format(segIdx))
  4016                                                   spikeMatDF = segSpikeMat.reset_index().rename(
  4017                                                       columns={'bin': 't'})
  4018                                           
  4019                                                   dataSeg = dataBlock.segments[segIdx]
  4020                                                   dummyAsig = dataSeg.filter(
  4021                                                       objects=AnalogSignalProxy)[0].load(channel_indexes=[0])
  4022                                                   samplingRate = dummyAsig.sampling_rate
  4023                                                   newT = dummyAsig.times.magnitude
  4024                                                   spikeMatDF['t'] = spikeMatDF['t'] + newT[0]
  4025                                           
  4026                                                   segSpikeMatInterp = hf.interpolateDF(
  4027                                                       spikeMatDF, pd.Series(newT),
  4028                                                       kind='linear', fill_value=(0, 0),
  4029                                                       x='t')
  4030                                                   spikeMatBlockInterp = dataFrameToAnalogSignals(
  4031                                                       segSpikeMatInterp,
  4032                                                       idxT='t', useColNames=True,
  4033                                                       dataCol=segSpikeMatInterp.drop(columns='t').columns,
  4034                                                       samplingRate=samplingRate)
  4035                                                   spikeMatBlockInterp.name = dataBlock.annotations['neo_name']
  4036                                                   spikeMatBlockInterp.annotate(
  4037                                                       nix_name=dataBlock.annotations['neo_name'])
  4038                                                   spikeMatBlockInterp.segments[0].name = dataSeg.annotations['neo_name']
  4039                                                   spikeMatBlockInterp.segments[0].annotate(
  4040                                                       nix_name=dataSeg.annotations['neo_name'])
  4041                                                   asigList = spikeMatBlockInterp.filter(objects=AnalogSignal)
  4042                                                   for asig in asigList:
  4043                                                       asig.annotate(binWidth=rasterOpts['binWidth'])
  4044                                                       if '_raster' in asig.name:
  4045                                                           asig.name = asig.name.replace('_raster', '_' + suffix)
  4046                                                       asig.name = 'seg{}_{}'.format(segIdx, childBaseName(asig.name, 'seg'))
  4047                                                       asig.annotate(nix_name=asig.name)
  4048                                                   chanIdxList = spikeMatBlockInterp.filter(objects=ChannelIndex)
  4049                                                   for chanIdx in chanIdxList:
  4050                                                       if '_raster' in chanIdx.name:
  4051                                                           chanIdx.name = chanIdx.name.replace('_raster', '_' + suffix)
  4052                                                       chanIdx.annotate(nix_name=chanIdx.name)
  4053                                           
  4054                                                   # masterBlock.merge(spikeMatBlockInterp)
  4055                                                   frBlockPath = dataPath.replace('_analyze.nix', '_fr.nix')
  4056                                                   writer = NixIO(filename=frBlockPath)
  4057                                                   writer.write_block(spikeMatBlockInterp, use_obj_names=True)
  4058                                                   writer.close()
  4059                                               #
  4060                                               dataReader.file.close()
  4061                                               return masterBlock

Timer unit: 1e-07 s

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: analogSignalsToDataFrame at line 43

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    43                                           @profile
    44                                           def analogSignalsToDataFrame(
    45                                                   analogsignals, idxT='t', useChanNames=False):
    46                                               asigList = []
    47                                               for asig in analogsignals:
    48                                                   if asig.shape[1] == 1:
    49                                                       if useChanNames:
    50                                                           colNames = [str(asig.channel_index.name)]
    51                                                       else:
    52                                                           colNames = [str(asig.name)]
    53                                                   else:
    54                                                       colNames = [
    55                                                           asig.name +
    56                                                           '_{}'.format(i) for i in
    57                                                           asig.channel_index.channel_ids
    58                                                           ]
    59                                                   asigList.append(
    60                                                       pd.DataFrame(
    61                                                           asig.magnitude, columns=colNames,
    62                                                           index=range(asig.shape[0])))
    63                                               asigList.append(
    64                                                   pd.DataFrame(
    65                                                       asig.times.magnitude, columns=[idxT],
    66                                                       index=range(asig.shape[0])))
    67                                               return pd.concat(asigList, axis=1)

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: listChanNames at line 69

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    69                                           @profile
    70                                           def listChanNames(
    71                                                   dataBlock, chanQuery,
    72                                                   objType=AnalogSignalProxy, condition=None):
    73                                               allChanList = [
    74                                                   i.name
    75                                                   for i in dataBlock.filter(objects=objType)]
    76                                               if condition == 'hasAsigs':
    77                                                   allChanList = [
    78                                                       i
    79                                                       for i in allChanList
    80                                                       if len(dataBlock.filter(objects=objType, name=i)[0].analogsignals)
    81                                                   ]
    82                                               chansToTrigger = pd.DataFrame(
    83                                                   np.unique(allChanList),
    84                                                   columns=['chanName'])
    85                                               if chanQuery is not None:
    86                                                   chansToTrigger = chansToTrigger.query(
    87                                                       chanQuery, engine='python')['chanName'].to_list()
    88                                               else:
    89                                                   chansToTrigger = chansToTrigger['chanName'].to_list()
    90                                               return chansToTrigger

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: spikeDictToSpikeTrains at line 92

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    92                                           @profile
    93                                           def spikeDictToSpikeTrains(
    94                                                   spikes, block=None, seg=None,
    95                                                   probeName='insTD', t_stop=None,
    96                                                   waveformUnits=pq.uV,
    97                                                   sampling_rate=3e4 * pq.Hz):
    98                                           
    99                                               if block is None:
   100                                                   assert seg is None
   101                                                   block = Block()
   102                                                   seg = Segment(name=probeName + ' segment')
   103                                                   block.segments.append(seg)
   104                                           
   105                                               if t_stop is None:
   106                                                   t_stop = hf.getLastSpikeTime(spikes) + 1
   107                                           
   108                                               for idx, chanName in enumerate(spikes['ChannelID']):
   109                                                   #  unique units on this channel
   110                                                   unitsOnThisChan = pd.unique(spikes['Classification'][idx])
   111                                                   nixChanName = probeName + '{}'.format(chanName)
   112                                                   chanIdx = ChannelIndex(
   113                                                       name=nixChanName,
   114                                                       index=np.asarray([idx]),
   115                                                       channel_names=np.asarray([nixChanName]))
   116                                                   block.channel_indexes.append(chanIdx)
   117                                                   
   118                                                   for unitIdx, unitName in enumerate(unitsOnThisChan):
   119                                                       unitMask = spikes['Classification'][idx] == unitName
   120                                                       # this unit's spike timestamps
   121                                                       theseTimes = spikes['TimeStamps'][idx][unitMask]
   122                                                       # this unit's waveforms
   123                                                       if len(spikes['Waveforms'][idx].shape) == 3:
   124                                                           theseWaveforms = spikes['Waveforms'][idx][unitMask, :, :]
   125                                                           theseWaveforms = np.swapaxes(theseWaveforms, 1, 2)
   126                                                       elif len(spikes['Waveforms'][idx].shape) == 2:
   127                                                           theseWaveforms = (
   128                                                               spikes['Waveforms'][idx][unitMask, np.newaxis, :])
   129                                                       else:
   130                                                           raise(Exception('spikes[Waveforms] has bad shape'))
   131                                           
   132                                                       unitName = '{}#{}'.format(nixChanName, unitIdx)
   133                                                       unit = Unit(name=unitName)
   134                                                       unit.channel_index = chanIdx
   135                                                       chanIdx.units.append(unit)
   136                                           
   137                                                       train = SpikeTrain(
   138                                                           times=theseTimes, t_stop=t_stop, units='sec',
   139                                                           name=unitName, sampling_rate=sampling_rate,
   140                                                           waveforms=theseWaveforms*waveformUnits,
   141                                                           left_sweep=0, dtype=np.float32)
   142                                                       unit.spiketrains.append(train)
   143                                                       seg.spiketrains.append(train)
   144                                           
   145                                                       unit.create_relationship()
   146                                                   chanIdx.create_relationship()
   147                                               seg.create_relationship()
   148                                               block.create_relationship()
   149                                               return block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: spikeTrainsToSpikeDict at line 151

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   151                                           @profile
   152                                           def spikeTrainsToSpikeDict(
   153                                                   spiketrains):
   154                                               nCh = len(spiketrains)
   155                                               spikes = {
   156                                                   'ChannelID': [i for i in range(nCh)],
   157                                                   'Classification': [np.asarray([]) for i in range(nCh)],
   158                                                   'NEUEVWAV_HeaderIndices': [None for i in range(nCh)],
   159                                                   'TimeStamps': [np.asarray([]) for i in range(nCh)],
   160                                                   'Units': 'uV',
   161                                                   'Waveforms': [np.asarray([]) for i in range(nCh)],
   162                                                   'basic_headers': {'TimeStampResolution': 3e4},
   163                                                   'extended_headers': []
   164                                                   }
   165                                               for idx, st in enumerate(spiketrains):
   166                                                   spikes['ChannelID'][idx] = st.name
   167                                                   if len(spikes['TimeStamps'][idx]):
   168                                                       spikes['TimeStamps'][idx] = np.stack((
   169                                                           spikes['TimeStamps'][idx],
   170                                                           st.times.magnitude), axis=-1)
   171                                                   else:
   172                                                       spikes['TimeStamps'][idx] = st.times.magnitude
   173                                                   
   174                                                   theseWaveforms = np.swapaxes(
   175                                                       st.waveforms, 1, 2)
   176                                                   theseWaveforms = np.atleast_2d(np.squeeze(
   177                                                       theseWaveforms))
   178                                                       
   179                                                   if len(spikes['Waveforms'][idx]):
   180                                                       spikes['Waveforms'][idx] = np.stack((
   181                                                           spikes['Waveforms'][idx],
   182                                                           theseWaveforms.magnitude), axis=-1)
   183                                                   else:
   184                                                       spikes['Waveforms'][idx] = theseWaveforms.magnitude
   185                                                   
   186                                                   classVals = st.times.magnitude ** 0 * idx
   187                                                   if len(spikes['Classification'][idx]):
   188                                                       spikes['Classification'][idx] = np.stack((
   189                                                           spikes['Classification'][idx],
   190                                                           classVals), axis=-1)
   191                                                   else:
   192                                                       spikes['Classification'][idx] = classVals
   193                                               return spikes

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: channelIndexesToSpikeDict at line 195

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   195                                           @profile
   196                                           def channelIndexesToSpikeDict(
   197                                                   channel_indexes):
   198                                               nCh = len(channel_indexes)
   199                                               spikes = {
   200                                                   'ChannelID': [i for i in range(nCh)],
   201                                                   'Classification': [np.asarray([]) for i in range(nCh)],
   202                                                   'NEUEVWAV_HeaderIndices': [None for i in range(nCh)],
   203                                                   'TimeStamps': [np.asarray([]) for i in range(nCh)],
   204                                                   'Units': 'uV',
   205                                                   'Waveforms': [np.asarray([]) for i in range(nCh)],
   206                                                   'basic_headers': {'TimeStampResolution': 3e4},
   207                                                   'extended_headers': []
   208                                                   }
   209                                               #  allocate fields for annotations
   210                                               for dummyCh in channel_indexes:
   211                                                   if len(dummyCh.units):
   212                                                       dummyUnit = dummyCh.units[0]
   213                                                       if len(dummyUnit.spiketrains):
   214                                                           if len(dummyUnit.spiketrains[0].times):
   215                                                               break
   216                                               dummySt = [
   217                                                   st
   218                                                   for st in dummyUnit.spiketrains
   219                                                   if len(st.times)][0]
   220                                               #  allocate fields for array annotations (per spike)
   221                                               if dummySt.array_annotations:
   222                                                   for key in dummySt.array_annotations.keys():
   223                                                       spikes.update({key: [np.asarray([]) for i in range(nCh)]})
   224                                                   
   225                                               maxUnitIdx = 0
   226                                               for idx, chIdx in enumerate(channel_indexes):
   227                                                   spikes['ChannelID'][idx] = chIdx.name
   228                                                   for unitIdx, thisUnit in enumerate(chIdx.units):
   229                                                       for stIdx, st in enumerate(thisUnit.spiketrains):
   230                                                           if not len(st.times):
   231                                                               continue
   232                                                           #  print(
   233                                                           #      'unit {} has {} spiketrains'.format(
   234                                                           #          thisUnit.name,
   235                                                           #          len(thisUnit.spiketrains)))
   236                                                           if len(spikes['TimeStamps'][idx]):
   237                                                               spikes['TimeStamps'][idx] = np.concatenate((
   238                                                                   spikes['TimeStamps'][idx],
   239                                                                   st.times.magnitude), axis=0)
   240                                                           else:
   241                                                               spikes['TimeStamps'][idx] = st.times.magnitude
   242                                                           #  reshape waveforms to comply with BRM convention
   243                                                           theseWaveforms = np.swapaxes(
   244                                                               st.waveforms, 1, 2)
   245                                                           theseWaveforms = np.atleast_2d(np.squeeze(
   246                                                               theseWaveforms))
   247                                                           #  append waveforms
   248                                                           if len(spikes['Waveforms'][idx]):
   249                                                               try:
   250                                                                   spikes['Waveforms'][idx] = np.concatenate((
   251                                                                       spikes['Waveforms'][idx],
   252                                                                       theseWaveforms.magnitude), axis=0)
   253                                                               except Exception:
   254                                                                   traceback.print_exc()
   255                                                           else:
   256                                                               spikes['Waveforms'][idx] = theseWaveforms.magnitude
   257                                                           #  give each unit a global index
   258                                                           classVals = st.times.magnitude ** 0 * maxUnitIdx
   259                                                           st.array_annotations.update({'Classification': classVals})
   260                                                           #  expand array_annotations into spikes dict
   261                                                           for key, value in st.array_annotations.items():
   262                                                               if len(spikes[key][idx]):
   263                                                                   spikes[key][idx] = np.concatenate((
   264                                                                       spikes[key][idx],
   265                                                                       value), axis=0)
   266                                                               else:
   267                                                                   spikes[key][idx] = value
   268                                                           for key, value in st.annotations.items():
   269                                                               if key not in spikes['basic_headers']:
   270                                                                   spikes['basic_headers'].update({key: {}})
   271                                                               try:
   272                                                                   spikes['basic_headers'][key].update({maxUnitIdx: value})
   273                                                               except Exception:
   274                                                                   pass
   275                                                           maxUnitIdx += 1
   276                                               return spikes

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: unitSpikeTrainArrayAnnToDF at line 278

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   278                                           @profile
   279                                           def unitSpikeTrainArrayAnnToDF(
   280                                                   spikeTrainContainer):
   281                                               #  list contains different segments
   282                                               if isinstance(spikeTrainContainer, ChannelIndex):
   283                                                   assert len(spikeTrainContainer.units) == 0
   284                                                   spiketrains = spikeTrainContainer.units[0].spiketrains
   285                                               elif isinstance(spikeTrainContainer, Unit):
   286                                                   spiketrains = spikeTrainContainer.spiketrains
   287                                               elif isinstance(spikeTrainContainer, list):
   288                                                   spiketrains = spikeTrainContainer
   289                                               fullAnnotationsDict = {}
   290                                               for segIdx, st in enumerate(spiketrains):
   291                                                   theseAnnDF = pd.DataFrame(st.array_annotations)
   292                                                   theseAnnDF['t'] = st.times.magnitude
   293                                                   fullAnnotationsDict.update({segIdx: theseAnnDF})
   294                                               annotationsDF = pd.concat(
   295                                                   fullAnnotationsDict, names=['segment', 'index'], sort=True)
   296                                               return annotationsDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getSpikeDFMetadata at line 298

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   298                                           @profile
   299                                           def getSpikeDFMetadata(spikeDF, metaDataCols):
   300                                               spikeDF.reset_index(inplace=True)
   301                                               metaDataCols = np.atleast_1d(metaDataCols)
   302                                               spikeDF.index.name = 'metaDataIdx'
   303                                               metaDataDF = spikeDF.loc[:, metaDataCols].copy()
   304                                               newSpikeDF = spikeDF.drop(columns=metaDataCols).reset_index()
   305                                               return newSpikeDF, metaDataDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: transposeSpikeDF at line 307

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   307                                           @profile
   308                                           def transposeSpikeDF(
   309                                                   spikeDF, transposeToColumns,
   310                                                   fastTranspose=False):
   311                                               newColumnNames = np.atleast_1d(transposeToColumns).tolist()
   312                                               originalColumnNames = np.atleast_1d(spikeDF.columns.names)
   313                                               metaDataCols = np.setdiff1d(spikeDF.index.names, newColumnNames).tolist()
   314                                               if fastTranspose:
   315                                                   #  fast but memory inefficient
   316                                                   return spikeDF.stack().unstack(transposeToColumns)
   317                                               else:
   318                                                   raise(Warning('Caution! transposeSpikeDF might not be working, needs testing RD 06252019'))
   319                                                   #  stash annotations, transpose, recover annotations
   320                                                   newSpikeDF, metaDataDF = getSpikeDFMetadata(spikeDF, metaDataCols)
   321                                                   del spikeDF
   322                                                   gc.collect()
   323                                                   #
   324                                                   newSpikeDF = newSpikeDF.stack().unstack(newColumnNames)
   325                                                   newSpikeDF.reset_index(inplace=True)
   326                                                   #  set the index
   327                                                   newIdxLabels = np.concatenate(
   328                                                       [originalColumnNames, metaDataCols]).tolist()
   329                                                   newSpikeDF.loc[:, metaDataCols] = (
   330                                                       metaDataDF
   331                                                       .loc[newSpikeDF['metaDataIdx'].to_list(), metaDataCols]
   332                                                       .to_numpy())
   333                                                   newSpikeDF = (
   334                                                       newSpikeDF
   335                                                       .drop(columns=['metaDataIdx'])
   336                                                       .set_index(newIdxLabels))
   337                                                   return newSpikeDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: concatenateBlocks at line 339

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   339                                           @profile
   340                                           def concatenateBlocks(
   341                                                   asigBlocks, spikeBlocks, eventBlocks, chunkingMetadata,
   342                                                   samplingRate, chanQuery, lazy, trackMemory, verbose
   343                                                   ):
   344                                               # Scan ahead through all files and ensure that
   345                                               # spikeTrains and units are present across all assembled files
   346                                               channelIndexCache = {}
   347                                               unitCache = {}
   348                                               asigCache = []
   349                                               asigAnnCache = {}
   350                                               spiketrainCache = {}
   351                                               eventCache = {}
   352                                               # get list of channels and units
   353                                               for idx, (chunkIdxStr, chunkMeta) in enumerate(chunkingMetadata.items()):
   354                                                   gc.collect()
   355                                                   chunkIdx = int(chunkIdxStr)
   356                                                   asigBlock = asigBlocks[chunkIdx]
   357                                                   asigSeg = asigBlock.segments[0]
   358                                                   spikeBlock = spikeBlocks[chunkIdx]
   359                                                   eventBlock = eventBlocks[chunkIdx]
   360                                                   eventSeg = eventBlock.segments[0]
   361                                                   for chIdx in asigBlock.filter(objects=ChannelIndex):
   362                                                       chAlreadyThere = (chIdx.name in channelIndexCache.keys())
   363                                                       if not chAlreadyThere:
   364                                                           newChIdx = copy(chIdx)
   365                                                           newChIdx.analogsignals = []
   366                                                           newChIdx.units = []
   367                                                           channelIndexCache[chIdx.name] = newChIdx
   368                                                   for unit in (spikeBlock.filter(objects=Unit)):
   369                                                       if lazy:
   370                                                           theseSpiketrains = []
   371                                                           for stP in unit.spiketrains:
   372                                                               st = loadStProxy(stP)
   373                                                               if len(st.times) > 0:
   374                                                                   theseSpiketrains.append(st)
   375                                                       else:
   376                                                           theseSpiketrains = [
   377                                                               st
   378                                                               for st in unit.spiketrains
   379                                                               if len(st.times)
   380                                                               ]
   381                                                       for st in theseSpiketrains:
   382                                                           st = loadObjArrayAnn(st)
   383                                                           if len(st.times):
   384                                                               st.magnitude[:] = st.times.magnitude + spikeBlock.annotations['chunkTStart']
   385                                                               st.t_start = min(0 * pq.s, st.times[0] * 0.999)
   386                                                               st.t_stop = max(
   387                                                                   st.t_stop + spikeBlock.annotations['chunkTStart'] * pq.s,
   388                                                                   st.times[-1] * 1.001)
   389                                                           else:
   390                                                               st.t_start += spikeBlock.annotations['chunkTStart'] * pq.s
   391                                                               st.t_stop += spikeBlock.annotations['chunkTStart'] * pq.s
   392                                                       uAlreadyThere = (unit.name in unitCache.keys())
   393                                                       if not uAlreadyThere:
   394                                                           newUnit = copy(unit)
   395                                                           newUnit.spiketrains = []
   396                                                           newUnit.annotations['parentChanName'] = unit.channel_index.name
   397                                                           unitCache[unit.name] = newUnit
   398                                                           spiketrainCache[unit.name] = theseSpiketrains
   399                                                       else:
   400                                                           spiketrainCache[unit.name] = spiketrainCache[unit.name] + theseSpiketrains
   401                                                   #
   402                                                   if lazy:
   403                                                       evList = [
   404                                                           evP.load()
   405                                                           for evP in eventSeg.events]
   406                                                   else:
   407                                                       evList = eventSeg.events
   408                                                   for event in evList:
   409                                                       event.magnitude[:] = event.magnitude + eventBlock.annotations['chunkTStart']
   410                                                       if event.name in eventCache.keys():
   411                                                           eventCache[event.name].append(event)
   412                                                       else:
   413                                                           eventCache[event.name] = [event]
   414                                                   # take the requested analog signal channels
   415                                                   if lazy:
   416                                                       tdChanNames = listChanNames(
   417                                                           asigBlock, chanQuery, objType=AnalogSignalProxy)
   418                                                       #############
   419                                                       # tdChanNames = ['seg0_utah1', 'seg0_utah10']
   420                                                       ##############
   421                                                       asigList = []
   422                                                       for asigP in asigSeg.analogsignals:
   423                                                           if asigP.name in tdChanNames:
   424                                                               asig = asigP.load()
   425                                                               asig.channel_index = asigP.channel_index
   426                                                               asigList.append(asig)
   427                                                               if trackMemory:
   428                                                                   print('loading {} from proxy object. memory usage: {:.1f} MB'.format(
   429                                                                       asigP.name, prf.memory_usage_psutil()))
   430                                                   else:
   431                                                       tdChanNames = listChanNames(
   432                                                           asigBlock, chanQuery, objType=AnalogSignal)
   433                                                       asigList = [
   434                                                           asig
   435                                                           for asig in asigSeg.analogsignals
   436                                                           if asig.name in tdChanNames
   437                                                           ]
   438                                                   for asig in asigList:
   439                                                       if asig.size > 0:
   440                                                           dummyAsig = asig
   441                                                   if idx == 0:
   442                                                       outputBlock = Block(
   443                                                           name=asigBlock.name,
   444                                                           file_origin=asigBlock.file_origin,
   445                                                           file_datetime=asigBlock.file_datetime,
   446                                                           rec_datetime=asigBlock.rec_datetime,
   447                                                           **asigBlock.annotations
   448                                                       )
   449                                                       newSeg = Segment(
   450                                                           index=0, name=asigSeg.name,
   451                                                           description=asigSeg.description,
   452                                                           file_origin=asigSeg.file_origin,
   453                                                           file_datetime=asigSeg.file_datetime,
   454                                                           rec_datetime=asigSeg.rec_datetime,
   455                                                           **asigSeg.annotations
   456                                                       )
   457                                                       outputBlock.segments = [newSeg]
   458                                                       for asig in asigList:
   459                                                           asigAnnCache[asig.name] = asig.annotations
   460                                                           asigAnnCache[asig.name]['parentChanName'] = asig.channel_index.name
   461                                                       asigUnits = dummyAsig.units
   462                                                   tdDF = analogSignalsToDataFrame(asigList)
   463                                                   del asigList  # asigs saved to dataframe, no longer needed
   464                                                   tdDF.loc[:, 't'] += asigBlock.annotations['chunkTStart']
   465                                                   tdDF.set_index('t', inplace=True)
   466                                                   if samplingRate != dummyAsig.sampling_rate:
   467                                                       lowPassOpts = {
   468                                                           'low': {
   469                                                               'Wn': float(samplingRate / 2),
   470                                                               'N': 4,
   471                                                               'btype': 'low',
   472                                                               'ftype': 'bessel'
   473                                                           }
   474                                                       }
   475                                                       newT = pd.Series(
   476                                                           np.arange(
   477                                                               dummyAsig.t_start + asigBlock.annotations['chunkTStart'] * pq.s,
   478                                                               dummyAsig.t_stop + asigBlock.annotations['chunkTStart'] * pq.s,
   479                                                               1/samplingRate))
   480                                                       if samplingRate < dummyAsig.sampling_rate:
   481                                                           filterCoeffs = hf.makeFilterCoeffsSOS(
   482                                                               lowPassOpts, float(dummyAsig.sampling_rate))
   483                                                           if trackMemory:
   484                                                               print('Filtering analog data before downsampling. memory usage: {:.1f} MB'.format(
   485                                                                   prf.memory_usage_psutil()))
   486                                                           '''
   487                                                           ### check that axis=0 is the correct option
   488                                                           dummyDF = tdDF.iloc[:, :4].copy()
   489                                                           filteredAsigs0 = signal.sosfiltfilt( filterCoeffs, dummyDF.to_numpy(), axis=0)
   490                                                           filteredAsigs1 = signal.sosfiltfilt( filterCoeffs, dummyDF.to_numpy(), axis=1)
   491                                                           ###
   492                                                           '''
   493                                                           filteredAsigs = signal.sosfiltfilt(
   494                                                               filterCoeffs, tdDF.to_numpy(),
   495                                                               axis=0)
   496                                                           tdDF = pd.DataFrame(
   497                                                               filteredAsigs,
   498                                                               index=tdDF.index,
   499                                                               columns=tdDF.columns)
   500                                                           if trackMemory:
   501                                                               print('Just finished analog data filtering before downsampling. memory usage: {:.1f} MB'.format(
   502                                                                   prf.memory_usage_psutil()))
   503                                                       tdInterp = hf.interpolateDF(
   504                                                           tdDF, newT,
   505                                                           kind='linear', fill_value='extrapolate',
   506                                                           verbose=verbose)
   507                                                       # free up memory used by full resolution asigs
   508                                                       del tdDF
   509                                                   else:
   510                                                       tdInterp = tdDF
   511                                                   #
   512                                                   asigCache.append(tdInterp)
   513                                                   #
   514                                                   print('Finished chunk {}'.format(chunkIdxStr))
   515                                               allTdDF = pd.concat(asigCache)
   516                                               # TODO: check for nans, if, for example a signal is partially missing
   517                                               allTdDF.fillna(method='bfill', inplace=True)
   518                                               allTdDF.fillna(method='ffill', inplace=True)
   519                                               for asigName in allTdDF.columns:
   520                                                   newAsig = AnalogSignal(
   521                                                       allTdDF[asigName].to_numpy() * asigUnits,
   522                                                       name=asigName,
   523                                                       sampling_rate=samplingRate,
   524                                                       dtype=np.float32,
   525                                                       **asigAnnCache[asigName])
   526                                                   chIdxName = asigAnnCache[asigName]['parentChanName']
   527                                                   chIdx = channelIndexCache[chIdxName]
   528                                                   # cross-assign ownership to containers
   529                                                   chIdx.analogsignals.append(newAsig)
   530                                                   newSeg.analogsignals.append(newAsig)
   531                                                   newAsig.channel_index = chIdx
   532                                                   newAsig.segment = newSeg
   533                                               #
   534                                               for uName, unit in unitCache.items():
   535                                                   # concatenate spike times, waveforms, etc.
   536                                                   if len(spiketrainCache[unit.name]):
   537                                                       consolidatedTimes = np.concatenate([
   538                                                               st.times.magnitude
   539                                                               for st in spiketrainCache[unit.name]
   540                                                           ])
   541                                                       # TODO:   decide whether to include this step
   542                                                       #         which snaps the spike times to the nearest
   543                                                       #         *sampled* data point
   544                                                       #
   545                                                       # consolidatedTimes, timesIndex = hf.closestSeries(
   546                                                       #     takeFrom=pd.Series(consolidatedTimes),
   547                                                       #     compareTo=pd.Series(allTdDF.index))
   548                                                       #
   549                                                       # find an example spiketrain with array_annotations
   550                                                       for st in spiketrainCache[unit.name]:
   551                                                           if len(st.times):
   552                                                               dummySt = st
   553                                                               break
   554                                                       consolidatedAnn = {
   555                                                           key: np.array([])
   556                                                           for key, value in dummySt.array_annotations.items()
   557                                                           }
   558                                                       for key, value in consolidatedAnn.items():
   559                                                           consolidatedAnn[key] = np.concatenate([
   560                                                               st.annotations[key]
   561                                                               for st in spiketrainCache[unit.name]
   562                                                           ])
   563                                                       consolidatedWaveforms = np.concatenate([
   564                                                           st.waveforms
   565                                                           for st in spiketrainCache[unit.name]
   566                                                           ])
   567                                                       spikeTStop = max([
   568                                                           st.t_stop
   569                                                           for st in spiketrainCache[unit.name]
   570                                                           ])
   571                                                       spikeTStart = max([
   572                                                           st.t_start
   573                                                           for st in spiketrainCache[unit.name]
   574                                                           ])
   575                                                       spikeAnnotations = {
   576                                                           key: value
   577                                                           for key, value in dummySt.annotations.items()
   578                                                           if key not in dummySt.annotations['arrayAnnNames']
   579                                                       }
   580                                                       newSt = SpikeTrain(
   581                                                           name=dummySt.name,
   582                                                           times=consolidatedTimes, units='sec', t_stop=spikeTStop,
   583                                                           waveforms=consolidatedWaveforms * dummySt.waveforms.units,
   584                                                           left_sweep=dummySt.left_sweep,
   585                                                           sampling_rate=dummySt.sampling_rate,
   586                                                           t_start=spikeTStart, **spikeAnnotations,
   587                                                           array_annotations=consolidatedAnn)
   588                                                       # cross-assign ownership to containers
   589                                                       unit.spiketrains.append(newSt)
   590                                                       newSt.unit = unit
   591                                                       newSeg.spiketrains.append(newSt)
   592                                                       newSt.segment = newSeg
   593                                                       # link chIdxes and Units
   594                                                       if unit.annotations['parentChanName'] in channelIndexCache:
   595                                                           chIdx = channelIndexCache[unit.annotations['parentChanName']]
   596                                                           if unit not in chIdx.units:
   597                                                               chIdx.units.append(unit)
   598                                                               unit.channel_index = chIdx
   599                                                       else:
   600                                                           newChIdx = ChannelIndex(
   601                                                               name=unit.annotations['parentChanName'], index=0)
   602                                                           channelIndexCache[unit.annotations['parentChanName']] = newChIdx
   603                                                           if unit not in newChIdx.units:
   604                                                               newChIdx.units.append(unit)
   605                                                               unit.channel_index = newChIdx
   606                                               #
   607                                               for evName, eventList in eventCache.items():
   608                                                   consolidatedTimes = np.concatenate([
   609                                                       ev.times.magnitude
   610                                                       for ev in eventList
   611                                                       ])
   612                                                   consolidatedLabels = np.concatenate([
   613                                                       ev.labels
   614                                                       for ev in eventList
   615                                                       ])
   616                                                   newEvent = Event(
   617                                                       name=evName,
   618                                                       times=consolidatedTimes * pq.s,
   619                                                       labels=consolidatedLabels
   620                                                       )
   621                                                   # if len(newEvent):
   622                                                   newEvent.segment = newSeg
   623                                                   newSeg.events.append(newEvent)
   624                                               for chIdxName, chIdx in channelIndexCache.items():
   625                                                   if len(chIdx.analogsignals) or len(chIdx.units):
   626                                                       outputBlock.channel_indexes.append(chIdx)
   627                                                       chIdx.block = outputBlock
   628                                               #
   629                                               outputBlock = purgeNixAnn(outputBlock)
   630                                               createRelationship = False
   631                                               if createRelationship:
   632                                                   outputBlock.create_relationship()
   633                                               return outputBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: concatenateEventsContainer at line 660

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   660                                           @profile
   661                                           def concatenateEventsContainer(eventContainer, linkParents=True):
   662                                               if isinstance(eventContainer, dict):
   663                                                   listOfEvents = list(eventContainer.values())
   664                                               else:
   665                                                   listOfEvents = eventContainer
   666                                               nonEmptyEvents = [ev for ev in listOfEvents if len(ev.times)]
   667                                               if not len(nonEmptyEvents) > 0:
   668                                                   return listOfEvents[0]
   669                                               masterEvent = listOfEvents[0]
   670                                               for evIdx, ev in enumerate(listOfEvents[1:]):
   671                                                   try:
   672                                                       masterEvent = masterEvent.merge(ev)
   673                                                   except Exception:
   674                                                       traceback.print_exc()
   675                                                       pdb.set_trace()
   676                                               if masterEvent.array_annotations is not None:
   677                                                   arrayAnnNames = list(masterEvent.array_annotations.keys())
   678                                                   masterEvent.annotations.update(masterEvent.array_annotations)
   679                                                   masterEvent.annotations['arrayAnnNames'] = arrayAnnNames
   680                                               if linkParents:
   681                                                   masterEvent.segment = listOfEvents[0].segment
   682                                                   if isinstance(masterEvent, SpikeTrain):
   683                                                       masterEvent.unit = listOfEvents[0].unit
   684                                               return masterEvent

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: unitSpikeTrainWaveformsToDF at line 743

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   743                                           @profile
   744                                           def unitSpikeTrainWaveformsToDF(
   745                                                   spikeTrainContainer,
   746                                                   dataQuery=None,
   747                                                   transposeToColumns='bin', fastTranspose=True,
   748                                                   lags=None, decimate=1, rollingWindow=None,
   749                                                   getMetaData=True, verbose=False,
   750                                                   whichSegments=None, windowSize=None, procFun=None):
   751                                               #  list contains different segments from *one* unit
   752                                               if isinstance(spikeTrainContainer, ChannelIndex):
   753                                                   assert len(spikeTrainContainer.units) == 0
   754                                                   spiketrains = spikeTrainContainer.units[0].spiketrains
   755                                               elif isinstance(spikeTrainContainer, Unit):
   756                                                   spiketrains = spikeTrainContainer.spiketrains
   757                                               else:
   758                                                   raise(Exception('not a valid container'))
   759                                               # TODO check if really need to assert uniqueness?
   760                                               uniqueSpiketrains = []
   761                                               for st in spiketrains:
   762                                                   if not np.any([st is i for i in uniqueSpiketrains]):
   763                                                       uniqueSpiketrains.append(st)
   764                                               #  subsampling options
   765                                               decimate = int(decimate)
   766                                               if whichSegments is not None:
   767                                                   uniqueSpiketrains = [
   768                                                       uniqueSpiketrains[i]
   769                                                       for i in whichSegments
   770                                                   ]
   771                                               #
   772                                               waveformsList = []
   773                                               #
   774                                               for segIdx, stIn in enumerate(uniqueSpiketrains):
   775                                                   if verbose:
   776                                                       print('extracting spiketrain from {}'.format(stIn.segment))
   777                                                   #  make sure is not a proxyObj
   778                                                   if isinstance(stIn, SpikeTrainProxy):
   779                                                       st = loadStProxy(stIn)
   780                                                       if (getMetaData) or (dataQuery is not None):
   781                                                           # if there's a query, get metadata temporarily to resolve it
   782                                                           st = loadObjArrayAnn(st)
   783                                                   else:
   784                                                       st = stIn
   785                                                   #  extract bins spaced by decimate argument
   786                                                   if not st.times.any():
   787                                                       continue
   788                                                   if verbose:
   789                                                       print('extracting wf from {}'.format(stIn.segment))
   790                                                   wf = np.asarray(
   791                                                       np.squeeze(st.waveforms),
   792                                                       dtype='float32')
   793                                                   if wf.ndim == 3:
   794                                                       print('Waveforms from more than one channel!')
   795                                                       if wf.shape[1] > 0:
   796                                                           wf = wf[:, 0, :]
   797                                                   wfDF = pd.DataFrame(wf)
   798                                                   samplingRate = st.sampling_rate
   799                                                   bins = (
   800                                                       np.asarray(wfDF.columns) / samplingRate -
   801                                                       st.left_sweep)
   802                                                   wfDF.columns = np.around(bins.magnitude, decimals=6)
   803                                                   if windowSize is not None:
   804                                                       winMask = (
   805                                                           (wfDF.columns >= windowSize[0]) &
   806                                                           (wfDF.columns <= windowSize[1]))
   807                                                       wfDF = wfDF.loc[:, winMask]
   808                                                   if procFun is not None:
   809                                                       wfDF = procFun(wfDF, st)
   810                                                   idxLabels = ['segment', 'originalIndex', 't']
   811                                                   wfDF.loc[:, 't'] = np.asarray(st.times.magnitude)
   812                                                   if (getMetaData) or (dataQuery is not None):
   813                                                       # if there's a query, get metadata temporarily to resolve it
   814                                                       annDict = {}
   815                                                       for k, values in st.array_annotations.items():
   816                                                           if isinstance(getMetaData, Iterable):
   817                                                               # if selecting metadata fields, check that
   818                                                               # the key is in the provided list
   819                                                               if k not in getMetaData:
   820                                                                   continue
   821                                                           if isinstance(values[0], str):
   822                                                               v = np.asarray(values, dtype='str')
   823                                                           else:
   824                                                               v = np.asarray(values)
   825                                                           annDict.update({k: v})
   826                                                       skipAnnNames = (
   827                                                           st.annotations['arrayAnnNames'] +
   828                                                           [
   829                                                               'arrayAnnNames', 'arrayAnnDTypes',
   830                                                               'nix_name', 'neo_name', 'id',
   831                                                               'cell_label', 'cluster_label', 'max_on_channel', 'binWidth']
   832                                                           )
   833                                                       annDF = pd.DataFrame(annDict)
   834                                                       for k, value in st.annotations.items():
   835                                                           if isinstance(getMetaData, Iterable):
   836                                                               # if selecting metadata fields, check that
   837                                                               # the key is in the provided list
   838                                                               if k not in getMetaData:
   839                                                                   continue
   840                                                           if k not in skipAnnNames:
   841                                                               annDF.loc[:, k] = value
   842                                                       #
   843                                                       if isinstance(getMetaData, Iterable):
   844                                                           doNotFillList = idxLabels + ['feature', 'bin']
   845                                                           fieldsNeedFiller = [
   846                                                               mdn
   847                                                               for mdn in getMetaData
   848                                                               if (mdn not in doNotFillList) and (mdn not in annDF.columns)]
   849                                                           for mdName in fieldsNeedFiller:
   850                                                               annDF.loc[:, mdName] = 'NA'
   851                                                       annColumns = annDF.columns.to_list()
   852                                                       if getMetaData:
   853                                                           for annNm in annColumns:
   854                                                               if annNm not in idxLabels:
   855                                                                   idxLabels.append(annNm)
   856                                                           # idxLabels += annColumns
   857                                                       spikeDF = annDF.join(wfDF)
   858                                                   else:
   859                                                       spikeDF = wfDF
   860                                                       del wfDF, st
   861                                                   spikeDF.loc[:, 'segment'] = segIdx
   862                                                   spikeDF.loc[:, 'originalIndex'] = spikeDF.index
   863                                                   spikeDF.columns.name = 'bin'
   864                                                   #
   865                                                   if dataQuery is not None:
   866                                                       spikeDF.query(dataQuery, inplace=True)
   867                                                       if not getMetaData:
   868                                                           spikeDF.drop(columns=annColumns, inplace=True)
   869                                                   waveformsList.append(spikeDF)
   870                                               #
   871                                               zeroLagWaveformsDF = pd.concat(waveformsList, axis='index')
   872                                               if verbose:
   873                                                   prf.print_memory_usage('before transposing waveforms')
   874                                               # TODO implement lags and rolling window addition here
   875                                               metaDF = zeroLagWaveformsDF.loc[:, idxLabels].copy()
   876                                               zeroLagWaveformsDF.drop(columns=idxLabels, inplace=True)
   877                                               if lags is None:
   878                                                   lags = [0]
   879                                               laggedWaveformsDict = {
   880                                                   (spikeTrainContainer.name, k): None for k in lags}
   881                                               for lag in lags:
   882                                                   if isinstance(lag, int):
   883                                                       shiftedWaveform = zeroLagWaveformsDF.shift(
   884                                                           lag, axis='columns')
   885                                                       if rollingWindow is not None:
   886                                                           halfRollingWin = int(np.ceil(rollingWindow/2))
   887                                                           seekIdx = slice(
   888                                                               halfRollingWin, -halfRollingWin+1, decimate)
   889                                                           # seekIdx = slice(None, None, decimate)
   890                                                           #shiftedWaveform = (
   891                                                           #    shiftedWaveform
   892                                                           #    .rolling(
   893                                                           #        window=rollingWindow, win_type='gaussian',
   894                                                           #        axis='columns', center=True)
   895                                                           #    .mean(std=halfRollingWin))
   896                                                           shiftedWaveform = (
   897                                                               shiftedWaveform
   898                                                               .rolling(
   899                                                                   window=rollingWindow, 
   900                                                                   axis='columns', center=True)
   901                                                               .mean())
   902                                                       else:
   903                                                           halfRollingWin = 0
   904                                                           seekIdx = slice(None, None, decimate)
   905                                                           if False:
   906                                                               oldShiftedWaveform = zeroLagWaveformsDF.shift(
   907                                                                   lag, axis='columns')
   908                                                               plt.plot(oldShiftedWaveform.iloc[0, :])
   909                                                               plt.plot(shiftedWaveform.iloc[0, :])
   910                                                               plt.show()
   911                                                       laggedWaveformsDict[
   912                                                           (spikeTrainContainer.name, lag)] = (
   913                                                               shiftedWaveform.iloc[:, seekIdx].copy())
   914                                                   if isinstance(lag, tuple):
   915                                                       halfRollingWin = int(np.ceil(lag[1]/2))
   916                                                       seekIdx = slice(
   917                                                           halfRollingWin, -halfRollingWin+1, decimate)
   918                                                       # seekIdx = slice(None, None, decimate)
   919                                                       shiftedWaveform = (
   920                                                           zeroLagWaveformsDF
   921                                                           .shift(lag[0], axis='columns')
   922                                                           .rolling(
   923                                                               window=lag[1], win_type='gaussian',
   924                                                               axis='columns', center=True)
   925                                                           .mean(std=halfRollingWin))
   926                                                       laggedWaveformsDict[
   927                                                           (spikeTrainContainer.name, lag)] = (
   928                                                               shiftedWaveform.iloc[:, seekIdx].copy())
   929                                               #
   930                                               if transposeToColumns == 'feature':
   931                                                   # stack the bin, name the feature column
   932                                                   # 
   933                                                   for idx, (key, value) in enumerate(laggedWaveformsDict.items()):
   934                                                       if idx == 0:
   935                                                           stackedIndexDF = pd.concat(
   936                                                               [metaDF, value], axis='columns')
   937                                                           stackedIndexDF.set_index(idxLabels, inplace=True)
   938                                                           # don't drop nans for now - might need to keep track of them
   939                                                           # if we need to equalize to another array later
   940                                                           newIndex = stackedIndexDF.stack(dropna=False).index
   941                                                           idxLabels.append('bin')
   942                                                       laggedWaveformsDict[key] = value.stack(dropna=False).to_frame(name=key).reset_index(drop=True)
   943                                                   waveformsDF = pd.concat(
   944                                                       laggedWaveformsDict.values(),
   945                                                       axis='columns')
   946                                                   waveformsDF.columns.names = ['feature', 'lag']
   947                                                   waveformsDF.index = newIndex
   948                                                   waveformsDF.columns.name = 'feature'
   949                                               elif transposeToColumns == 'bin':
   950                                                   # add the feature column
   951                                                   waveformsDF = pd.concat(
   952                                                       laggedWaveformsDict,
   953                                                       names=['feature', 'lag', 'originalDummy']).reset_index()
   954                                                   waveformsDF = pd.concat(
   955                                                       [
   956                                                           metaDF.reset_index(drop=True),
   957                                                           waveformsDF.drop(columns='originalDummy')],
   958                                                       axis='columns')
   959                                                   idxLabels += ['feature', 'lag']
   960                                                   waveformsDF.columns.name = 'bin'
   961                                                   waveformsDF.set_index(idxLabels, inplace=True)
   962                                               #
   963                                               if transposeToColumns != waveformsDF.columns.name:
   964                                                   waveformsDF = transposeSpikeDF(
   965                                                       waveformsDF, transposeToColumns,
   966                                                       fastTranspose=fastTranspose)
   967                                               return waveformsDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: concatenateUnitSpikeTrainWaveformsDF at line 969

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   969                                           @profile
   970                                           def concatenateUnitSpikeTrainWaveformsDF(
   971                                                   units, dataQuery=None,
   972                                                   transposeToColumns='bin', concatOn='index',
   973                                                   fastTranspose=True, getMetaData=True, verbose=False,
   974                                                   addLags=None, decimate=1, rollingWindow=None,
   975                                                   metaDataToCategories=False, windowSize=None,
   976                                                   whichSegments=None, procFun=None):
   977                                               allUnits = []
   978                                               for thisUnit in units:
   979                                                   hasAnySpikes = []
   980                                                   for stIn in thisUnit.spiketrains:
   981                                                       if isinstance(stIn, SpikeTrainProxy):
   982                                                           st = stIn.load(
   983                                                               magnitude_mode='rescaled',
   984                                                               load_waveforms=False)
   985                                                       else:
   986                                                           st = stIn
   987                                                       hasAnySpikes.append(st.times.any())
   988                                                   if np.any(hasAnySpikes):
   989                                                       allUnits.append(thisUnit)
   990                                               waveformsList = []
   991                                               for idx, thisUnit in enumerate(allUnits):
   992                                                   if verbose:
   993                                                       print('concatenating unitDF {}'.format(thisUnit.name))
   994                                                   lags = None
   995                                                   if addLags is not None:
   996                                                       if thisUnit.name in addLags:
   997                                                           lags = addLags[thisUnit.name]
   998                                                   unitWaveforms = unitSpikeTrainWaveformsToDF(
   999                                                       thisUnit, dataQuery=dataQuery,
  1000                                                       transposeToColumns=transposeToColumns,
  1001                                                       fastTranspose=fastTranspose, getMetaData=getMetaData,
  1002                                                       lags=lags, decimate=decimate, rollingWindow=rollingWindow,
  1003                                                       verbose=verbose, windowSize=windowSize,
  1004                                                       whichSegments=whichSegments, procFun=procFun)
  1005                                                   if idx == 0:
  1006                                                       idxLabels = unitWaveforms.index.names
  1007                                                   if (concatOn == 'columns') and (idx > 0):
  1008                                                       # other than first time, we already have the metadata
  1009                                                       unitWaveforms.reset_index(drop=True, inplace=True)
  1010                                                   else:
  1011                                                       # first time, or if concatenating indices,
  1012                                                       # keep the the metadata
  1013                                                       unitWaveforms.reset_index(inplace=True)
  1014                                                       if metaDataToCategories:
  1015                                                           # convert metadata to categoricals to free memory
  1016                                                           #
  1017                                                           unitWaveforms[idxLabels] = (
  1018                                                               unitWaveforms[idxLabels]
  1019                                                               .astype('category')
  1020                                                               )
  1021                                                   waveformsList.append(unitWaveforms)
  1022                                                   del unitWaveforms
  1023                                                   if verbose:
  1024                                                       print('memory usage: {:.1f} MB'.format(prf.memory_usage_psutil()))
  1025                                               if verbose:
  1026                                                   print(
  1027                                                       'about to join all, memory usage: {:.1f} MB'
  1028                                                       .format(prf.memory_usage_psutil()))
  1029                                               #  if concatenating indexes, reset the index of the result
  1030                                               #  ignoreIndex = (concatOn == 'index')
  1031                                               allWaveforms = pd.concat(
  1032                                                   waveformsList, axis=concatOn,
  1033                                                   # ignore_index=ignoreIndex
  1034                                                   )
  1035                                               del waveformsList
  1036                                               if verbose:
  1037                                                   print(
  1038                                                       'finished concatenating, memory usage: {:.1f} MB'
  1039                                                       .format(prf.memory_usage_psutil()))
  1040                                               try:
  1041                                                   allWaveforms.set_index(idxLabels, inplace=True)
  1042                                                   allWaveforms.sort_index(
  1043                                                       level=['segment', 'originalIndex', 't'],
  1044                                                       axis='index', inplace=True, kind='mergesort')
  1045                                                   allWaveforms.sort_index(
  1046                                                       axis='columns', inplace=True, kind='mergesort')
  1047                                               except Exception:
  1048                                                   pdb.set_trace()
  1049                                               return allWaveforms

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: alignedAsigsToDF at line 1051

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1051                                           @profile
  1052                                           def alignedAsigsToDF(
  1053                                                   dataBlock, unitNames=None,
  1054                                                   unitQuery=None, dataQuery=None,
  1055                                                   collapseSizes=False, verbose=False,
  1056                                                   duplicateControlsByProgram=False,
  1057                                                   amplitudeColumn='amplitude',
  1058                                                   programColumn='program',
  1059                                                   electrodeColumn='electrode',
  1060                                                   transposeToColumns='bin', concatOn='index', fastTranspose=True,
  1061                                                   addLags=None, decimate=1, rollingWindow=None,
  1062                                                   whichSegments=None, windowSize=None,
  1063                                                   getMetaData=True, metaDataToCategories=True,
  1064                                                   outlierTrials=None, invertOutlierMask=False,
  1065                                                   makeControlProgram=False, removeFuzzyName=False, procFun=None):
  1066                                               #  channels to trigger
  1067                                               if unitNames is None:
  1068                                                   unitNames = listChanNames(dataBlock, unitQuery, objType=Unit)
  1069                                               allUnits = []
  1070                                               for uName in unitNames:
  1071                                                   allUnits += dataBlock.filter(objects=Unit, name=uName)
  1072                                               allWaveforms = concatenateUnitSpikeTrainWaveformsDF(
  1073                                                   allUnits, dataQuery=dataQuery,
  1074                                                   transposeToColumns=transposeToColumns, concatOn=concatOn,
  1075                                                   fastTranspose=fastTranspose,
  1076                                                   addLags=addLags, decimate=decimate, rollingWindow=rollingWindow,
  1077                                                   verbose=verbose, whichSegments=whichSegments,
  1078                                                   windowSize=windowSize, procFun=procFun,
  1079                                                   getMetaData=getMetaData, metaDataToCategories=metaDataToCategories)
  1080                                               #
  1081                                               manipulateIndex = np.any(
  1082                                                   [
  1083                                                       collapseSizes, duplicateControlsByProgram,
  1084                                                       makeControlProgram, removeFuzzyName
  1085                                                       ])
  1086                                               if outlierTrials is not None:
  1087                                                   def rejectionLookup(entry):
  1088                                                       key = []
  1089                                                       for subKey in outlierTrials.index.names:
  1090                                                           keyIdx = allWaveforms.index.names.index(subKey)
  1091                                                           key.append(entry[keyIdx])
  1092                                                       # print(key)
  1093                                                       # outlierTrials.iloc[1, :]
  1094                                                       # allWaveforms.iloc[1, :]
  1095                                                       return outlierTrials[tuple(key)]
  1096                                                   #
  1097                                                   outlierMask = np.asarray(
  1098                                                       allWaveforms.index.map(rejectionLookup),
  1099                                                       dtype=np.bool)
  1100                                                   if invertOutlierMask:
  1101                                                       outlierMask = ~outlierMask
  1102                                                   allWaveforms = allWaveforms.loc[~outlierMask, :]
  1103                                               if manipulateIndex and getMetaData:
  1104                                                   idxLabels = allWaveforms.index.names
  1105                                                   allWaveforms.reset_index(inplace=True)
  1106                                                   # 
  1107                                                   if collapseSizes:
  1108                                                       try:
  1109                                                           allWaveforms.loc[allWaveforms['pedalSizeCat'] == 'XL', 'pedalSizeCat'] = 'L'
  1110                                                           allWaveforms.loc[allWaveforms['pedalSizeCat'] == 'XS', 'pedalSizeCat'] = 'S'
  1111                                                       except Exception:
  1112                                                           traceback.print_exc()
  1113                                                   if makeControlProgram:
  1114                                                       try:
  1115                                                           allWaveforms.loc[allWaveforms[amplitudeColumn] == 0, programColumn] = 999
  1116                                                           allWaveforms.loc[allWaveforms[amplitudeColumn] == 0, electrodeColumn] = 'control'
  1117                                                       except Exception:
  1118                                                           traceback.print_exc()
  1119                                                   if duplicateControlsByProgram:
  1120                                                       #
  1121                                                       noStimWaveforms = (
  1122                                                           allWaveforms
  1123                                                           .loc[allWaveforms[amplitudeColumn] == 0, :]
  1124                                                           )
  1125                                                       stimWaveforms = (
  1126                                                           allWaveforms
  1127                                                           .loc[allWaveforms[amplitudeColumn] != 0, :]
  1128                                                           .copy()
  1129                                                           )
  1130                                                       uniqProgs = stimWaveforms[programColumn].unique()
  1131                                                       progElecLookup = {}
  1132                                                       #pdb.set_trace()
  1133                                                       for progIdx in uniqProgs:
  1134                                                           theseStimDF = stimWaveforms.loc[
  1135                                                               stimWaveforms[programColumn] == progIdx,
  1136                                                               electrodeColumn]
  1137                                                           elecIdx = theseStimDF.iloc[0]
  1138                                                           progElecLookup.update({progIdx: elecIdx})
  1139                                                       #
  1140                                                       if makeControlProgram:
  1141                                                           uniqProgs = np.append(uniqProgs, 999)
  1142                                                           progElecLookup.update({999: 'control'})
  1143                                                       #
  1144                                                       for progIdx in uniqProgs:
  1145                                                           dummyWaveforms = noStimWaveforms.copy()
  1146                                                           dummyWaveforms.loc[:, programColumn] = progIdx
  1147                                                           dummyWaveforms.loc[:, electrodeColumn] = progElecLookup[progIdx]
  1148                                                           stimWaveforms = pd.concat([stimWaveforms, dummyWaveforms])
  1149                                                       stimWaveforms.reset_index(drop=True, inplace=True)
  1150                                                       allWaveforms = stimWaveforms
  1151                                                   #
  1152                                                   if removeFuzzyName:
  1153                                                       fuzzyNamesBase = [
  1154                                                           i.replace('Fuzzy', '')
  1155                                                           for i in idxLabels
  1156                                                           if 'Fuzzy' in i]
  1157                                                       colRenamer = {n + 'Fuzzy': n for n in fuzzyNamesBase}
  1158                                                       fuzzyNamesBasePresent = [
  1159                                                           i
  1160                                                           for i in fuzzyNamesBase
  1161                                                           if i in allWaveforms.columns]
  1162                                                       allWaveforms.drop(columns=fuzzyNamesBasePresent, inplace=True)
  1163                                                       allWaveforms.rename(columns=colRenamer, inplace=True)
  1164                                                       idxLabels = np.unique(
  1165                                                           [i.replace('Fuzzy', '') for i in idxLabels])
  1166                                                   #
  1167                                                   allWaveforms.set_index(
  1168                                                       list(idxLabels),
  1169                                                       inplace=True)
  1170                                                   if isinstance(allWaveforms.columns, pd.MultiIndex):
  1171                                                       allWaveforms.columns = allWaveforms.columns.remove_unused_levels()
  1172                                               #
  1173                                               if transposeToColumns == 'feature':
  1174                                                   zipNames = zip(pd.unique(allWaveforms.columns.get_level_values('feature')).tolist(), unitNames)
  1175                                                   try:
  1176                                                       assert np.all([i == j for i, j in zipNames]), 'columns out of requested order!'
  1177                                                   except Exception:
  1178                                                       traceback.print_exc()
  1179                                                       allWaveforms.reindex(columns=unitNames)
  1180                                               if isinstance(allWaveforms.columns, pd.MultiIndex):
  1181                                                   allWaveforms.columns = allWaveforms.columns.remove_unused_levels()
  1182                                               allWaveforms.sort_index(
  1183                                                   axis='columns', inplace=True, kind='mergesort')
  1184                                               return allWaveforms

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getAsigsAlignedToEvents at line 1186

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1186                                           @profile
  1187                                           def getAsigsAlignedToEvents(
  1188                                                   eventBlock=None, signalBlock=None,
  1189                                                   chansToTrigger=None, chanQuery=None,
  1190                                                   eventName=None, windowSize=None,
  1191                                                   minNReps=None,
  1192                                                   appendToExisting=False,
  1193                                                   checkReferences=True, verbose=False,
  1194                                                   fileName=None, folderPath=None, chunkSize=None
  1195                                                   ):
  1196                                               #  get signals from same block as events?
  1197                                               if signalBlock is None:
  1198                                                   signalBlock = eventBlock
  1199                                               #  channels to trigger
  1200                                               if chansToTrigger is None:
  1201                                                   chansToTrigger = listChanNames(
  1202                                                       signalBlock, chanQuery, objType=ChannelIndex, condition='hasAsigs')
  1203                                               #  allocate block for spiketrains
  1204                                               masterBlock = Block()
  1205                                               try:
  1206                                                   masterBlock.name = signalBlock.annotations['neo_name']
  1207                                                   masterBlock.annotate(nix_name=signalBlock.annotations['neo_name'])
  1208                                               except Exception:
  1209                                                   masterBlock.name = signalBlock.name
  1210                                                   masterBlock.annotate(neo_name=signalBlock.name)
  1211                                                   masterBlock.annotate(nix_name=signalBlock.name)
  1212                                               #  make channels and units for triggered time series
  1213                                               for chanName in chansToTrigger:
  1214                                                   chanIdx = ChannelIndex(name=chanName + '#0', index=[0])
  1215                                                   chanIdx.annotate(nix_name=chanIdx.name)
  1216                                                   thisUnit = Unit(name=chanIdx.name)
  1217                                                   thisUnit.annotate(nix_name=chanIdx.name)
  1218                                                   chanIdx.units.append(thisUnit)
  1219                                                   thisUnit.channel_index = chanIdx
  1220                                                   masterBlock.channel_indexes.append(chanIdx)
  1221                                                   sigChanIdxList = signalBlock.filter(
  1222                                                       objects=ChannelIndex, name=chanName)
  1223                                                   if len(sigChanIdxList):
  1224                                                       sigChanIdx = sigChanIdxList[0]
  1225                                                       if sigChanIdx.coordinates is not None:
  1226                                                           coordUnits = sigChanIdx.coordinates[0][0].units
  1227                                                           chanIdx.coordinates = np.asarray(sigChanIdx.coordinates) * coordUnits
  1228                                                           thisUnit.annotations['parentChanXCoords'] = float(chanIdx.coordinates[:, 0].magnitude)
  1229                                                           thisUnit.annotations['parentChanYCoords'] = float(chanIdx.coordinates[:, 1].magnitude)
  1230                                                           thisUnit.annotations['parentChanCoordinateUnits'] = '{}'.format(coordUnits)
  1231                                               #
  1232                                               totalNSegs = 0
  1233                                               #  print([evSeg.events[3].name for evSeg in eventBlock.segments])
  1234                                               allAlignEventsList = []
  1235                                               for segIdx, eventSeg in enumerate(eventBlock.segments):
  1236                                                   thisEventName = 'seg{}_{}'.format(segIdx, eventName)
  1237                                                   try:
  1238                                                       assert len(eventSeg.filter(name=thisEventName)) == 1
  1239                                                   except Exception:
  1240                                                       traceback.print_exc()
  1241                                                   allEvIn = eventSeg.filter(name=thisEventName)[0]
  1242                                                   if isinstance(allEvIn, EventProxy):
  1243                                                       allAlignEvents = loadObjArrayAnn(allEvIn.load())
  1244                                                   elif isinstance(allEvIn, Event):
  1245                                                       allAlignEvents = allEvIn
  1246                                                   else:
  1247                                                       raise(Exception(
  1248                                                           '{} must be an Event or EventProxy!'
  1249                                                           .format(eventName)))
  1250                                                   allAlignEventsList.append(allAlignEvents)
  1251                                               allAlignEventsDF = unitSpikeTrainArrayAnnToDF(allAlignEventsList)
  1252                                               #
  1253                                               breakDownData = (
  1254                                                   allAlignEventsDF
  1255                                                   .groupby(minNReps['categories'])
  1256                                                   .agg('count')
  1257                                                   .iloc[:, 0]
  1258                                                   )
  1259                                               try:
  1260                                                   breakDownData[breakDownData > minNReps['n']].to_csv(
  1261                                                       os.path.join(
  1262                                                           folderPath, 'numRepetitionsEachCondition.csv'
  1263                                                       ), header=True
  1264                                                   )
  1265                                               except Exception:
  1266                                                   traceback.print_exc()
  1267                                               allAlignEventsDF.loc[:, 'keepMask'] = False
  1268                                               for name, group in allAlignEventsDF.groupby(minNReps['categories']):
  1269                                                   allAlignEventsDF.loc[group.index, 'keepMask'] = (
  1270                                                       breakDownData[name] > minNReps['n'])
  1271                                               for segIdx, group in allAlignEventsDF.groupby('segment'):
  1272                                                   allAlignEventsList[segIdx].array_annotations['keepMask'] = group['keepMask'].to_numpy()
  1273                                               #
  1274                                               for segIdx, eventSeg in enumerate(eventBlock.segments):
  1275                                                   if verbose:
  1276                                                       print(
  1277                                                           'getAsigsAlignedToEvents on segment {} of {}'
  1278                                                           .format(segIdx + 1, len(eventBlock.segments)))
  1279                                                   allAlignEvents = allAlignEventsList[segIdx]
  1280                                                   if chunkSize is None:
  1281                                                       alignEventGroups = [allAlignEvents]
  1282                                                   else:
  1283                                                       nChunks = max(
  1284                                                           int(np.floor(allAlignEvents.shape[0] / chunkSize)),
  1285                                                           1)
  1286                                                       alignEventGroups = []
  1287                                                       for i in range(nChunks):
  1288                                                           if not (i == (nChunks - 1)):
  1289                                                               # not last one
  1290                                                               alignEventGroups.append(
  1291                                                                   allAlignEvents[i * chunkSize: (i + 1) * chunkSize])
  1292                                                           else:
  1293                                                               alignEventGroups.append(
  1294                                                                   allAlignEvents[i * chunkSize:])
  1295                                                   signalSeg = signalBlock.segments[segIdx]
  1296                                                   for subSegIdx, alignEvents in enumerate(alignEventGroups):
  1297                                                       # seg to contain triggered time series
  1298                                                       if verbose:
  1299                                                           print(
  1300                                                               'getAsigsAlignedToEvents on subSegment {} of {}'
  1301                                                               .format(subSegIdx + 1, len(alignEventGroups)))
  1302                                                       if not alignEvents.shape[0] > 0:
  1303                                                           continue
  1304                                                       newSeg = Segment(name='seg{}_'.format(int(totalNSegs)))
  1305                                                       newSeg.annotate(nix_name=newSeg.name)
  1306                                                       masterBlock.segments.append(newSeg)
  1307                                                       for chanName in chansToTrigger:
  1308                                                           asigName = 'seg{}_{}'.format(segIdx, chanName)
  1309                                                           if verbose:
  1310                                                               print(
  1311                                                                   'getAsigsAlignedToEvents on channel {}'
  1312                                                                   .format(chanName))
  1313                                                           assert len(signalSeg.filter(name=asigName)) == 1
  1314                                                           asig = signalSeg.filter(name=asigName)[0]
  1315                                                           nominalWinLen = int(
  1316                                                               (windowSize[1] - windowSize[0]) *
  1317                                                               asig.sampling_rate - 1)
  1318                                                           validMask = (
  1319                                                               ((
  1320                                                                   alignEvents + windowSize[1] +
  1321                                                                   asig.sampling_rate ** (-1)) < asig.t_stop) &
  1322                                                               ((
  1323                                                                   alignEvents + windowSize[0] -
  1324                                                                   asig.sampling_rate ** (-1)) > asig.t_start)
  1325                                                               )
  1326                                                           thisKeepMask = alignEvents.array_annotations['keepMask']
  1327                                                           fullMask = (validMask & thisKeepMask)
  1328                                                           alignEvents = alignEvents[fullMask]
  1329                                                           # array_annotations get sliced with the event, but regular anns do not
  1330                                                           for annName in alignEvents.annotations['arrayAnnNames']:
  1331                                                               alignEvents.annotations[annName] = (
  1332                                                                   alignEvents.array_annotations[annName])
  1333                                                           if isinstance(asig, AnalogSignalProxy):
  1334                                                               if checkReferences:
  1335                                                                   da = (
  1336                                                                       asig
  1337                                                                       ._rawio
  1338                                                                       .da_list['blocks'][0]['segments'][segIdx]['data'])
  1339                                                                   print('segIdx {}, asig.name {}'.format(
  1340                                                                       segIdx, asig.name))
  1341                                                                   print('asig._global_channel_indexes = {}'.format(
  1342                                                                       asig._global_channel_indexes))
  1343                                                                   print('asig references {}'.format(
  1344                                                                       da[asig._global_channel_indexes[0]]))
  1345                                                                   try:
  1346                                                                       assert (
  1347                                                                           asig.name
  1348                                                                           in da[asig._global_channel_indexes[0]].name)
  1349                                                                   except Exception:
  1350                                                                       traceback.print_exc()
  1351                                                               rawWaveforms = [
  1352                                                                   asig.load(
  1353                                                                       time_slice=(t + windowSize[0], t + windowSize[1]))
  1354                                                                   for t in alignEvents]
  1355                                                               if any([rW.shape[0] < nominalWinLen for rW in rawWaveforms]):
  1356                                                                   rawWaveforms = [
  1357                                                                       asig.load(
  1358                                                                           time_slice=(t + windowSize[0], t + windowSize[1] + asig.sampling_period))
  1359                                                                       for t in alignEvents]
  1360                                                           elif isinstance(asig, AnalogSignal):
  1361                                                               rawWaveforms = []
  1362                                                               for t in alignEvents:
  1363                                                                   asigMask = (asig.times > t + windowSize[0]) & (asig.times < t + windowSize[1])
  1364                                                                   rawWaveforms.append(asig[asigMask[:, np.newaxis]])
  1365                                                           else:
  1366                                                               raise(Exception('{} must be an AnalogSignal or AnalogSignalProxy!'.format(asigName)))
  1367                                                           #
  1368                                                           samplingRate = asig.sampling_rate
  1369                                                           waveformUnits = rawWaveforms[0].units
  1370                                                           #  fix length if roundoff error
  1371                                                           #  minLen = min([rW.shape[0] for rW in rawWaveforms])
  1372                                                           rawWaveforms = [rW[:nominalWinLen] for rW in rawWaveforms]
  1373                                                           #
  1374                                                           spikeWaveforms = (
  1375                                                               np.hstack([rW.magnitude for rW in rawWaveforms])
  1376                                                               .transpose()[:, np.newaxis, :] * waveformUnits
  1377                                                               )
  1378                                                           #
  1379                                                           thisUnit = masterBlock.filter(
  1380                                                               objects=Unit, name=chanName + '#0')[0]
  1381                                                           skipEventAnnNames = (
  1382                                                               ['nix_name', 'neo_name']
  1383                                                               )
  1384                                                           stAnn = {
  1385                                                               k: v
  1386                                                               for k, v in alignEvents.annotations.items()
  1387                                                               if k not in skipEventAnnNames
  1388                                                               }
  1389                                                           skipAsigAnnNames = (
  1390                                                               ['channel_id', 'nix_name', 'neo_name']
  1391                                                               )
  1392                                                           stAnn.update({
  1393                                                               k: v
  1394                                                               for k, v in asig.annotations.items()
  1395                                                               if k not in skipAsigAnnNames
  1396                                                           })
  1397                                                           st = SpikeTrain(
  1398                                                               name='seg{}_{}'.format(int(totalNSegs), thisUnit.name),
  1399                                                               times=alignEvents.times,
  1400                                                               waveforms=spikeWaveforms,
  1401                                                               t_start=asig.t_start, t_stop=asig.t_stop,
  1402                                                               left_sweep=windowSize[0] * (-1),
  1403                                                               sampling_rate=samplingRate,
  1404                                                               **stAnn
  1405                                                               )
  1406                                                           st.annotate(nix_name=st.name)
  1407                                                           st.annotations['unitAnnotations'] = json.dumps(
  1408                                                               thisUnit.annotations.copy())
  1409                                                           thisUnit.spiketrains.append(st)
  1410                                                           newSeg.spiketrains.append(st)
  1411                                                           st.unit = thisUnit
  1412                                                       totalNSegs += 1
  1413                                               try:
  1414                                                   eventBlock.filter(
  1415                                                       objects=EventProxy)[0]._rawio.file.close()
  1416                                               except Exception:
  1417                                                   traceback.print_exc()
  1418                                               if signalBlock is not eventBlock:
  1419                                                   try:
  1420                                                       signalBlock.filter(
  1421                                                           objects=AnalogSignalProxy)[0]._rawio.file.close()
  1422                                                   except Exception:
  1423                                                       traceback.print_exc()
  1424                                               triggeredPath = os.path.join(
  1425                                                   folderPath, fileName + '.nix')
  1426                                               if not os.path.exists(triggeredPath):
  1427                                                   appendToExisting = False
  1428                                           
  1429                                               if appendToExisting:
  1430                                                   allSegs = list(range(len(masterBlock.segments)))
  1431                                                   addBlockToNIX(
  1432                                                       masterBlock, neoSegIdx=allSegs,
  1433                                                       writeSpikes=True,
  1434                                                       fileName=fileName,
  1435                                                       folderPath=folderPath,
  1436                                                       purgeNixNames=False,
  1437                                                       nixBlockIdx=0, nixSegIdx=allSegs)
  1438                                               else:
  1439                                                   if os.path.exists(triggeredPath):
  1440                                                       os.remove(triggeredPath)
  1441                                                   masterBlock = purgeNixAnn(masterBlock)
  1442                                                   writer = NixIO(filename=triggeredPath)
  1443                                                   writer.write_block(masterBlock, use_obj_names=True)
  1444                                                   writer.close()
  1445                                               return masterBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: alignedAsigDFtoSpikeTrain at line 1447

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1447                                           @profile
  1448                                           def alignedAsigDFtoSpikeTrain(
  1449                                                   allWaveforms, dataBlock=None, matchSamplingRate=True):
  1450                                               masterBlock = Block()
  1451                                               masterBlock.name = dataBlock.annotations['neo_name']
  1452                                               masterBlock.annotate(nix_name=dataBlock.annotations['neo_name'])
  1453                                               for segIdx, group in allWaveforms.groupby('segment'):
  1454                                                   print('Saving trajectoriess for segment {}'.format(segIdx))
  1455                                                   dataSeg = dataBlock.segments[segIdx]
  1456                                                   exSt = dataSeg.spiketrains[0]
  1457                                                   if isinstance(exSt, SpikeTrainProxy):
  1458                                                       print(
  1459                                                           'alignedAsigDFtoSpikeTrain basing seg {} on {}'
  1460                                                           .format(segIdx, exSt.name))
  1461                                                       stProxy = exSt
  1462                                                       exSt = loadStProxy(stProxy)
  1463                                                       exSt = loadObjArrayAnn(exSt)
  1464                                                   print('exSt.left_sweep is {}'.format(exSt.left_sweep))
  1465                                                   wfBins = ((np.arange(exSt.waveforms.shape[2]) / (exSt.sampling_rate)) - exSt.left_sweep).magnitude
  1466                                                   # seg to contain triggered time series
  1467                                                   newSeg = Segment(name=dataSeg.annotations['neo_name'])
  1468                                                   newSeg.annotate(nix_name=dataSeg.annotations['neo_name'])
  1469                                                   masterBlock.segments.append(newSeg)
  1470                                                   #
  1471                                                   if group.columns.name == 'bin':
  1472                                                       grouper = group.groupby('feature')
  1473                                                       colsAre = 'bin'
  1474                                                   elif group.columns.name == 'feature':
  1475                                                       grouper = group.iteritems()
  1476                                                       colsAre = 'feature'
  1477                                                   for featName, featGroup in grouper:
  1478                                                       print('Saving {}...'.format(featName))
  1479                                                       if featName[-2:] == '#0':
  1480                                                           cleanFeatName = featName
  1481                                                       else:
  1482                                                           cleanFeatName = featName + '#0'
  1483                                                       if segIdx == 0:
  1484                                                           #  allocate units
  1485                                                           chanIdx = ChannelIndex(
  1486                                                               name=cleanFeatName, index=[0])
  1487                                                           chanIdx.annotate(nix_name=chanIdx.name)
  1488                                                           thisUnit = Unit(name=chanIdx.name)
  1489                                                           thisUnit.annotate(nix_name=chanIdx.name)
  1490                                                           chanIdx.units.append(thisUnit)
  1491                                                           thisUnit.channel_index = chanIdx
  1492                                                           masterBlock.channel_indexes.append(chanIdx)
  1493                                                       else:
  1494                                                           thisUnit = masterBlock.filter(
  1495                                                               objects=Unit, name=cleanFeatName)[0]
  1496                                                       if colsAre == 'bin':
  1497                                                           spikeWaveformsDF = featGroup
  1498                                                       elif colsAre == 'feature':
  1499                                                           if isinstance(featGroup, pd.Series):
  1500                                                               featGroup = featGroup.to_frame(name=featName)
  1501                                                               featGroup.columns.name = 'feature'
  1502                                                           spikeWaveformsDF = transposeSpikeDF(
  1503                                                               featGroup,
  1504                                                               'bin', fastTranspose=True)
  1505                                                       if matchSamplingRate:
  1506                                                           if len(spikeWaveformsDF.columns) != len(wfBins):
  1507                                                               wfDF = spikeWaveformsDF.reset_index(drop=True).T
  1508                                                               wfDF = hf.interpolateDF(wfDF, wfBins)
  1509                                                               spikeWaveformsDF = wfDF.T.set_index(spikeWaveformsDF.index)
  1510                                                       spikeWaveforms = spikeWaveformsDF.to_numpy()[:, np.newaxis, :]
  1511                                                       arrAnnDF = spikeWaveformsDF.index.to_frame()
  1512                                                       spikeTimes = arrAnnDF['t']
  1513                                                       arrAnnDF.drop(columns='t', inplace=True)
  1514                                                       arrAnn = {}
  1515                                                       colsToKeep = arrAnnDF.columns.drop(['originalIndex', 'feature', 'segment', 'lag'])
  1516                                                       for cName in colsToKeep:
  1517                                                           values = arrAnnDF[cName].to_numpy()
  1518                                                           if isinstance(values[0], str):
  1519                                                               values = values.astype('U')
  1520                                                           arrAnn.update({str(cName): values.flatten()})
  1521                                                       arrayAnnNames = {
  1522                                                           'arrayAnnNames': list(arrAnn.keys())}
  1523                                                       st = SpikeTrain(
  1524                                                           name='seg{}_{}'.format(int(segIdx), thisUnit.name),
  1525                                                           times=spikeTimes.to_numpy() * exSt.units,
  1526                                                           waveforms=spikeWaveforms * pq.dimensionless,
  1527                                                           t_start=exSt.t_start, t_stop=exSt.t_stop,
  1528                                                           left_sweep=exSt.left_sweep,
  1529                                                           sampling_rate=exSt.sampling_rate,
  1530                                                           **arrAnn, **arrayAnnNames
  1531                                                           )
  1532                                                       st.annotate(nix_name=st.name)
  1533                                                       thisUnit.spiketrains.append(st)
  1534                                                       newSeg.spiketrains.append(st)
  1535                                                       st.unit = thisUnit
  1536                                               return masterBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: dataFrameToAnalogSignals at line 1538

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1538                                           @profile
  1539                                           def dataFrameToAnalogSignals(
  1540                                                   df,
  1541                                                   block=None, seg=None,
  1542                                                   idxT='NSPTime',
  1543                                                   probeName='insTD', samplingRate=500*pq.Hz,
  1544                                                   timeUnits=pq.s, measureUnits=pq.mV,
  1545                                                   dataCol=['channel_0', 'channel_1'],
  1546                                                   useColNames=False, forceColNames=None,
  1547                                                   namePrefix='', nameSuffix='', verbose=False):
  1548                                               if block is None:
  1549                                                   assert seg is None
  1550                                                   block = Block(name=probeName)
  1551                                                   seg = Segment(name='seg0_' + probeName)
  1552                                                   block.segments.append(seg)
  1553                                               if verbose:
  1554                                                   print('in dataFrameToAnalogSignals...')
  1555                                               for idx, colName in enumerate(dataCol):
  1556                                                   if verbose:
  1557                                                       print('    {}'.format(colName))
  1558                                                   if forceColNames is not None:
  1559                                                       chanName = forceColNames[idx]
  1560                                                   elif useColNames:
  1561                                                       chanName = namePrefix + colName + nameSuffix
  1562                                                   else:
  1563                                                       chanName = namePrefix + (probeName.lower() + '{}'.format(idx)) + nameSuffix
  1564                                                   #
  1565                                                   chanIdx = ChannelIndex(
  1566                                                       name=chanName,
  1567                                                       # index=None,
  1568                                                       index=np.asarray([idx]),
  1569                                                       # channel_names=np.asarray([chanName])
  1570                                                       )
  1571                                                   block.channel_indexes.append(chanIdx)
  1572                                                   asig = AnalogSignal(
  1573                                                       df[colName].to_numpy() * measureUnits,
  1574                                                       name='seg0_' + chanName,
  1575                                                       sampling_rate=samplingRate,
  1576                                                       dtype=np.float32,
  1577                                                       # **ann
  1578                                                       )
  1579                                                   if idxT is not None:
  1580                                                       asig.t_start = df[idxT].iloc[0] * timeUnits
  1581                                                   else:
  1582                                                       asig.t_start = df.index[0] * timeUnits
  1583                                                   asig.channel_index = chanIdx
  1584                                                   # assign ownership to containers
  1585                                                   chanIdx.analogsignals.append(asig)
  1586                                                   seg.analogsignals.append(asig)
  1587                                                   chanIdx.create_relationship()
  1588                                               # assign parent to children
  1589                                               block.create_relationship()
  1590                                               seg.create_relationship()
  1591                                               return block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: eventDataFrameToEvents at line 1593

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1593                                           @profile
  1594                                           def eventDataFrameToEvents(
  1595                                                   eventDF, idxT=None,
  1596                                                   annCol=None,
  1597                                                   eventName='', tUnits=pq.s,
  1598                                                   makeList=True
  1599                                                   ):
  1600                                               if makeList:
  1601                                                   eventList = []
  1602                                                   for colName in annCol:
  1603                                                       originalDType = type(eventDF[colName].to_numpy()[0]).__name__
  1604                                                       event = Event(
  1605                                                           name=eventName + colName,
  1606                                                           times=eventDF[idxT].to_numpy() * tUnits,
  1607                                                           labels=eventDF[colName].astype(originalDType).to_numpy()
  1608                                                           )
  1609                                                       event.annotate(originalDType=originalDType)
  1610                                                       eventList.append(event)
  1611                                                   return eventList
  1612                                               else:
  1613                                                   if annCol is None:
  1614                                                       annCol = eventDF.drop(columns=idxT).columns
  1615                                                   event = Event(
  1616                                                       name=eventName,
  1617                                                       times=eventDF[idxT].to_numpy() * tUnits,
  1618                                                       labels=np.asarray(eventDF.index)
  1619                                                       )
  1620                                                   event.annotations.update(
  1621                                                       {
  1622                                                           'arrayAnnNames': [],
  1623                                                           'arrayAnnDTypes': []
  1624                                                           })
  1625                                                   for colName in annCol:
  1626                                                       originalDType = type(eventDF[colName].to_numpy()[0]).__name__
  1627                                                       arrayAnn = eventDF[colName].astype(originalDType).to_numpy()
  1628                                                       event.array_annotations.update(
  1629                                                           {colName: arrayAnn})
  1630                                                       event.annotations['arrayAnnNames'].append(colName)
  1631                                                       event.annotations['arrayAnnDTypes'].append(originalDType)
  1632                                                       event.annotations.update(
  1633                                                           {colName: arrayAnn})
  1634                                                   return event

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: eventsToDataFrame at line 1636

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1636                                           @profile
  1637                                           def eventsToDataFrame(
  1638                                                   events, idxT='t', names=None
  1639                                                   ):
  1640                                               eventDict = {}
  1641                                               calculatedT = False
  1642                                               for event in events:
  1643                                                   if names is not None:
  1644                                                       if event.name not in names:
  1645                                                           continue
  1646                                                   if len(event.times):
  1647                                                       if not calculatedT:
  1648                                                           t = pd.Series(event.times.magnitude)
  1649                                                           calculatedT = True
  1650                                                       try:
  1651                                                           values = event.array_annotations['labels']
  1652                                                       except Exception:
  1653                                                           values = event.labels
  1654                                                       if isinstance(values[0], bytes):
  1655                                                           #  event came from hdf, need to recover dtype
  1656                                                           if 'originalDType' in event.annotations:
  1657                                                               dtypeStr = event.annotations['originalDType'].split(';')[-1]
  1658                                                               if 'np.' not in dtypeStr:
  1659                                                                   dtypeStr = 'np.' + dtypeStr
  1660                                                               originalDType = eval(dtypeStr)
  1661                                                               values = np.asarray(values, dtype=originalDType)
  1662                                                           else:
  1663                                                               values = np.asarray(values, dtype=np.str)
  1664                                                       #  print(values.dtype)
  1665                                                       eventDict.update({
  1666                                                           event.name: pd.Series(values)})
  1667                                               eventDict.update({idxT: t})
  1668                                               return pd.concat(eventDict, axis=1)

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadSpikeMats at line 1670

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1670                                           @profile
  1671                                           def loadSpikeMats(
  1672                                                   dataPath, rasterOpts,
  1673                                                   alignTimes=None, chans=None, loadAll=False,
  1674                                                   absoluteBins=False, transposeSpikeMat=False,
  1675                                                   checkReferences=False,
  1676                                                   aggregateFun=None):
  1677                                           
  1678                                               reader = nixio_fr.NixIO(filename=dataPath)
  1679                                               chanNames = reader.header['signal_channels']['name']
  1680                                               
  1681                                               if chans is not None:
  1682                                                   sigMask = np.isin(chanNames, chans)
  1683                                                   chanNames = chanNames[sigMask]
  1684                                                   
  1685                                               chanIdx = reader.channel_name_to_index(chanNames)
  1686                                               
  1687                                               if not loadAll:
  1688                                                   assert alignTimes is not None
  1689                                                   spikeMats = {i: None for i in alignTimes.index}
  1690                                                   validTrials = pd.Series(True, index=alignTimes.index)
  1691                                               else:
  1692                                                   spikeMats = {
  1693                                                       i: None for i in range(reader.segment_count(block_index=0))}
  1694                                                   validTrials = None
  1695                                               
  1696                                               for segIdx in range(reader.segment_count(block_index=0)):
  1697                                                   if checkReferences:
  1698                                                       for i, cIdx in enumerate(chanIdx):
  1699                                                           da = reader.da_list['blocks'][0]['segments'][segIdx]['data'][cIdx]
  1700                                                           print('name {}, da.name {}'.format(chanNames[i], da.name))
  1701                                                           try:
  1702                                                               assert chanNames[i] in da.name, 'reference problem!!'
  1703                                                           except Exception:
  1704                                                               traceback.print_exc()
  1705                                                   tStart = reader.get_signal_t_start(
  1706                                                       block_index=0, seg_index=segIdx)
  1707                                                   fs = reader.get_signal_sampling_rate(
  1708                                                       channel_indexes=chanIdx
  1709                                                       )
  1710                                                   sigSize = reader.get_signal_size(
  1711                                                       block_index=0, seg_index=segIdx
  1712                                                       )
  1713                                                   tStop = sigSize / fs + tStart
  1714                                                   #  convert to indices early to avoid floating point problems
  1715                                                   
  1716                                                   intervalIdx = int(round(rasterOpts['binInterval'] * fs))
  1717                                                   #  halfIntervalIdx = int(round(intervalIdx / 2))
  1718                                                   
  1719                                                   widthIdx = int(round(rasterOpts['binWidth'] * fs))
  1720                                                   halfWidthIdx = int(round(widthIdx / 2))
  1721                                                   
  1722                                                   if rasterOpts['smoothKernelWidth'] is not None:
  1723                                                       kernWidthIdx = int(round(rasterOpts['smoothKernelWidth'] * fs))
  1724                                                   
  1725                                                   theBins = None
  1726                                           
  1727                                                   if not loadAll:
  1728                                                       winStartIdx = int(round(rasterOpts['windowSize'][0] * fs))
  1729                                                       winStopIdx = int(round(rasterOpts['windowSize'][1] * fs))
  1730                                                       timeMask = (alignTimes > tStart) & (alignTimes < tStop)
  1731                                                       maskedTimes = alignTimes[timeMask]
  1732                                                   else:
  1733                                                       #  irrelevant, will load all
  1734                                                       maskedTimes = pd.Series(np.nan)
  1735                                           
  1736                                                   for idx, tOnset in maskedTimes.iteritems():
  1737                                                       if not loadAll:
  1738                                                           idxOnset = int(round((tOnset - tStart) * fs))
  1739                                                           #  can't not be ints
  1740                                                           iStart = idxOnset + winStartIdx - int(3 * halfWidthIdx)
  1741                                                           iStop = idxOnset + winStopIdx + int(3 * halfWidthIdx)
  1742                                                       else:
  1743                                                           winStartIdx = 0
  1744                                                           iStart = 0
  1745                                                           iStop = sigSize
  1746                                           
  1747                                                       if iStart < 0:
  1748                                                           #  near the first edge
  1749                                                           validTrials[idx] = False
  1750                                                       elif (sigSize < iStop):
  1751                                                           #  near the ending edge
  1752                                                           validTrials[idx] = False
  1753                                                       else:
  1754                                                           #  valid slices
  1755                                                           try:
  1756                                                               rawSpikeMat = pd.DataFrame(
  1757                                                                   reader.get_analogsignal_chunk(
  1758                                                                       block_index=0, seg_index=segIdx,
  1759                                                                       i_start=iStart, i_stop=iStop,
  1760                                                                       channel_names=chanNames))
  1761                                                           except Exception:
  1762                                                               traceback.print_exc()
  1763                                                               #
  1764                                                           if aggregateFun is None:
  1765                                                               procSpikeMat = rawSpikeMat.rolling(
  1766                                                                   window=3 * widthIdx, center=True,
  1767                                                                   win_type='gaussian'
  1768                                                                   ).mean(std=halfWidthIdx)
  1769                                                           else:
  1770                                                               procSpikeMat = rawSpikeMat.rolling(
  1771                                                                   window=widthIdx, center=True
  1772                                                                   ).apply(
  1773                                                                       aggregateFun,
  1774                                                                       raw=True,
  1775                                                                       kwargs={'fs': fs, 'nSamp': widthIdx})
  1776                                                           #
  1777                                                           if rasterOpts['smoothKernelWidth'] is not None:
  1778                                                               procSpikeMat = (
  1779                                                                   procSpikeMat
  1780                                                                   .rolling(
  1781                                                                       window=3 * kernWidthIdx, center=True,
  1782                                                                       win_type='gaussian')
  1783                                                                   .mean(std=kernWidthIdx/2)
  1784                                                                   .dropna().iloc[::intervalIdx, :]
  1785                                                               )
  1786                                                           else:
  1787                                                               procSpikeMat = (
  1788                                                                   procSpikeMat
  1789                                                                   .dropna().iloc[::intervalIdx, :]
  1790                                                               )
  1791                                           
  1792                                                           procSpikeMat.columns = chanNames
  1793                                                           procSpikeMat.columns.name = 'unit'
  1794                                                           if theBins is None:
  1795                                                               theBins = np.asarray(
  1796                                                                   procSpikeMat.index + winStartIdx) / fs
  1797                                                           if absoluteBins:
  1798                                                               procSpikeMat.index = theBins + idxOnset / fs
  1799                                                           else:
  1800                                                               procSpikeMat.index = theBins
  1801                                                           procSpikeMat.index.name = 'bin'
  1802                                                           if loadAll:
  1803                                                               smIdx = segIdx
  1804                                                           else:
  1805                                                               smIdx = idx
  1806                                                               
  1807                                                           spikeMats[smIdx] = procSpikeMat
  1808                                                           if transposeSpikeMat:
  1809                                                               spikeMats[smIdx] = spikeMats[smIdx].transpose()
  1810                                                       #  plt.imshow(rawSpikeMat.to_numpy(), aspect='equal'); plt.show()
  1811                                               return spikeMats, validTrials

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: synchronizeINStoNSP at line 1813

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1813                                           @profile
  1814                                           def synchronizeINStoNSP(
  1815                                                   tapTimestampsNSP=None, tapTimestampsINS=None,
  1816                                                   precalculatedFun=None,
  1817                                                   NSPTimeRanges=(None, None),
  1818                                                   td=None, accel=None, insBlock=None, trialSegment=None, degree=1,
  1819                                                   trimSpiketrains=False
  1820                                                   ):
  1821                                               print('Trial Segment {}'.format(trialSegment))
  1822                                               if precalculatedFun is None:
  1823                                                   assert ((tapTimestampsNSP is not None) & (tapTimestampsINS is not None))
  1824                                                   # sanity check that the intervals match
  1825                                                   insDiff = tapTimestampsINS.diff().dropna().values
  1826                                                   nspDiff = tapTimestampsNSP.diff().dropna().values
  1827                                                   print('On the INS, the diff() between taps was\n{}'.format(insDiff))
  1828                                                   print('On the NSP, the diff() between taps was\n{}'.format(nspDiff))
  1829                                                   print('This amounts to a msec difference of\n{}'.format(
  1830                                                       (insDiff - nspDiff) * 1e3))
  1831                                                   if (insDiff - nspDiff > 20e-3).any():
  1832                                                       raise(Exception('Tap trains too different!'))
  1833                                                   #
  1834                                                   if degree > 0:
  1835                                                       synchPolyCoeffsINStoNSP = np.polyfit(
  1836                                                           x=tapTimestampsINS.values, y=tapTimestampsNSP.values,
  1837                                                           deg=degree)
  1838                                                   else:
  1839                                                       timeOffset = tapTimestampsNSP.values - tapTimestampsINS.values
  1840                                                       synchPolyCoeffsINStoNSP = np.array([1, np.mean(timeOffset)])
  1841                                                   timeInterpFunINStoNSP = np.poly1d(synchPolyCoeffsINStoNSP)
  1842                                               else:
  1843                                                   timeInterpFunINStoNSP = precalculatedFun
  1844                                               if td is not None:
  1845                                                   td.loc[:, 'NSPTime'] = pd.Series(
  1846                                                       timeInterpFunINStoNSP(td['t']), index=td['t'].index)
  1847                                                   td.loc[:, 'NSPTime'] = timeInterpFunINStoNSP(td['t'].to_numpy())
  1848                                               if accel is not None:
  1849                                                   accel.loc[:, 'NSPTime'] = pd.Series(
  1850                                                       timeInterpFunINStoNSP(accel['t']), index=accel['t'].index)
  1851                                               if insBlock is not None:
  1852                                                   # allUnits = [st.unit for st in insBlock.segments[0].spiketrains]
  1853                                                   # [un.name for un in insBlock.filter(objects=Unit)]
  1854                                                   for unit in insBlock.filter(objects=Unit):
  1855                                                       tStart = NSPTimeRanges[0]
  1856                                                       tStop = NSPTimeRanges[1]
  1857                                                       uniqueSt = []
  1858                                                       for st in unit.spiketrains:
  1859                                                           if st not in uniqueSt:
  1860                                                               uniqueSt.append(st)
  1861                                                           else:
  1862                                                               continue
  1863                                                           print('Synchronizing {}'.format(st.name))
  1864                                                           if len(st.times):
  1865                                                               segMaskSt = np.array(
  1866                                                                   st.array_annotations['trialSegment'],
  1867                                                                   dtype=np.int) == trialSegment
  1868                                                               st.magnitude[segMaskSt] = (
  1869                                                                   timeInterpFunINStoNSP(st.times[segMaskSt].magnitude))
  1870                                                               if trimSpiketrains:
  1871                                                                   print('Trimming spiketrain')
  1872                                                                   #  kludgey fix for weirdness concerning t_start
  1873                                                                   st.t_start = min(tStart, st.times[0] * 0.999)
  1874                                                                   st.t_stop = min(tStop, st.times[-1] * 1.001)
  1875                                                                   validMask = st < st.t_stop
  1876                                                                   if ~validMask.all():
  1877                                                                       print('Deleted some spikes')
  1878                                                                       st = st[validMask]
  1879                                                                       # delete invalid spikes
  1880                                                                       if 'arrayAnnNames' in st.annotations.keys():
  1881                                                                           for key in st.annotations['arrayAnnNames']:
  1882                                                                               try:
  1883                                                                                   # st.annotations[key] = np.array(st.array_annotations[key])
  1884                                                                                   st.annotations[key] = np.delete(st.annotations[key], ~validMask)
  1885                                                                               except Exception:
  1886                                                                                   traceback.print_exc()
  1887                                                                                   pdb.set_trace()
  1888                                                           else:
  1889                                                               if trimSpiketrains:
  1890                                                                   st.t_start = tStart
  1891                                                                   st.t_stop = tStop
  1892                                                   #
  1893                                                   allEvents = [
  1894                                                       ev
  1895                                                       for ev in insBlock.filter(objects=Event)
  1896                                                       if ('ins' in ev.name) and ('concatenate' not in ev.name)]
  1897                                                   concatEvents = [
  1898                                                       ev
  1899                                                       for ev in insBlock.filter(objects=Event)
  1900                                                       if ('ins' in ev.name) and ('concatenate' in ev.name)]
  1901                                                   eventsDF = eventsToDataFrame(allEvents, idxT='t')
  1902                                                   newNames = {i: childBaseName(i, 'seg') for i in eventsDF.columns}
  1903                                                   eventsDF.rename(columns=newNames, inplace=True)
  1904                                                   segMask = hf.getStimSerialTrialSegMask(eventsDF, trialSegment)
  1905                                                   evTStart = eventsDF.loc[segMask, 't'].min() * pq.s
  1906                                                   evTStop = eventsDF.loc[segMask, 't'].max() * pq.s
  1907                                                   # print('allEvents[0].shape = {}'.format(allEvents[0].shape))
  1908                                                   # print('allEvents[0].magnitude[segMask][0] = {}'.format(allEvents[0].magnitude[segMask][0]))
  1909                                                   for event in (allEvents + concatEvents):
  1910                                                       if trimSpiketrains:
  1911                                                           thisSegMask = (event.times >= evTStart) & (event.times <= evTStop)
  1912                                                       else:
  1913                                                           thisSegMask = (event.times >= evTStart) & (event.times < evTStop)
  1914                                                       event.magnitude[thisSegMask] = (
  1915                                                           timeInterpFunINStoNSP(event.times[thisSegMask].magnitude))
  1916                                                   # print('allEvents[0].magnitude[segMask][0] = {}'.format(allEvents[0].magnitude[segMask][0]))
  1917                                                   # if len(concatEvents) > trialSegment:
  1918                                                   #     concatEvents[trialSegment].magnitude[:] = timeInterpFunINStoNSP(
  1919                                                   #         concatEvents[trialSegment].times[:].magnitude)
  1920                                               return td, accel, insBlock, timeInterpFunINStoNSP

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: findSegsIncluding at line 1922

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1922                                           @profile
  1923                                           def findSegsIncluding(
  1924                                                   block, timeSlice=None):
  1925                                               segBoundsList = []
  1926                                               for segIdx, seg in enumerate(block.segments):
  1927                                                   segBoundsList.append(pd.DataFrame({
  1928                                                       't_start': seg.t_start,
  1929                                                       't_stop': seg.t_stop
  1930                                                       }, index=[segIdx]))
  1931                                           
  1932                                               segBounds = pd.concat(segBoundsList)
  1933                                               if timeSlice[0] is not None:
  1934                                                   segMask = (segBounds['t_start'] * pq.s >= timeSlice[0]) & (
  1935                                                       segBounds['t_stop'] * pq.s <= timeSlice[1])
  1936                                                   requestedSegs = segBounds.loc[segMask, :]
  1937                                               else:
  1938                                                   timeSlice = (None, None)
  1939                                                   requestedSegs = segBounds
  1940                                               return segBounds, requestedSegs

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: findSegsIncluded at line 1942

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1942                                           @profile
  1943                                           def findSegsIncluded(
  1944                                                   block, timeSlice=None):
  1945                                               segBoundsList = []
  1946                                               for segIdx, seg in enumerate(block.segments):
  1947                                                   segBoundsList.append(pd.DataFrame({
  1948                                                       't_start': seg.t_start,
  1949                                                       't_stop': seg.t_stop
  1950                                                       }, index=[segIdx]))
  1951                                           
  1952                                               segBounds = pd.concat(segBoundsList)
  1953                                               if timeSlice[0] is not None:
  1954                                                   segMask = (segBounds['t_start'] * pq.s <= timeSlice[0]) | (
  1955                                                       segBounds['t_stop'] * pq.s >= timeSlice[1])
  1956                                                   requestedSegs = segBounds.loc[segMask, :]
  1957                                               else:
  1958                                                   timeSlice = (None, None)
  1959                                                   requestedSegs = segBounds
  1960                                               return segBounds, requestedSegs

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getElecLookupTable at line 1962

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1962                                           @profile
  1963                                           def getElecLookupTable(
  1964                                                   block, elecIds=None):
  1965                                               lookupTableList = []
  1966                                               for metaIdx, chanIdx in enumerate(block.channel_indexes):
  1967                                                   if chanIdx.analogsignals:
  1968                                                       #  print(chanIdx.name)
  1969                                                       lookupTableList.append(pd.DataFrame({
  1970                                                           'channelNames': np.asarray(chanIdx.channel_names, dtype=np.str),
  1971                                                           'index': chanIdx.index,
  1972                                                           'metaIndex': metaIdx * chanIdx.index**0,
  1973                                                           'localIndex': (
  1974                                                               list(range(chanIdx.analogsignals[0].shape[1])))
  1975                                                           }))
  1976                                               lookupTable = pd.concat(lookupTableList, ignore_index=True)
  1977                                           
  1978                                               if elecIds is None:
  1979                                                   requestedIndices = lookupTable
  1980                                               else:
  1981                                                   if isinstance(elecIds[0], str):
  1982                                                       idxMask = lookupTable['channelNames'].isin(elecIds)
  1983                                                       requestedIndices = lookupTable.loc[idxMask, :]
  1984                                               return lookupTable, requestedIndices

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getNIXData at line 1986

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1986                                           @profile
  1987                                           def getNIXData(
  1988                                                   fileName=None,
  1989                                                   folderPath=None,
  1990                                                   reader=None, blockIdx=0,
  1991                                                   elecIds=None, startTime_s=None,
  1992                                                   dataLength_s=None, downsample=1,
  1993                                                   signal_group_mode='group-by-same-units',
  1994                                                   closeReader=False):
  1995                                               #  Open file and extract headers
  1996                                               if reader is None:
  1997                                                   assert (fileName is not None) and (folderPath is not None)
  1998                                                   filePath = os.path.join(folderPath, fileName) + '.nix'
  1999                                                   reader = nixio_fr.NixIO(filename=filePath)
  2000                                           
  2001                                               block = reader.read_block(
  2002                                                   block_index=blockIdx, lazy=True,
  2003                                                   signal_group_mode=signal_group_mode)
  2004                                           
  2005                                               for segIdx, seg in enumerate(block.segments):
  2006                                                   seg.events = [i.load() for i in seg.events]
  2007                                                   seg.epochs = [i.load() for i in seg.epochs]
  2008                                           
  2009                                               # find elecIds
  2010                                               lookupTable, requestedIndices = getElecLookupTable(
  2011                                                   block, elecIds=elecIds)
  2012                                           
  2013                                               # find segments that contain the requested times
  2014                                               if dataLength_s is not None:
  2015                                                   assert startTime_s is not None
  2016                                                   timeSlice = (
  2017                                                       startTime_s * pq.s,
  2018                                                       (startTime_s + dataLength_s) * pq.s)
  2019                                               else:
  2020                                                   timeSlice = (None, None)
  2021                                               segBounds, requestedSegs = findSegsIncluding(block, timeSlice)
  2022                                               #
  2023                                               data = pd.DataFrame(columns=elecIds + ['t'])
  2024                                               for segIdx in requestedSegs.index:
  2025                                                   seg = block.segments[segIdx]
  2026                                                   if dataLength_s is not None:
  2027                                                       timeSlice = (
  2028                                                           max(timeSlice[0], seg.t_start),
  2029                                                           min(timeSlice[1], seg.t_stop)
  2030                                                           )
  2031                                                   else:
  2032                                                       timeSlice = (seg.t_start, seg.t_stop)
  2033                                                   segData = pd.DataFrame()
  2034                                                   for metaIdx in pd.unique(requestedIndices['metaIndex']):
  2035                                                       metaIdxMatch = requestedIndices['metaIndex'] == metaIdx
  2036                                                       theseRequestedIndices = requestedIndices.loc[
  2037                                                           metaIdxMatch, :]
  2038                                                       theseElecIds = theseRequestedIndices['channelNames']
  2039                                                       asig = seg.analogsignals[metaIdx]
  2040                                                       thisTimeSlice = (
  2041                                                           max(timeSlice[0], asig.t_start),
  2042                                                           min(timeSlice[1], asig.t_stop)
  2043                                                           )
  2044                                                       reqData = asig.load(
  2045                                                           time_slice=thisTimeSlice,
  2046                                                           channel_indexes=theseRequestedIndices['localIndex'].to_numpy())
  2047                                                       segData = pd.concat((
  2048                                                               segData,
  2049                                                               pd.DataFrame(
  2050                                                                   reqData.magnitude, columns=theseElecIds.to_numpy())),
  2051                                                           axis=1)
  2052                                                   segT = reqData.times
  2053                                                   segData['t'] = segT
  2054                                                   data = pd.concat(
  2055                                                       (data, segData),
  2056                                                       axis=0, ignore_index=True)
  2057                                               channelData = {
  2058                                                   'data': data,
  2059                                                   't': data['t']
  2060                                                   }
  2061                                               if closeReader:
  2062                                                   reader.file.close()
  2063                                                   block = None
  2064                                                   # closing the reader breaks its connection to the block
  2065                                               return channelData, block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: childBaseName at line 2067

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2067                                           @profile
  2068                                           def childBaseName(
  2069                                                   childName, searchTerm):
  2070                                               if searchTerm in childName:
  2071                                                   baseName = '_'.join(childName.split('_')[1:])
  2072                                               else:
  2073                                                   baseName = childName
  2074                                               return baseName

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: readBlockFixNames at line 2076

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2076                                           @profile
  2077                                           def readBlockFixNames(
  2078                                                   rawioReader,
  2079                                                   block_index=0, signal_group_mode='split-all',
  2080                                                   lazy=True, mapDF=None, reduceChannelIndexes=False,
  2081                                                   loadList=None, purgeNixNames=False
  2082                                                   ):
  2083                                               headerSignalChan = pd.DataFrame(
  2084                                                   rawioReader.header['signal_channels']).set_index('id')
  2085                                               headerUnitChan = pd.DataFrame(
  2086                                                   rawioReader.header['unit_channels']).set_index('id')
  2087                                               dataBlock = rawioReader.read_block(
  2088                                                   block_index=block_index, lazy=lazy,
  2089                                                   signal_group_mode=signal_group_mode)
  2090                                               if dataBlock.name is None:
  2091                                                   if 'neo_name' in dataBlock.annotations:
  2092                                                       dataBlock.name = dataBlock.annotations['neo_name']
  2093                                               #  on first segment, rename the chan_indexes and units
  2094                                               seg0 = dataBlock.segments[0]
  2095                                               asigLikeList = (
  2096                                                   seg0.filter(objects=AnalogSignalProxy) +
  2097                                                   seg0.filter(objects=AnalogSignal))
  2098                                               if mapDF is not None:
  2099                                                   if headerSignalChan.size > 0:
  2100                                                       asigNameChanger = {}
  2101                                                       for nevID in mapDF['nevID']:
  2102                                                           if int(nevID) in headerSignalChan.index:
  2103                                                               labelFromMap = (
  2104                                                                   mapDF
  2105                                                                   .loc[mapDF['nevID'] == nevID, 'label']
  2106                                                                   .iloc[0])
  2107                                                               asigNameChanger[
  2108                                                                   headerSignalChan.loc[int(nevID), 'name']] = labelFromMap
  2109                                                   else:
  2110                                                       asigOrigNames = np.unique(
  2111                                                           [i.split('#')[0] for i in headerUnitChan['name']])
  2112                                                       asigNameChanger = {}
  2113                                                       for origName in asigOrigNames:
  2114                                                           # ripple specific
  2115                                                           formattedName = origName.replace('.', '_').replace(' raw', '')
  2116                                                           if mapDF['label'].str.contains(formattedName).any():
  2117                                                               asigNameChanger[origName] = formattedName
  2118                                               else:
  2119                                                   asigNameChanger = dict()
  2120                                               for asig in asigLikeList:
  2121                                                   asigBaseName = childBaseName(asig.name, 'seg')
  2122                                                   asig.name = (
  2123                                                       asigNameChanger[asigBaseName]
  2124                                                       if asigBaseName in asigNameChanger
  2125                                                       else asigBaseName)
  2126                                                   if mapDF is not None:
  2127                                                       if (mapDF['label'] == asig.name).any():
  2128                                                           asig.annotations['xCoords'] = float(mapDF.loc[mapDF['label'] == asig.name, 'xcoords'].iloc[0])
  2129                                                           asig.annotations['yCoords'] = float(mapDF.loc[mapDF['label'] == asig.name, 'ycoords'].iloc[0])
  2130                                                           asig.annotations['zCoords'] = float(mapDF.loc[mapDF['label'] == asig.name, 'zcoords'].iloc[0])
  2131                                                   if 'Channel group ' in asig.channel_index.name:
  2132                                                       newChanName = (
  2133                                                           asigNameChanger[asigBaseName]
  2134                                                           if asigBaseName in asigNameChanger
  2135                                                           else asigBaseName)
  2136                                                       asig.channel_index.name = newChanName
  2137                                                       if 'neo_name' in asig.channel_index.annotations:
  2138                                                           asig.channel_index.annotations['neo_name'] = newChanName
  2139                                                       if 'nix_name' in asig.channel_index.annotations:
  2140                                                           asig.channel_index.annotations['nix_name'] = newChanName
  2141                                                       if mapDF is not None:
  2142                                                           try:
  2143                                                               asig.channel_index.coordinates = np.asarray([
  2144                                                                   asig.annotations['xCoords'], asig.annotations['yCoords'], asig.annotations['zCoords']
  2145                                                               ])[np.newaxis, :] * pq.um
  2146                                                           except Exception:
  2147                                                               pass
  2148                                               spikeTrainLikeList = (
  2149                                                   seg0.filter(objects=SpikeTrainProxy) +
  2150                                                   seg0.filter(objects=SpikeTrain))
  2151                                               # add channels for channelIndex that has no asigs but has spikes
  2152                                               nExtraChans = 0
  2153                                               for stp in spikeTrainLikeList:
  2154                                                   stpBaseName = childBaseName(stp.name, 'seg')
  2155                                                   nameParser = re.search(r'ch(\d*)#(\d*)', stpBaseName)
  2156                                                   if nameParser is not None:
  2157                                                       # first time at this unit, rename it
  2158                                                       chanId = int(nameParser.group(1))
  2159                                                       unitId = int(nameParser.group(2))
  2160                                                       if chanId >= 5121:
  2161                                                           isRippleStimChan = True
  2162                                                           chanId = chanId - 5120
  2163                                                       else:
  2164                                                           isRippleStimChan = False
  2165                                                       ####################
  2166                                                       # asigBaseName = headerSignalChan.loc[chanId, 'name']
  2167                                                       # if mapDF is not None:
  2168                                                       #     if asigBaseName in asigNameChanger:
  2169                                                       #         chanIdLabel = (
  2170                                                       #             asigNameChanger[asigBaseName]
  2171                                                       #             if asigBaseName in asigNameChanger
  2172                                                       #             else asigBaseName)
  2173                                                       #     else:
  2174                                                       #         chanIdLabel = asigBaseName
  2175                                                       # else:
  2176                                                       #     chanIdLabel = asigBaseName
  2177                                                       ###################
  2178                                                       # if swapMaps is not None:
  2179                                                       #     nameCandidates = (swapMaps['to'].loc[swapMaps['to']['nevID'] == chanId, 'label']).to_list()
  2180                                                       # elif mapDF is not None:
  2181                                                       #     nameCandidates = (mapDF.loc[mapDF['nevID'] == chanId, 'label']).to_list()
  2182                                                       # else:
  2183                                                       #     nameCandidates = []
  2184                                                       ##############################
  2185                                                       if mapDF is not None:
  2186                                                           nameCandidates = (
  2187                                                               mapDF
  2188                                                               .loc[mapDF['nevID'] == chanId, 'label']
  2189                                                               .to_list())
  2190                                                       else:
  2191                                                           nameCandidates = []
  2192                                                       if len(nameCandidates) == 1:
  2193                                                           chanIdLabel = nameCandidates[0]
  2194                                                       elif chanId in headerSignalChan:
  2195                                                           chanIdLabel = headerSignalChan.loc[chanId, 'name']
  2196                                                       else:
  2197                                                           chanIdLabel = 'ch{}'.format(chanId)
  2198                                                       #
  2199                                                       if isRippleStimChan:
  2200                                                           stp.name = '{}_stim#{}'.format(chanIdLabel, unitId)
  2201                                                       else:
  2202                                                           stp.name = '{}#{}'.format(chanIdLabel, unitId)
  2203                                                       stp.unit.name = stp.name
  2204                                                   ########################################
  2205                                                   # sanitize ripple names ####
  2206                                                   stp.name = stp.name.replace('.', '_').replace(' raw', '')
  2207                                                   stp.unit.name = stp.unit.name.replace('.', '_').replace(' raw', '')
  2208                                                   ###########################################
  2209                                                   if 'ChannelIndex for ' in stp.unit.channel_index.name:
  2210                                                       newChanName = stp.name.replace('_stim#0', '')
  2211                                                       # remove unit #
  2212                                                       newChanName = re.sub(r'#\d', '', newChanName)
  2213                                                       stp.unit.channel_index.name = newChanName
  2214                                                       # units and analogsignals have different channel_indexes when loaded by nix
  2215                                                       # add them to each other's parent list
  2216                                                       allMatchingChIdx = dataBlock.filter(
  2217                                                           objects=ChannelIndex, name=newChanName)
  2218                                                       if (len(allMatchingChIdx) > 1) and reduceChannelIndexes:
  2219                                                           assert len(allMatchingChIdx) == 2
  2220                                                           targetChIdx = [
  2221                                                               ch
  2222                                                               for ch in allMatchingChIdx
  2223                                                               if ch is not stp.unit.channel_index][0]
  2224                                                           oldChIdx = stp.unit.channel_index
  2225                                                           targetChIdx.units.append(stp.unit)
  2226                                                           stp.unit.channel_index = targetChIdx
  2227                                                           oldChIdx.units.remove(stp.unit)
  2228                                                           if not (len(oldChIdx.units) or len(oldChIdx.analogsignals)):
  2229                                                               dataBlock.channel_indexes.remove(oldChIdx)
  2230                                                           del oldChIdx
  2231                                                           targetChIdx.create_relationship()
  2232                                                       elif reduceChannelIndexes:
  2233                                                           if newChanName not in headerSignalChan['name']:
  2234                                                               stp.unit.channel_index.index = np.asarray(
  2235                                                                   [headerSignalChan['name'].size + nExtraChans])
  2236                                                               stp.unit.channel_index.channel_ids = np.asarray(
  2237                                                                   [headerSignalChan['name'].size + nExtraChans])
  2238                                                               stp.unit.channel_index.channel_names = np.asarray(
  2239                                                                   [newChanName])
  2240                                                               nExtraChans += 1
  2241                                                           if 'neo_name' not in allMatchingChIdx[0].annotations:
  2242                                                               allMatchingChIdx[0].annotations['neo_name'] = allMatchingChIdx[0].name
  2243                                                           if 'nix_name' not in allMatchingChIdx[0].annotations:
  2244                                                               allMatchingChIdx[0].annotations['nix_name'] = allMatchingChIdx[0].name
  2245                                                   stp.unit.channel_index.name = stp.unit.channel_index.name.replace('.', '_').replace(' raw', '')
  2246                                               #  rename the children
  2247                                               typesNeedRenaming = [
  2248                                                   SpikeTrainProxy, AnalogSignalProxy, EventProxy,
  2249                                                   SpikeTrain, AnalogSignal, Event]
  2250                                               for segIdx, seg in enumerate(dataBlock.segments):
  2251                                                   if seg.name is None:
  2252                                                       seg.name = 'seg{}_'.format(segIdx)
  2253                                                   else:
  2254                                                       if 'seg{}_'.format(segIdx) not in seg.name:
  2255                                                           seg.name = (
  2256                                                               'seg{}_{}'
  2257                                                               .format(
  2258                                                                   segIdx,
  2259                                                                   childBaseName(seg.name, 'seg')))
  2260                                                   for objType in typesNeedRenaming:
  2261                                                       for child in seg.filter(objects=objType):
  2262                                                           if 'seg{}_'.format(segIdx) not in child.name:
  2263                                                               child.name = (
  2264                                                                   'seg{}_{}'
  2265                                                                   .format(
  2266                                                                       segIdx, childBaseName(child.name, 'seg')))
  2267                                                           #  todo: decide if below is needed
  2268                                                           #  elif 'seg' in child.name:
  2269                                                           #      childBaseName = '_'.join(child.name.split('_')[1:])
  2270                                                           #      child.name = 'seg{}_{}'.format(segIdx, childBaseName)
  2271                                               # [i.name for i in dataBlock.filter(objects=Unit)]
  2272                                               # [i.name for i in dataBlock.filter(objects=ChannelIndex)]
  2273                                               # [i.name for i in dataBlock.filter(objects=SpikeTrain)]
  2274                                               # [i.name for i in dataBlock.filter(objects=SpikeTrainProxy)]
  2275                                               if lazy:
  2276                                                   for stP in dataBlock.filter(objects=SpikeTrainProxy):
  2277                                                       if 'unitAnnotations' in stP.annotations:
  2278                                                           unAnnStr = stP.annotations['unitAnnotations']
  2279                                                           stP.unit.annotations.update(json.loads(unAnnStr))
  2280                                               if (loadList is not None) and lazy:
  2281                                                   if 'asigs' in loadList:
  2282                                                       loadAsigList(
  2283                                                           dataBlock, listOfAsigProxyNames=loadList['asigs'],
  2284                                                           replaceInParents=True)
  2285                                                   if 'events' in loadList:
  2286                                                       loadEventList(
  2287                                                           dataBlock,
  2288                                                           listOfEventNames=loadList['events'],
  2289                                                           replaceInParents=True)
  2290                                                   if 'spiketrains' in loadList:
  2291                                                       loadSpikeTrainList(
  2292                                                           dataBlock,
  2293                                                           listOfSpikeTrainNames=loadList['spiketrains'],
  2294                                                           replaceInParents=True)
  2295                                               if purgeNixNames:
  2296                                                   dataBlock = purgeNixAnn(dataBlock)
  2297                                               return dataBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadSpikeTrainList at line 2299

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2299                                           @profile
  2300                                           def loadSpikeTrainList(
  2301                                                   dataBlock, listOfSpikeTrainNames=None,
  2302                                                   replaceInParents=True):
  2303                                               listOfSpikeTrains = []
  2304                                               if listOfSpikeTrainNames is None:
  2305                                                   listOfSpikeTrainNames = [
  2306                                                       stp.name
  2307                                                       for stp in dataBlock.filter(objects=SpikeTrainProxy)]
  2308                                               for stP in dataBlock.filter(objects=SpikeTrainProxy):
  2309                                                   if stP.name in listOfSpikeTrainNames:
  2310                                                       st = loadObjArrayAnn(stP.load())
  2311                                                       listOfSpikeTrains.append(st)
  2312                                                       if replaceInParents:
  2313                                                           seg = stP.segment
  2314                                                           segStNames = [s.name for s in seg.spiketrains]
  2315                                                           idxInSeg = segStNames.index(stP.name)
  2316                                                           seg.spiketrains[idxInSeg] = st
  2317                                                           #
  2318                                                           unit = stP.unit
  2319                                                           unitStNames = [s.name for s in unit.spiketrains]
  2320                                                           st.unit = unit
  2321                                                           idxInUnit = unitStNames.index(stP.name)
  2322                                                           unit.spiketrains[idxInUnit] = st
  2323                                               return listOfSpikeTrains

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadEventList at line 2325

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2325                                           @profile
  2326                                           def loadEventList(
  2327                                                   dataBlock,
  2328                                                   listOfEventNames=None, replaceInParents=True):
  2329                                               listOfEvents = []
  2330                                               if listOfEventNames is None:
  2331                                                   listOfEventNames = [
  2332                                                       evp.name
  2333                                                       for evp in dataBlock.filter(objects=EventProxy)]
  2334                                               for evP in dataBlock.filter(objects=EventProxy):
  2335                                                   if evP.name in listOfEventNames:
  2336                                                       ev = loadObjArrayAnn(evP.load())
  2337                                                       listOfEvents.append(ev)
  2338                                                       if replaceInParents:
  2339                                                           seg = evP.segment
  2340                                                           segEvNames = [e.name for e in seg.events]
  2341                                                           idxInSeg = segEvNames.index(evP.name)
  2342                                                           seg.events[idxInSeg] = ev
  2343                                               return listOfEvents

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadAsigList at line 2345

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2345                                           @profile
  2346                                           def loadAsigList(
  2347                                                   dataBlock, listOfAsigProxyNames=None, replaceInParents=True):
  2348                                               listOfAsigs = []
  2349                                               if listOfAsigProxyNames is None:
  2350                                                   listOfAsigProxyNames = [
  2351                                                       asigp.name
  2352                                                       for asigp in dataBlock.filter(objects=AnalogSignalProxy)]
  2353                                               for asigP in dataBlock.filter(objects=AnalogSignalProxy):
  2354                                                   if asigP.name in listOfAsigProxyNames:
  2355                                                       asig = asigP.load()
  2356                                                       asig.annotations = asigP.annotations.copy()
  2357                                                       listOfAsigs.append(asig)
  2358                                                       #
  2359                                                       if replaceInParents:
  2360                                                           seg = asigP.segment
  2361                                                           segAsigNames = [ag.name for ag in seg.analogsignals]
  2362                                                           asig.segment = seg
  2363                                                           idxInSeg = segAsigNames.index(asigP.name)
  2364                                                           seg.analogsignals[idxInSeg] = asig
  2365                                                           #
  2366                                                           chIdx = asigP.channel_index
  2367                                                           chIdxAsigNames = [ag.name for ag in chIdx.analogsignals]
  2368                                                           asig.channel_index = chIdx
  2369                                                           idxInChIdx = chIdxAsigNames.index(asigP.name)
  2370                                                           chIdx.analogsignals[idxInChIdx] = asig
  2371                                               return listOfAsigs

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: addBlockToNIX at line 2373

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2373                                           @profile
  2374                                           def addBlockToNIX(
  2375                                                   newBlock, neoSegIdx=[0],
  2376                                                   writeAsigs=True, writeSpikes=True, writeEvents=True,
  2377                                                   asigNameList=None,
  2378                                                   purgeNixNames=False,
  2379                                                   fileName=None,
  2380                                                   folderPath=None,
  2381                                                   nixBlockIdx=0, nixSegIdx=[0],
  2382                                                   ):
  2383                                               #  base file name
  2384                                               trialBasePath = os.path.join(folderPath, fileName)
  2385                                               if writeAsigs:
  2386                                                   # peek at file to ensure compatibility
  2387                                                   reader = nixio_fr.NixIO(filename=trialBasePath + '.nix')
  2388                                                   tempBlock = reader.read_block(
  2389                                                       block_index=nixBlockIdx,
  2390                                                       lazy=True, signal_group_mode='split-all')
  2391                                                   checkCompatible = {i: False for i in nixSegIdx}
  2392                                                   forceShape = {i: None for i in nixSegIdx}
  2393                                                   forceType = {i: None for i in nixSegIdx}
  2394                                                   forceFS = {i: None for i in nixSegIdx}
  2395                                                   for nixIdx in nixSegIdx:
  2396                                                       tempAsigList = tempBlock.segments[nixIdx].filter(
  2397                                                           objects=AnalogSignalProxy)
  2398                                                       if len(tempAsigList) > 0:
  2399                                                           tempAsig = tempAsigList[0]
  2400                                                           checkCompatible[nixIdx] = True
  2401                                                           forceType[nixIdx] = tempAsig.dtype
  2402                                                           forceShape[nixIdx] = tempAsig.shape[0]  # ? docs say shape[1], but that's confusing
  2403                                                           forceFS[nixIdx] = tempAsig.sampling_rate
  2404                                                   reader.file.close()
  2405                                               #  if newBlock was loaded from a nix file, strip the old nix_names away:
  2406                                               #  todo: replace with function from this module
  2407                                               if purgeNixNames:
  2408                                                   newBlock = purgeNixAnn(newBlock)
  2409                                               #
  2410                                               writer = NixIO(filename=trialBasePath + '.nix')
  2411                                               nixblock = writer.nix_file.blocks[nixBlockIdx]
  2412                                               nixblockName = nixblock.name
  2413                                               if 'nix_name' in newBlock.annotations.keys():
  2414                                                   try:
  2415                                                       assert newBlock.annotations['nix_name'] == nixblockName
  2416                                                   except Exception:
  2417                                                       newBlock.annotations['nix_name'] = nixblockName
  2418                                               else:
  2419                                                   newBlock.annotate(nix_name=nixblockName)
  2420                                               #
  2421                                               for idx, segIdx in enumerate(neoSegIdx):
  2422                                                   nixIdx = nixSegIdx[idx]
  2423                                                   newSeg = newBlock.segments[segIdx]
  2424                                                   nixgroup = nixblock.groups[nixIdx]
  2425                                                   nixSegName = nixgroup.name
  2426                                                   if 'nix_name' in newSeg.annotations.keys():
  2427                                                       try:
  2428                                                           assert newSeg.annotations['nix_name'] == nixSegName
  2429                                                       except Exception:
  2430                                                           newSeg.annotations['nix_name'] = nixSegName
  2431                                                   else:
  2432                                                       newSeg.annotate(nix_name=nixSegName)
  2433                                                   #
  2434                                                   if writeEvents:
  2435                                                       eventList = newSeg.events
  2436                                                       eventOrder = np.argsort([i.name for i in eventList])
  2437                                                       for event in [eventList[i] for i in eventOrder]:
  2438                                                           event = writer._write_event(event, nixblock, nixgroup)
  2439                                                   #
  2440                                                   if writeAsigs:
  2441                                                       asigList = newSeg.filter(objects=AnalogSignal)
  2442                                                       asigOrder = np.argsort([i.name for i in asigList])
  2443                                                       for asig in [asigList[i] for i in asigOrder]:
  2444                                                           if checkCompatible[nixIdx]:
  2445                                                               assert asig.dtype == forceType[nixIdx]
  2446                                                               assert asig.sampling_rate == forceFS[nixIdx]
  2447                                                               #  print('asig.shape[0] = {}'.format(asig.shape[0]))
  2448                                                               #  print('forceShape[nixIdx] = {}'.format(forceShape[nixIdx]))
  2449                                                               assert asig.shape[0] == forceShape[nixIdx]
  2450                                                           asig = writer._write_analogsignal(asig, nixblock, nixgroup)
  2451                                                       #  for isig in newSeg.filter(objects=IrregularlySampledSignal):
  2452                                                       #      isig = writer._write_irregularlysampledsignal(
  2453                                                       #          isig, nixblock, nixgroup)
  2454                                                   #
  2455                                                   if writeSpikes:
  2456                                                       stList = newSeg.filter(objects=SpikeTrain)
  2457                                                       stOrder = np.argsort([i.name for i in stList])
  2458                                                       for st in [stList[i] for i in stOrder]:
  2459                                                           st = writer._write_spiketrain(st, nixblock, nixgroup)
  2460                                               #
  2461                                               for chanIdx in newBlock.filter(objects=ChannelIndex):
  2462                                                   chanIdx = writer._write_channelindex(chanIdx, nixblock)
  2463                                                   #  auto descends into units inside of _write_channelindex
  2464                                               writer._create_source_links(newBlock, nixblock)
  2465                                               writer.close()
  2466                                               print('Done adding block to Nix.')
  2467                                               return newBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadStProxy at line 2469

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2469                                           @profile
  2470                                           def loadStProxy(stProxy):
  2471                                               try:
  2472                                                   st = stProxy.load(
  2473                                                       magnitude_mode='rescaled',
  2474                                                       load_waveforms=True)
  2475                                               except Exception:
  2476                                                   st = stProxy.load(
  2477                                                       magnitude_mode='rescaled',
  2478                                                       load_waveforms=False)
  2479                                                   st.waveforms = np.asarray([]).reshape((0, 0, 0))*pq.mV
  2480                                               return st

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: preproc at line 2482

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2482                                           @profile
  2483                                           def preproc(
  2484                                                   fileName='Trial001',
  2485                                                   rawFolderPath='./',
  2486                                                   outputFolderPath='./', mapDF=None,
  2487                                                   # swapMaps=None,
  2488                                                   electrodeArrayName='utah',
  2489                                                   fillOverflow=True, removeJumps=True,
  2490                                                   removeMeanAcross=False,
  2491                                                   linearDetrend=False,
  2492                                                   interpolateOutliers=False, calcOutliers=False,
  2493                                                   outlierMaskFilterOpts=None,
  2494                                                   outlierThreshold=1,
  2495                                                   calcArtifactTrace=False,
  2496                                                   motorEncoderMask=None,
  2497                                                   calcAverageLFP=False,
  2498                                                   eventInfo=None,
  2499                                                   spikeSourceType='', spikePath=None,
  2500                                                   chunkSize=1800, equalChunks=True, chunkList=None, chunkOffset=0,
  2501                                                   writeMode='rw',
  2502                                                   signal_group_mode='split-all', trialInfo=None,
  2503                                                   asigNameList=None, ainpNameList=None, nameSuffix='',
  2504                                                   saveFromAsigNameList=True,
  2505                                                   calcRigEvents=True, normalizeByImpedance=False,
  2506                                                   LFPFilterOpts=None, encoderCountPerDegree=180e2,
  2507                                                   outlierRemovalDebugFlag=False, impedanceFilePath=None
  2508                                                   ):
  2509                                               #  base file name
  2510                                               rawBasePath = os.path.join(rawFolderPath, fileName)
  2511                                               outputFilePath = os.path.join(
  2512                                                   outputFolderPath,
  2513                                                   fileName + nameSuffix + '.nix')
  2514                                               if os.path.exists(outputFilePath):
  2515                                                   os.remove(outputFilePath)
  2516                                               #  instantiate reader, get metadata
  2517                                               print('Loading\n{}\n'.format(rawBasePath))
  2518                                               reader = BlackrockIO(
  2519                                                   filename=rawBasePath, nsx_to_load=5)
  2520                                               reader.parse_header()
  2521                                               # metadata = reader.header
  2522                                               #  absolute section index
  2523                                               dummyBlock = readBlockFixNames(
  2524                                                   reader,
  2525                                                   block_index=0, lazy=True,
  2526                                                   signal_group_mode=signal_group_mode,
  2527                                                   mapDF=mapDF, reduceChannelIndexes=True,
  2528                                                   # swapMaps=swapMaps
  2529                                                   )
  2530                                               segLen = dummyBlock.segments[0].analogsignals[0].shape[0] / (
  2531                                                   dummyBlock.segments[0].analogsignals[0].sampling_rate)
  2532                                               nChunks = math.ceil(segLen / chunkSize)
  2533                                               #
  2534                                               if equalChunks:
  2535                                                   actualChunkSize = (segLen / nChunks).magnitude
  2536                                               else:
  2537                                                   actualChunkSize = chunkSize
  2538                                               if chunkList is None:
  2539                                                   chunkList = range(nChunks)
  2540                                               chunkingMetadata = {}
  2541                                               for chunkIdx in chunkList:
  2542                                                   print('preproc on chunk {}'.format(chunkIdx))
  2543                                                   #  instantiate spike reader if requested
  2544                                                   if spikeSourceType == 'tdc':
  2545                                                       if spikePath is None:
  2546                                                           spikePath = os.path.join(
  2547                                                               outputFolderPath, 'tdc_' + fileName,
  2548                                                               'tdc_' + fileName + '.nix')
  2549                                                       print('loading {}'.format(spikePath))
  2550                                                       spikeReader = nixio_fr.NixIO(filename=spikePath)
  2551                                                   else:
  2552                                                       spikeReader = None
  2553                                                   #  absolute section index
  2554                                                   block = readBlockFixNames(
  2555                                                       reader,
  2556                                                       block_index=0, lazy=True,
  2557                                                       signal_group_mode=signal_group_mode,
  2558                                                       mapDF=mapDF, reduceChannelIndexes=True,
  2559                                                       # swapMaps=swapMaps
  2560                                                       )
  2561                                                   if spikeReader is not None:
  2562                                                       spikeBlock = readBlockFixNames(
  2563                                                           spikeReader, block_index=0, lazy=True,
  2564                                                           signal_group_mode=signal_group_mode,
  2565                                                           mapDF=mapDF, reduceChannelIndexes=True,
  2566                                                           # swapMaps=swapMaps
  2567                                                           )
  2568                                                       spikeBlock = purgeNixAnn(spikeBlock)
  2569                                                   else:
  2570                                                       spikeBlock = None
  2571                                                   #
  2572                                                   #  instantiate writer
  2573                                                   if (nChunks == 1) or (len(chunkList) == 1):
  2574                                                       partNameSuffix = ""
  2575                                                       thisChunkOutFilePath = outputFilePath
  2576                                                   else:
  2577                                                       partNameSuffix = '_pt{:0>3}'.format(chunkIdx)
  2578                                                       thisChunkOutFilePath = (
  2579                                                           outputFilePath
  2580                                                           .replace('.nix', partNameSuffix + '.nix'))
  2581                                                   #
  2582                                                   if os.path.exists(thisChunkOutFilePath):
  2583                                                       os.remove(thisChunkOutFilePath)
  2584                                                   writer = NixIO(
  2585                                                       filename=thisChunkOutFilePath, mode=writeMode)
  2586                                                   chunkTStart = chunkIdx * actualChunkSize + chunkOffset
  2587                                                   chunkTStop = (chunkIdx + 1) * actualChunkSize + chunkOffset
  2588                                                   chunkingMetadata[chunkIdx] = {
  2589                                                       'filename': thisChunkOutFilePath,
  2590                                                       'partNameSuffix': partNameSuffix,
  2591                                                       'chunkTStart': chunkTStart,
  2592                                                       'chunkTStop': chunkTStop}
  2593                                                   block.annotate(chunkTStart=chunkTStart)
  2594                                                   block.annotate(chunkTStop=chunkTStop)
  2595                                                   block.annotate(
  2596                                                       recDatetimeStr=(
  2597                                                           block
  2598                                                           .rec_datetime
  2599                                                           .replace(tzinfo=timezone.utc)
  2600                                                           .isoformat())
  2601                                                       )
  2602                                                   #
  2603                                                   preprocBlockToNix(
  2604                                                       block, writer,
  2605                                                       chunkTStart=chunkTStart,
  2606                                                       chunkTStop=chunkTStop,
  2607                                                       fillOverflow=fillOverflow,
  2608                                                       removeJumps=removeJumps,
  2609                                                       interpolateOutliers=interpolateOutliers,
  2610                                                       calcOutliers=calcOutliers,
  2611                                                       outlierThreshold=outlierThreshold,
  2612                                                       outlierMaskFilterOpts=outlierMaskFilterOpts,
  2613                                                       calcArtifactTrace=calcArtifactTrace,
  2614                                                       linearDetrend=linearDetrend,
  2615                                                       motorEncoderMask=motorEncoderMask,
  2616                                                       electrodeArrayName=electrodeArrayName,
  2617                                                       calcAverageLFP=calcAverageLFP,
  2618                                                       eventInfo=eventInfo,
  2619                                                       asigNameList=asigNameList, ainpNameList=ainpNameList,
  2620                                                       saveFromAsigNameList=saveFromAsigNameList,
  2621                                                       spikeSourceType=spikeSourceType,
  2622                                                       spikeBlock=spikeBlock,
  2623                                                       calcRigEvents=calcRigEvents,
  2624                                                       normalizeByImpedance=normalizeByImpedance,
  2625                                                       removeMeanAcross=removeMeanAcross,
  2626                                                       LFPFilterOpts=LFPFilterOpts,
  2627                                                       encoderCountPerDegree=encoderCountPerDegree,
  2628                                                       outlierRemovalDebugFlag=outlierRemovalDebugFlag,
  2629                                                       impedanceFilePath=impedanceFilePath,
  2630                                                       )
  2631                                                   #### diagnostics
  2632                                                   diagnosticFolder = os.path.join(
  2633                                                       outputFolderPath,
  2634                                                       'preprocDiagnostics',
  2635                                                       # fileName + nameSuffix + partNameSuffix
  2636                                                       )
  2637                                                   if not os.path.exists(diagnosticFolder):
  2638                                                       os.mkdir(diagnosticFolder)
  2639                                                   asigDiagnostics = {}
  2640                                                   outlierDiagnostics = {}
  2641                                                   diagnosticText = ''
  2642                                                   for asig in block.filter(objects=AnalogSignal):
  2643                                                       annNames = ['mean_removal_r2', 'mean_removal_group']
  2644                                                       for annName in annNames:
  2645                                                           if annName in asig.annotations:
  2646                                                               if asig.name not in asigDiagnostics:
  2647                                                                   asigDiagnostics[asig.name] = {}
  2648                                                               asigDiagnostics[asig.name].update({
  2649                                                                   annName: asig.annotations[annName]})
  2650                                                       annNames = [
  2651                                                           'outlierProportion', 'nDim',
  2652                                                           'noveltyThreshold', 'outlierThreshold'
  2653                                                           ]
  2654                                                       for annName in annNames:
  2655                                                           if annName in asig.annotations:
  2656                                                               if asig.name not in outlierDiagnostics:
  2657                                                                   outlierDiagnostics[asig.name] = {}
  2658                                                               outlierDiagnostics[asig.name].update({
  2659                                                                   annName: '{}'.format(asig.annotations[annName])
  2660                                                               })
  2661                                                   if removeMeanAcross:
  2662                                                       asigDiagnosticsDF = pd.DataFrame(asigDiagnostics).T
  2663                                                       asigDiagnosticsDF.sort_values(by='mean_removal_r2', inplace=True)
  2664                                                       diagnosticText += '<h2>LFP Diagnostics</h2>\n'
  2665                                                       diagnosticText += asigDiagnosticsDF.to_html()
  2666                                                       fig, ax = plt.subplots()
  2667                                                       sns.distplot(asigDiagnosticsDF['mean_removal_r2'], ax=ax)
  2668                                                       ax.set_ylabel('Count of analog signals')
  2669                                                       ax.set_xlabel('R^2 of regressing mean against signal')
  2670                                                       fig.savefig(os.path.join(
  2671                                                               diagnosticFolder,
  2672                                                               fileName + nameSuffix + partNameSuffix + '_meanRemovalR2.png'
  2673                                                           ))
  2674                                                   if interpolateOutliers:
  2675                                                       outlierDiagnosticsDF = pd.DataFrame(outlierDiagnostics).T
  2676                                                       diagnosticText += '<h2>Outlier Diagnostics</h2>\n'
  2677                                                       diagnosticText += outlierDiagnosticsDF.to_html()
  2678                                                   diagnosticTextPath = os.path.join(
  2679                                                       diagnosticFolder,
  2680                                                       fileName + nameSuffix + partNameSuffix + '_asigDiagnostics.html'
  2681                                                       )
  2682                                                   with open(diagnosticTextPath, 'w') as _f:
  2683                                                       _f.write(diagnosticText)
  2684                                                   writer.close()
  2685                                               chunkingInfoPath = os.path.join(
  2686                                                   outputFolderPath,
  2687                                                   fileName + nameSuffix +
  2688                                                   '_chunkingInfo.json'
  2689                                                   )
  2690                                               if os.path.exists(chunkingInfoPath):
  2691                                                   os.remove(chunkingInfoPath)
  2692                                               with open(chunkingInfoPath, 'w') as f:
  2693                                                   json.dump(chunkingMetadata, f)
  2694                                               return

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: preprocBlockToNix at line 2696

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2696                                           @profile
  2697                                           def preprocBlockToNix(
  2698                                                   block, writer,
  2699                                                   chunkTStart=None,
  2700                                                   chunkTStop=None,
  2701                                                   eventInfo=None,
  2702                                                   fillOverflow=False, calcAverageLFP=False,
  2703                                                   interpolateOutliers=False, calcOutliers=False,
  2704                                                   calcArtifactTrace=False,
  2705                                                   outlierMaskFilterOpts=None,
  2706                                                   useMeanToCenter=False,   # mean center? median center?
  2707                                                   linearDetrend=False,
  2708                                                   zScoreEachTrace=False,
  2709                                                   outlierThreshold=1,
  2710                                                   motorEncoderMask=None,
  2711                                                   electrodeArrayName='utah',
  2712                                                   removeJumps=False, trackMemory=True,
  2713                                                   asigNameList=None, ainpNameList=None,
  2714                                                   saveFromAsigNameList=True,
  2715                                                   spikeSourceType='', spikeBlock=None,
  2716                                                   calcRigEvents=True,
  2717                                                   normalizeByImpedance=True,
  2718                                                   impedanceFilePath=None,
  2719                                                   removeMeanAcross=False,
  2720                                                   LFPFilterOpts=None, encoderCountPerDegree=180e2,
  2721                                                   outlierRemovalDebugFlag=False,
  2722                                                   ):
  2723                                               #  prune out nev spike placeholders
  2724                                               #  (will get added back on a chunk by chunk basis,
  2725                                               #  if not pruning units)
  2726                                               if spikeSourceType == 'nev':
  2727                                                   pruneOutUnits = False
  2728                                               else:
  2729                                                   pruneOutUnits = True
  2730                                               #
  2731                                               for chanIdx in block.channel_indexes:
  2732                                                   if chanIdx.units:
  2733                                                       for unit in chanIdx.units:
  2734                                                           if unit.spiketrains:
  2735                                                               unit.spiketrains = []
  2736                                                       if pruneOutUnits:
  2737                                                           chanIdx.units = []
  2738                                               #
  2739                                               if spikeBlock is not None:
  2740                                                   for chanIdx in spikeBlock.channel_indexes:
  2741                                                       if chanIdx.units:
  2742                                                           for unit in chanIdx.units:
  2743                                                               if unit.spiketrains:
  2744                                                                   unit.spiketrains = []
  2745                                               #  precalculate new segment
  2746                                               seg = block.segments[0]
  2747                                               #  remove chanIndexes assigned to units; makes more sense to
  2748                                               #  only use chanIdx for asigs and spikes on that asig
  2749                                               #  block.channel_indexes = (
  2750                                               #      [chanIdx for chanIdx in block.channel_indexes if (
  2751                                               #          chanIdx.analogsignals)])
  2752                                               if calcAverageLFP:
  2753                                                   lastIndex = len(block.channel_indexes)
  2754                                                   lastID = block.channel_indexes[-1].channel_ids[0] + 1
  2755                                                   if asigNameList is None:
  2756                                                       asigNameList = [
  2757                                                           [
  2758                                                               childBaseName(a.name, 'seg')
  2759                                                               for a in seg.analogsignals
  2760                                                               if not (('ainp' in a.name) or ('analog' in a.name))]
  2761                                                           ]
  2762                                                   nMeanChans = len(asigNameList)
  2763                                                   #
  2764                                                   meanChIdxList = []
  2765                                                   for meanChIdx in range(nMeanChans):
  2766                                                       tempChIdx = ChannelIndex(
  2767                                                           index=[lastIndex + meanChIdx],
  2768                                                           channel_names=['{}_rawAverage_{}'.format(electrodeArrayName, meanChIdx)],
  2769                                                           channel_ids=[lastID + meanChIdx],
  2770                                                           name='{}_rawAverage_{}'.format(electrodeArrayName, meanChIdx),
  2771                                                           file_origin=block.channel_indexes[-1].file_origin
  2772                                                           )
  2773                                                       tempChIdx.merge_annotations(block.channel_indexes[-1])
  2774                                                       block.channel_indexes.append(tempChIdx)
  2775                                                       meanChIdxList.append(tempChIdx)
  2776                                                       lastIndex += 1
  2777                                                       lastID += 1
  2778                                                   lastIndex = len(block.channel_indexes)
  2779                                                   lastID = block.channel_indexes[-1].channel_ids[0] + 1
  2780                                                   # if calcArtifactTrace:
  2781                                                   if True:
  2782                                                       artChIdxList = []
  2783                                                       for artChIdx in range(nMeanChans):
  2784                                                           tempChIdx = ChannelIndex(
  2785                                                               index=[lastIndex + artChIdx],
  2786                                                               channel_names=['{}_artifact_{}'.format(electrodeArrayName, artChIdx)],
  2787                                                               channel_ids=[lastID + artChIdx],
  2788                                                               name='{}_artifact_{}'.format(electrodeArrayName, artChIdx),
  2789                                                               file_origin=block.channel_indexes[-1].file_origin
  2790                                                               )
  2791                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2792                                                           block.channel_indexes.append(tempChIdx)
  2793                                                           artChIdxList.append(tempChIdx)
  2794                                                           lastIndex += 1
  2795                                                           lastID += 1
  2796                                                   # if calcOutliers:
  2797                                                   if True:
  2798                                                       devChIdxList = []
  2799                                                       for devChIdx in range(nMeanChans):
  2800                                                           tempChIdx = ChannelIndex(
  2801                                                               index=[lastIndex + devChIdx],
  2802                                                               channel_names=['{}_deviation_{}'.format(electrodeArrayName, devChIdx)],
  2803                                                               channel_ids=[lastID + devChIdx],
  2804                                                               name='{}_deviation_{}'.format(electrodeArrayName, devChIdx),
  2805                                                               file_origin=block.channel_indexes[-1].file_origin
  2806                                                               )
  2807                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2808                                                           block.channel_indexes.append(tempChIdx)
  2809                                                           devChIdxList.append(tempChIdx)
  2810                                                           lastIndex += 1
  2811                                                           lastID += 1
  2812                                                       smDevChIdxList = []
  2813                                                       for devChIdx in range(nMeanChans):
  2814                                                           tempChIdx = ChannelIndex(
  2815                                                               index=[lastIndex + devChIdx],
  2816                                                               channel_names=['{}_smoothed_deviation_{}'.format(electrodeArrayName, devChIdx)],
  2817                                                               channel_ids=[lastID + devChIdx],
  2818                                                               name='{}_smoothed_deviation_{}'.format(electrodeArrayName, devChIdx),
  2819                                                               file_origin=block.channel_indexes[-1].file_origin
  2820                                                               )
  2821                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2822                                                           block.channel_indexes.append(tempChIdx)
  2823                                                           smDevChIdxList.append(tempChIdx)
  2824                                                           lastIndex += 1
  2825                                                           lastID += 1
  2826                                                       outMaskChIdxList = []
  2827                                                       for outMaskChIdx in range(nMeanChans):
  2828                                                           tempChIdx = ChannelIndex(
  2829                                                               index=[lastIndex + outMaskChIdx],
  2830                                                               channel_names=['{}_outlierMask_{}'.format(
  2831                                                                   electrodeArrayName, outMaskChIdx)],
  2832                                                               channel_ids=[lastID + outMaskChIdx],
  2833                                                               name='{}_outlierMask_{}'.format(
  2834                                                                   electrodeArrayName, outMaskChIdx),
  2835                                                               file_origin=block.channel_indexes[-1].file_origin
  2836                                                               )
  2837                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2838                                                           block.channel_indexes.append(tempChIdx)
  2839                                                           outMaskChIdxList.append(tempChIdx)
  2840                                                           lastIndex += 1
  2841                                                           lastID += 1
  2842                                               #  delete asig and irsig proxies from channel index list
  2843                                               for metaIdx, chanIdx in enumerate(block.channel_indexes):
  2844                                                   if chanIdx.analogsignals:
  2845                                                       chanIdx.analogsignals = []
  2846                                                   if chanIdx.irregularlysampledsignals:
  2847                                                       chanIdx.irregularlysampledsignals = []
  2848                                               newSeg = Segment(
  2849                                                       index=0, name=seg.name,
  2850                                                       description=seg.description,
  2851                                                       file_origin=seg.file_origin,
  2852                                                       file_datetime=seg.file_datetime,
  2853                                                       rec_datetime=seg.rec_datetime,
  2854                                                       **seg.annotations
  2855                                                   )
  2856                                               block.segments = [newSeg]
  2857                                               block, nixblock = writer.write_block_meta(block)
  2858                                               # descend into Segments
  2859                                               if impedanceFilePath is not None:
  2860                                                   try:
  2861                                                       impedances = prb_meta.getLatestImpedance(
  2862                                                           block=block, impedanceFilePath=impedanceFilePath)
  2863                                                       averageImpedance = impedances['impedance'].median()
  2864                                                   except Exception:
  2865                                                       traceback.print_exc()
  2866                                               # for segIdx, seg in enumerate(oldSegList):
  2867                                               if spikeBlock is not None:
  2868                                                   spikeSeg = spikeBlock.segments[0]
  2869                                               else:
  2870                                                   spikeSeg = seg
  2871                                               #
  2872                                               if trackMemory:
  2873                                                   print('memory usage: {:.1f} MB'.format(
  2874                                                       prf.memory_usage_psutil()))
  2875                                               newSeg, nixgroup = writer._write_segment_meta(newSeg, nixblock)
  2876                                               #  trim down list of analog signals if necessary
  2877                                               asigNameListSeg = []
  2878                                               if (removeMeanAcross or calcAverageLFP):
  2879                                                   meanGroups = {}
  2880                                               for subListIdx, subList in enumerate(asigNameList):
  2881                                                   subListSeg = [
  2882                                                       'seg{}_{}'.format(0, a)
  2883                                                       for a in subList]
  2884                                                   asigNameListSeg += subListSeg
  2885                                                   if (removeMeanAcross or calcAverageLFP):
  2886                                                       meanGroups[subListIdx] = subListSeg
  2887                                               aSigList = []
  2888                                               # [asig.name for asig in seg.analogsignals]
  2889                                               for a in seg.analogsignals:
  2890                                                   # if np.any([n in a.name for n in asigNameListSeg]):
  2891                                                   if a.name in asigNameListSeg:
  2892                                                       aSigList.append(a)
  2893                                               if ainpNameList is not None:
  2894                                                   ainpNameListSeg = [
  2895                                                       'seg{}_{}'.format(0, a)
  2896                                                       for a in ainpNameList]
  2897                                                   ainpList = []
  2898                                                   for a in seg.analogsignals:
  2899                                                       if np.any([n == a.name for n in ainpNameListSeg]):
  2900                                                           ainpList.append(a)
  2901                                               else:
  2902                                                   ainpList = [
  2903                                                       a
  2904                                                       for a in seg.analogsignals
  2905                                                       if (('ainp' in a.name) or ('analog' in a.name))]
  2906                                                   ainpNameListSeg = [a.name for a in aSigList]
  2907                                               nAsigs = len(aSigList)
  2908                                               if LFPFilterOpts is not None:
  2909                                                   def filterFun(sig, filterCoeffs=None):
  2910                                                       # sig[:] = signal.sosfiltfilt(
  2911                                                       sig[:] = signal.sosfilt(
  2912                                                           filterCoeffs, sig.magnitude.flatten())[:, np.newaxis] * sig.units
  2913                                                       return sig
  2914                                                   filterCoeffs = hf.makeFilterCoeffsSOS(
  2915                                                       LFPFilterOpts, float(seg.analogsignals[0].sampling_rate))
  2916                                                   if False:
  2917                                                       fig, ax1, ax2 = hf.plotFilterResponse(
  2918                                                           filterCoeffs,
  2919                                                           float(seg.analogsignals[0].sampling_rate))
  2920                                                       fig2, ax3, ax4 = hf.plotFilterImpulseResponse(
  2921                                                           LFPFilterOpts,
  2922                                                           float(seg.analogsignals[0].sampling_rate))
  2923                                                       plt.show()
  2924                                               # first pass through asigs, if removing mean across channels
  2925                                               if (removeMeanAcross or calcAverageLFP):
  2926                                                   for aSigIdx, aSigProxy in enumerate(seg.analogsignals):
  2927                                                       if aSigIdx == 0:
  2928                                                           # check bounds
  2929                                                           tStart = max(chunkTStart * pq.s, aSigProxy.t_start)
  2930                                                           tStop = min(chunkTStop * pq.s, aSigProxy.t_stop)
  2931                                                       loadThisOne = (aSigProxy in aSigList)
  2932                                                       if loadThisOne:
  2933                                                           if trackMemory:
  2934                                                               print(
  2935                                                                   'Extracting asig for mean, memory usage: {:.1f} MB'.format(
  2936                                                                       prf.memory_usage_psutil()))
  2937                                                           chanIdx = aSigProxy.channel_index
  2938                                                           asig = aSigProxy.load(
  2939                                                               time_slice=(tStart, tStop),
  2940                                                               magnitude_mode='rescaled')
  2941                                                           if 'tempLFPStore' not in locals():
  2942                                                               tempLFPStore = pd.DataFrame(
  2943                                                                   np.zeros(
  2944                                                                       (asig.shape[0], nAsigs),
  2945                                                                       dtype=np.float32),
  2946                                                                   columns=asigNameListSeg)
  2947                                                           if 'dummyAsig' not in locals():
  2948                                                               dummyAsig = asig.copy()
  2949                                                           #  perform requested preproc operations
  2950                                                           #  if LFPFilterOpts is not None:
  2951                                                           #      asig[:] = filterFun(
  2952                                                           #          asig, filterCoeffs=filterCoeffs)
  2953                                                           if normalizeByImpedance:
  2954                                                               elNmMatchMsk = impedances['elec'] == chanIdx.name
  2955                                                               '''
  2956                                                               asig.magnitude[:] = (
  2957                                                                   (asig.magnitude - np.median(asig.magnitude)) /
  2958                                                                   np.min(
  2959                                                                       impedances.loc[elNmMatchMsk, 'impedance']
  2960                                                                       ))
  2961                                                               '''
  2962                                                               asig.magnitude[:] = (
  2963                                                                   (asig.magnitude) * averageImpedance /
  2964                                                                   np.min(
  2965                                                                       impedances.loc[elNmMatchMsk, 'impedance']
  2966                                                                       ))
  2967                                                           # if fillOverflow:
  2968                                                           #     # fill in overflow:
  2969                                                           #     '''
  2970                                                           #     timeSection['data'], overflowMask = hf.fillInOverflow(
  2971                                                           #         timeSection['data'], fillMethod = 'average')
  2972                                                           #     badData.update({'overflow': overflowMask})
  2973                                                           #     '''
  2974                                                           #     pass
  2975                                                           # if removeJumps:
  2976                                                           #     # find unusual jumps in derivative or amplitude
  2977                                                           #     '''
  2978                                                           #     timeSection['data'], newBadData = hf.fillInJumps(timeSection['data'],
  2979                                                           #     timeSection['samp_per_s'], smoothing_ms = 0.5, nStdDiff = 50,
  2980                                                           #     nStdAmp = 100)
  2981                                                           #     badData.update(newBadData)
  2982                                                           #     '''
  2983                                                           #     pass
  2984                                                           tempLFPStore.loc[:, aSigProxy.name] = asig.magnitude.flatten()
  2985                                                           del asig
  2986                                                           gc.collect()
  2987                                                   # end of first pass
  2988                                                   if (removeMeanAcross or calcAverageLFP):
  2989                                                       centerLFP = np.zeros(
  2990                                                           (tempLFPStore.shape[0], len(asigNameList)),
  2991                                                           dtype=np.float32)
  2992                                                       spreadLFP = np.zeros(
  2993                                                           (tempLFPStore.shape[0], len(asigNameList)),
  2994                                                           dtype=np.float32)
  2995                                                       # if calcOutliers:
  2996                                                       if True:
  2997                                                           if outlierMaskFilterOpts is not None:
  2998                                                               filterCoeffsOutlierMask = hf.makeFilterCoeffsSOS(
  2999                                                                   outlierMaskFilterOpts, float(dummyAsig.sampling_rate))
  3000                                                           lfpDeviation = np.zeros(
  3001                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3002                                                               dtype=np.float32)
  3003                                                           smoothedDeviation = np.zeros(
  3004                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3005                                                               dtype=np.float32)
  3006                                                           outlierMask = np.zeros(
  3007                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3008                                                               dtype=np.bool)
  3009                                                           outlierMetadata = {}
  3010                                                       # if calcArtifactTrace:
  3011                                                       if True:
  3012                                                           artifactSignal = np.zeros(
  3013                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3014                                                               dtype=np.float32)
  3015                                                       ###############
  3016                                                       # tempLFPStore.iloc[:, 0] = np.nan  # for debugging axes
  3017                                                       #############
  3018                                                       plotDevFilterDebug = False
  3019                                                       if plotDevFilterDebug:
  3020                                                           try:
  3021                                                               devFiltDebugMask = (dummyAsig.times > 90 * pq.s) & (dummyAsig.times < 92 * pq.s)
  3022                                                           except Exception:
  3023                                                               pdb.set_trace()
  3024                                                           plotColIdx = 1
  3025                                                           ddfFig, ddfAx = plt.subplots(len(asigNameList), 1)
  3026                                                           ddfFig2, ddfAx2 = plt.subplots()
  3027                                                           ddfFig3, ddfAx3 = plt.subplots(
  3028                                                               1, len(asigNameList),
  3029                                                               sharey=True)
  3030                                                           if len(asigNameList) == 1:
  3031                                                               ddfAx = np.asarray([ddfAx])
  3032                                                               ddfAx3 = np.asarray([ddfAx3])
  3033                                                       for subListIdx, subList in enumerate(asigNameList):
  3034                                                           columnsForThisGroup = meanGroups[subListIdx]
  3035                                                           if trackMemory:
  3036                                                               print(
  3037                                                                   'asig group {}: calculating mean, memory usage: {:.1f} MB'.format(
  3038                                                                       subListIdx, prf.memory_usage_psutil()))
  3039                                                               print('this group contains\n{}'.format(columnsForThisGroup))
  3040                                                           if plotDevFilterDebug:
  3041                                                               ddfAx3[subListIdx].plot(
  3042                                                                   dummyAsig.times[devFiltDebugMask],
  3043                                                                   tempLFPStore.loc[:, columnsForThisGroup].iloc[devFiltDebugMask, plotColIdx],
  3044                                                                   label='original ch'
  3045                                                                   )
  3046                                                           if fillOverflow:
  3047                                                               print('Filling overflow...')
  3048                                                               # fill in overflow:
  3049                                                               tempLFPStore.loc[:, columnsForThisGroup], pltHandles = hf.fillInOverflow2(
  3050                                                                   tempLFPStore.loc[:, columnsForThisGroup].to_numpy(),
  3051                                                                   overFlowFillType='average',
  3052                                                                   overFlowThreshold=8000,
  3053                                                                   debuggingPlots=plotDevFilterDebug
  3054                                                                   )
  3055                                                               if plotDevFilterDebug:
  3056                                                                   pltHandles['ax'].set_title('ch grp {}'.format(subListIdx))
  3057                                                                   ddfAx3[subListIdx].plot(
  3058                                                                       dummyAsig.times[devFiltDebugMask],
  3059                                                                       tempLFPStore.loc[:, columnsForThisGroup].iloc[devFiltDebugMask, plotColIdx],
  3060                                                                       label='filled ch'
  3061                                                                       )
  3062                                                           # zscore of each trace
  3063                                                           if zScoreEachTrace:
  3064                                                               print('About to calculate zscore of each trace (along columns) for prelim outlier detection')
  3065                                                               columnZScore = pd.DataFrame(
  3066                                                                   stats.zscore(
  3067                                                                       tempLFPStore.loc[:, columnsForThisGroup],
  3068                                                                       axis=1),
  3069                                                                   index=tempLFPStore.index,
  3070                                                                   columns=columnsForThisGroup
  3071                                                                   )
  3072                                                               excludeFromMeanMask = columnZScore.abs() > 6
  3073                                                               if useMeanToCenter:
  3074                                                                   centerLFP[:, subListIdx] = (
  3075                                                                       tempLFPStore
  3076                                                                       .loc[:, columnsForThisGroup]
  3077                                                                       .mask(excludeFromMeanMask)
  3078                                                                       .mean(axis=1).to_numpy()
  3079                                                                       )
  3080                                                               else:
  3081                                                                   centerLFP[:, subListIdx] = (
  3082                                                                       tempLFPStore
  3083                                                                       .loc[:, columnsForThisGroup]
  3084                                                                       .mask(excludeFromMeanMask)
  3085                                                                       .median(axis=1).to_numpy()
  3086                                                                       )
  3087                                                           else:
  3088                                                               if useMeanToCenter:
  3089                                                                   centerLFP[:, subListIdx] = (
  3090                                                                       tempLFPStore
  3091                                                                       .loc[:, columnsForThisGroup]
  3092                                                                       .mean(axis=1).to_numpy()
  3093                                                                       )
  3094                                                               else:
  3095                                                                   centerLFP[:, subListIdx] = (
  3096                                                                       tempLFPStore
  3097                                                                       .loc[:, columnsForThisGroup]
  3098                                                                       .median(axis=1).to_numpy()
  3099                                                                       )
  3100                                                           if calcArtifactTrace:
  3101                                                               if LFPFilterOpts is not None:
  3102                                                                   print('applying LFPFilterOpts to cached asigs for artifact ID')
  3103                                                                   # tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfilt(
  3104                                                                   tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfiltfilt(
  3105                                                                       filterCoeffs, tempLFPStore.loc[:, columnsForThisGroup],
  3106                                                                       axis=0)
  3107                                                                   if useMeanToCenter:
  3108                                                                       tempCenter = (
  3109                                                                           tempLFPStore
  3110                                                                           .loc[:, columnsForThisGroup]
  3111                                                                           .mean(axis=1).diff().fillna(0)
  3112                                                                           )
  3113                                                                   else:
  3114                                                                       tempCenter = (
  3115                                                                           tempLFPStore
  3116                                                                           .loc[:, columnsForThisGroup]
  3117                                                                           .median(axis=1).diff().fillna(0)
  3118                                                                           )
  3119                                                               artifactSignal[:, subListIdx] = np.abs(stats.zscore(tempCenter.to_numpy()))
  3120                                                           if calcOutliers:
  3121                                                               if plotDevFilterDebug:
  3122                                                                   ddfAx[subListIdx].plot(
  3123                                                                       dummyAsig.times[devFiltDebugMask],
  3124                                                                       centerLFP[devFiltDebugMask, subListIdx],
  3125                                                                       label='mean of ch group'
  3126                                                                       )
  3127                                                               # filter the traces, if needed
  3128                                                               if LFPFilterOpts is not None:
  3129                                                                   print('applying LFPFilterOpts to cached asigs before outlier detection')
  3130                                                                   # tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfiltfilt(
  3131                                                                   tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfilt(
  3132                                                                       filterCoeffs, tempLFPStore.loc[:, columnsForThisGroup],
  3133                                                                       axis=0)
  3134                                                                   if plotDevFilterDebug:
  3135                                                                       ddfAx3[subListIdx].plot(
  3136                                                                           dummyAsig.times[devFiltDebugMask],
  3137                                                                           tempLFPStore.loc[:, columnsForThisGroup].iloc[devFiltDebugMask, plotColIdx],
  3138                                                                           label='filtered ch'
  3139                                                                           )
  3140                                                               ##################################
  3141                                                               print('Whitening cached traces before outlier detection')
  3142                                                               whitenByPCA = True
  3143                                                               if whitenByPCA:
  3144                                                                   projector = PCA(
  3145                                                                       n_components=None, whiten=True)
  3146                                                                   pcs = projector.fit_transform(
  3147                                                                       tempLFPStore.loc[:, columnsForThisGroup])
  3148                                                                   explVarMask = (
  3149                                                                       np.cumsum(projector.explained_variance_ratio_) < 1 - 1e-2)
  3150                                                                   explVarMask[0] = True  # (keep at least 1)
  3151                                                                   pcs = pcs[:, explVarMask]
  3152                                                                   nDim = pcs.shape[1]
  3153                                                                   lfpDeviation[:, subListIdx] = (pcs ** 2).sum(axis=1)
  3154                                                               else:  # whiten by mahalanobis distance
  3155                                                                   est = EmpiricalCovariance()
  3156                                                                   est.fit(tempLFPStore.loc[:, columnsForThisGroup].to_numpy())
  3157                                                                   lfpDeviation[:, subListIdx] = est.mahalanobis(
  3158                                                                       tempLFPStore.loc[:, columnsForThisGroup].to_numpy())
  3159                                                                   nDim = tempLFPStore.loc[:, columnsForThisGroup].shape[1]
  3160                                                               #
  3161                                                               transformedDeviation = stats.norm.isf(stats.chi2.sf(lfpDeviation[:, subListIdx], nDim))
  3162                                                               infMask = np.isinf(transformedDeviation)
  3163                                                               if infMask.any():
  3164                                                                   transformedDeviation[infMask] = transformedDeviation[~infMask].max()
  3165                                                               debugProbaTrans = False
  3166                                                               if debugProbaTrans:
  3167                                                                   fig, ax = plt.subplots()
  3168                                                                   tAx = ax.twinx()
  3169                                                                   plotMask = (dummyAsig.times >= 60 * pq.s) & (dummyAsig.times < 95 * pq.s)
  3170                                                                   ax.plot(dummyAsig.times[plotMask], transformedDeviation[plotMask], c='b', label='transformed deviation')
  3171                                                                   tAx.plot(dummyAsig.times[plotMask], lfpDeviation[plotMask, subListIdx], c='r', label='original deviation')
  3172                                                                   ax.legend(loc='upper left')
  3173                                                                   tAx.legend(loc='upper right')
  3174                                                                   plt.show()
  3175                                                               lfpDeviation[:, subListIdx] = transformedDeviation
  3176                                                               noveltyThreshold = stats.norm.interval(outlierThreshold)[1]
  3177                                                               # chi2Bounds = stats.chi2.interval(outlierThreshold, nDim)
  3178                                                               # lfpDeviation[:, subListIdx] = lfpDeviation[:, subListIdx] / chi2Bounds[1]
  3179                                                               # print('nDim = {}, chi2Lim = {}'.format(nDim, chi2Bounds))
  3180                                                               # noveltyThreshold = 1
  3181                                                               #
  3182                                                               outlierMetadata[subListIdx] = {
  3183                                                                   'nDim': nDim,
  3184                                                                   'noveltyThreshold': noveltyThreshold,
  3185                                                                   'outlierThreshold': outlierThreshold
  3186                                                                   }
  3187                                                               # smoothedDeviation = signal.sosfilt(
  3188                                                               print('Smoothing deviation')
  3189                                                               tempSmDev = signal.sosfiltfilt(
  3190                                                                   filterCoeffsOutlierMask, lfpDeviation[:, subListIdx])
  3191                                                               smoothedDeviation[:, subListIdx] = tempSmDev
  3192                                                               if plotDevFilterDebug:
  3193                                                                   ddfAx[subListIdx].plot(
  3194                                                                       dummyAsig.times[devFiltDebugMask],
  3195                                                                       lfpDeviation[devFiltDebugMask, subListIdx],
  3196                                                                       label='original deviation (ch grp {})'.format(subListIdx))
  3197                                                                   ddfAx[subListIdx].plot(
  3198                                                                       dummyAsig.times[devFiltDebugMask],
  3199                                                                       smoothedDeviation[devFiltDebugMask, subListIdx],
  3200                                                                       label='filtered deviation (ch grp {})'.format(subListIdx))
  3201                                                               ##
  3202                                                               print('Calculating outlier mask')
  3203                                                               outlierMask[:, subListIdx] = (
  3204                                                                   smoothedDeviation[:, subListIdx] > noveltyThreshold)
  3205                                                               if plotDevFilterDebug:
  3206                                                                   ddfAx[subListIdx].axhline(noveltyThreshold, c='r')
  3207                                                       if plotDevFilterDebug and calcOutliers:
  3208                                                           for subListIdx, subList in enumerate(asigNameList):
  3209                                                               ddfAx[subListIdx].legend(loc='upper right')
  3210                                                               ddfAx[subListIdx].set_title('Deviation')
  3211                                                               ddfAx3[subListIdx].legend(loc='upper right')
  3212                                                               ddfAx3[subListIdx].set_title('Example channel')
  3213                                                               ddfAx2.plot(
  3214                                                                   dummyAsig.times[devFiltDebugMask],
  3215                                                                   smoothedDeviation[devFiltDebugMask, subListIdx],
  3216                                                                   label='ch grp {}'.format(subListIdx))
  3217                                                               ddfAx2.set_title('Smoothed Deviation')
  3218                                                           ddfAx2.legend(loc='upper right')
  3219                                                           plt.show()
  3220                                                       #############
  3221                                                       del tempLFPStore
  3222                                                       gc.collect()
  3223                                               if (removeMeanAcross or calcAverageLFP):
  3224                                                   for mIdx, meanChIdx in enumerate(meanChIdxList):
  3225                                                       meanAsig = AnalogSignal(
  3226                                                           centerLFP[:, mIdx],
  3227                                                           units=dummyAsig.units,
  3228                                                           sampling_rate=dummyAsig.sampling_rate,
  3229                                                           # name='seg{}_{}'.format(idx, meanChIdx.name)
  3230                                                           name='seg{}_{}'.format(0, meanChIdx.name),
  3231                                                           t_start=tStart
  3232                                                       )
  3233                                                       # assign ownership to containers
  3234                                                       meanChIdx.analogsignals.append(meanAsig)
  3235                                                       newSeg.analogsignals.append(meanAsig)
  3236                                                       # assign parent to children
  3237                                                       meanChIdx.create_relationship()
  3238                                                       newSeg.create_relationship()
  3239                                                       # write out to file
  3240                                                       if LFPFilterOpts is not None:
  3241                                                           meanAsig[:] = filterFun(
  3242                                                               meanAsig, filterCoeffs=filterCoeffs)
  3243                                                       meanAsig = writer._write_analogsignal(
  3244                                                           meanAsig, nixblock, nixgroup)
  3245                                                   # if calcArtifactTrace:
  3246                                                   if True:
  3247                                                       for mIdx, artChIdx in enumerate(artChIdxList):
  3248                                                           artAsig = AnalogSignal(
  3249                                                               artifactSignal[:, mIdx],
  3250                                                               units=dummyAsig.units,
  3251                                                               sampling_rate=dummyAsig.sampling_rate,
  3252                                                               # name='seg{}_{}'.format(idx, devChIdx.name)
  3253                                                               name='seg{}_{}'.format(0, artChIdx.name),
  3254                                                               t_start=tStart
  3255                                                               )
  3256                                                           # assign ownership to containers
  3257                                                           artChIdx.analogsignals.append(artAsig)
  3258                                                           newSeg.analogsignals.append(artAsig)
  3259                                                           # assign parent to children
  3260                                                           artChIdx.create_relationship()
  3261                                                           newSeg.create_relationship()
  3262                                                           # write out to file
  3263                                                           artAsig = writer._write_analogsignal(
  3264                                                               artAsig, nixblock, nixgroup)
  3265                                                           #########################################################
  3266                                                   # if calcOutliers:
  3267                                                   if True:
  3268                                                       for mIdx, devChIdx in enumerate(devChIdxList):
  3269                                                           devAsig = AnalogSignal(
  3270                                                               lfpDeviation[:, mIdx],
  3271                                                               units=dummyAsig.units,
  3272                                                               sampling_rate=dummyAsig.sampling_rate,
  3273                                                               # name='seg{}_{}'.format(idx, devChIdx.name)
  3274                                                               name='seg{}_{}'.format(0, devChIdx.name),
  3275                                                               t_start=tStart
  3276                                                               )
  3277                                                           # assign ownership to containers
  3278                                                           devChIdx.analogsignals.append(devAsig)
  3279                                                           newSeg.analogsignals.append(devAsig)
  3280                                                           # assign parent to children
  3281                                                           devChIdx.create_relationship()
  3282                                                           newSeg.create_relationship()
  3283                                                           # write out to file
  3284                                                           devAsig = writer._write_analogsignal(
  3285                                                               devAsig, nixblock, nixgroup)
  3286                                                           #########################################################
  3287                                                       for mIdx, smDevChIdx in enumerate(smDevChIdxList):
  3288                                                           smDevAsig = AnalogSignal(
  3289                                                               smoothedDeviation[:, mIdx],
  3290                                                               units=dummyAsig.units,
  3291                                                               sampling_rate=dummyAsig.sampling_rate,
  3292                                                               # name='seg{}_{}'.format(idx, devChIdx.name)
  3293                                                               name='seg{}_{}'.format(0, smDevChIdx.name),
  3294                                                               t_start=tStart
  3295                                                               )
  3296                                                           # assign ownership to containers
  3297                                                           smDevChIdx.analogsignals.append(smDevAsig)
  3298                                                           newSeg.analogsignals.append(smDevAsig)
  3299                                                           # assign parent to children
  3300                                                           smDevChIdx.create_relationship()
  3301                                                           newSeg.create_relationship()
  3302                                                           # write out to file
  3303                                                           smDevAsig = writer._write_analogsignal(
  3304                                                               smDevAsig, nixblock, nixgroup)
  3305                                                           #########################################################
  3306                                                       for mIdx, outMaskChIdx in enumerate(outMaskChIdxList):
  3307                                                           outMaskAsig = AnalogSignal(
  3308                                                               outlierMask[:, mIdx],
  3309                                                               units=dummyAsig.units,
  3310                                                               sampling_rate=dummyAsig.sampling_rate,
  3311                                                               # name='seg{}_{}'.format(idx, outMaskChIdx.name)
  3312                                                               name='seg{}_{}'.format(0, outMaskChIdx.name),
  3313                                                               t_start=tStart, dtype=np.float32
  3314                                                               )
  3315                                                           outMaskAsig.annotations['outlierProportion'] = np.mean(outlierMask[:, mIdx])
  3316                                                           if calcOutliers:
  3317                                                               outMaskAsig.annotations.update(outlierMetadata[mIdx])
  3318                                                           # assign ownership to containers
  3319                                                           outMaskChIdx.analogsignals.append(outMaskAsig)
  3320                                                           newSeg.analogsignals.append(outMaskAsig)
  3321                                                           # assign parent to children
  3322                                                           outMaskChIdx.create_relationship()
  3323                                                           newSeg.create_relationship()
  3324                                                           # write out to file
  3325                                                           outMaskAsig = writer._write_analogsignal(
  3326                                                               outMaskAsig, nixblock, nixgroup)
  3327                                                   #
  3328                                                   w0 = 60
  3329                                                   bandQ = 20
  3330                                                   bw = w0/bandQ
  3331                                                   noiseSos = signal.iirfilter(
  3332                                                       N=8, Wn=[w0 - bw/2, w0 + bw/2],
  3333                                                       btype='band', ftype='butter',
  3334                                                       analog=False, fs=float(dummyAsig.sampling_rate),
  3335                                                       output='sos')
  3336                                                   # signal.hilbert does not have an option to zero pad
  3337                                                   nextLen = fftpack.helper.next_fast_len(dummyAsig.shape[0])
  3338                                                   deficit = int(nextLen - dummyAsig.shape[0])
  3339                                                   lDef = int(np.floor(deficit / 2))
  3340                                                   rDef = int(np.ceil(deficit / 2)) + 1
  3341                                                   temp = np.pad(
  3342                                                       dummyAsig.magnitude.flatten(),
  3343                                                       (lDef, rDef), mode='constant')
  3344                                                   # lineNoise = signal.sosfiltfilt(
  3345                                                   lineNoise = signal.sosfilt(
  3346                                                       noiseSos, temp, axis=0)
  3347                                                   lineNoiseH = signal.hilbert(lineNoise)
  3348                                                   lineNoise = lineNoise[lDef:-rDef]
  3349                                                   lineNoiseH = lineNoiseH[lDef:-rDef]
  3350                                                   lineNoisePhase = np.angle(lineNoiseH)
  3351                                                   lineNoisePhaseDF = pd.DataFrame(
  3352                                                       lineNoisePhase,
  3353                                                       index=dummyAsig.times,
  3354                                                       columns=['phase']
  3355                                                       )
  3356                                                   plotHilbert = False
  3357                                                   if plotHilbert:
  3358                                                       lineNoiseFreq = (
  3359                                                           np.diff(np.unwrap(lineNoisePhase)) /
  3360                                                           (2.0*np.pi) * float(dummyAsig.sampling_rate))
  3361                                                       lineNoiseEnvelope = np.abs(lineNoiseH)
  3362                                                       i1 = 300000; i2 = 330000
  3363                                                       fig, ax = plt.subplots(2, 1, sharex=True)
  3364                                                       ax[0].plot(dummyAsig.times[devFiltDebugMask], dummyAsig.magnitude[devFiltDebugMask, :])
  3365                                                       ax[0].plot(dummyAsig.times[devFiltDebugMask], lineNoise[devFiltDebugMask])
  3366                                                       ax[0].plot(dummyAsig.times[devFiltDebugMask], lineNoiseEnvelope[devFiltDebugMask])
  3367                                                       axFr = ax[1].twinx()
  3368                                                       ax[1].plot(
  3369                                                           dummyAsig.times[devFiltDebugMask], lineNoisePhase[devFiltDebugMask],
  3370                                                           c='r', label='phase')
  3371                                                       ax[1].legend()
  3372                                                       axFr.plot(
  3373                                                           dummyAsig.times[devFiltDebugMask], lineNoiseFreq[devFiltDebugMask],
  3374                                                           label='freq')
  3375                                                       axFr.set_ylim([59, 61])
  3376                                                       axFr.legend()
  3377                                                       plt.show()
  3378                                               # second pass through asigs, to save
  3379                                               for aSigIdx, aSigProxy in enumerate(seg.analogsignals):
  3380                                                   if aSigIdx == 0:
  3381                                                       # check bounds
  3382                                                       tStart = max(chunkTStart * pq.s, aSigProxy.t_start)
  3383                                                       tStop = min(chunkTStop * pq.s, aSigProxy.t_stop)
  3384                                                   loadThisOne = (
  3385                                                       (saveFromAsigNameList and (aSigProxy in aSigList)) or
  3386                                                       (aSigProxy in ainpList)
  3387                                                       )
  3388                                                   if loadThisOne:
  3389                                                       if trackMemory:
  3390                                                           print('writing asig {} ({}) memory usage: {:.1f} MB'.format(
  3391                                                               aSigIdx, aSigProxy.name, prf.memory_usage_psutil()))
  3392                                                       chanIdx = aSigProxy.channel_index
  3393                                                       asig = aSigProxy.load(
  3394                                                           time_slice=(tStart, tStop),
  3395                                                           magnitude_mode='rescaled')
  3396                                                       #  link AnalogSignal and ID providing channel_index
  3397                                                       asig.channel_index = chanIdx
  3398                                                       #  perform requested preproc operations
  3399                                                       if 'impedances' in locals():
  3400                                                           elNmMatchMsk = impedances['elec'] == chanIdx.name
  3401                                                           if elNmMatchMsk.any():
  3402                                                               originalImpedance = np.min(
  3403                                                                   impedances.loc[elNmMatchMsk, 'impedance']
  3404                                                                   )
  3405                                                               asig.annotations['originalImpedance'] = originalImpedance
  3406                                                               if normalizeByImpedance and (aSigProxy not in ainpList):
  3407                                                                   '''
  3408                                                                   asig.magnitude[:] = (
  3409                                                                       (asig.magnitude - np.median(asig.magnitude)) /
  3410                                                                       np.min(
  3411                                                                           impedances.loc[elNmMatchMsk, 'impedance']
  3412                                                                           )
  3413                                                                       )
  3414                                                                   '''
  3415                                                                   print('Normalizing {} by {} kOhms'.format(asig.name, originalImpedance))
  3416                                                                   asig.magnitude[:] = (
  3417                                                                       (asig.magnitude * averageImpedance) / originalImpedance
  3418                                                                       )
  3419                                                       if fillOverflow:
  3420                                                           # fill in overflow:
  3421                                                           asig.magnitude[:], _ = hf.fillInOverflow2(
  3422                                                               asig.magnitude[:],
  3423                                                               overFlowFillType='average',
  3424                                                               overFlowThreshold=8000,
  3425                                                               debuggingPlots=False
  3426                                                               )
  3427                                                       if removeJumps:
  3428                                                           # find unusual jumps in derivative or amplitude
  3429                                                           '''
  3430                                                           timeSection['data'], newBadData = hf.fillInJumps(timeSection['data'],
  3431                                                           timeSection['samp_per_s'], smoothing_ms = 0.5, nStdDiff = 50,
  3432                                                           nStdAmp = 100)
  3433                                                           badData.update(newBadData)
  3434                                                           '''
  3435                                                           pass
  3436                                                       if calcAverageLFP and (aSigProxy not in ainpList):
  3437                                                           for k, cols in meanGroups.items():
  3438                                                               if asig.name in cols:
  3439                                                                   whichColumnToSubtract = k
  3440                                                           noiseModel = np.polyfit(
  3441                                                               centerLFP[:, whichColumnToSubtract],
  3442                                                               asig.magnitude.flatten(), 1, full=True)
  3443                                                           rSq = 1 - noiseModel[1][0] / np.sum(asig.magnitude.flatten() ** 2)
  3444                                                           asig.annotations['mean_removal_r2'] = rSq
  3445                                                           asig.annotations['mean_removal_group'] = whichColumnToSubtract
  3446                                                           if linearDetrend:
  3447                                                               noiseTerm = np.polyval(
  3448                                                                   noiseModel[0],
  3449                                                                   centerLFP[:, whichColumnToSubtract])
  3450                                                           else:
  3451                                                               noiseTerm = centerLFP[:, whichColumnToSubtract]
  3452                                                           ###
  3453                                                           plotMeanSubtraction = False
  3454                                                           if plotMeanSubtraction:
  3455                                                               i1 = 300000; i2 = 330000
  3456                                                               fig, ax = plt.subplots(1, 1)
  3457                                                               ax.plot(asig.times[devFiltDebugMask], asig.magnitude[devFiltDebugMask, :], label='channel')
  3458                                                               ax.plot(asig.times[devFiltDebugMask], centerLFP[devFiltDebugMask, whichColumnToSubtract], label='mean')
  3459                                                               ax.plot(asig.times[devFiltDebugMask], noiseTerm[devFiltDebugMask], label='adjusted mean')
  3460                                                               ax.legend()
  3461                                                               plt.show()
  3462                                                           ###
  3463                                                           if removeMeanAcross:
  3464                                                               asig.magnitude[:] = np.atleast_2d(
  3465                                                                   asig.magnitude.flatten() - noiseTerm).transpose()
  3466                                                               # asig.magnitude[:] = (
  3467                                                               #     asig.magnitude - np.median(asig.magnitude))
  3468                                                       if (LFPFilterOpts is not None) and (aSigProxy not in ainpList):
  3469                                                           asig.magnitude[:] = filterFun(asig, filterCoeffs=filterCoeffs)
  3470                                                       if (interpolateOutliers) and (aSigProxy not in ainpList) and (not outlierRemovalDebugFlag):
  3471                                                           for k, cols in meanGroups.items():
  3472                                                               if asig.name in cols:
  3473                                                                   whichColumnToSubtract = k
  3474                                                           tempSer = pd.Series(asig.magnitude.flatten())
  3475                                                           tempSer.loc[outlierMask[:, whichColumnToSubtract]] = np.nan
  3476                                                           tempSer = (
  3477                                                               tempSer
  3478                                                               .interpolate(method='linear', limit_area='inside')
  3479                                                               .fillna(method='ffill')
  3480                                                               .fillna(method='bfill')
  3481                                                               )
  3482                                                           asig.magnitude[:, 0] = tempSer.to_numpy()
  3483                                                       # pdb.set_trace()
  3484                                                       if (aSigProxy in aSigList) or (aSigProxy in ainpList):
  3485                                                           # assign ownership to containers
  3486                                                           chanIdx.analogsignals.append(asig)
  3487                                                           newSeg.analogsignals.append(asig)
  3488                                                           # assign parent to children
  3489                                                           chanIdx.create_relationship()
  3490                                                           newSeg.create_relationship()
  3491                                                           # write out to file
  3492                                                           asig = writer._write_analogsignal(
  3493                                                               asig, nixblock, nixgroup)
  3494                                                       del asig
  3495                                                       gc.collect()
  3496                                               for irSigIdx, irSigProxy in enumerate(
  3497                                                       seg.irregularlysampledsignals):
  3498                                                   chanIdx = irSigProxy.channel_index
  3499                                                   #
  3500                                                   isig = irSigProxy.load(
  3501                                                       time_slice=(tStart, tStop),
  3502                                                       magnitude_mode='rescaled')
  3503                                                   #  link irregularlysampledSignal
  3504                                                   #  and ID providing channel_index
  3505                                                   isig.channel_index = chanIdx
  3506                                                   # assign ownership to containers
  3507                                                   chanIdx.irregularlysampledsignals.append(isig)
  3508                                                   newSeg.irregularlysampledsignals.append(isig)
  3509                                                   # assign parent to children
  3510                                                   chanIdx.create_relationship()
  3511                                                   newSeg.create_relationship()
  3512                                                   # write out to file
  3513                                                   isig = writer._write_irregularlysampledsignal(
  3514                                                       isig, nixblock, nixgroup)
  3515                                                   del isig
  3516                                                   gc.collect()
  3517                                               #
  3518                                               if len(spikeSourceType):
  3519                                                   for stIdx, stProxy in enumerate(spikeSeg.spiketrains):
  3520                                                       if trackMemory:
  3521                                                           print('writing spiketrains mem usage: {}'.format(
  3522                                                               prf.memory_usage_psutil()))
  3523                                                       unit = stProxy.unit
  3524                                                       st = loadStProxy(stProxy)
  3525                                                       #  have to manually slice tStop and tStart because
  3526                                                       #  array annotations are not saved natively in the nix file
  3527                                                       #  (we're getting them as plain annotations)
  3528                                                       timeMask = np.asarray(
  3529                                                           (st.times >= tStart) & (st.times < tStop),
  3530                                                           dtype=np.bool)
  3531                                                       try:
  3532                                                           if 'arrayAnnNames' in st.annotations:
  3533                                                               for key in st.annotations['arrayAnnNames']:
  3534                                                                   st.annotations[key] = np.asarray(
  3535                                                                       st.annotations[key])[timeMask]
  3536                                                           st = st[timeMask]
  3537                                                           st.t_start = tStart
  3538                                                           st.t_stop = tStop
  3539                                                       except Exception:
  3540                                                           traceback.print_exc()
  3541                                                       #  tdc may or may not have the same channel ids, but
  3542                                                       #  it will have consistent channel names
  3543                                                       nameParser = re.search(
  3544                                                           r'([a-zA-Z0-9]*)#(\d*)', unit.name)
  3545                                                       chanLabel = nameParser.group(1)
  3546                                                       unitId = nameParser.group(2)
  3547                                                       #
  3548                                                       chIdxName = unit.name.replace('_stim', '').split('#')[0]
  3549                                                       chanIdx = block.filter(objects=ChannelIndex, name=chIdxName)[0]
  3550                                                       # [i.name for i in block.filter(objects=ChannelIndex)]
  3551                                                       # [i.name for i in spikeBlock.filter(objects=Unit)]
  3552                                                       #  print(unit.name)
  3553                                                       if not (unit in chanIdx.units):
  3554                                                           # first time at this unit, add to its chanIdx
  3555                                                           unit.channel_index = chanIdx
  3556                                                           chanIdx.units.append(unit)
  3557                                                       #  except Exception:
  3558                                                       #      traceback.print_exc()
  3559                                                       st.name = 'seg{}_{}'.format(0, unit.name)
  3560                                                       # st.name = 'seg{}_{}'.format(idx, unit.name)
  3561                                                       #  link SpikeTrain and ID providing unit
  3562                                                       if calcAverageLFP:
  3563                                                           if 'arrayAnnNames' in st.annotations:
  3564                                                               st.annotations['arrayAnnNames'] = list(st.annotations['arrayAnnNames'])
  3565                                                           else:
  3566                                                               st.annotations['arrayAnnNames'] = []
  3567                                                           st.annotations['arrayAnnNames'].append('phase60hz')
  3568                                                           phase60hz = hf.interpolateDF(
  3569                                                               lineNoisePhaseDF,
  3570                                                               newX=st.times, columns=['phase']).to_numpy().flatten()
  3571                                                           st.annotations.update({'phase60hz': phase60hz})
  3572                                                           plotPhaseDist = False
  3573                                                           if plotPhaseDist:
  3574                                                               sns.distplot(phase60hz)
  3575                                                               plt.show()
  3576                                                       st.unit = unit
  3577                                                       # assign ownership to containers
  3578                                                       unit.spiketrains.append(st)
  3579                                                       newSeg.spiketrains.append(st)
  3580                                                       # assign parent to children
  3581                                                       unit.create_relationship()
  3582                                                       newSeg.create_relationship()
  3583                                                       # write out to file
  3584                                                       st = writer._write_spiketrain(st, nixblock, nixgroup)
  3585                                                       del st
  3586                                               #  process proprio trial related events
  3587                                               if calcRigEvents:
  3588                                                   print('Processing rig events...')
  3589                                                   analogData = []
  3590                                                   for key, value in eventInfo['inputIDs'].items():
  3591                                                       searchName = 'seg{}_'.format(0) + value
  3592                                                       ainpAsig = seg.filter(
  3593                                                           objects=AnalogSignalProxy,
  3594                                                           name=searchName)[0]
  3595                                                       ainpData = ainpAsig.load(
  3596                                                           time_slice=(tStart, tStop),
  3597                                                           magnitude_mode='rescaled')
  3598                                                       analogData.append(
  3599                                                           pd.DataFrame(ainpData.magnitude, columns=[key]))
  3600                                                       del ainpData
  3601                                                       gc.collect()
  3602                                                   motorData = pd.concat(analogData, axis=1)
  3603                                                   del analogData
  3604                                                   gc.collect()
  3605                                                   if motorEncoderMask is not None:
  3606                                                       ainpData = ainpAsig.load(
  3607                                                           time_slice=(tStart, tStop),
  3608                                                           magnitude_mode='rescaled')
  3609                                                       ainpTime = ainpData.times.magnitude
  3610                                                       meTimeMask = np.zeros_like(ainpTime, dtype=np.bool)
  3611                                                       for meTimeBounds in motorEncoderMask:
  3612                                                           meTimeMask = (
  3613                                                               meTimeMask |
  3614                                                               (
  3615                                                                   (ainpTime > meTimeBounds[0]) &
  3616                                                                   (ainpTime < meTimeBounds[1])
  3617                                                                   )
  3618                                                               )
  3619                                                       columnsToOverride = ['A-', 'A+', 'B-', 'B+', 'Z-', 'Z+']
  3620                                                       for colName in columnsToOverride:
  3621                                                           motorData.loc[~meTimeMask, colName] = motorData.loc[:, colName].quantile(q=0.05)
  3622                                                       del ainpData, ainpTime
  3623                                                       gc.collect()
  3624                                                   motorData = mea.processMotorData(
  3625                                                       motorData, ainpAsig.sampling_rate.magnitude,
  3626                                                       encoderCountPerDegree=encoderCountPerDegree
  3627                                                       )
  3628                                                   keepCols = [
  3629                                                       'position', 'velocity', 'velocityCat',
  3630                                                       'rightBut_int', 'leftBut_int',
  3631                                                       'rightLED_int', 'leftLED_int', 'simiTrigs_int']
  3632                                                   for colName in keepCols:
  3633                                                       if trackMemory:
  3634                                                           print('writing motorData memory usage: {:.1f} MB'.format(
  3635                                                               prf.memory_usage_psutil()))
  3636                                                       chanIdx = ChannelIndex(
  3637                                                           name=colName,
  3638                                                           index=np.asarray([0]),
  3639                                                           channel_names=np.asarray([0]))
  3640                                                       block.channel_indexes.append(chanIdx)
  3641                                                       motorAsig = AnalogSignal(
  3642                                                           motorData[colName].to_numpy() * pq.mV,
  3643                                                           name=colName,
  3644                                                           sampling_rate=ainpAsig.sampling_rate,
  3645                                                           dtype=np.float32)
  3646                                                       motorAsig.t_start = ainpAsig.t_start
  3647                                                       motorAsig.channel_index = chanIdx
  3648                                                       # assign ownership to containers
  3649                                                       chanIdx.analogsignals.append(motorAsig)
  3650                                                       newSeg.analogsignals.append(motorAsig)
  3651                                                       chanIdx.create_relationship()
  3652                                                       newSeg.create_relationship()
  3653                                                       # write out to file
  3654                                                       motorAsig = writer._write_analogsignal(
  3655                                                           motorAsig, nixblock, nixgroup)
  3656                                                       del motorAsig
  3657                                                       gc.collect()
  3658                                                   _, trialEvents = mea.getTrials(
  3659                                                       motorData, ainpAsig.sampling_rate.magnitude,
  3660                                                       float(tStart.magnitude), trialType=None)
  3661                                                   trialEvents.fillna(0)
  3662                                                   trialEvents.rename(
  3663                                                       columns={
  3664                                                           'Label': 'rig_property',
  3665                                                           'Details': 'rig_value'},
  3666                                                       inplace=True)
  3667                                                   del motorData
  3668                                                   gc.collect()
  3669                                                   eventList = eventDataFrameToEvents(
  3670                                                       trialEvents,
  3671                                                       idxT='Time',
  3672                                                       annCol=['rig_property', 'rig_value'])
  3673                                                   for event in eventList:
  3674                                                       if trackMemory:
  3675                                                           print(
  3676                                                               'writing motor events memory usage: {:.1f} MB'
  3677                                                               .format(prf.memory_usage_psutil()))
  3678                                                       event.segment = newSeg
  3679                                                       newSeg.events.append(event)
  3680                                                       newSeg.create_relationship()
  3681                                                       # write out to file
  3682                                                       event = writer._write_event(event, nixblock, nixgroup)
  3683                                                       del event
  3684                                                       gc.collect()
  3685                                                   del trialEvents, eventList
  3686                                               #
  3687                                               for eventProxy in seg.events:
  3688                                                   event = eventProxy.load(
  3689                                                       time_slice=(tStart, tStop))
  3690                                                   event.t_start = tStart
  3691                                                   event.t_stop = tStop
  3692                                                   event.segment = newSeg
  3693                                                   newSeg.events.append(event)
  3694                                                   newSeg.create_relationship()
  3695                                                   # write out to file
  3696                                                   event = writer._write_event(event, nixblock, nixgroup)
  3697                                                   del event
  3698                                                   gc.collect()
  3699                                               #
  3700                                               for epochProxy in seg.epochs:
  3701                                                   epoch = epochProxy.load(
  3702                                                       time_slice=(tStart, tStop))
  3703                                                   epoch.t_start = tStart
  3704                                                   epoch.t_stop = tStop
  3705                                                   epoch.segment = newSeg
  3706                                                   newSeg.events.append(epoch)
  3707                                                   newSeg.create_relationship()
  3708                                                   # write out to file
  3709                                                   epoch = writer._write_epoch(epoch, nixblock, nixgroup)
  3710                                                   del epoch
  3711                                                   gc.collect()
  3712                                               #
  3713                                               chanIdxDiscardNames = []
  3714                                               # descend into ChannelIndexes
  3715                                               for chanIdx in block.channel_indexes:
  3716                                                   if chanIdx.analogsignals or chanIdx.units:
  3717                                                       chanIdx = writer._write_channelindex(chanIdx, nixblock)
  3718                                                   else:
  3719                                                       chanIdxDiscardNames.append(chanIdx.name)
  3720                                               block.channel_indexes = [
  3721                                                   i
  3722                                                   for i in block.channel_indexes
  3723                                                   if i.name not in chanIdxDiscardNames
  3724                                                   ]
  3725                                               writer._create_source_links(block, nixblock)
  3726                                               return

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: purgeNixAnn at line 3728

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3728                                           @profile
  3729                                           def purgeNixAnn(
  3730                                                   block, annNames=['nix_name', 'neo_name']):
  3731                                               for annName in annNames:
  3732                                                   block.annotations.pop(annName, None)
  3733                                               for child in block.children_recur:
  3734                                                   if child.annotations:
  3735                                                       child.annotations = {
  3736                                                           k: v
  3737                                                           for k, v in child.annotations.items()
  3738                                                           if k not in annNames}
  3739                                               for child in block.data_children_recur:
  3740                                                   if child.annotations:
  3741                                                       child.annotations = {
  3742                                                           k: v
  3743                                                           for k, v in child.annotations.items()
  3744                                                           if k not in annNames}
  3745                                               return block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadContainerArrayAnn at line 3747

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3747                                           @profile
  3748                                           def loadContainerArrayAnn(
  3749                                                   container=None, trainList=None
  3750                                                   ):
  3751                                               assert (container is not None) or (trainList is not None)
  3752                                               #
  3753                                               spikesAndEvents = []
  3754                                               returnObj = []
  3755                                               if container is not None:
  3756                                                   #  need the line below! (RD: don't remember why, consider removing)
  3757                                                   container.create_relationship()
  3758                                                   #
  3759                                                   spikesAndEvents += (
  3760                                                       container.filter(objects=SpikeTrain) +
  3761                                                       container.filter(objects=Event)
  3762                                                       )
  3763                                                   returnObj.append(container)
  3764                                               if trainList is not None:
  3765                                                   spikesAndEvents += trainList
  3766                                                   returnObj.append(trainList)
  3767                                               #
  3768                                               if len(returnObj) == 1:
  3769                                                   returnObj = returnObj[0]
  3770                                               else:
  3771                                                   returnObj = tuple(returnObj)
  3772                                               #
  3773                                               for st in spikesAndEvents:
  3774                                                   st = loadObjArrayAnn(st)
  3775                                               return returnObj

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadObjArrayAnn at line 3777

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3777                                           @profile
  3778                                           def loadObjArrayAnn(st):
  3779                                               if 'arrayAnnNames' in st.annotations.keys():
  3780                                                   if isinstance(st.annotations['arrayAnnNames'], str):
  3781                                                       st.annotations['arrayAnnNames'] = [st.annotations['arrayAnnNames']]
  3782                                                   elif isinstance(st.annotations['arrayAnnNames'], tuple):
  3783                                                       st.annotations['arrayAnnNames'] = [i for i in st.annotations['arrayAnnNames']]
  3784                                                   #
  3785                                                   for key in st.annotations['arrayAnnNames']:
  3786                                                       #  fromRaw, the ann come back as tuple, need to recast
  3787                                                       try:
  3788                                                           if len(st.times) == 1:
  3789                                                               st.annotations[key] = np.atleast_1d(st.annotations[key]).flatten()
  3790                                                           st.array_annotations.update(
  3791                                                               {key: np.asarray(st.annotations[key])})
  3792                                                           st.annotations[key] = np.asarray(st.annotations[key])
  3793                                                       except Exception:
  3794                                                           print('Error with {}'.format(st.name))
  3795                                                           traceback.print_exc()
  3796                                                           pdb.set_trace()
  3797                                               if hasattr(st, 'waveforms'):
  3798                                                   if st.waveforms is None:
  3799                                                       st.waveforms = np.asarray([]).reshape((0, 0, 0)) * pq.mV
  3800                                                   elif not len(st.waveforms):
  3801                                                       st.waveforms = np.asarray([]).reshape((0, 0, 0)) * pq.mV
  3802                                               return st

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadWithArrayAnn at line 3804

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3804                                           @profile
  3805                                           def loadWithArrayAnn(
  3806                                                   dataPath, fromRaw=False,
  3807                                                   mapDF=None, reduceChannelIndexes=False):
  3808                                               if fromRaw:
  3809                                                   reader = nixio_fr.NixIO(filename=dataPath)
  3810                                                   block = readBlockFixNames(
  3811                                                       reader, lazy=False,
  3812                                                       mapDF=mapDF,
  3813                                                       reduceChannelIndexes=reduceChannelIndexes)
  3814                                               else:
  3815                                                   reader = NixIO(filename=dataPath)
  3816                                                   block = reader.read_block()
  3817                                                   # [un.name for un in block.filter(objects=Unit)]
  3818                                                   # [len(un.spiketrains) for un in block.filter(objects=Unit)]
  3819                                               
  3820                                               block = loadContainerArrayAnn(container=block)
  3821                                               
  3822                                               if fromRaw:
  3823                                                   reader.file.close()
  3824                                               else:
  3825                                                   reader.close()
  3826                                               return block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: blockFromPath at line 3828

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3828                                           @profile
  3829                                           def blockFromPath(
  3830                                                   dataPath, lazy=False, mapDF=None,
  3831                                                   reduceChannelIndexes=False, loadList=None,
  3832                                                   purgeNixNames=False, chunkingInfoPath=None):
  3833                                               chunkingMetadata = None
  3834                                               if chunkingInfoPath is not None:
  3835                                                   if os.path.exists(chunkingInfoPath):
  3836                                                       with open(chunkingInfoPath, 'r') as f:
  3837                                                           chunkingMetadata = json.load(f)
  3838                                               if chunkingMetadata is None:
  3839                                                   chunkingMetadata = {
  3840                                                       '0': {
  3841                                                           'filename': dataPath,
  3842                                                           'partNameSuffix': '',
  3843                                                           'chunkTStart': 0,
  3844                                                           'chunkTStop': 'NaN'
  3845                                                       }}
  3846                                               for idx, (chunkIdxStr, chunkMeta) in enumerate(chunkingMetadata.items()):   
  3847                                                   thisDataPath = chunkMeta['filename']
  3848                                                   assert os.path.exists(thisDataPath)
  3849                                                   if idx == 0:
  3850                                                       if lazy:
  3851                                                           dataReader = nixio_fr.NixIO(
  3852                                                               filename=thisDataPath)
  3853                                                           dataBlock = readBlockFixNames(
  3854                                                               dataReader, lazy=lazy, mapDF=mapDF,
  3855                                                               reduceChannelIndexes=reduceChannelIndexes,
  3856                                                               purgeNixNames=purgeNixNames, loadList=loadList)
  3857                                                       else:
  3858                                                           dataReader = None
  3859                                                           dataBlock = loadWithArrayAnn(thisDataPath)
  3860                                                   else:
  3861                                                       if lazy:
  3862                                                           dataReader2 = nixio_fr.NixIO(
  3863                                                               filename=thisDataPath)
  3864                                                           dataBlock2 = readBlockFixNames(
  3865                                                               dataReader2, lazy=lazy, mapDF=mapDF,
  3866                                                               reduceChannelIndexes=reduceChannelIndexes, loadList=loadList)
  3867                                                       else:
  3868                                                           dataReader2 = None
  3869                                                           dataBlock2 = loadWithArrayAnn(thisDataPath)
  3870                                                       maxSegIdx = len(dataBlock.segments)
  3871                                                       typesNeedRenaming = [
  3872                                                           SpikeTrainProxy, AnalogSignalProxy, EventProxy,
  3873                                                           SpikeTrain, AnalogSignal, Event]
  3874                                                       for segIdx, seg in enumerate(dataBlock2.segments):
  3875                                                           if seg.name is None:
  3876                                                               seg.name = 'seg{}_'.format(maxSegIdx + segIdx)
  3877                                                           else:
  3878                                                               if 'seg{}_'.format(maxSegIdx + segIdx) not in seg.name:
  3879                                                                   seg.name = (
  3880                                                                       'seg{}_{}'
  3881                                                                       .format(
  3882                                                                           maxSegIdx + segIdx,
  3883                                                                           childBaseName(seg.name, 'seg')))
  3884                                                           for objType in typesNeedRenaming:
  3885                                                               for child in seg.filter(objects=objType):
  3886                                                                   if 'seg{}_'.format(maxSegIdx + segIdx) not in child.name:
  3887                                                                       child.name = (
  3888                                                                           'seg{}_{}'
  3889                                                                           .format(
  3890                                                                               maxSegIdx + segIdx, childBaseName(child.name, 'seg')))
  3891                                                       dataBlock.merge(dataBlock2)
  3892                                               return dataReader, dataBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: calcBinarizedArray at line 3894

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3894                                           @profile
  3895                                           def calcBinarizedArray(
  3896                                                   dataBlock, samplingRate,
  3897                                                   binnedSpikePath=None,
  3898                                                   saveToFile=True, matchT=None):
  3899                                               #
  3900                                               spikeMatBlock = Block(name=dataBlock.name + '_binarized')
  3901                                               spikeMatBlock.merge_annotations(dataBlock)
  3902                                               #
  3903                                               allSpikeTrains = [
  3904                                                   i for i in dataBlock.filter(objects=SpikeTrain)]
  3905                                               #
  3906                                               for st in allSpikeTrains:
  3907                                                   chanList = spikeMatBlock.filter(
  3908                                                       objects=ChannelIndex, name=st.unit.name)
  3909                                                   if not len(chanList):
  3910                                                       chanIdx = ChannelIndex(name=st.unit.name, index=np.asarray([0]))
  3911                                                       #  print(chanIdx.name)
  3912                                                       spikeMatBlock.channel_indexes.append(chanIdx)
  3913                                                       thisUnit = Unit(name=st.unit.name)
  3914                                                       chanIdx.units.append(thisUnit)
  3915                                                       thisUnit.channel_index = chanIdx
  3916                                               #
  3917                                               for segIdx, seg in enumerate(dataBlock.segments):
  3918                                                   newSeg = Segment(name='seg{}_{}'.format(segIdx, spikeMatBlock.name))
  3919                                                   newSeg.merge_annotations(seg)
  3920                                                   spikeMatBlock.segments.append(newSeg)
  3921                                                   #  tStart = dataBlock.segments[0].t_start
  3922                                                   #  tStop = dataBlock.segments[0].t_stop
  3923                                                   tStart = seg.t_start
  3924                                                   tStop = seg.t_stop
  3925                                                   # make dummy binary spike train, in case ths chan didn't fire
  3926                                                   segSpikeTrains = [
  3927                                                       i for i in seg.filter(objects=SpikeTrain) if '#' in i.name]
  3928                                                   dummyBin = binarize(
  3929                                                       segSpikeTrains[0],
  3930                                                       sampling_rate=samplingRate,
  3931                                                       t_start=tStart,
  3932                                                       t_stop=tStop + samplingRate ** -1) * 0
  3933                                                   for chanIdx in spikeMatBlock.channel_indexes:
  3934                                                       #  print(chanIdx.name)
  3935                                                       stList = seg.filter(
  3936                                                           objects=SpikeTrain,
  3937                                                           name='seg{}_{}'.format(segIdx, chanIdx.name)
  3938                                                           )
  3939                                                       if len(stList):
  3940                                                           st = stList[0]
  3941                                                           print('binarizing {}'.format(st.name))
  3942                                                           stBin = binarize(
  3943                                                               st,
  3944                                                               sampling_rate=samplingRate,
  3945                                                               t_start=tStart,
  3946                                                               t_stop=tStop + samplingRate ** -1)
  3947                                                           spikeMatBlock.segments[segIdx].spiketrains.append(st)
  3948                                                           #  to do: link st to spikematblock's chidx and units
  3949                                                           assert len(chanIdx.filter(objects=Unit)) == 1
  3950                                                           thisUnit = chanIdx.filter(objects=Unit)[0]
  3951                                                           thisUnit.spiketrains.append(st)
  3952                                                           st.unit = thisUnit
  3953                                                           st.segment = spikeMatBlock.segments[segIdx]
  3954                                                       else:
  3955                                                           print('{} has no spikes'.format(st.name))
  3956                                                           stBin = dummyBin
  3957                                                       skipStAnnNames = [
  3958                                                           'nix_name', 'neo_name', 'arrayAnnNames']
  3959                                                       if 'arrayAnnNames' in st.annotations:
  3960                                                           skipStAnnNames += list(st.annotations['arrayAnnNames'])
  3961                                                       asigAnn = {
  3962                                                           k: v
  3963                                                           for k, v in st.annotations.items()
  3964                                                           if k not in skipStAnnNames
  3965                                                           }
  3966                                                       asig = AnalogSignal(
  3967                                                           stBin * samplingRate,
  3968                                                           name='seg{}_{}_raster'.format(segIdx, st.unit.name),
  3969                                                           sampling_rate=samplingRate,
  3970                                                           dtype=np.int,
  3971                                                           **asigAnn)
  3972                                                       if matchT is not None:
  3973                                                           asig = asig[:matchT.shape[0], :]
  3974                                                       asig.t_start = tStart
  3975                                                       asig.annotate(binWidth=1 / samplingRate.magnitude)
  3976                                                       chanIdx.analogsignals.append(asig)
  3977                                                       asig.channel_index = chanIdx
  3978                                                       spikeMatBlock.segments[segIdx].analogsignals.append(asig)
  3979                                               #
  3980                                               for chanIdx in spikeMatBlock.channel_indexes:
  3981                                                   chanIdx.name = chanIdx.name + '_raster'
  3982                                               #
  3983                                               spikeMatBlock.create_relationship()
  3984                                               spikeMatBlock = purgeNixAnn(spikeMatBlock)
  3985                                               if saveToFile:
  3986                                                   if os.path.exists(binnedSpikePath):
  3987                                                       os.remove(binnedSpikePath)
  3988                                                   writer = NixIO(filename=binnedSpikePath)
  3989                                                   writer.write_block(spikeMatBlock, use_obj_names=True)
  3990                                                   writer.close()
  3991                                               return spikeMatBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: calcFR at line 3993

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3993                                           @profile
  3994                                           def calcFR(
  3995                                                   binnedPath, dataPath,
  3996                                                   suffix='fr', aggregateFun=None,
  3997                                                   chanNames=None, rasterOpts=None, verbose=False
  3998                                                   ):
  3999                                               print('Loading rasters...')
  4000                                               masterSpikeMats, _ = loadSpikeMats(
  4001                                                   binnedPath, rasterOpts,
  4002                                                   aggregateFun=aggregateFun,
  4003                                                   chans=chanNames,
  4004                                                   loadAll=True, checkReferences=False)
  4005                                               print('Loading data file...')
  4006                                               dataReader = nixio_fr.NixIO(
  4007                                                   filename=dataPath)
  4008                                               dataBlock = dataReader.read_block(
  4009                                                   block_index=0, lazy=True,
  4010                                                   signal_group_mode='split-all')
  4011                                               masterBlock = Block()
  4012                                               masterBlock.name = dataBlock.annotations['neo_name']
  4013                                               #
  4014                                               for segIdx, segSpikeMat in masterSpikeMats.items():
  4015                                                   print('Calculating FR for segment {}'.format(segIdx))
  4016                                                   spikeMatDF = segSpikeMat.reset_index().rename(
  4017                                                       columns={'bin': 't'})
  4018                                           
  4019                                                   dataSeg = dataBlock.segments[segIdx]
  4020                                                   dummyAsig = dataSeg.filter(
  4021                                                       objects=AnalogSignalProxy)[0].load(channel_indexes=[0])
  4022                                                   samplingRate = dummyAsig.sampling_rate
  4023                                                   newT = dummyAsig.times.magnitude
  4024                                                   spikeMatDF['t'] = spikeMatDF['t'] + newT[0]
  4025                                           
  4026                                                   segSpikeMatInterp = hf.interpolateDF(
  4027                                                       spikeMatDF, pd.Series(newT),
  4028                                                       kind='linear', fill_value=(0, 0),
  4029                                                       x='t')
  4030                                                   spikeMatBlockInterp = dataFrameToAnalogSignals(
  4031                                                       segSpikeMatInterp,
  4032                                                       idxT='t', useColNames=True,
  4033                                                       dataCol=segSpikeMatInterp.drop(columns='t').columns,
  4034                                                       samplingRate=samplingRate)
  4035                                                   spikeMatBlockInterp.name = dataBlock.annotations['neo_name']
  4036                                                   spikeMatBlockInterp.annotate(
  4037                                                       nix_name=dataBlock.annotations['neo_name'])
  4038                                                   spikeMatBlockInterp.segments[0].name = dataSeg.annotations['neo_name']
  4039                                                   spikeMatBlockInterp.segments[0].annotate(
  4040                                                       nix_name=dataSeg.annotations['neo_name'])
  4041                                                   asigList = spikeMatBlockInterp.filter(objects=AnalogSignal)
  4042                                                   for asig in asigList:
  4043                                                       asig.annotate(binWidth=rasterOpts['binWidth'])
  4044                                                       if '_raster' in asig.name:
  4045                                                           asig.name = asig.name.replace('_raster', '_' + suffix)
  4046                                                       asig.name = 'seg{}_{}'.format(segIdx, childBaseName(asig.name, 'seg'))
  4047                                                       asig.annotate(nix_name=asig.name)
  4048                                                   chanIdxList = spikeMatBlockInterp.filter(objects=ChannelIndex)
  4049                                                   for chanIdx in chanIdxList:
  4050                                                       if '_raster' in chanIdx.name:
  4051                                                           chanIdx.name = chanIdx.name.replace('_raster', '_' + suffix)
  4052                                                       chanIdx.annotate(nix_name=chanIdx.name)
  4053                                           
  4054                                                   # masterBlock.merge(spikeMatBlockInterp)
  4055                                                   frBlockPath = dataPath.replace('_analyze.nix', '_fr.nix')
  4056                                                   writer = NixIO(filename=frBlockPath)
  4057                                                   writer.write_block(spikeMatBlockInterp, use_obj_names=True)
  4058                                                   writer.close()
  4059                                               #
  4060                                               dataReader.file.close()
  4061                                               return masterBlock

Timer unit: 1e-07 s

Total time: 31.6217 s
File: C\../../analysis-code/calcTrialOutliers.py
Function: findOutliers at line 147

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   147                                           @profile
   148                                           def findOutliers(
   149                                                   mahalDistDF, groupBy=None,
   150                                                   qThresh=None, sdThresh=None, sdThreshInner=None,
   151                                                   devQuantile=None, nDim=1, multiplier=1, twoTailed=False):
   152                                               #
   153         1         22.0     22.0      0.0      if sdThresh is None:
   154         1         11.0     11.0      0.0          if qThresh is None:
   155                                                       qThresh = 1 - 1e-6
   156         1       7677.0   7677.0      0.0          chi2Bounds = chi2.interval(qThresh, nDim)
   157         1         26.0     26.0      0.0          sdThresh = multiplier * chi2Bounds[1]
   158                                               #
   159         1          6.0      6.0      0.0      if twoTailed:
   160         1         17.0     17.0      0.0          chiProba = pd.Series(
   161         1    3335589.0 3335589.0      1.1              -np.log(np.squeeze(chi2.pdf(mahalDistDF, nDim))),
   162         1       2279.0   2279.0      0.0              index=mahalDistDF.index)
   163         1       3227.0   3227.0      0.0          chiProbaLim = -np.log(chi2.pdf(sdThresh, nDim))
   164         1         10.0     10.0      0.0          if devQuantile is not None:
   165         1  312816573.0 312816573.0     98.9              deviation = chiProba.groupby(groupBy).quantile(q=devQuantile)
   166                                                   else:
   167                                                       deviation = chiProba.groupby(groupBy).max()
   168                                               else:
   169                                                   if devQuantile is not None:
   170                                                       deviation = mahalDistDF['mahalDist'].groupby(groupBy).quantile(q=devQuantile)
   171                                                   else:
   172                                                       deviation = mahalDistDF['mahalDist'].groupby(groupBy).max()
   173         1      17157.0  17157.0      0.0      deviationDF = deviation.to_frame(name='deviation')
   174                                               #
   175         1         30.0     30.0      0.0      if twoTailed:
   176         1      34615.0  34615.0      0.0          deviationDF['rejectBlock'] = (deviationDF['deviation'] > chiProbaLim)
   177                                               else:
   178                                                   deviationDF['rejectBlock'] = (deviationDF['deviation'] > sdThresh)
   179                                               #
   180         1         29.0     29.0      0.0      return deviationDF

Total time: 134.071 s
File: C\../../analysis-code/calcTrialOutliers.py
Function: calcCovMat at line 182

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   182                                           @profile
   183                                           def calcCovMat(
   184                                                   partition, dataColNames=None,
   185                                                   useMinCovDet=True,
   186                                                   supportFraction=None, verbose=False):
   187     13746   34616303.0   2518.3      2.6      dataColMask = partition.columns.isin(dataColNames)
   188     13746  116119035.0   8447.5      8.7      partitionData = partition.loc[:, dataColMask]
   189                                               # print('partition shape = {}'.format(partitionData.shape))
   190     13746     215714.0     15.7      0.0      if useMinCovDet:
   191                                                   try:
   192                                                       est = MinCovDet(support_fraction=supportFraction)
   193                                                       est.fit(partitionData.values)
   194                                                   except Exception:
   195                                                       traceback.print_exc()
   196                                                       print('\npartition shape = {}\n'.format(partitionData.shape))
   197                                                       est = EmpiricalCovariance()
   198                                                       est.fit(partitionData.values)
   199                                               else:
   200     13746    2069512.0    150.6      0.2          est = EmpiricalCovariance()
   201     13746  142724396.0  10383.0     10.6          est.fit(partitionData.values)
   202     13746     359777.0     26.2      0.0      result = pd.DataFrame(
   203     13746   99314284.0   7225.0      7.4          est.mahalanobis(partitionData.values),
   204     13746  135293329.0   9842.4     10.1          index=partition.index, columns=['mahalDist'])
   205                                               # print('result shape is {}'.format(result.shape))
   206     13746     338649.0     24.6      0.0      result = pd.concat(
   207     13746  173251409.0  12603.8     12.9          [result, partition.loc[:, ~dataColMask]],
   208     13746  627469547.0  45647.4     46.8          axis=1)
   209     13746    8799134.0    640.1      0.7      result.name = 'mahalanobisDistance'
   210                                               #
   211                                               # if result['electrode'].iloc[0] == 'foo':
   212                                               #     pdb.set_trace()
   213                                               # print('result type is {}'.format(type(result)))
   214                                               # print(result.T)
   215                                               # print('partition shape = {}'.format(partitionData.shape))
   216     13746     143576.0     10.4      0.0      return result

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: analogSignalsToDataFrame at line 43

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    43                                           @profile
    44                                           def analogSignalsToDataFrame(
    45                                                   analogsignals, idxT='t', useChanNames=False):
    46                                               asigList = []
    47                                               for asig in analogsignals:
    48                                                   if asig.shape[1] == 1:
    49                                                       if useChanNames:
    50                                                           colNames = [str(asig.channel_index.name)]
    51                                                       else:
    52                                                           colNames = [str(asig.name)]
    53                                                   else:
    54                                                       colNames = [
    55                                                           asig.name +
    56                                                           '_{}'.format(i) for i in
    57                                                           asig.channel_index.channel_ids
    58                                                           ]
    59                                                   asigList.append(
    60                                                       pd.DataFrame(
    61                                                           asig.magnitude, columns=colNames,
    62                                                           index=range(asig.shape[0])))
    63                                               asigList.append(
    64                                                   pd.DataFrame(
    65                                                       asig.times.magnitude, columns=[idxT],
    66                                                       index=range(asig.shape[0])))
    67                                               return pd.concat(asigList, axis=1)

Total time: 0.016549 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: listChanNames at line 69

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    69                                           @profile
    70                                           def listChanNames(
    71                                                   dataBlock, chanQuery,
    72                                                   objType=AnalogSignalProxy, condition=None):
    73                                               allChanList = [
    74         1         32.0     32.0      0.0          i.name
    75         1      25364.0  25364.0     15.3          for i in dataBlock.filter(objects=objType)]
    76         1         28.0     28.0      0.0      if condition == 'hasAsigs':
    77                                                   allChanList = [
    78                                                       i
    79                                                       for i in allChanList
    80                                                       if len(dataBlock.filter(objects=objType, name=i)[0].analogsignals)
    81                                                   ]
    82         1         41.0     41.0      0.0      chansToTrigger = pd.DataFrame(
    83         1       1789.0   1789.0      1.1          np.unique(allChanList),
    84         1      22668.0  22668.0     13.7          columns=['chanName'])
    85         1         33.0     33.0      0.0      if chanQuery is not None:
    86         1         37.0     37.0      0.0          chansToTrigger = chansToTrigger.query(
    87         1     115469.0 115469.0     69.8              chanQuery, engine='python')['chanName'].to_list()
    88                                               else:
    89                                                   chansToTrigger = chansToTrigger['chanName'].to_list()
    90         1         29.0     29.0      0.0      return chansToTrigger

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: spikeDictToSpikeTrains at line 92

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    92                                           @profile
    93                                           def spikeDictToSpikeTrains(
    94                                                   spikes, block=None, seg=None,
    95                                                   probeName='insTD', t_stop=None,
    96                                                   waveformUnits=pq.uV,
    97                                                   sampling_rate=3e4 * pq.Hz):
    98                                           
    99                                               if block is None:
   100                                                   assert seg is None
   101                                                   block = Block()
   102                                                   seg = Segment(name=probeName + ' segment')
   103                                                   block.segments.append(seg)
   104                                           
   105                                               if t_stop is None:
   106                                                   t_stop = hf.getLastSpikeTime(spikes) + 1
   107                                           
   108                                               for idx, chanName in enumerate(spikes['ChannelID']):
   109                                                   #  unique units on this channel
   110                                                   unitsOnThisChan = pd.unique(spikes['Classification'][idx])
   111                                                   nixChanName = probeName + '{}'.format(chanName)
   112                                                   chanIdx = ChannelIndex(
   113                                                       name=nixChanName,
   114                                                       index=np.asarray([idx]),
   115                                                       channel_names=np.asarray([nixChanName]))
   116                                                   block.channel_indexes.append(chanIdx)
   117                                                   
   118                                                   for unitIdx, unitName in enumerate(unitsOnThisChan):
   119                                                       unitMask = spikes['Classification'][idx] == unitName
   120                                                       # this unit's spike timestamps
   121                                                       theseTimes = spikes['TimeStamps'][idx][unitMask]
   122                                                       # this unit's waveforms
   123                                                       if len(spikes['Waveforms'][idx].shape) == 3:
   124                                                           theseWaveforms = spikes['Waveforms'][idx][unitMask, :, :]
   125                                                           theseWaveforms = np.swapaxes(theseWaveforms, 1, 2)
   126                                                       elif len(spikes['Waveforms'][idx].shape) == 2:
   127                                                           theseWaveforms = (
   128                                                               spikes['Waveforms'][idx][unitMask, np.newaxis, :])
   129                                                       else:
   130                                                           raise(Exception('spikes[Waveforms] has bad shape'))
   131                                           
   132                                                       unitName = '{}#{}'.format(nixChanName, unitIdx)
   133                                                       unit = Unit(name=unitName)
   134                                                       unit.channel_index = chanIdx
   135                                                       chanIdx.units.append(unit)
   136                                           
   137                                                       train = SpikeTrain(
   138                                                           times=theseTimes, t_stop=t_stop, units='sec',
   139                                                           name=unitName, sampling_rate=sampling_rate,
   140                                                           waveforms=theseWaveforms*waveformUnits,
   141                                                           left_sweep=0, dtype=np.float32)
   142                                                       unit.spiketrains.append(train)
   143                                                       seg.spiketrains.append(train)
   144                                           
   145                                                       unit.create_relationship()
   146                                                   chanIdx.create_relationship()
   147                                               seg.create_relationship()
   148                                               block.create_relationship()
   149                                               return block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: spikeTrainsToSpikeDict at line 151

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   151                                           @profile
   152                                           def spikeTrainsToSpikeDict(
   153                                                   spiketrains):
   154                                               nCh = len(spiketrains)
   155                                               spikes = {
   156                                                   'ChannelID': [i for i in range(nCh)],
   157                                                   'Classification': [np.asarray([]) for i in range(nCh)],
   158                                                   'NEUEVWAV_HeaderIndices': [None for i in range(nCh)],
   159                                                   'TimeStamps': [np.asarray([]) for i in range(nCh)],
   160                                                   'Units': 'uV',
   161                                                   'Waveforms': [np.asarray([]) for i in range(nCh)],
   162                                                   'basic_headers': {'TimeStampResolution': 3e4},
   163                                                   'extended_headers': []
   164                                                   }
   165                                               for idx, st in enumerate(spiketrains):
   166                                                   spikes['ChannelID'][idx] = st.name
   167                                                   if len(spikes['TimeStamps'][idx]):
   168                                                       spikes['TimeStamps'][idx] = np.stack((
   169                                                           spikes['TimeStamps'][idx],
   170                                                           st.times.magnitude), axis=-1)
   171                                                   else:
   172                                                       spikes['TimeStamps'][idx] = st.times.magnitude
   173                                                   
   174                                                   theseWaveforms = np.swapaxes(
   175                                                       st.waveforms, 1, 2)
   176                                                   theseWaveforms = np.atleast_2d(np.squeeze(
   177                                                       theseWaveforms))
   178                                                       
   179                                                   if len(spikes['Waveforms'][idx]):
   180                                                       spikes['Waveforms'][idx] = np.stack((
   181                                                           spikes['Waveforms'][idx],
   182                                                           theseWaveforms.magnitude), axis=-1)
   183                                                   else:
   184                                                       spikes['Waveforms'][idx] = theseWaveforms.magnitude
   185                                                   
   186                                                   classVals = st.times.magnitude ** 0 * idx
   187                                                   if len(spikes['Classification'][idx]):
   188                                                       spikes['Classification'][idx] = np.stack((
   189                                                           spikes['Classification'][idx],
   190                                                           classVals), axis=-1)
   191                                                   else:
   192                                                       spikes['Classification'][idx] = classVals
   193                                               return spikes

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: channelIndexesToSpikeDict at line 195

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   195                                           @profile
   196                                           def channelIndexesToSpikeDict(
   197                                                   channel_indexes):
   198                                               nCh = len(channel_indexes)
   199                                               spikes = {
   200                                                   'ChannelID': [i for i in range(nCh)],
   201                                                   'Classification': [np.asarray([]) for i in range(nCh)],
   202                                                   'NEUEVWAV_HeaderIndices': [None for i in range(nCh)],
   203                                                   'TimeStamps': [np.asarray([]) for i in range(nCh)],
   204                                                   'Units': 'uV',
   205                                                   'Waveforms': [np.asarray([]) for i in range(nCh)],
   206                                                   'basic_headers': {'TimeStampResolution': 3e4},
   207                                                   'extended_headers': []
   208                                                   }
   209                                               #  allocate fields for annotations
   210                                               for dummyCh in channel_indexes:
   211                                                   if len(dummyCh.units):
   212                                                       dummyUnit = dummyCh.units[0]
   213                                                       if len(dummyUnit.spiketrains):
   214                                                           if len(dummyUnit.spiketrains[0].times):
   215                                                               break
   216                                               dummySt = [
   217                                                   st
   218                                                   for st in dummyUnit.spiketrains
   219                                                   if len(st.times)][0]
   220                                               #  allocate fields for array annotations (per spike)
   221                                               if dummySt.array_annotations:
   222                                                   for key in dummySt.array_annotations.keys():
   223                                                       spikes.update({key: [np.asarray([]) for i in range(nCh)]})
   224                                                   
   225                                               maxUnitIdx = 0
   226                                               for idx, chIdx in enumerate(channel_indexes):
   227                                                   spikes['ChannelID'][idx] = chIdx.name
   228                                                   for unitIdx, thisUnit in enumerate(chIdx.units):
   229                                                       for stIdx, st in enumerate(thisUnit.spiketrains):
   230                                                           if not len(st.times):
   231                                                               continue
   232                                                           #  print(
   233                                                           #      'unit {} has {} spiketrains'.format(
   234                                                           #          thisUnit.name,
   235                                                           #          len(thisUnit.spiketrains)))
   236                                                           if len(spikes['TimeStamps'][idx]):
   237                                                               spikes['TimeStamps'][idx] = np.concatenate((
   238                                                                   spikes['TimeStamps'][idx],
   239                                                                   st.times.magnitude), axis=0)
   240                                                           else:
   241                                                               spikes['TimeStamps'][idx] = st.times.magnitude
   242                                                           #  reshape waveforms to comply with BRM convention
   243                                                           theseWaveforms = np.swapaxes(
   244                                                               st.waveforms, 1, 2)
   245                                                           theseWaveforms = np.atleast_2d(np.squeeze(
   246                                                               theseWaveforms))
   247                                                           #  append waveforms
   248                                                           if len(spikes['Waveforms'][idx]):
   249                                                               try:
   250                                                                   spikes['Waveforms'][idx] = np.concatenate((
   251                                                                       spikes['Waveforms'][idx],
   252                                                                       theseWaveforms.magnitude), axis=0)
   253                                                               except Exception:
   254                                                                   traceback.print_exc()
   255                                                           else:
   256                                                               spikes['Waveforms'][idx] = theseWaveforms.magnitude
   257                                                           #  give each unit a global index
   258                                                           classVals = st.times.magnitude ** 0 * maxUnitIdx
   259                                                           st.array_annotations.update({'Classification': classVals})
   260                                                           #  expand array_annotations into spikes dict
   261                                                           for key, value in st.array_annotations.items():
   262                                                               if len(spikes[key][idx]):
   263                                                                   spikes[key][idx] = np.concatenate((
   264                                                                       spikes[key][idx],
   265                                                                       value), axis=0)
   266                                                               else:
   267                                                                   spikes[key][idx] = value
   268                                                           for key, value in st.annotations.items():
   269                                                               if key not in spikes['basic_headers']:
   270                                                                   spikes['basic_headers'].update({key: {}})
   271                                                               try:
   272                                                                   spikes['basic_headers'][key].update({maxUnitIdx: value})
   273                                                               except Exception:
   274                                                                   pass
   275                                                           maxUnitIdx += 1
   276                                               return spikes

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: unitSpikeTrainArrayAnnToDF at line 278

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   278                                           @profile
   279                                           def unitSpikeTrainArrayAnnToDF(
   280                                                   spikeTrainContainer):
   281                                               #  list contains different segments
   282                                               if isinstance(spikeTrainContainer, ChannelIndex):
   283                                                   assert len(spikeTrainContainer.units) == 0
   284                                                   spiketrains = spikeTrainContainer.units[0].spiketrains
   285                                               elif isinstance(spikeTrainContainer, Unit):
   286                                                   spiketrains = spikeTrainContainer.spiketrains
   287                                               elif isinstance(spikeTrainContainer, list):
   288                                                   spiketrains = spikeTrainContainer
   289                                               fullAnnotationsDict = {}
   290                                               for segIdx, st in enumerate(spiketrains):
   291                                                   theseAnnDF = pd.DataFrame(st.array_annotations)
   292                                                   theseAnnDF['t'] = st.times.magnitude
   293                                                   fullAnnotationsDict.update({segIdx: theseAnnDF})
   294                                               annotationsDF = pd.concat(
   295                                                   fullAnnotationsDict, names=['segment', 'index'], sort=True)
   296                                               return annotationsDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getSpikeDFMetadata at line 298

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   298                                           @profile
   299                                           def getSpikeDFMetadata(spikeDF, metaDataCols):
   300                                               spikeDF.reset_index(inplace=True)
   301                                               metaDataCols = np.atleast_1d(metaDataCols)
   302                                               spikeDF.index.name = 'metaDataIdx'
   303                                               metaDataDF = spikeDF.loc[:, metaDataCols].copy()
   304                                               newSpikeDF = spikeDF.drop(columns=metaDataCols).reset_index()
   305                                               return newSpikeDF, metaDataDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: transposeSpikeDF at line 307

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   307                                           @profile
   308                                           def transposeSpikeDF(
   309                                                   spikeDF, transposeToColumns,
   310                                                   fastTranspose=False):
   311                                               newColumnNames = np.atleast_1d(transposeToColumns).tolist()
   312                                               originalColumnNames = np.atleast_1d(spikeDF.columns.names)
   313                                               metaDataCols = np.setdiff1d(spikeDF.index.names, newColumnNames).tolist()
   314                                               if fastTranspose:
   315                                                   #  fast but memory inefficient
   316                                                   return spikeDF.stack().unstack(transposeToColumns)
   317                                               else:
   318                                                   raise(Warning('Caution! transposeSpikeDF might not be working, needs testing RD 06252019'))
   319                                                   #  stash annotations, transpose, recover annotations
   320                                                   newSpikeDF, metaDataDF = getSpikeDFMetadata(spikeDF, metaDataCols)
   321                                                   del spikeDF
   322                                                   gc.collect()
   323                                                   #
   324                                                   newSpikeDF = newSpikeDF.stack().unstack(newColumnNames)
   325                                                   newSpikeDF.reset_index(inplace=True)
   326                                                   #  set the index
   327                                                   newIdxLabels = np.concatenate(
   328                                                       [originalColumnNames, metaDataCols]).tolist()
   329                                                   newSpikeDF.loc[:, metaDataCols] = (
   330                                                       metaDataDF
   331                                                       .loc[newSpikeDF['metaDataIdx'].to_list(), metaDataCols]
   332                                                       .to_numpy())
   333                                                   newSpikeDF = (
   334                                                       newSpikeDF
   335                                                       .drop(columns=['metaDataIdx'])
   336                                                       .set_index(newIdxLabels))
   337                                                   return newSpikeDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: concatenateBlocks at line 339

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   339                                           @profile
   340                                           def concatenateBlocks(
   341                                                   asigBlocks, spikeBlocks, eventBlocks, chunkingMetadata,
   342                                                   samplingRate, chanQuery, lazy, trackMemory, verbose
   343                                                   ):
   344                                               # Scan ahead through all files and ensure that
   345                                               # spikeTrains and units are present across all assembled files
   346                                               channelIndexCache = {}
   347                                               unitCache = {}
   348                                               asigCache = []
   349                                               asigAnnCache = {}
   350                                               spiketrainCache = {}
   351                                               eventCache = {}
   352                                               # get list of channels and units
   353                                               for idx, (chunkIdxStr, chunkMeta) in enumerate(chunkingMetadata.items()):
   354                                                   gc.collect()
   355                                                   chunkIdx = int(chunkIdxStr)
   356                                                   asigBlock = asigBlocks[chunkIdx]
   357                                                   asigSeg = asigBlock.segments[0]
   358                                                   spikeBlock = spikeBlocks[chunkIdx]
   359                                                   eventBlock = eventBlocks[chunkIdx]
   360                                                   eventSeg = eventBlock.segments[0]
   361                                                   for chIdx in asigBlock.filter(objects=ChannelIndex):
   362                                                       chAlreadyThere = (chIdx.name in channelIndexCache.keys())
   363                                                       if not chAlreadyThere:
   364                                                           newChIdx = copy(chIdx)
   365                                                           newChIdx.analogsignals = []
   366                                                           newChIdx.units = []
   367                                                           channelIndexCache[chIdx.name] = newChIdx
   368                                                   for unit in (spikeBlock.filter(objects=Unit)):
   369                                                       if lazy:
   370                                                           theseSpiketrains = []
   371                                                           for stP in unit.spiketrains:
   372                                                               st = loadStProxy(stP)
   373                                                               if len(st.times) > 0:
   374                                                                   theseSpiketrains.append(st)
   375                                                       else:
   376                                                           theseSpiketrains = [
   377                                                               st
   378                                                               for st in unit.spiketrains
   379                                                               if len(st.times)
   380                                                               ]
   381                                                       for st in theseSpiketrains:
   382                                                           st = loadObjArrayAnn(st)
   383                                                           if len(st.times):
   384                                                               st.magnitude[:] = st.times.magnitude + spikeBlock.annotations['chunkTStart']
   385                                                               st.t_start = min(0 * pq.s, st.times[0] * 0.999)
   386                                                               st.t_stop = max(
   387                                                                   st.t_stop + spikeBlock.annotations['chunkTStart'] * pq.s,
   388                                                                   st.times[-1] * 1.001)
   389                                                           else:
   390                                                               st.t_start += spikeBlock.annotations['chunkTStart'] * pq.s
   391                                                               st.t_stop += spikeBlock.annotations['chunkTStart'] * pq.s
   392                                                       uAlreadyThere = (unit.name in unitCache.keys())
   393                                                       if not uAlreadyThere:
   394                                                           newUnit = copy(unit)
   395                                                           newUnit.spiketrains = []
   396                                                           newUnit.annotations['parentChanName'] = unit.channel_index.name
   397                                                           unitCache[unit.name] = newUnit
   398                                                           spiketrainCache[unit.name] = theseSpiketrains
   399                                                       else:
   400                                                           spiketrainCache[unit.name] = spiketrainCache[unit.name] + theseSpiketrains
   401                                                   #
   402                                                   if lazy:
   403                                                       evList = [
   404                                                           evP.load()
   405                                                           for evP in eventSeg.events]
   406                                                   else:
   407                                                       evList = eventSeg.events
   408                                                   for event in evList:
   409                                                       event.magnitude[:] = event.magnitude + eventBlock.annotations['chunkTStart']
   410                                                       if event.name in eventCache.keys():
   411                                                           eventCache[event.name].append(event)
   412                                                       else:
   413                                                           eventCache[event.name] = [event]
   414                                                   # take the requested analog signal channels
   415                                                   if lazy:
   416                                                       tdChanNames = listChanNames(
   417                                                           asigBlock, chanQuery, objType=AnalogSignalProxy)
   418                                                       #############
   419                                                       # tdChanNames = ['seg0_utah1', 'seg0_utah10']
   420                                                       ##############
   421                                                       asigList = []
   422                                                       for asigP in asigSeg.analogsignals:
   423                                                           if asigP.name in tdChanNames:
   424                                                               asig = asigP.load()
   425                                                               asig.channel_index = asigP.channel_index
   426                                                               asigList.append(asig)
   427                                                               if trackMemory:
   428                                                                   print('loading {} from proxy object. memory usage: {:.1f} MB'.format(
   429                                                                       asigP.name, prf.memory_usage_psutil()))
   430                                                   else:
   431                                                       tdChanNames = listChanNames(
   432                                                           asigBlock, chanQuery, objType=AnalogSignal)
   433                                                       asigList = [
   434                                                           asig
   435                                                           for asig in asigSeg.analogsignals
   436                                                           if asig.name in tdChanNames
   437                                                           ]
   438                                                   for asig in asigList:
   439                                                       if asig.size > 0:
   440                                                           dummyAsig = asig
   441                                                   if idx == 0:
   442                                                       outputBlock = Block(
   443                                                           name=asigBlock.name,
   444                                                           file_origin=asigBlock.file_origin,
   445                                                           file_datetime=asigBlock.file_datetime,
   446                                                           rec_datetime=asigBlock.rec_datetime,
   447                                                           **asigBlock.annotations
   448                                                       )
   449                                                       newSeg = Segment(
   450                                                           index=0, name=asigSeg.name,
   451                                                           description=asigSeg.description,
   452                                                           file_origin=asigSeg.file_origin,
   453                                                           file_datetime=asigSeg.file_datetime,
   454                                                           rec_datetime=asigSeg.rec_datetime,
   455                                                           **asigSeg.annotations
   456                                                       )
   457                                                       outputBlock.segments = [newSeg]
   458                                                       for asig in asigList:
   459                                                           asigAnnCache[asig.name] = asig.annotations
   460                                                           asigAnnCache[asig.name]['parentChanName'] = asig.channel_index.name
   461                                                       asigUnits = dummyAsig.units
   462                                                   tdDF = analogSignalsToDataFrame(asigList)
   463                                                   del asigList  # asigs saved to dataframe, no longer needed
   464                                                   tdDF.loc[:, 't'] += asigBlock.annotations['chunkTStart']
   465                                                   tdDF.set_index('t', inplace=True)
   466                                                   if samplingRate != dummyAsig.sampling_rate:
   467                                                       lowPassOpts = {
   468                                                           'low': {
   469                                                               'Wn': float(samplingRate / 2),
   470                                                               'N': 4,
   471                                                               'btype': 'low',
   472                                                               'ftype': 'bessel'
   473                                                           }
   474                                                       }
   475                                                       newT = pd.Series(
   476                                                           np.arange(
   477                                                               dummyAsig.t_start + asigBlock.annotations['chunkTStart'] * pq.s,
   478                                                               dummyAsig.t_stop + asigBlock.annotations['chunkTStart'] * pq.s,
   479                                                               1/samplingRate))
   480                                                       if samplingRate < dummyAsig.sampling_rate:
   481                                                           filterCoeffs = hf.makeFilterCoeffsSOS(
   482                                                               lowPassOpts, float(dummyAsig.sampling_rate))
   483                                                           if trackMemory:
   484                                                               print('Filtering analog data before downsampling. memory usage: {:.1f} MB'.format(
   485                                                                   prf.memory_usage_psutil()))
   486                                                           '''
   487                                                           ### check that axis=0 is the correct option
   488                                                           dummyDF = tdDF.iloc[:, :4].copy()
   489                                                           filteredAsigs0 = signal.sosfiltfilt( filterCoeffs, dummyDF.to_numpy(), axis=0)
   490                                                           filteredAsigs1 = signal.sosfiltfilt( filterCoeffs, dummyDF.to_numpy(), axis=1)
   491                                                           ###
   492                                                           '''
   493                                                           filteredAsigs = signal.sosfiltfilt(
   494                                                               filterCoeffs, tdDF.to_numpy(),
   495                                                               axis=0)
   496                                                           tdDF = pd.DataFrame(
   497                                                               filteredAsigs,
   498                                                               index=tdDF.index,
   499                                                               columns=tdDF.columns)
   500                                                           if trackMemory:
   501                                                               print('Just finished analog data filtering before downsampling. memory usage: {:.1f} MB'.format(
   502                                                                   prf.memory_usage_psutil()))
   503                                                       tdInterp = hf.interpolateDF(
   504                                                           tdDF, newT,
   505                                                           kind='linear', fill_value='extrapolate',
   506                                                           verbose=verbose)
   507                                                       # free up memory used by full resolution asigs
   508                                                       del tdDF
   509                                                   else:
   510                                                       tdInterp = tdDF
   511                                                   #
   512                                                   asigCache.append(tdInterp)
   513                                                   #
   514                                                   print('Finished chunk {}'.format(chunkIdxStr))
   515                                               allTdDF = pd.concat(asigCache)
   516                                               # TODO: check for nans, if, for example a signal is partially missing
   517                                               allTdDF.fillna(method='bfill', inplace=True)
   518                                               allTdDF.fillna(method='ffill', inplace=True)
   519                                               for asigName in allTdDF.columns:
   520                                                   newAsig = AnalogSignal(
   521                                                       allTdDF[asigName].to_numpy() * asigUnits,
   522                                                       name=asigName,
   523                                                       sampling_rate=samplingRate,
   524                                                       dtype=np.float32,
   525                                                       **asigAnnCache[asigName])
   526                                                   chIdxName = asigAnnCache[asigName]['parentChanName']
   527                                                   chIdx = channelIndexCache[chIdxName]
   528                                                   # cross-assign ownership to containers
   529                                                   chIdx.analogsignals.append(newAsig)
   530                                                   newSeg.analogsignals.append(newAsig)
   531                                                   newAsig.channel_index = chIdx
   532                                                   newAsig.segment = newSeg
   533                                               #
   534                                               for uName, unit in unitCache.items():
   535                                                   # concatenate spike times, waveforms, etc.
   536                                                   if len(spiketrainCache[unit.name]):
   537                                                       consolidatedTimes = np.concatenate([
   538                                                               st.times.magnitude
   539                                                               for st in spiketrainCache[unit.name]
   540                                                           ])
   541                                                       # TODO:   decide whether to include this step
   542                                                       #         which snaps the spike times to the nearest
   543                                                       #         *sampled* data point
   544                                                       #
   545                                                       # consolidatedTimes, timesIndex = hf.closestSeries(
   546                                                       #     takeFrom=pd.Series(consolidatedTimes),
   547                                                       #     compareTo=pd.Series(allTdDF.index))
   548                                                       #
   549                                                       # find an example spiketrain with array_annotations
   550                                                       for st in spiketrainCache[unit.name]:
   551                                                           if len(st.times):
   552                                                               dummySt = st
   553                                                               break
   554                                                       consolidatedAnn = {
   555                                                           key: np.array([])
   556                                                           for key, value in dummySt.array_annotations.items()
   557                                                           }
   558                                                       for key, value in consolidatedAnn.items():
   559                                                           consolidatedAnn[key] = np.concatenate([
   560                                                               st.annotations[key]
   561                                                               for st in spiketrainCache[unit.name]
   562                                                           ])
   563                                                       consolidatedWaveforms = np.concatenate([
   564                                                           st.waveforms
   565                                                           for st in spiketrainCache[unit.name]
   566                                                           ])
   567                                                       spikeTStop = max([
   568                                                           st.t_stop
   569                                                           for st in spiketrainCache[unit.name]
   570                                                           ])
   571                                                       spikeTStart = max([
   572                                                           st.t_start
   573                                                           for st in spiketrainCache[unit.name]
   574                                                           ])
   575                                                       spikeAnnotations = {
   576                                                           key: value
   577                                                           for key, value in dummySt.annotations.items()
   578                                                           if key not in dummySt.annotations['arrayAnnNames']
   579                                                       }
   580                                                       newSt = SpikeTrain(
   581                                                           name=dummySt.name,
   582                                                           times=consolidatedTimes, units='sec', t_stop=spikeTStop,
   583                                                           waveforms=consolidatedWaveforms * dummySt.waveforms.units,
   584                                                           left_sweep=dummySt.left_sweep,
   585                                                           sampling_rate=dummySt.sampling_rate,
   586                                                           t_start=spikeTStart, **spikeAnnotations,
   587                                                           array_annotations=consolidatedAnn)
   588                                                       # cross-assign ownership to containers
   589                                                       unit.spiketrains.append(newSt)
   590                                                       newSt.unit = unit
   591                                                       newSeg.spiketrains.append(newSt)
   592                                                       newSt.segment = newSeg
   593                                                       # link chIdxes and Units
   594                                                       if unit.annotations['parentChanName'] in channelIndexCache:
   595                                                           chIdx = channelIndexCache[unit.annotations['parentChanName']]
   596                                                           if unit not in chIdx.units:
   597                                                               chIdx.units.append(unit)
   598                                                               unit.channel_index = chIdx
   599                                                       else:
   600                                                           newChIdx = ChannelIndex(
   601                                                               name=unit.annotations['parentChanName'], index=0)
   602                                                           channelIndexCache[unit.annotations['parentChanName']] = newChIdx
   603                                                           if unit not in newChIdx.units:
   604                                                               newChIdx.units.append(unit)
   605                                                               unit.channel_index = newChIdx
   606                                               #
   607                                               for evName, eventList in eventCache.items():
   608                                                   consolidatedTimes = np.concatenate([
   609                                                       ev.times.magnitude
   610                                                       for ev in eventList
   611                                                       ])
   612                                                   consolidatedLabels = np.concatenate([
   613                                                       ev.labels
   614                                                       for ev in eventList
   615                                                       ])
   616                                                   newEvent = Event(
   617                                                       name=evName,
   618                                                       times=consolidatedTimes * pq.s,
   619                                                       labels=consolidatedLabels
   620                                                       )
   621                                                   # if len(newEvent):
   622                                                   newEvent.segment = newSeg
   623                                                   newSeg.events.append(newEvent)
   624                                               for chIdxName, chIdx in channelIndexCache.items():
   625                                                   if len(chIdx.analogsignals) or len(chIdx.units):
   626                                                       outputBlock.channel_indexes.append(chIdx)
   627                                                       chIdx.block = outputBlock
   628                                               #
   629                                               outputBlock = purgeNixAnn(outputBlock)
   630                                               createRelationship = False
   631                                               if createRelationship:
   632                                                   outputBlock.create_relationship()
   633                                               return outputBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: concatenateEventsContainer at line 660

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   660                                           @profile
   661                                           def concatenateEventsContainer(eventContainer, linkParents=True):
   662                                               if isinstance(eventContainer, dict):
   663                                                   listOfEvents = list(eventContainer.values())
   664                                               else:
   665                                                   listOfEvents = eventContainer
   666                                               nonEmptyEvents = [ev for ev in listOfEvents if len(ev.times)]
   667                                               if not len(nonEmptyEvents) > 0:
   668                                                   return listOfEvents[0]
   669                                               masterEvent = listOfEvents[0]
   670                                               for evIdx, ev in enumerate(listOfEvents[1:]):
   671                                                   try:
   672                                                       masterEvent = masterEvent.merge(ev)
   673                                                   except Exception:
   674                                                       traceback.print_exc()
   675                                                       pdb.set_trace()
   676                                               if masterEvent.array_annotations is not None:
   677                                                   arrayAnnNames = list(masterEvent.array_annotations.keys())
   678                                                   masterEvent.annotations.update(masterEvent.array_annotations)
   679                                                   masterEvent.annotations['arrayAnnNames'] = arrayAnnNames
   680                                               if linkParents:
   681                                                   masterEvent.segment = listOfEvents[0].segment
   682                                                   if isinstance(masterEvent, SpikeTrain):
   683                                                       masterEvent.unit = listOfEvents[0].unit
   684                                               return masterEvent

Total time: 8.6476 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: unitSpikeTrainWaveformsToDF at line 743

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   743                                           @profile
   744                                           def unitSpikeTrainWaveformsToDF(
   745                                                   spikeTrainContainer,
   746                                                   dataQuery=None,
   747                                                   transposeToColumns='bin', fastTranspose=True,
   748                                                   lags=None, decimate=1, rollingWindow=None,
   749                                                   getMetaData=True, verbose=False,
   750                                                   whichSegments=None, windowSize=None, procFun=None):
   751                                               #  list contains different segments from *one* unit
   752        13        516.0     39.7      0.0      if isinstance(spikeTrainContainer, ChannelIndex):
   753                                                   assert len(spikeTrainContainer.units) == 0
   754                                                   spiketrains = spikeTrainContainer.units[0].spiketrains
   755        13        343.0     26.4      0.0      elif isinstance(spikeTrainContainer, Unit):
   756        13        352.0     27.1      0.0          spiketrains = spikeTrainContainer.spiketrains
   757                                               else:
   758                                                   raise(Exception('not a valid container'))
   759                                               # TODO check if really need to assert uniqueness?
   760        13        310.0     23.8      0.0      uniqueSpiketrains = []
   761        91       2162.0     23.8      0.0      for st in spiketrains:
   762        78      15671.0    200.9      0.0          if not np.any([st is i for i in uniqueSpiketrains]):
   763        39        973.0     24.9      0.0              uniqueSpiketrains.append(st)
   764                                               #  subsampling options
   765        13        361.0     27.8      0.0      decimate = int(decimate)
   766        13        294.0     22.6      0.0      if whichSegments is not None:
   767                                                   uniqueSpiketrains = [
   768                                                       uniqueSpiketrains[i]
   769                                                       for i in whichSegments
   770                                                   ]
   771                                               #
   772        13        288.0     22.2      0.0      waveformsList = []
   773                                               #
   774        52       2491.0     47.9      0.0      for segIdx, stIn in enumerate(uniqueSpiketrains):
   775        39       1330.0     34.1      0.0          if verbose:
   776                                                       print('extracting spiketrain from {}'.format(stIn.segment))
   777                                                   #  make sure is not a proxyObj
   778        39       2729.0     70.0      0.0          if isinstance(stIn, SpikeTrainProxy):
   779                                                       st = loadStProxy(stIn)
   780                                                       if (getMetaData) or (dataQuery is not None):
   781                                                           # if there's a query, get metadata temporarily to resolve it
   782                                                           st = loadObjArrayAnn(st)
   783                                                   else:
   784        39       1400.0     35.9      0.0              st = stIn
   785                                                   #  extract bins spaced by decimate argument
   786        39      77979.0   1999.5      0.1          if not st.times.any():
   787                                                       continue
   788        39       1577.0     40.4      0.0          if verbose:
   789                                                       print('extracting wf from {}'.format(stIn.segment))
   790        39       1584.0     40.6      0.0          wf = np.asarray(
   791        39      16849.0    432.0      0.0              np.squeeze(st.waveforms),
   792        39       5744.0    147.3      0.0              dtype='float32')
   793        39       1608.0     41.2      0.0          if wf.ndim == 3:
   794                                                       print('Waveforms from more than one channel!')
   795                                                       if wf.shape[1] > 0:
   796                                                           wf = wf[:, 0, :]
   797        39     186974.0   4794.2      0.2          wfDF = pd.DataFrame(wf)
   798        39       1831.0     46.9      0.0          samplingRate = st.sampling_rate
   799                                                   bins = (
   800        39      35936.0    921.4      0.0              np.asarray(wfDF.columns) / samplingRate -
   801        39      95348.0   2444.8      0.1              st.left_sweep)
   802        39     132899.0   3407.7      0.2          wfDF.columns = np.around(bins.magnitude, decimals=6)
   803        39       1809.0     46.4      0.0          if windowSize is not None:
   804                                                       winMask = (
   805        39      77912.0   1997.7      0.1                  (wfDF.columns >= windowSize[0]) &
   806        39      54853.0   1406.5      0.1                  (wfDF.columns <= windowSize[1]))
   807        39    2247204.0  57620.6      2.6              wfDF = wfDF.loc[:, winMask]
   808        39       1857.0     47.6      0.0          if procFun is not None:
   809        39   35538657.0 911247.6     41.1              wfDF = procFun(wfDF, st)
   810        39       1879.0     48.2      0.0          idxLabels = ['segment', 'originalIndex', 't']
   811        39     996809.0  25559.2      1.2          wfDF.loc[:, 't'] = np.asarray(st.times.magnitude)
   812        39       1592.0     40.8      0.0          if (getMetaData) or (dataQuery is not None):
   813                                                       # if there's a query, get metadata temporarily to resolve it
   814        39       1179.0     30.2      0.0              annDict = {}
   815       507      15503.0     30.6      0.0              for k, values in st.array_annotations.items():
   816       468      18213.0     38.9      0.0                  if isinstance(getMetaData, Iterable):
   817                                                               # if selecting metadata fields, check that
   818                                                               # the key is in the provided list
   819       468      14171.0     30.3      0.0                      if k not in getMetaData:
   820       273       7139.0     26.2      0.0                          continue
   821       195       8209.0     42.1      0.0                  if isinstance(values[0], str):
   822        78       6016.0     77.1      0.0                      v = np.asarray(values, dtype='str')
   823                                                           else:
   824       117       4743.0     40.5      0.0                      v = np.asarray(values)
   825       195       6456.0     33.1      0.0                  annDict.update({k: v})
   826                                                       skipAnnNames = (
   827        39       1306.0     33.5      0.0                  st.annotations['arrayAnnNames'] +
   828                                                           [
   829        39       1038.0     26.6      0.0                      'arrayAnnNames', 'arrayAnnDTypes',
   830        39       1037.0     26.6      0.0                      'nix_name', 'neo_name', 'id',
   831        39       1363.0     34.9      0.0                      'cell_label', 'cluster_label', 'max_on_channel', 'binWidth']
   832                                                           )
   833        39    1392897.0  35715.3      1.6              annDF = pd.DataFrame(annDict)
   834       702      20283.0     28.9      0.0              for k, value in st.annotations.items():
   835       663      24704.0     37.3      0.0                  if isinstance(getMetaData, Iterable):
   836                                                               # if selecting metadata fields, check that
   837                                                               # the key is in the provided list
   838       663      19063.0     28.8      0.0                      if k not in getMetaData:
   839       468      11805.0     25.2      0.0                          continue
   840       195       5270.0     27.0      0.0                  if k not in skipAnnNames:
   841                                                               annDF.loc[:, k] = value
   842                                                       #
   843        39       1308.0     33.5      0.0              if isinstance(getMetaData, Iterable):
   844        39       1422.0     36.5      0.0                  doNotFillList = idxLabels + ['feature', 'bin']
   845                                                           fieldsNeedFiller = [
   846        39       1160.0     29.7      0.0                      mdn
   847        39      22031.0    564.9      0.0                      for mdn in getMetaData
   848                                                               if (mdn not in doNotFillList) and (mdn not in annDF.columns)]
   849       156       5545.0     35.5      0.0                  for mdName in fieldsNeedFiller:
   850       117    3676276.0  31421.2      4.3                      annDF.loc[:, mdName] = 'NA'
   851        39      14912.0    382.4      0.0              annColumns = annDF.columns.to_list()
   852        39       1017.0     26.1      0.0              if getMetaData:
   853       351       8915.0     25.4      0.0                  for annNm in annColumns:
   854       312       8304.0     26.6      0.0                      if annNm not in idxLabels:
   855       312       8349.0     26.8      0.0                          idxLabels.append(annNm)
   856                                                           # idxLabels += annColumns
   857        39    3941700.0 101069.2      4.6              spikeDF = annDF.join(wfDF)
   858                                                   else:
   859                                                       spikeDF = wfDF
   860                                                       del wfDF, st
   861        39    1306927.0  33510.9      1.5          spikeDF.loc[:, 'segment'] = segIdx
   862        39     953550.0  24450.0      1.1          spikeDF.loc[:, 'originalIndex'] = spikeDF.index
   863        39       1311.0     33.6      0.0          spikeDF.columns.name = 'bin'
   864                                                   #
   865        39        900.0     23.1      0.0          if dataQuery is not None:
   866        39   11436851.0 293252.6     13.2              spikeDF.query(dataQuery, inplace=True)
   867        39       2774.0     71.1      0.0              if not getMetaData:
   868                                                           spikeDF.drop(columns=annColumns, inplace=True)
   869        39       2038.0     52.3      0.0          waveformsList.append(spikeDF)
   870                                               #
   871        13    2734502.0 210346.3      3.2      zeroLagWaveformsDF = pd.concat(waveformsList, axis='index')
   872        13        565.0     43.5      0.0      if verbose:
   873                                                   prf.print_memory_usage('before transposing waveforms')
   874                                               # TODO implement lags and rolling window addition here
   875        13     593106.0  45623.5      0.7      metaDF = zeroLagWaveformsDF.loc[:, idxLabels].copy()
   876        13     609412.0  46877.8      0.7      zeroLagWaveformsDF.drop(columns=idxLabels, inplace=True)
   877        13        484.0     37.2      0.0      if lags is None:
   878        13        309.0     23.8      0.0          lags = [0]
   879        13        424.0     32.6      0.0      laggedWaveformsDict = {
   880        13        717.0     55.2      0.0          (spikeTrainContainer.name, k): None for k in lags}
   881        26        593.0     22.8      0.0      for lag in lags:
   882        13        345.0     26.5      0.0          if isinstance(lag, int):
   883        13        336.0     25.8      0.0              shiftedWaveform = zeroLagWaveformsDF.shift(
   884        13     312255.0  24019.6      0.4                  lag, axis='columns')
   885        13        434.0     33.4      0.0              if rollingWindow is not None:
   886                                                           halfRollingWin = int(np.ceil(rollingWindow/2))
   887                                                           seekIdx = slice(
   888                                                               halfRollingWin, -halfRollingWin+1, decimate)
   889                                                           # seekIdx = slice(None, None, decimate)
   890                                                           #shiftedWaveform = (
   891                                                           #    shiftedWaveform
   892                                                           #    .rolling(
   893                                                           #        window=rollingWindow, win_type='gaussian',
   894                                                           #        axis='columns', center=True)
   895                                                           #    .mean(std=halfRollingWin))
   896                                                           shiftedWaveform = (
   897                                                               shiftedWaveform
   898                                                               .rolling(
   899                                                                   window=rollingWindow, 
   900                                                                   axis='columns', center=True)
   901                                                               .mean())
   902                                                       else:
   903        13        258.0     19.8      0.0                  halfRollingWin = 0
   904        13        403.0     31.0      0.0                  seekIdx = slice(None, None, decimate)
   905                                                           if False:
   906                                                               oldShiftedWaveform = zeroLagWaveformsDF.shift(
   907                                                                   lag, axis='columns')
   908                                                               plt.plot(oldShiftedWaveform.iloc[0, :])
   909                                                               plt.plot(shiftedWaveform.iloc[0, :])
   910                                                               plt.show()
   911                                                       laggedWaveformsDict[
   912                                                           (spikeTrainContainer.name, lag)] = (
   913        13     338623.0  26047.9      0.4                      shiftedWaveform.iloc[:, seekIdx].copy())
   914        13        577.0     44.4      0.0          if isinstance(lag, tuple):
   915                                                       halfRollingWin = int(np.ceil(lag[1]/2))
   916                                                       seekIdx = slice(
   917                                                           halfRollingWin, -halfRollingWin+1, decimate)
   918                                                       # seekIdx = slice(None, None, decimate)
   919                                                       shiftedWaveform = (
   920                                                           zeroLagWaveformsDF
   921                                                           .shift(lag[0], axis='columns')
   922                                                           .rolling(
   923                                                               window=lag[1], win_type='gaussian',
   924                                                               axis='columns', center=True)
   925                                                           .mean(std=halfRollingWin))
   926                                                       laggedWaveformsDict[
   927                                                           (spikeTrainContainer.name, lag)] = (
   928                                                               shiftedWaveform.iloc[:, seekIdx].copy())
   929                                               #
   930        13        293.0     22.5      0.0      if transposeToColumns == 'feature':
   931                                                   # stack the bin, name the feature column
   932                                                   # 
   933        26       1023.0     39.3      0.0          for idx, (key, value) in enumerate(laggedWaveformsDict.items()):
   934        13        279.0     21.5      0.0              if idx == 0:
   935        13        434.0     33.4      0.0                  stackedIndexDF = pd.concat(
   936        13     773973.0  59536.4      0.9                      [metaDF, value], axis='columns')
   937        13    2747265.0 211328.1      3.2                  stackedIndexDF.set_index(idxLabels, inplace=True)
   938                                                           # don't drop nans for now - might need to keep track of them
   939                                                           # if we need to equalize to another array later
   940        13   11239253.0 864557.9     13.0                  newIndex = stackedIndexDF.stack(dropna=False).index
   941        13        565.0     43.5      0.0                  idxLabels.append('bin')
   942        13    4012571.0 308659.3      4.6              laggedWaveformsDict[key] = value.stack(dropna=False).to_frame(name=key).reset_index(drop=True)
   943        13        508.0     39.1      0.0          waveformsDF = pd.concat(
   944        13        326.0     25.1      0.0              laggedWaveformsDict.values(),
   945        13     629967.0  48459.0      0.7              axis='columns')
   946        13       7096.0    545.8      0.0          waveformsDF.columns.names = ['feature', 'lag']
   947        13       2438.0    187.5      0.0          waveformsDF.index = newIndex
   948        13        362.0     27.8      0.0          waveformsDF.columns.name = 'feature'
   949                                               elif transposeToColumns == 'bin':
   950                                                   # add the feature column
   951                                                   waveformsDF = pd.concat(
   952                                                       laggedWaveformsDict,
   953                                                       names=['feature', 'lag', 'originalDummy']).reset_index()
   954                                                   waveformsDF = pd.concat(
   955                                                       [
   956                                                           metaDF.reset_index(drop=True),
   957                                                           waveformsDF.drop(columns='originalDummy')],
   958                                                       axis='columns')
   959                                                   idxLabels += ['feature', 'lag']
   960                                                   waveformsDF.columns.name = 'bin'
   961                                                   waveformsDF.set_index(idxLabels, inplace=True)
   962                                               #
   963        13        305.0     23.5      0.0      if transposeToColumns != waveformsDF.columns.name:
   964                                                   waveformsDF = transposeSpikeDF(
   965                                                       waveformsDF, transposeToColumns,
   966                                                       fastTranspose=fastTranspose)
   967        13        214.0     16.5      0.0      return waveformsDF

Total time: 16.5898 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: concatenateUnitSpikeTrainWaveformsDF at line 969

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   969                                           @profile
   970                                           def concatenateUnitSpikeTrainWaveformsDF(
   971                                                   units, dataQuery=None,
   972                                                   transposeToColumns='bin', concatOn='index',
   973                                                   fastTranspose=True, getMetaData=True, verbose=False,
   974                                                   addLags=None, decimate=1, rollingWindow=None,
   975                                                   metaDataToCategories=False, windowSize=None,
   976                                                   whichSegments=None, procFun=None):
   977         1          9.0      9.0      0.0      allUnits = []
   978        14        113.0      8.1      0.0      for thisUnit in units:
   979        13        276.0     21.2      0.0          hasAnySpikes = []
   980        91        846.0      9.3      0.0          for stIn in thisUnit.spiketrains:
   981        78        873.0     11.2      0.0              if isinstance(stIn, SpikeTrainProxy):
   982                                                           st = stIn.load(
   983                                                               magnitude_mode='rescaled',
   984                                                               load_waveforms=False)
   985                                                       else:
   986        78        616.0      7.9      0.0                  st = stIn
   987        78      36660.0    470.0      0.0              hasAnySpikes.append(st.times.any())
   988        13       1790.0    137.7      0.0          if np.any(hasAnySpikes):
   989        13        134.0     10.3      0.0              allUnits.append(thisUnit)
   990         1          8.0      8.0      0.0      waveformsList = []
   991        14        255.0     18.2      0.0      for idx, thisUnit in enumerate(allUnits):
   992        13        154.0     11.8      0.0          if verbose:
   993                                                       print('concatenating unitDF {}'.format(thisUnit.name))
   994        13        141.0     10.8      0.0          lags = None
   995        13        146.0     11.2      0.0          if addLags is not None:
   996                                                       if thisUnit.name in addLags:
   997                                                           lags = addLags[thisUnit.name]
   998        13        205.0     15.8      0.0          unitWaveforms = unitSpikeTrainWaveformsToDF(
   999        13        150.0     11.5      0.0              thisUnit, dataQuery=dataQuery,
  1000        13        148.0     11.4      0.0              transposeToColumns=transposeToColumns,
  1001        13        136.0     10.5      0.0              fastTranspose=fastTranspose, getMetaData=getMetaData,
  1002        13        156.0     12.0      0.0              lags=lags, decimate=decimate, rollingWindow=rollingWindow,
  1003        13        148.0     11.4      0.0              verbose=verbose, windowSize=windowSize,
  1004        13   87565346.0 6735795.8     52.8              whichSegments=whichSegments, procFun=procFun)
  1005        13        277.0     21.3      0.0          if idx == 0:
  1006         1        131.0    131.0      0.0              idxLabels = unitWaveforms.index.names
  1007        13        180.0     13.8      0.0          if (concatOn == 'columns') and (idx > 0):
  1008                                                       # other than first time, we already have the metadata
  1009        12     363814.0  30317.8      0.2              unitWaveforms.reset_index(drop=True, inplace=True)
  1010                                                   else:
  1011                                                       # first time, or if concatenating indices,
  1012                                                       # keep the the metadata
  1013         1    3838082.0 3838082.0      2.3              unitWaveforms.reset_index(inplace=True)
  1014         1         46.0     46.0      0.0              if metaDataToCategories:
  1015                                                           # convert metadata to categoricals to free memory
  1016                                                           #
  1017                                                           unitWaveforms[idxLabels] = (
  1018                                                               unitWaveforms[idxLabels]
  1019                                                               .astype('category')
  1020                                                               )
  1021        13        334.0     25.7      0.0          waveformsList.append(unitWaveforms)
  1022        13        160.0     12.3      0.0          del unitWaveforms
  1023        13        149.0     11.5      0.0          if verbose:
  1024                                                       print('memory usage: {:.1f} MB'.format(prf.memory_usage_psutil()))
  1025         1          8.0      8.0      0.0      if verbose:
  1026                                                   print(
  1027                                                       'about to join all, memory usage: {:.1f} MB'
  1028                                                       .format(prf.memory_usage_psutil()))
  1029                                               #  if concatenating indexes, reset the index of the result
  1030                                               #  ignoreIndex = (concatOn == 'index')
  1031         1         21.0     21.0      0.0      allWaveforms = pd.concat(
  1032         1    3318702.0 3318702.0      2.0          waveformsList, axis=concatOn,
  1033                                                   # ignore_index=ignoreIndex
  1034                                                   )
  1035         1     519112.0 519112.0      0.3      del waveformsList
  1036         1         61.0     61.0      0.0      if verbose:
  1037                                                   print(
  1038                                                       'finished concatenating, memory usage: {:.1f} MB'
  1039                                                       .format(prf.memory_usage_psutil()))
  1040         1         32.0     32.0      0.0      try:
  1041         1   17701793.0 17701793.0     10.7          allWaveforms.set_index(idxLabels, inplace=True)
  1042         1         54.0     54.0      0.0          allWaveforms.sort_index(
  1043         1         17.0     17.0      0.0              level=['segment', 'originalIndex', 't'],
  1044         1   52080871.0 52080871.0     31.4              axis='index', inplace=True, kind='mergesort')
  1045         1         29.0     29.0      0.0          allWaveforms.sort_index(
  1046         1     465485.0 465485.0      0.3              axis='columns', inplace=True, kind='mergesort')
  1047                                               except Exception:
  1048                                                   pdb.set_trace()
  1049         1         20.0     20.0      0.0      return allWaveforms

Total time: 16.667 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: alignedAsigsToDF at line 1051

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1051                                           @profile
  1052                                           def alignedAsigsToDF(
  1053                                                   dataBlock, unitNames=None,
  1054                                                   unitQuery=None, dataQuery=None,
  1055                                                   collapseSizes=False, verbose=False,
  1056                                                   duplicateControlsByProgram=False,
  1057                                                   amplitudeColumn='amplitude',
  1058                                                   programColumn='program',
  1059                                                   electrodeColumn='electrode',
  1060                                                   transposeToColumns='bin', concatOn='index', fastTranspose=True,
  1061                                                   addLags=None, decimate=1, rollingWindow=None,
  1062                                                   whichSegments=None, windowSize=None,
  1063                                                   getMetaData=True, metaDataToCategories=True,
  1064                                                   outlierTrials=None, invertOutlierMask=False,
  1065                                                   makeControlProgram=False, removeFuzzyName=False, procFun=None):
  1066                                               #  channels to trigger
  1067         1         55.0     55.0      0.0      if unitNames is None:
  1068         1     166167.0 166167.0      0.1          unitNames = listChanNames(dataBlock, unitQuery, objType=Unit)
  1069         1         50.0     50.0      0.0      allUnits = []
  1070        14        339.0     24.2      0.0      for uName in unitNames:
  1071        13     155950.0  11996.2      0.1          allUnits += dataBlock.filter(objects=Unit, name=uName)
  1072         1         16.0     16.0      0.0      allWaveforms = concatenateUnitSpikeTrainWaveformsDF(
  1073         1         16.0     16.0      0.0          allUnits, dataQuery=dataQuery,
  1074         1         15.0     15.0      0.0          transposeToColumns=transposeToColumns, concatOn=concatOn,
  1075         1         14.0     14.0      0.0          fastTranspose=fastTranspose,
  1076         1         14.0     14.0      0.0          addLags=addLags, decimate=decimate, rollingWindow=rollingWindow,
  1077         1         14.0     14.0      0.0          verbose=verbose, whichSegments=whichSegments,
  1078         1         15.0     15.0      0.0          windowSize=windowSize, procFun=procFun,
  1079         1  165905950.0 165905950.0     99.5          getMetaData=getMetaData, metaDataToCategories=metaDataToCategories)
  1080                                               #
  1081         1         37.0     37.0      0.0      manipulateIndex = np.any(
  1082                                                   [
  1083         1         15.0     15.0      0.0              collapseSizes, duplicateControlsByProgram,
  1084         1        385.0    385.0      0.0              makeControlProgram, removeFuzzyName
  1085                                                       ])
  1086         1         17.0     17.0      0.0      if outlierTrials is not None:
  1087                                                   def rejectionLookup(entry):
  1088                                                       key = []
  1089                                                       for subKey in outlierTrials.index.names:
  1090                                                           keyIdx = allWaveforms.index.names.index(subKey)
  1091                                                           key.append(entry[keyIdx])
  1092                                                       # print(key)
  1093                                                       # outlierTrials.iloc[1, :]
  1094                                                       # allWaveforms.iloc[1, :]
  1095                                                       return outlierTrials[tuple(key)]
  1096                                                   #
  1097                                                   outlierMask = np.asarray(
  1098                                                       allWaveforms.index.map(rejectionLookup),
  1099                                                       dtype=np.bool)
  1100                                                   if invertOutlierMask:
  1101                                                       outlierMask = ~outlierMask
  1102                                                   allWaveforms = allWaveforms.loc[~outlierMask, :]
  1103         1         19.0     19.0      0.0      if manipulateIndex and getMetaData:
  1104                                                   idxLabels = allWaveforms.index.names
  1105                                                   allWaveforms.reset_index(inplace=True)
  1106                                                   # 
  1107                                                   if collapseSizes:
  1108                                                       try:
  1109                                                           allWaveforms.loc[allWaveforms['pedalSizeCat'] == 'XL', 'pedalSizeCat'] = 'L'
  1110                                                           allWaveforms.loc[allWaveforms['pedalSizeCat'] == 'XS', 'pedalSizeCat'] = 'S'
  1111                                                       except Exception:
  1112                                                           traceback.print_exc()
  1113                                                   if makeControlProgram:
  1114                                                       try:
  1115                                                           allWaveforms.loc[allWaveforms[amplitudeColumn] == 0, programColumn] = 999
  1116                                                           allWaveforms.loc[allWaveforms[amplitudeColumn] == 0, electrodeColumn] = 'control'
  1117                                                       except Exception:
  1118                                                           traceback.print_exc()
  1119                                                   if duplicateControlsByProgram:
  1120                                                       #
  1121                                                       noStimWaveforms = (
  1122                                                           allWaveforms
  1123                                                           .loc[allWaveforms[amplitudeColumn] == 0, :]
  1124                                                           )
  1125                                                       stimWaveforms = (
  1126                                                           allWaveforms
  1127                                                           .loc[allWaveforms[amplitudeColumn] != 0, :]
  1128                                                           .copy()
  1129                                                           )
  1130                                                       uniqProgs = stimWaveforms[programColumn].unique()
  1131                                                       progElecLookup = {}
  1132                                                       #pdb.set_trace()
  1133                                                       for progIdx in uniqProgs:
  1134                                                           theseStimDF = stimWaveforms.loc[
  1135                                                               stimWaveforms[programColumn] == progIdx,
  1136                                                               electrodeColumn]
  1137                                                           elecIdx = theseStimDF.iloc[0]
  1138                                                           progElecLookup.update({progIdx: elecIdx})
  1139                                                       #
  1140                                                       if makeControlProgram:
  1141                                                           uniqProgs = np.append(uniqProgs, 999)
  1142                                                           progElecLookup.update({999: 'control'})
  1143                                                       #
  1144                                                       for progIdx in uniqProgs:
  1145                                                           dummyWaveforms = noStimWaveforms.copy()
  1146                                                           dummyWaveforms.loc[:, programColumn] = progIdx
  1147                                                           dummyWaveforms.loc[:, electrodeColumn] = progElecLookup[progIdx]
  1148                                                           stimWaveforms = pd.concat([stimWaveforms, dummyWaveforms])
  1149                                                       stimWaveforms.reset_index(drop=True, inplace=True)
  1150                                                       allWaveforms = stimWaveforms
  1151                                                   #
  1152                                                   if removeFuzzyName:
  1153                                                       fuzzyNamesBase = [
  1154                                                           i.replace('Fuzzy', '')
  1155                                                           for i in idxLabels
  1156                                                           if 'Fuzzy' in i]
  1157                                                       colRenamer = {n + 'Fuzzy': n for n in fuzzyNamesBase}
  1158                                                       fuzzyNamesBasePresent = [
  1159                                                           i
  1160                                                           for i in fuzzyNamesBase
  1161                                                           if i in allWaveforms.columns]
  1162                                                       allWaveforms.drop(columns=fuzzyNamesBasePresent, inplace=True)
  1163                                                       allWaveforms.rename(columns=colRenamer, inplace=True)
  1164                                                       idxLabels = np.unique(
  1165                                                           [i.replace('Fuzzy', '') for i in idxLabels])
  1166                                                   #
  1167                                                   allWaveforms.set_index(
  1168                                                       list(idxLabels),
  1169                                                       inplace=True)
  1170                                                   if isinstance(allWaveforms.columns, pd.MultiIndex):
  1171                                                       allWaveforms.columns = allWaveforms.columns.remove_unused_levels()
  1172                                               #
  1173         1         18.0     18.0      0.0      if transposeToColumns == 'feature':
  1174         1       5037.0   5037.0      0.0          zipNames = zip(pd.unique(allWaveforms.columns.get_level_values('feature')).tolist(), unitNames)
  1175         1         19.0     19.0      0.0          try:
  1176         1        311.0    311.0      0.0              assert np.all([i == j for i, j in zipNames]), 'columns out of requested order!'
  1177                                                   except Exception:
  1178                                                       traceback.print_exc()
  1179                                                       allWaveforms.reindex(columns=unitNames)
  1180         1         31.0     31.0      0.0      if isinstance(allWaveforms.columns, pd.MultiIndex):
  1181         1       8201.0   8201.0      0.0          allWaveforms.columns = allWaveforms.columns.remove_unused_levels()
  1182         1         22.0     22.0      0.0      allWaveforms.sort_index(
  1183         1     427407.0 427407.0      0.3          axis='columns', inplace=True, kind='mergesort')
  1184         1         23.0     23.0      0.0      return allWaveforms

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getAsigsAlignedToEvents at line 1186

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1186                                           @profile
  1187                                           def getAsigsAlignedToEvents(
  1188                                                   eventBlock=None, signalBlock=None,
  1189                                                   chansToTrigger=None, chanQuery=None,
  1190                                                   eventName=None, windowSize=None,
  1191                                                   minNReps=None,
  1192                                                   appendToExisting=False,
  1193                                                   checkReferences=True, verbose=False,
  1194                                                   fileName=None, folderPath=None, chunkSize=None
  1195                                                   ):
  1196                                               #  get signals from same block as events?
  1197                                               if signalBlock is None:
  1198                                                   signalBlock = eventBlock
  1199                                               #  channels to trigger
  1200                                               if chansToTrigger is None:
  1201                                                   chansToTrigger = listChanNames(
  1202                                                       signalBlock, chanQuery, objType=ChannelIndex, condition='hasAsigs')
  1203                                               #  allocate block for spiketrains
  1204                                               masterBlock = Block()
  1205                                               try:
  1206                                                   masterBlock.name = signalBlock.annotations['neo_name']
  1207                                                   masterBlock.annotate(nix_name=signalBlock.annotations['neo_name'])
  1208                                               except Exception:
  1209                                                   masterBlock.name = signalBlock.name
  1210                                                   masterBlock.annotate(neo_name=signalBlock.name)
  1211                                                   masterBlock.annotate(nix_name=signalBlock.name)
  1212                                               #  make channels and units for triggered time series
  1213                                               for chanName in chansToTrigger:
  1214                                                   chanIdx = ChannelIndex(name=chanName + '#0', index=[0])
  1215                                                   chanIdx.annotate(nix_name=chanIdx.name)
  1216                                                   thisUnit = Unit(name=chanIdx.name)
  1217                                                   thisUnit.annotate(nix_name=chanIdx.name)
  1218                                                   chanIdx.units.append(thisUnit)
  1219                                                   thisUnit.channel_index = chanIdx
  1220                                                   masterBlock.channel_indexes.append(chanIdx)
  1221                                                   sigChanIdxList = signalBlock.filter(
  1222                                                       objects=ChannelIndex, name=chanName)
  1223                                                   if len(sigChanIdxList):
  1224                                                       sigChanIdx = sigChanIdxList[0]
  1225                                                       if sigChanIdx.coordinates is not None:
  1226                                                           coordUnits = sigChanIdx.coordinates[0][0].units
  1227                                                           chanIdx.coordinates = np.asarray(sigChanIdx.coordinates) * coordUnits
  1228                                                           thisUnit.annotations['parentChanXCoords'] = float(chanIdx.coordinates[:, 0].magnitude)
  1229                                                           thisUnit.annotations['parentChanYCoords'] = float(chanIdx.coordinates[:, 1].magnitude)
  1230                                                           thisUnit.annotations['parentChanCoordinateUnits'] = '{}'.format(coordUnits)
  1231                                               #
  1232                                               totalNSegs = 0
  1233                                               #  print([evSeg.events[3].name for evSeg in eventBlock.segments])
  1234                                               allAlignEventsList = []
  1235                                               for segIdx, eventSeg in enumerate(eventBlock.segments):
  1236                                                   thisEventName = 'seg{}_{}'.format(segIdx, eventName)
  1237                                                   try:
  1238                                                       assert len(eventSeg.filter(name=thisEventName)) == 1
  1239                                                   except Exception:
  1240                                                       traceback.print_exc()
  1241                                                   allEvIn = eventSeg.filter(name=thisEventName)[0]
  1242                                                   if isinstance(allEvIn, EventProxy):
  1243                                                       allAlignEvents = loadObjArrayAnn(allEvIn.load())
  1244                                                   elif isinstance(allEvIn, Event):
  1245                                                       allAlignEvents = allEvIn
  1246                                                   else:
  1247                                                       raise(Exception(
  1248                                                           '{} must be an Event or EventProxy!'
  1249                                                           .format(eventName)))
  1250                                                   allAlignEventsList.append(allAlignEvents)
  1251                                               allAlignEventsDF = unitSpikeTrainArrayAnnToDF(allAlignEventsList)
  1252                                               #
  1253                                               breakDownData = (
  1254                                                   allAlignEventsDF
  1255                                                   .groupby(minNReps['categories'])
  1256                                                   .agg('count')
  1257                                                   .iloc[:, 0]
  1258                                                   )
  1259                                               try:
  1260                                                   breakDownData[breakDownData > minNReps['n']].to_csv(
  1261                                                       os.path.join(
  1262                                                           folderPath, 'numRepetitionsEachCondition.csv'
  1263                                                       ), header=True
  1264                                                   )
  1265                                               except Exception:
  1266                                                   traceback.print_exc()
  1267                                               allAlignEventsDF.loc[:, 'keepMask'] = False
  1268                                               for name, group in allAlignEventsDF.groupby(minNReps['categories']):
  1269                                                   allAlignEventsDF.loc[group.index, 'keepMask'] = (
  1270                                                       breakDownData[name] > minNReps['n'])
  1271                                               for segIdx, group in allAlignEventsDF.groupby('segment'):
  1272                                                   allAlignEventsList[segIdx].array_annotations['keepMask'] = group['keepMask'].to_numpy()
  1273                                               #
  1274                                               for segIdx, eventSeg in enumerate(eventBlock.segments):
  1275                                                   if verbose:
  1276                                                       print(
  1277                                                           'getAsigsAlignedToEvents on segment {} of {}'
  1278                                                           .format(segIdx + 1, len(eventBlock.segments)))
  1279                                                   allAlignEvents = allAlignEventsList[segIdx]
  1280                                                   if chunkSize is None:
  1281                                                       alignEventGroups = [allAlignEvents]
  1282                                                   else:
  1283                                                       nChunks = max(
  1284                                                           int(np.floor(allAlignEvents.shape[0] / chunkSize)),
  1285                                                           1)
  1286                                                       alignEventGroups = []
  1287                                                       for i in range(nChunks):
  1288                                                           if not (i == (nChunks - 1)):
  1289                                                               # not last one
  1290                                                               alignEventGroups.append(
  1291                                                                   allAlignEvents[i * chunkSize: (i + 1) * chunkSize])
  1292                                                           else:
  1293                                                               alignEventGroups.append(
  1294                                                                   allAlignEvents[i * chunkSize:])
  1295                                                   signalSeg = signalBlock.segments[segIdx]
  1296                                                   for subSegIdx, alignEvents in enumerate(alignEventGroups):
  1297                                                       # seg to contain triggered time series
  1298                                                       if verbose:
  1299                                                           print(
  1300                                                               'getAsigsAlignedToEvents on subSegment {} of {}'
  1301                                                               .format(subSegIdx + 1, len(alignEventGroups)))
  1302                                                       if not alignEvents.shape[0] > 0:
  1303                                                           continue
  1304                                                       newSeg = Segment(name='seg{}_'.format(int(totalNSegs)))
  1305                                                       newSeg.annotate(nix_name=newSeg.name)
  1306                                                       masterBlock.segments.append(newSeg)
  1307                                                       for chanName in chansToTrigger:
  1308                                                           asigName = 'seg{}_{}'.format(segIdx, chanName)
  1309                                                           if verbose:
  1310                                                               print(
  1311                                                                   'getAsigsAlignedToEvents on channel {}'
  1312                                                                   .format(chanName))
  1313                                                           assert len(signalSeg.filter(name=asigName)) == 1
  1314                                                           asig = signalSeg.filter(name=asigName)[0]
  1315                                                           nominalWinLen = int(
  1316                                                               (windowSize[1] - windowSize[0]) *
  1317                                                               asig.sampling_rate - 1)
  1318                                                           validMask = (
  1319                                                               ((
  1320                                                                   alignEvents + windowSize[1] +
  1321                                                                   asig.sampling_rate ** (-1)) < asig.t_stop) &
  1322                                                               ((
  1323                                                                   alignEvents + windowSize[0] -
  1324                                                                   asig.sampling_rate ** (-1)) > asig.t_start)
  1325                                                               )
  1326                                                           thisKeepMask = alignEvents.array_annotations['keepMask']
  1327                                                           fullMask = (validMask & thisKeepMask)
  1328                                                           alignEvents = alignEvents[fullMask]
  1329                                                           # array_annotations get sliced with the event, but regular anns do not
  1330                                                           for annName in alignEvents.annotations['arrayAnnNames']:
  1331                                                               alignEvents.annotations[annName] = (
  1332                                                                   alignEvents.array_annotations[annName])
  1333                                                           if isinstance(asig, AnalogSignalProxy):
  1334                                                               if checkReferences:
  1335                                                                   da = (
  1336                                                                       asig
  1337                                                                       ._rawio
  1338                                                                       .da_list['blocks'][0]['segments'][segIdx]['data'])
  1339                                                                   print('segIdx {}, asig.name {}'.format(
  1340                                                                       segIdx, asig.name))
  1341                                                                   print('asig._global_channel_indexes = {}'.format(
  1342                                                                       asig._global_channel_indexes))
  1343                                                                   print('asig references {}'.format(
  1344                                                                       da[asig._global_channel_indexes[0]]))
  1345                                                                   try:
  1346                                                                       assert (
  1347                                                                           asig.name
  1348                                                                           in da[asig._global_channel_indexes[0]].name)
  1349                                                                   except Exception:
  1350                                                                       traceback.print_exc()
  1351                                                               rawWaveforms = [
  1352                                                                   asig.load(
  1353                                                                       time_slice=(t + windowSize[0], t + windowSize[1]))
  1354                                                                   for t in alignEvents]
  1355                                                               if any([rW.shape[0] < nominalWinLen for rW in rawWaveforms]):
  1356                                                                   rawWaveforms = [
  1357                                                                       asig.load(
  1358                                                                           time_slice=(t + windowSize[0], t + windowSize[1] + asig.sampling_period))
  1359                                                                       for t in alignEvents]
  1360                                                           elif isinstance(asig, AnalogSignal):
  1361                                                               rawWaveforms = []
  1362                                                               for t in alignEvents:
  1363                                                                   asigMask = (asig.times > t + windowSize[0]) & (asig.times < t + windowSize[1])
  1364                                                                   rawWaveforms.append(asig[asigMask[:, np.newaxis]])
  1365                                                           else:
  1366                                                               raise(Exception('{} must be an AnalogSignal or AnalogSignalProxy!'.format(asigName)))
  1367                                                           #
  1368                                                           samplingRate = asig.sampling_rate
  1369                                                           waveformUnits = rawWaveforms[0].units
  1370                                                           #  fix length if roundoff error
  1371                                                           #  minLen = min([rW.shape[0] for rW in rawWaveforms])
  1372                                                           rawWaveforms = [rW[:nominalWinLen] for rW in rawWaveforms]
  1373                                                           #
  1374                                                           spikeWaveforms = (
  1375                                                               np.hstack([rW.magnitude for rW in rawWaveforms])
  1376                                                               .transpose()[:, np.newaxis, :] * waveformUnits
  1377                                                               )
  1378                                                           #
  1379                                                           thisUnit = masterBlock.filter(
  1380                                                               objects=Unit, name=chanName + '#0')[0]
  1381                                                           skipEventAnnNames = (
  1382                                                               ['nix_name', 'neo_name']
  1383                                                               )
  1384                                                           stAnn = {
  1385                                                               k: v
  1386                                                               for k, v in alignEvents.annotations.items()
  1387                                                               if k not in skipEventAnnNames
  1388                                                               }
  1389                                                           skipAsigAnnNames = (
  1390                                                               ['channel_id', 'nix_name', 'neo_name']
  1391                                                               )
  1392                                                           stAnn.update({
  1393                                                               k: v
  1394                                                               for k, v in asig.annotations.items()
  1395                                                               if k not in skipAsigAnnNames
  1396                                                           })
  1397                                                           st = SpikeTrain(
  1398                                                               name='seg{}_{}'.format(int(totalNSegs), thisUnit.name),
  1399                                                               times=alignEvents.times,
  1400                                                               waveforms=spikeWaveforms,
  1401                                                               t_start=asig.t_start, t_stop=asig.t_stop,
  1402                                                               left_sweep=windowSize[0] * (-1),
  1403                                                               sampling_rate=samplingRate,
  1404                                                               **stAnn
  1405                                                               )
  1406                                                           st.annotate(nix_name=st.name)
  1407                                                           st.annotations['unitAnnotations'] = json.dumps(
  1408                                                               thisUnit.annotations.copy())
  1409                                                           thisUnit.spiketrains.append(st)
  1410                                                           newSeg.spiketrains.append(st)
  1411                                                           st.unit = thisUnit
  1412                                                       totalNSegs += 1
  1413                                               try:
  1414                                                   eventBlock.filter(
  1415                                                       objects=EventProxy)[0]._rawio.file.close()
  1416                                               except Exception:
  1417                                                   traceback.print_exc()
  1418                                               if signalBlock is not eventBlock:
  1419                                                   try:
  1420                                                       signalBlock.filter(
  1421                                                           objects=AnalogSignalProxy)[0]._rawio.file.close()
  1422                                                   except Exception:
  1423                                                       traceback.print_exc()
  1424                                               triggeredPath = os.path.join(
  1425                                                   folderPath, fileName + '.nix')
  1426                                               if not os.path.exists(triggeredPath):
  1427                                                   appendToExisting = False
  1428                                           
  1429                                               if appendToExisting:
  1430                                                   allSegs = list(range(len(masterBlock.segments)))
  1431                                                   addBlockToNIX(
  1432                                                       masterBlock, neoSegIdx=allSegs,
  1433                                                       writeSpikes=True,
  1434                                                       fileName=fileName,
  1435                                                       folderPath=folderPath,
  1436                                                       purgeNixNames=False,
  1437                                                       nixBlockIdx=0, nixSegIdx=allSegs)
  1438                                               else:
  1439                                                   if os.path.exists(triggeredPath):
  1440                                                       os.remove(triggeredPath)
  1441                                                   masterBlock = purgeNixAnn(masterBlock)
  1442                                                   writer = NixIO(filename=triggeredPath)
  1443                                                   writer.write_block(masterBlock, use_obj_names=True)
  1444                                                   writer.close()
  1445                                               return masterBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: alignedAsigDFtoSpikeTrain at line 1447

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1447                                           @profile
  1448                                           def alignedAsigDFtoSpikeTrain(
  1449                                                   allWaveforms, dataBlock=None, matchSamplingRate=True):
  1450                                               masterBlock = Block()
  1451                                               masterBlock.name = dataBlock.annotations['neo_name']
  1452                                               masterBlock.annotate(nix_name=dataBlock.annotations['neo_name'])
  1453                                               for segIdx, group in allWaveforms.groupby('segment'):
  1454                                                   print('Saving trajectoriess for segment {}'.format(segIdx))
  1455                                                   dataSeg = dataBlock.segments[segIdx]
  1456                                                   exSt = dataSeg.spiketrains[0]
  1457                                                   if isinstance(exSt, SpikeTrainProxy):
  1458                                                       print(
  1459                                                           'alignedAsigDFtoSpikeTrain basing seg {} on {}'
  1460                                                           .format(segIdx, exSt.name))
  1461                                                       stProxy = exSt
  1462                                                       exSt = loadStProxy(stProxy)
  1463                                                       exSt = loadObjArrayAnn(exSt)
  1464                                                   print('exSt.left_sweep is {}'.format(exSt.left_sweep))
  1465                                                   wfBins = ((np.arange(exSt.waveforms.shape[2]) / (exSt.sampling_rate)) - exSt.left_sweep).magnitude
  1466                                                   # seg to contain triggered time series
  1467                                                   newSeg = Segment(name=dataSeg.annotations['neo_name'])
  1468                                                   newSeg.annotate(nix_name=dataSeg.annotations['neo_name'])
  1469                                                   masterBlock.segments.append(newSeg)
  1470                                                   #
  1471                                                   if group.columns.name == 'bin':
  1472                                                       grouper = group.groupby('feature')
  1473                                                       colsAre = 'bin'
  1474                                                   elif group.columns.name == 'feature':
  1475                                                       grouper = group.iteritems()
  1476                                                       colsAre = 'feature'
  1477                                                   for featName, featGroup in grouper:
  1478                                                       print('Saving {}...'.format(featName))
  1479                                                       if featName[-2:] == '#0':
  1480                                                           cleanFeatName = featName
  1481                                                       else:
  1482                                                           cleanFeatName = featName + '#0'
  1483                                                       if segIdx == 0:
  1484                                                           #  allocate units
  1485                                                           chanIdx = ChannelIndex(
  1486                                                               name=cleanFeatName, index=[0])
  1487                                                           chanIdx.annotate(nix_name=chanIdx.name)
  1488                                                           thisUnit = Unit(name=chanIdx.name)
  1489                                                           thisUnit.annotate(nix_name=chanIdx.name)
  1490                                                           chanIdx.units.append(thisUnit)
  1491                                                           thisUnit.channel_index = chanIdx
  1492                                                           masterBlock.channel_indexes.append(chanIdx)
  1493                                                       else:
  1494                                                           thisUnit = masterBlock.filter(
  1495                                                               objects=Unit, name=cleanFeatName)[0]
  1496                                                       if colsAre == 'bin':
  1497                                                           spikeWaveformsDF = featGroup
  1498                                                       elif colsAre == 'feature':
  1499                                                           if isinstance(featGroup, pd.Series):
  1500                                                               featGroup = featGroup.to_frame(name=featName)
  1501                                                               featGroup.columns.name = 'feature'
  1502                                                           spikeWaveformsDF = transposeSpikeDF(
  1503                                                               featGroup,
  1504                                                               'bin', fastTranspose=True)
  1505                                                       if matchSamplingRate:
  1506                                                           if len(spikeWaveformsDF.columns) != len(wfBins):
  1507                                                               wfDF = spikeWaveformsDF.reset_index(drop=True).T
  1508                                                               wfDF = hf.interpolateDF(wfDF, wfBins)
  1509                                                               spikeWaveformsDF = wfDF.T.set_index(spikeWaveformsDF.index)
  1510                                                       spikeWaveforms = spikeWaveformsDF.to_numpy()[:, np.newaxis, :]
  1511                                                       arrAnnDF = spikeWaveformsDF.index.to_frame()
  1512                                                       spikeTimes = arrAnnDF['t']
  1513                                                       arrAnnDF.drop(columns='t', inplace=True)
  1514                                                       arrAnn = {}
  1515                                                       colsToKeep = arrAnnDF.columns.drop(['originalIndex', 'feature', 'segment', 'lag'])
  1516                                                       for cName in colsToKeep:
  1517                                                           values = arrAnnDF[cName].to_numpy()
  1518                                                           if isinstance(values[0], str):
  1519                                                               values = values.astype('U')
  1520                                                           arrAnn.update({str(cName): values.flatten()})
  1521                                                       arrayAnnNames = {
  1522                                                           'arrayAnnNames': list(arrAnn.keys())}
  1523                                                       st = SpikeTrain(
  1524                                                           name='seg{}_{}'.format(int(segIdx), thisUnit.name),
  1525                                                           times=spikeTimes.to_numpy() * exSt.units,
  1526                                                           waveforms=spikeWaveforms * pq.dimensionless,
  1527                                                           t_start=exSt.t_start, t_stop=exSt.t_stop,
  1528                                                           left_sweep=exSt.left_sweep,
  1529                                                           sampling_rate=exSt.sampling_rate,
  1530                                                           **arrAnn, **arrayAnnNames
  1531                                                           )
  1532                                                       st.annotate(nix_name=st.name)
  1533                                                       thisUnit.spiketrains.append(st)
  1534                                                       newSeg.spiketrains.append(st)
  1535                                                       st.unit = thisUnit
  1536                                               return masterBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: dataFrameToAnalogSignals at line 1538

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1538                                           @profile
  1539                                           def dataFrameToAnalogSignals(
  1540                                                   df,
  1541                                                   block=None, seg=None,
  1542                                                   idxT='NSPTime',
  1543                                                   probeName='insTD', samplingRate=500*pq.Hz,
  1544                                                   timeUnits=pq.s, measureUnits=pq.mV,
  1545                                                   dataCol=['channel_0', 'channel_1'],
  1546                                                   useColNames=False, forceColNames=None,
  1547                                                   namePrefix='', nameSuffix='', verbose=False):
  1548                                               if block is None:
  1549                                                   assert seg is None
  1550                                                   block = Block(name=probeName)
  1551                                                   seg = Segment(name='seg0_' + probeName)
  1552                                                   block.segments.append(seg)
  1553                                               if verbose:
  1554                                                   print('in dataFrameToAnalogSignals...')
  1555                                               for idx, colName in enumerate(dataCol):
  1556                                                   if verbose:
  1557                                                       print('    {}'.format(colName))
  1558                                                   if forceColNames is not None:
  1559                                                       chanName = forceColNames[idx]
  1560                                                   elif useColNames:
  1561                                                       chanName = namePrefix + colName + nameSuffix
  1562                                                   else:
  1563                                                       chanName = namePrefix + (probeName.lower() + '{}'.format(idx)) + nameSuffix
  1564                                                   #
  1565                                                   chanIdx = ChannelIndex(
  1566                                                       name=chanName,
  1567                                                       # index=None,
  1568                                                       index=np.asarray([idx]),
  1569                                                       # channel_names=np.asarray([chanName])
  1570                                                       )
  1571                                                   block.channel_indexes.append(chanIdx)
  1572                                                   asig = AnalogSignal(
  1573                                                       df[colName].to_numpy() * measureUnits,
  1574                                                       name='seg0_' + chanName,
  1575                                                       sampling_rate=samplingRate,
  1576                                                       dtype=np.float32,
  1577                                                       # **ann
  1578                                                       )
  1579                                                   if idxT is not None:
  1580                                                       asig.t_start = df[idxT].iloc[0] * timeUnits
  1581                                                   else:
  1582                                                       asig.t_start = df.index[0] * timeUnits
  1583                                                   asig.channel_index = chanIdx
  1584                                                   # assign ownership to containers
  1585                                                   chanIdx.analogsignals.append(asig)
  1586                                                   seg.analogsignals.append(asig)
  1587                                                   chanIdx.create_relationship()
  1588                                               # assign parent to children
  1589                                               block.create_relationship()
  1590                                               seg.create_relationship()
  1591                                               return block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: eventDataFrameToEvents at line 1593

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1593                                           @profile
  1594                                           def eventDataFrameToEvents(
  1595                                                   eventDF, idxT=None,
  1596                                                   annCol=None,
  1597                                                   eventName='', tUnits=pq.s,
  1598                                                   makeList=True
  1599                                                   ):
  1600                                               if makeList:
  1601                                                   eventList = []
  1602                                                   for colName in annCol:
  1603                                                       originalDType = type(eventDF[colName].to_numpy()[0]).__name__
  1604                                                       event = Event(
  1605                                                           name=eventName + colName,
  1606                                                           times=eventDF[idxT].to_numpy() * tUnits,
  1607                                                           labels=eventDF[colName].astype(originalDType).to_numpy()
  1608                                                           )
  1609                                                       event.annotate(originalDType=originalDType)
  1610                                                       eventList.append(event)
  1611                                                   return eventList
  1612                                               else:
  1613                                                   if annCol is None:
  1614                                                       annCol = eventDF.drop(columns=idxT).columns
  1615                                                   event = Event(
  1616                                                       name=eventName,
  1617                                                       times=eventDF[idxT].to_numpy() * tUnits,
  1618                                                       labels=np.asarray(eventDF.index)
  1619                                                       )
  1620                                                   event.annotations.update(
  1621                                                       {
  1622                                                           'arrayAnnNames': [],
  1623                                                           'arrayAnnDTypes': []
  1624                                                           })
  1625                                                   for colName in annCol:
  1626                                                       originalDType = type(eventDF[colName].to_numpy()[0]).__name__
  1627                                                       arrayAnn = eventDF[colName].astype(originalDType).to_numpy()
  1628                                                       event.array_annotations.update(
  1629                                                           {colName: arrayAnn})
  1630                                                       event.annotations['arrayAnnNames'].append(colName)
  1631                                                       event.annotations['arrayAnnDTypes'].append(originalDType)
  1632                                                       event.annotations.update(
  1633                                                           {colName: arrayAnn})
  1634                                                   return event

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: eventsToDataFrame at line 1636

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1636                                           @profile
  1637                                           def eventsToDataFrame(
  1638                                                   events, idxT='t', names=None
  1639                                                   ):
  1640                                               eventDict = {}
  1641                                               calculatedT = False
  1642                                               for event in events:
  1643                                                   if names is not None:
  1644                                                       if event.name not in names:
  1645                                                           continue
  1646                                                   if len(event.times):
  1647                                                       if not calculatedT:
  1648                                                           t = pd.Series(event.times.magnitude)
  1649                                                           calculatedT = True
  1650                                                       try:
  1651                                                           values = event.array_annotations['labels']
  1652                                                       except Exception:
  1653                                                           values = event.labels
  1654                                                       if isinstance(values[0], bytes):
  1655                                                           #  event came from hdf, need to recover dtype
  1656                                                           if 'originalDType' in event.annotations:
  1657                                                               dtypeStr = event.annotations['originalDType'].split(';')[-1]
  1658                                                               if 'np.' not in dtypeStr:
  1659                                                                   dtypeStr = 'np.' + dtypeStr
  1660                                                               originalDType = eval(dtypeStr)
  1661                                                               values = np.asarray(values, dtype=originalDType)
  1662                                                           else:
  1663                                                               values = np.asarray(values, dtype=np.str)
  1664                                                       #  print(values.dtype)
  1665                                                       eventDict.update({
  1666                                                           event.name: pd.Series(values)})
  1667                                               eventDict.update({idxT: t})
  1668                                               return pd.concat(eventDict, axis=1)

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadSpikeMats at line 1670

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1670                                           @profile
  1671                                           def loadSpikeMats(
  1672                                                   dataPath, rasterOpts,
  1673                                                   alignTimes=None, chans=None, loadAll=False,
  1674                                                   absoluteBins=False, transposeSpikeMat=False,
  1675                                                   checkReferences=False,
  1676                                                   aggregateFun=None):
  1677                                           
  1678                                               reader = nixio_fr.NixIO(filename=dataPath)
  1679                                               chanNames = reader.header['signal_channels']['name']
  1680                                               
  1681                                               if chans is not None:
  1682                                                   sigMask = np.isin(chanNames, chans)
  1683                                                   chanNames = chanNames[sigMask]
  1684                                                   
  1685                                               chanIdx = reader.channel_name_to_index(chanNames)
  1686                                               
  1687                                               if not loadAll:
  1688                                                   assert alignTimes is not None
  1689                                                   spikeMats = {i: None for i in alignTimes.index}
  1690                                                   validTrials = pd.Series(True, index=alignTimes.index)
  1691                                               else:
  1692                                                   spikeMats = {
  1693                                                       i: None for i in range(reader.segment_count(block_index=0))}
  1694                                                   validTrials = None
  1695                                               
  1696                                               for segIdx in range(reader.segment_count(block_index=0)):
  1697                                                   if checkReferences:
  1698                                                       for i, cIdx in enumerate(chanIdx):
  1699                                                           da = reader.da_list['blocks'][0]['segments'][segIdx]['data'][cIdx]
  1700                                                           print('name {}, da.name {}'.format(chanNames[i], da.name))
  1701                                                           try:
  1702                                                               assert chanNames[i] in da.name, 'reference problem!!'
  1703                                                           except Exception:
  1704                                                               traceback.print_exc()
  1705                                                   tStart = reader.get_signal_t_start(
  1706                                                       block_index=0, seg_index=segIdx)
  1707                                                   fs = reader.get_signal_sampling_rate(
  1708                                                       channel_indexes=chanIdx
  1709                                                       )
  1710                                                   sigSize = reader.get_signal_size(
  1711                                                       block_index=0, seg_index=segIdx
  1712                                                       )
  1713                                                   tStop = sigSize / fs + tStart
  1714                                                   #  convert to indices early to avoid floating point problems
  1715                                                   
  1716                                                   intervalIdx = int(round(rasterOpts['binInterval'] * fs))
  1717                                                   #  halfIntervalIdx = int(round(intervalIdx / 2))
  1718                                                   
  1719                                                   widthIdx = int(round(rasterOpts['binWidth'] * fs))
  1720                                                   halfWidthIdx = int(round(widthIdx / 2))
  1721                                                   
  1722                                                   if rasterOpts['smoothKernelWidth'] is not None:
  1723                                                       kernWidthIdx = int(round(rasterOpts['smoothKernelWidth'] * fs))
  1724                                                   
  1725                                                   theBins = None
  1726                                           
  1727                                                   if not loadAll:
  1728                                                       winStartIdx = int(round(rasterOpts['windowSize'][0] * fs))
  1729                                                       winStopIdx = int(round(rasterOpts['windowSize'][1] * fs))
  1730                                                       timeMask = (alignTimes > tStart) & (alignTimes < tStop)
  1731                                                       maskedTimes = alignTimes[timeMask]
  1732                                                   else:
  1733                                                       #  irrelevant, will load all
  1734                                                       maskedTimes = pd.Series(np.nan)
  1735                                           
  1736                                                   for idx, tOnset in maskedTimes.iteritems():
  1737                                                       if not loadAll:
  1738                                                           idxOnset = int(round((tOnset - tStart) * fs))
  1739                                                           #  can't not be ints
  1740                                                           iStart = idxOnset + winStartIdx - int(3 * halfWidthIdx)
  1741                                                           iStop = idxOnset + winStopIdx + int(3 * halfWidthIdx)
  1742                                                       else:
  1743                                                           winStartIdx = 0
  1744                                                           iStart = 0
  1745                                                           iStop = sigSize
  1746                                           
  1747                                                       if iStart < 0:
  1748                                                           #  near the first edge
  1749                                                           validTrials[idx] = False
  1750                                                       elif (sigSize < iStop):
  1751                                                           #  near the ending edge
  1752                                                           validTrials[idx] = False
  1753                                                       else:
  1754                                                           #  valid slices
  1755                                                           try:
  1756                                                               rawSpikeMat = pd.DataFrame(
  1757                                                                   reader.get_analogsignal_chunk(
  1758                                                                       block_index=0, seg_index=segIdx,
  1759                                                                       i_start=iStart, i_stop=iStop,
  1760                                                                       channel_names=chanNames))
  1761                                                           except Exception:
  1762                                                               traceback.print_exc()
  1763                                                               #
  1764                                                           if aggregateFun is None:
  1765                                                               procSpikeMat = rawSpikeMat.rolling(
  1766                                                                   window=3 * widthIdx, center=True,
  1767                                                                   win_type='gaussian'
  1768                                                                   ).mean(std=halfWidthIdx)
  1769                                                           else:
  1770                                                               procSpikeMat = rawSpikeMat.rolling(
  1771                                                                   window=widthIdx, center=True
  1772                                                                   ).apply(
  1773                                                                       aggregateFun,
  1774                                                                       raw=True,
  1775                                                                       kwargs={'fs': fs, 'nSamp': widthIdx})
  1776                                                           #
  1777                                                           if rasterOpts['smoothKernelWidth'] is not None:
  1778                                                               procSpikeMat = (
  1779                                                                   procSpikeMat
  1780                                                                   .rolling(
  1781                                                                       window=3 * kernWidthIdx, center=True,
  1782                                                                       win_type='gaussian')
  1783                                                                   .mean(std=kernWidthIdx/2)
  1784                                                                   .dropna().iloc[::intervalIdx, :]
  1785                                                               )
  1786                                                           else:
  1787                                                               procSpikeMat = (
  1788                                                                   procSpikeMat
  1789                                                                   .dropna().iloc[::intervalIdx, :]
  1790                                                               )
  1791                                           
  1792                                                           procSpikeMat.columns = chanNames
  1793                                                           procSpikeMat.columns.name = 'unit'
  1794                                                           if theBins is None:
  1795                                                               theBins = np.asarray(
  1796                                                                   procSpikeMat.index + winStartIdx) / fs
  1797                                                           if absoluteBins:
  1798                                                               procSpikeMat.index = theBins + idxOnset / fs
  1799                                                           else:
  1800                                                               procSpikeMat.index = theBins
  1801                                                           procSpikeMat.index.name = 'bin'
  1802                                                           if loadAll:
  1803                                                               smIdx = segIdx
  1804                                                           else:
  1805                                                               smIdx = idx
  1806                                                               
  1807                                                           spikeMats[smIdx] = procSpikeMat
  1808                                                           if transposeSpikeMat:
  1809                                                               spikeMats[smIdx] = spikeMats[smIdx].transpose()
  1810                                                       #  plt.imshow(rawSpikeMat.to_numpy(), aspect='equal'); plt.show()
  1811                                               return spikeMats, validTrials

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: synchronizeINStoNSP at line 1813

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1813                                           @profile
  1814                                           def synchronizeINStoNSP(
  1815                                                   tapTimestampsNSP=None, tapTimestampsINS=None,
  1816                                                   precalculatedFun=None,
  1817                                                   NSPTimeRanges=(None, None),
  1818                                                   td=None, accel=None, insBlock=None, trialSegment=None, degree=1,
  1819                                                   trimSpiketrains=False
  1820                                                   ):
  1821                                               print('Trial Segment {}'.format(trialSegment))
  1822                                               if precalculatedFun is None:
  1823                                                   assert ((tapTimestampsNSP is not None) & (tapTimestampsINS is not None))
  1824                                                   # sanity check that the intervals match
  1825                                                   insDiff = tapTimestampsINS.diff().dropna().values
  1826                                                   nspDiff = tapTimestampsNSP.diff().dropna().values
  1827                                                   print('On the INS, the diff() between taps was\n{}'.format(insDiff))
  1828                                                   print('On the NSP, the diff() between taps was\n{}'.format(nspDiff))
  1829                                                   print('This amounts to a msec difference of\n{}'.format(
  1830                                                       (insDiff - nspDiff) * 1e3))
  1831                                                   if (insDiff - nspDiff > 20e-3).any():
  1832                                                       raise(Exception('Tap trains too different!'))
  1833                                                   #
  1834                                                   if degree > 0:
  1835                                                       synchPolyCoeffsINStoNSP = np.polyfit(
  1836                                                           x=tapTimestampsINS.values, y=tapTimestampsNSP.values,
  1837                                                           deg=degree)
  1838                                                   else:
  1839                                                       timeOffset = tapTimestampsNSP.values - tapTimestampsINS.values
  1840                                                       synchPolyCoeffsINStoNSP = np.array([1, np.mean(timeOffset)])
  1841                                                   timeInterpFunINStoNSP = np.poly1d(synchPolyCoeffsINStoNSP)
  1842                                               else:
  1843                                                   timeInterpFunINStoNSP = precalculatedFun
  1844                                               if td is not None:
  1845                                                   td.loc[:, 'NSPTime'] = pd.Series(
  1846                                                       timeInterpFunINStoNSP(td['t']), index=td['t'].index)
  1847                                                   td.loc[:, 'NSPTime'] = timeInterpFunINStoNSP(td['t'].to_numpy())
  1848                                               if accel is not None:
  1849                                                   accel.loc[:, 'NSPTime'] = pd.Series(
  1850                                                       timeInterpFunINStoNSP(accel['t']), index=accel['t'].index)
  1851                                               if insBlock is not None:
  1852                                                   # allUnits = [st.unit for st in insBlock.segments[0].spiketrains]
  1853                                                   # [un.name for un in insBlock.filter(objects=Unit)]
  1854                                                   for unit in insBlock.filter(objects=Unit):
  1855                                                       tStart = NSPTimeRanges[0]
  1856                                                       tStop = NSPTimeRanges[1]
  1857                                                       uniqueSt = []
  1858                                                       for st in unit.spiketrains:
  1859                                                           if st not in uniqueSt:
  1860                                                               uniqueSt.append(st)
  1861                                                           else:
  1862                                                               continue
  1863                                                           print('Synchronizing {}'.format(st.name))
  1864                                                           if len(st.times):
  1865                                                               segMaskSt = np.array(
  1866                                                                   st.array_annotations['trialSegment'],
  1867                                                                   dtype=np.int) == trialSegment
  1868                                                               st.magnitude[segMaskSt] = (
  1869                                                                   timeInterpFunINStoNSP(st.times[segMaskSt].magnitude))
  1870                                                               if trimSpiketrains:
  1871                                                                   print('Trimming spiketrain')
  1872                                                                   #  kludgey fix for weirdness concerning t_start
  1873                                                                   st.t_start = min(tStart, st.times[0] * 0.999)
  1874                                                                   st.t_stop = min(tStop, st.times[-1] * 1.001)
  1875                                                                   validMask = st < st.t_stop
  1876                                                                   if ~validMask.all():
  1877                                                                       print('Deleted some spikes')
  1878                                                                       st = st[validMask]
  1879                                                                       # delete invalid spikes
  1880                                                                       if 'arrayAnnNames' in st.annotations.keys():
  1881                                                                           for key in st.annotations['arrayAnnNames']:
  1882                                                                               try:
  1883                                                                                   # st.annotations[key] = np.array(st.array_annotations[key])
  1884                                                                                   st.annotations[key] = np.delete(st.annotations[key], ~validMask)
  1885                                                                               except Exception:
  1886                                                                                   traceback.print_exc()
  1887                                                                                   pdb.set_trace()
  1888                                                           else:
  1889                                                               if trimSpiketrains:
  1890                                                                   st.t_start = tStart
  1891                                                                   st.t_stop = tStop
  1892                                                   #
  1893                                                   allEvents = [
  1894                                                       ev
  1895                                                       for ev in insBlock.filter(objects=Event)
  1896                                                       if ('ins' in ev.name) and ('concatenate' not in ev.name)]
  1897                                                   concatEvents = [
  1898                                                       ev
  1899                                                       for ev in insBlock.filter(objects=Event)
  1900                                                       if ('ins' in ev.name) and ('concatenate' in ev.name)]
  1901                                                   eventsDF = eventsToDataFrame(allEvents, idxT='t')
  1902                                                   newNames = {i: childBaseName(i, 'seg') for i in eventsDF.columns}
  1903                                                   eventsDF.rename(columns=newNames, inplace=True)
  1904                                                   segMask = hf.getStimSerialTrialSegMask(eventsDF, trialSegment)
  1905                                                   evTStart = eventsDF.loc[segMask, 't'].min() * pq.s
  1906                                                   evTStop = eventsDF.loc[segMask, 't'].max() * pq.s
  1907                                                   # print('allEvents[0].shape = {}'.format(allEvents[0].shape))
  1908                                                   # print('allEvents[0].magnitude[segMask][0] = {}'.format(allEvents[0].magnitude[segMask][0]))
  1909                                                   for event in (allEvents + concatEvents):
  1910                                                       if trimSpiketrains:
  1911                                                           thisSegMask = (event.times >= evTStart) & (event.times <= evTStop)
  1912                                                       else:
  1913                                                           thisSegMask = (event.times >= evTStart) & (event.times < evTStop)
  1914                                                       event.magnitude[thisSegMask] = (
  1915                                                           timeInterpFunINStoNSP(event.times[thisSegMask].magnitude))
  1916                                                   # print('allEvents[0].magnitude[segMask][0] = {}'.format(allEvents[0].magnitude[segMask][0]))
  1917                                                   # if len(concatEvents) > trialSegment:
  1918                                                   #     concatEvents[trialSegment].magnitude[:] = timeInterpFunINStoNSP(
  1919                                                   #         concatEvents[trialSegment].times[:].magnitude)
  1920                                               return td, accel, insBlock, timeInterpFunINStoNSP

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: findSegsIncluding at line 1922

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1922                                           @profile
  1923                                           def findSegsIncluding(
  1924                                                   block, timeSlice=None):
  1925                                               segBoundsList = []
  1926                                               for segIdx, seg in enumerate(block.segments):
  1927                                                   segBoundsList.append(pd.DataFrame({
  1928                                                       't_start': seg.t_start,
  1929                                                       't_stop': seg.t_stop
  1930                                                       }, index=[segIdx]))
  1931                                           
  1932                                               segBounds = pd.concat(segBoundsList)
  1933                                               if timeSlice[0] is not None:
  1934                                                   segMask = (segBounds['t_start'] * pq.s >= timeSlice[0]) & (
  1935                                                       segBounds['t_stop'] * pq.s <= timeSlice[1])
  1936                                                   requestedSegs = segBounds.loc[segMask, :]
  1937                                               else:
  1938                                                   timeSlice = (None, None)
  1939                                                   requestedSegs = segBounds
  1940                                               return segBounds, requestedSegs

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: findSegsIncluded at line 1942

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1942                                           @profile
  1943                                           def findSegsIncluded(
  1944                                                   block, timeSlice=None):
  1945                                               segBoundsList = []
  1946                                               for segIdx, seg in enumerate(block.segments):
  1947                                                   segBoundsList.append(pd.DataFrame({
  1948                                                       't_start': seg.t_start,
  1949                                                       't_stop': seg.t_stop
  1950                                                       }, index=[segIdx]))
  1951                                           
  1952                                               segBounds = pd.concat(segBoundsList)
  1953                                               if timeSlice[0] is not None:
  1954                                                   segMask = (segBounds['t_start'] * pq.s <= timeSlice[0]) | (
  1955                                                       segBounds['t_stop'] * pq.s >= timeSlice[1])
  1956                                                   requestedSegs = segBounds.loc[segMask, :]
  1957                                               else:
  1958                                                   timeSlice = (None, None)
  1959                                                   requestedSegs = segBounds
  1960                                               return segBounds, requestedSegs

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getElecLookupTable at line 1962

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1962                                           @profile
  1963                                           def getElecLookupTable(
  1964                                                   block, elecIds=None):
  1965                                               lookupTableList = []
  1966                                               for metaIdx, chanIdx in enumerate(block.channel_indexes):
  1967                                                   if chanIdx.analogsignals:
  1968                                                       #  print(chanIdx.name)
  1969                                                       lookupTableList.append(pd.DataFrame({
  1970                                                           'channelNames': np.asarray(chanIdx.channel_names, dtype=np.str),
  1971                                                           'index': chanIdx.index,
  1972                                                           'metaIndex': metaIdx * chanIdx.index**0,
  1973                                                           'localIndex': (
  1974                                                               list(range(chanIdx.analogsignals[0].shape[1])))
  1975                                                           }))
  1976                                               lookupTable = pd.concat(lookupTableList, ignore_index=True)
  1977                                           
  1978                                               if elecIds is None:
  1979                                                   requestedIndices = lookupTable
  1980                                               else:
  1981                                                   if isinstance(elecIds[0], str):
  1982                                                       idxMask = lookupTable['channelNames'].isin(elecIds)
  1983                                                       requestedIndices = lookupTable.loc[idxMask, :]
  1984                                               return lookupTable, requestedIndices

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getNIXData at line 1986

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1986                                           @profile
  1987                                           def getNIXData(
  1988                                                   fileName=None,
  1989                                                   folderPath=None,
  1990                                                   reader=None, blockIdx=0,
  1991                                                   elecIds=None, startTime_s=None,
  1992                                                   dataLength_s=None, downsample=1,
  1993                                                   signal_group_mode='group-by-same-units',
  1994                                                   closeReader=False):
  1995                                               #  Open file and extract headers
  1996                                               if reader is None:
  1997                                                   assert (fileName is not None) and (folderPath is not None)
  1998                                                   filePath = os.path.join(folderPath, fileName) + '.nix'
  1999                                                   reader = nixio_fr.NixIO(filename=filePath)
  2000                                           
  2001                                               block = reader.read_block(
  2002                                                   block_index=blockIdx, lazy=True,
  2003                                                   signal_group_mode=signal_group_mode)
  2004                                           
  2005                                               for segIdx, seg in enumerate(block.segments):
  2006                                                   seg.events = [i.load() for i in seg.events]
  2007                                                   seg.epochs = [i.load() for i in seg.epochs]
  2008                                           
  2009                                               # find elecIds
  2010                                               lookupTable, requestedIndices = getElecLookupTable(
  2011                                                   block, elecIds=elecIds)
  2012                                           
  2013                                               # find segments that contain the requested times
  2014                                               if dataLength_s is not None:
  2015                                                   assert startTime_s is not None
  2016                                                   timeSlice = (
  2017                                                       startTime_s * pq.s,
  2018                                                       (startTime_s + dataLength_s) * pq.s)
  2019                                               else:
  2020                                                   timeSlice = (None, None)
  2021                                               segBounds, requestedSegs = findSegsIncluding(block, timeSlice)
  2022                                               #
  2023                                               data = pd.DataFrame(columns=elecIds + ['t'])
  2024                                               for segIdx in requestedSegs.index:
  2025                                                   seg = block.segments[segIdx]
  2026                                                   if dataLength_s is not None:
  2027                                                       timeSlice = (
  2028                                                           max(timeSlice[0], seg.t_start),
  2029                                                           min(timeSlice[1], seg.t_stop)
  2030                                                           )
  2031                                                   else:
  2032                                                       timeSlice = (seg.t_start, seg.t_stop)
  2033                                                   segData = pd.DataFrame()
  2034                                                   for metaIdx in pd.unique(requestedIndices['metaIndex']):
  2035                                                       metaIdxMatch = requestedIndices['metaIndex'] == metaIdx
  2036                                                       theseRequestedIndices = requestedIndices.loc[
  2037                                                           metaIdxMatch, :]
  2038                                                       theseElecIds = theseRequestedIndices['channelNames']
  2039                                                       asig = seg.analogsignals[metaIdx]
  2040                                                       thisTimeSlice = (
  2041                                                           max(timeSlice[0], asig.t_start),
  2042                                                           min(timeSlice[1], asig.t_stop)
  2043                                                           )
  2044                                                       reqData = asig.load(
  2045                                                           time_slice=thisTimeSlice,
  2046                                                           channel_indexes=theseRequestedIndices['localIndex'].to_numpy())
  2047                                                       segData = pd.concat((
  2048                                                               segData,
  2049                                                               pd.DataFrame(
  2050                                                                   reqData.magnitude, columns=theseElecIds.to_numpy())),
  2051                                                           axis=1)
  2052                                                   segT = reqData.times
  2053                                                   segData['t'] = segT
  2054                                                   data = pd.concat(
  2055                                                       (data, segData),
  2056                                                       axis=0, ignore_index=True)
  2057                                               channelData = {
  2058                                                   'data': data,
  2059                                                   't': data['t']
  2060                                                   }
  2061                                               if closeReader:
  2062                                                   reader.file.close()
  2063                                                   block = None
  2064                                                   # closing the reader breaks its connection to the block
  2065                                               return channelData, block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: childBaseName at line 2067

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2067                                           @profile
  2068                                           def childBaseName(
  2069                                                   childName, searchTerm):
  2070                                               if searchTerm in childName:
  2071                                                   baseName = '_'.join(childName.split('_')[1:])
  2072                                               else:
  2073                                                   baseName = childName
  2074                                               return baseName

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: readBlockFixNames at line 2076

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2076                                           @profile
  2077                                           def readBlockFixNames(
  2078                                                   rawioReader,
  2079                                                   block_index=0, signal_group_mode='split-all',
  2080                                                   lazy=True, mapDF=None, reduceChannelIndexes=False,
  2081                                                   loadList=None, purgeNixNames=False
  2082                                                   ):
  2083                                               headerSignalChan = pd.DataFrame(
  2084                                                   rawioReader.header['signal_channels']).set_index('id')
  2085                                               headerUnitChan = pd.DataFrame(
  2086                                                   rawioReader.header['unit_channels']).set_index('id')
  2087                                               dataBlock = rawioReader.read_block(
  2088                                                   block_index=block_index, lazy=lazy,
  2089                                                   signal_group_mode=signal_group_mode)
  2090                                               if dataBlock.name is None:
  2091                                                   if 'neo_name' in dataBlock.annotations:
  2092                                                       dataBlock.name = dataBlock.annotations['neo_name']
  2093                                               #  on first segment, rename the chan_indexes and units
  2094                                               seg0 = dataBlock.segments[0]
  2095                                               asigLikeList = (
  2096                                                   seg0.filter(objects=AnalogSignalProxy) +
  2097                                                   seg0.filter(objects=AnalogSignal))
  2098                                               if mapDF is not None:
  2099                                                   if headerSignalChan.size > 0:
  2100                                                       asigNameChanger = {}
  2101                                                       for nevID in mapDF['nevID']:
  2102                                                           if int(nevID) in headerSignalChan.index:
  2103                                                               labelFromMap = (
  2104                                                                   mapDF
  2105                                                                   .loc[mapDF['nevID'] == nevID, 'label']
  2106                                                                   .iloc[0])
  2107                                                               asigNameChanger[
  2108                                                                   headerSignalChan.loc[int(nevID), 'name']] = labelFromMap
  2109                                                   else:
  2110                                                       asigOrigNames = np.unique(
  2111                                                           [i.split('#')[0] for i in headerUnitChan['name']])
  2112                                                       asigNameChanger = {}
  2113                                                       for origName in asigOrigNames:
  2114                                                           # ripple specific
  2115                                                           formattedName = origName.replace('.', '_').replace(' raw', '')
  2116                                                           if mapDF['label'].str.contains(formattedName).any():
  2117                                                               asigNameChanger[origName] = formattedName
  2118                                               else:
  2119                                                   asigNameChanger = dict()
  2120                                               for asig in asigLikeList:
  2121                                                   asigBaseName = childBaseName(asig.name, 'seg')
  2122                                                   asig.name = (
  2123                                                       asigNameChanger[asigBaseName]
  2124                                                       if asigBaseName in asigNameChanger
  2125                                                       else asigBaseName)
  2126                                                   if mapDF is not None:
  2127                                                       if (mapDF['label'] == asig.name).any():
  2128                                                           asig.annotations['xCoords'] = float(mapDF.loc[mapDF['label'] == asig.name, 'xcoords'].iloc[0])
  2129                                                           asig.annotations['yCoords'] = float(mapDF.loc[mapDF['label'] == asig.name, 'ycoords'].iloc[0])
  2130                                                           asig.annotations['zCoords'] = float(mapDF.loc[mapDF['label'] == asig.name, 'zcoords'].iloc[0])
  2131                                                   if 'Channel group ' in asig.channel_index.name:
  2132                                                       newChanName = (
  2133                                                           asigNameChanger[asigBaseName]
  2134                                                           if asigBaseName in asigNameChanger
  2135                                                           else asigBaseName)
  2136                                                       asig.channel_index.name = newChanName
  2137                                                       if 'neo_name' in asig.channel_index.annotations:
  2138                                                           asig.channel_index.annotations['neo_name'] = newChanName
  2139                                                       if 'nix_name' in asig.channel_index.annotations:
  2140                                                           asig.channel_index.annotations['nix_name'] = newChanName
  2141                                                       if mapDF is not None:
  2142                                                           try:
  2143                                                               asig.channel_index.coordinates = np.asarray([
  2144                                                                   asig.annotations['xCoords'], asig.annotations['yCoords'], asig.annotations['zCoords']
  2145                                                               ])[np.newaxis, :] * pq.um
  2146                                                           except Exception:
  2147                                                               pass
  2148                                               spikeTrainLikeList = (
  2149                                                   seg0.filter(objects=SpikeTrainProxy) +
  2150                                                   seg0.filter(objects=SpikeTrain))
  2151                                               # add channels for channelIndex that has no asigs but has spikes
  2152                                               nExtraChans = 0
  2153                                               for stp in spikeTrainLikeList:
  2154                                                   stpBaseName = childBaseName(stp.name, 'seg')
  2155                                                   nameParser = re.search(r'ch(\d*)#(\d*)', stpBaseName)
  2156                                                   if nameParser is not None:
  2157                                                       # first time at this unit, rename it
  2158                                                       chanId = int(nameParser.group(1))
  2159                                                       unitId = int(nameParser.group(2))
  2160                                                       if chanId >= 5121:
  2161                                                           isRippleStimChan = True
  2162                                                           chanId = chanId - 5120
  2163                                                       else:
  2164                                                           isRippleStimChan = False
  2165                                                       ####################
  2166                                                       # asigBaseName = headerSignalChan.loc[chanId, 'name']
  2167                                                       # if mapDF is not None:
  2168                                                       #     if asigBaseName in asigNameChanger:
  2169                                                       #         chanIdLabel = (
  2170                                                       #             asigNameChanger[asigBaseName]
  2171                                                       #             if asigBaseName in asigNameChanger
  2172                                                       #             else asigBaseName)
  2173                                                       #     else:
  2174                                                       #         chanIdLabel = asigBaseName
  2175                                                       # else:
  2176                                                       #     chanIdLabel = asigBaseName
  2177                                                       ###################
  2178                                                       # if swapMaps is not None:
  2179                                                       #     nameCandidates = (swapMaps['to'].loc[swapMaps['to']['nevID'] == chanId, 'label']).to_list()
  2180                                                       # elif mapDF is not None:
  2181                                                       #     nameCandidates = (mapDF.loc[mapDF['nevID'] == chanId, 'label']).to_list()
  2182                                                       # else:
  2183                                                       #     nameCandidates = []
  2184                                                       ##############################
  2185                                                       if mapDF is not None:
  2186                                                           nameCandidates = (
  2187                                                               mapDF
  2188                                                               .loc[mapDF['nevID'] == chanId, 'label']
  2189                                                               .to_list())
  2190                                                       else:
  2191                                                           nameCandidates = []
  2192                                                       if len(nameCandidates) == 1:
  2193                                                           chanIdLabel = nameCandidates[0]
  2194                                                       elif chanId in headerSignalChan:
  2195                                                           chanIdLabel = headerSignalChan.loc[chanId, 'name']
  2196                                                       else:
  2197                                                           chanIdLabel = 'ch{}'.format(chanId)
  2198                                                       #
  2199                                                       if isRippleStimChan:
  2200                                                           stp.name = '{}_stim#{}'.format(chanIdLabel, unitId)
  2201                                                       else:
  2202                                                           stp.name = '{}#{}'.format(chanIdLabel, unitId)
  2203                                                       stp.unit.name = stp.name
  2204                                                   ########################################
  2205                                                   # sanitize ripple names ####
  2206                                                   stp.name = stp.name.replace('.', '_').replace(' raw', '')
  2207                                                   stp.unit.name = stp.unit.name.replace('.', '_').replace(' raw', '')
  2208                                                   ###########################################
  2209                                                   if 'ChannelIndex for ' in stp.unit.channel_index.name:
  2210                                                       newChanName = stp.name.replace('_stim#0', '')
  2211                                                       # remove unit #
  2212                                                       newChanName = re.sub(r'#\d', '', newChanName)
  2213                                                       stp.unit.channel_index.name = newChanName
  2214                                                       # units and analogsignals have different channel_indexes when loaded by nix
  2215                                                       # add them to each other's parent list
  2216                                                       allMatchingChIdx = dataBlock.filter(
  2217                                                           objects=ChannelIndex, name=newChanName)
  2218                                                       if (len(allMatchingChIdx) > 1) and reduceChannelIndexes:
  2219                                                           assert len(allMatchingChIdx) == 2
  2220                                                           targetChIdx = [
  2221                                                               ch
  2222                                                               for ch in allMatchingChIdx
  2223                                                               if ch is not stp.unit.channel_index][0]
  2224                                                           oldChIdx = stp.unit.channel_index
  2225                                                           targetChIdx.units.append(stp.unit)
  2226                                                           stp.unit.channel_index = targetChIdx
  2227                                                           oldChIdx.units.remove(stp.unit)
  2228                                                           if not (len(oldChIdx.units) or len(oldChIdx.analogsignals)):
  2229                                                               dataBlock.channel_indexes.remove(oldChIdx)
  2230                                                           del oldChIdx
  2231                                                           targetChIdx.create_relationship()
  2232                                                       elif reduceChannelIndexes:
  2233                                                           if newChanName not in headerSignalChan['name']:
  2234                                                               stp.unit.channel_index.index = np.asarray(
  2235                                                                   [headerSignalChan['name'].size + nExtraChans])
  2236                                                               stp.unit.channel_index.channel_ids = np.asarray(
  2237                                                                   [headerSignalChan['name'].size + nExtraChans])
  2238                                                               stp.unit.channel_index.channel_names = np.asarray(
  2239                                                                   [newChanName])
  2240                                                               nExtraChans += 1
  2241                                                           if 'neo_name' not in allMatchingChIdx[0].annotations:
  2242                                                               allMatchingChIdx[0].annotations['neo_name'] = allMatchingChIdx[0].name
  2243                                                           if 'nix_name' not in allMatchingChIdx[0].annotations:
  2244                                                               allMatchingChIdx[0].annotations['nix_name'] = allMatchingChIdx[0].name
  2245                                                   stp.unit.channel_index.name = stp.unit.channel_index.name.replace('.', '_').replace(' raw', '')
  2246                                               #  rename the children
  2247                                               typesNeedRenaming = [
  2248                                                   SpikeTrainProxy, AnalogSignalProxy, EventProxy,
  2249                                                   SpikeTrain, AnalogSignal, Event]
  2250                                               for segIdx, seg in enumerate(dataBlock.segments):
  2251                                                   if seg.name is None:
  2252                                                       seg.name = 'seg{}_'.format(segIdx)
  2253                                                   else:
  2254                                                       if 'seg{}_'.format(segIdx) not in seg.name:
  2255                                                           seg.name = (
  2256                                                               'seg{}_{}'
  2257                                                               .format(
  2258                                                                   segIdx,
  2259                                                                   childBaseName(seg.name, 'seg')))
  2260                                                   for objType in typesNeedRenaming:
  2261                                                       for child in seg.filter(objects=objType):
  2262                                                           if 'seg{}_'.format(segIdx) not in child.name:
  2263                                                               child.name = (
  2264                                                                   'seg{}_{}'
  2265                                                                   .format(
  2266                                                                       segIdx, childBaseName(child.name, 'seg')))
  2267                                                           #  todo: decide if below is needed
  2268                                                           #  elif 'seg' in child.name:
  2269                                                           #      childBaseName = '_'.join(child.name.split('_')[1:])
  2270                                                           #      child.name = 'seg{}_{}'.format(segIdx, childBaseName)
  2271                                               # [i.name for i in dataBlock.filter(objects=Unit)]
  2272                                               # [i.name for i in dataBlock.filter(objects=ChannelIndex)]
  2273                                               # [i.name for i in dataBlock.filter(objects=SpikeTrain)]
  2274                                               # [i.name for i in dataBlock.filter(objects=SpikeTrainProxy)]
  2275                                               if lazy:
  2276                                                   for stP in dataBlock.filter(objects=SpikeTrainProxy):
  2277                                                       if 'unitAnnotations' in stP.annotations:
  2278                                                           unAnnStr = stP.annotations['unitAnnotations']
  2279                                                           stP.unit.annotations.update(json.loads(unAnnStr))
  2280                                               if (loadList is not None) and lazy:
  2281                                                   if 'asigs' in loadList:
  2282                                                       loadAsigList(
  2283                                                           dataBlock, listOfAsigProxyNames=loadList['asigs'],
  2284                                                           replaceInParents=True)
  2285                                                   if 'events' in loadList:
  2286                                                       loadEventList(
  2287                                                           dataBlock,
  2288                                                           listOfEventNames=loadList['events'],
  2289                                                           replaceInParents=True)
  2290                                                   if 'spiketrains' in loadList:
  2291                                                       loadSpikeTrainList(
  2292                                                           dataBlock,
  2293                                                           listOfSpikeTrainNames=loadList['spiketrains'],
  2294                                                           replaceInParents=True)
  2295                                               if purgeNixNames:
  2296                                                   dataBlock = purgeNixAnn(dataBlock)
  2297                                               return dataBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadSpikeTrainList at line 2299

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2299                                           @profile
  2300                                           def loadSpikeTrainList(
  2301                                                   dataBlock, listOfSpikeTrainNames=None,
  2302                                                   replaceInParents=True):
  2303                                               listOfSpikeTrains = []
  2304                                               if listOfSpikeTrainNames is None:
  2305                                                   listOfSpikeTrainNames = [
  2306                                                       stp.name
  2307                                                       for stp in dataBlock.filter(objects=SpikeTrainProxy)]
  2308                                               for stP in dataBlock.filter(objects=SpikeTrainProxy):
  2309                                                   if stP.name in listOfSpikeTrainNames:
  2310                                                       st = loadObjArrayAnn(stP.load())
  2311                                                       listOfSpikeTrains.append(st)
  2312                                                       if replaceInParents:
  2313                                                           seg = stP.segment
  2314                                                           segStNames = [s.name for s in seg.spiketrains]
  2315                                                           idxInSeg = segStNames.index(stP.name)
  2316                                                           seg.spiketrains[idxInSeg] = st
  2317                                                           #
  2318                                                           unit = stP.unit
  2319                                                           unitStNames = [s.name for s in unit.spiketrains]
  2320                                                           st.unit = unit
  2321                                                           idxInUnit = unitStNames.index(stP.name)
  2322                                                           unit.spiketrains[idxInUnit] = st
  2323                                               return listOfSpikeTrains

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadEventList at line 2325

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2325                                           @profile
  2326                                           def loadEventList(
  2327                                                   dataBlock,
  2328                                                   listOfEventNames=None, replaceInParents=True):
  2329                                               listOfEvents = []
  2330                                               if listOfEventNames is None:
  2331                                                   listOfEventNames = [
  2332                                                       evp.name
  2333                                                       for evp in dataBlock.filter(objects=EventProxy)]
  2334                                               for evP in dataBlock.filter(objects=EventProxy):
  2335                                                   if evP.name in listOfEventNames:
  2336                                                       ev = loadObjArrayAnn(evP.load())
  2337                                                       listOfEvents.append(ev)
  2338                                                       if replaceInParents:
  2339                                                           seg = evP.segment
  2340                                                           segEvNames = [e.name for e in seg.events]
  2341                                                           idxInSeg = segEvNames.index(evP.name)
  2342                                                           seg.events[idxInSeg] = ev
  2343                                               return listOfEvents

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadAsigList at line 2345

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2345                                           @profile
  2346                                           def loadAsigList(
  2347                                                   dataBlock, listOfAsigProxyNames=None, replaceInParents=True):
  2348                                               listOfAsigs = []
  2349                                               if listOfAsigProxyNames is None:
  2350                                                   listOfAsigProxyNames = [
  2351                                                       asigp.name
  2352                                                       for asigp in dataBlock.filter(objects=AnalogSignalProxy)]
  2353                                               for asigP in dataBlock.filter(objects=AnalogSignalProxy):
  2354                                                   if asigP.name in listOfAsigProxyNames:
  2355                                                       asig = asigP.load()
  2356                                                       asig.annotations = asigP.annotations.copy()
  2357                                                       listOfAsigs.append(asig)
  2358                                                       #
  2359                                                       if replaceInParents:
  2360                                                           seg = asigP.segment
  2361                                                           segAsigNames = [ag.name for ag in seg.analogsignals]
  2362                                                           asig.segment = seg
  2363                                                           idxInSeg = segAsigNames.index(asigP.name)
  2364                                                           seg.analogsignals[idxInSeg] = asig
  2365                                                           #
  2366                                                           chIdx = asigP.channel_index
  2367                                                           chIdxAsigNames = [ag.name for ag in chIdx.analogsignals]
  2368                                                           asig.channel_index = chIdx
  2369                                                           idxInChIdx = chIdxAsigNames.index(asigP.name)
  2370                                                           chIdx.analogsignals[idxInChIdx] = asig
  2371                                               return listOfAsigs

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: addBlockToNIX at line 2373

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2373                                           @profile
  2374                                           def addBlockToNIX(
  2375                                                   newBlock, neoSegIdx=[0],
  2376                                                   writeAsigs=True, writeSpikes=True, writeEvents=True,
  2377                                                   asigNameList=None,
  2378                                                   purgeNixNames=False,
  2379                                                   fileName=None,
  2380                                                   folderPath=None,
  2381                                                   nixBlockIdx=0, nixSegIdx=[0],
  2382                                                   ):
  2383                                               #  base file name
  2384                                               trialBasePath = os.path.join(folderPath, fileName)
  2385                                               if writeAsigs:
  2386                                                   # peek at file to ensure compatibility
  2387                                                   reader = nixio_fr.NixIO(filename=trialBasePath + '.nix')
  2388                                                   tempBlock = reader.read_block(
  2389                                                       block_index=nixBlockIdx,
  2390                                                       lazy=True, signal_group_mode='split-all')
  2391                                                   checkCompatible = {i: False for i in nixSegIdx}
  2392                                                   forceShape = {i: None for i in nixSegIdx}
  2393                                                   forceType = {i: None for i in nixSegIdx}
  2394                                                   forceFS = {i: None for i in nixSegIdx}
  2395                                                   for nixIdx in nixSegIdx:
  2396                                                       tempAsigList = tempBlock.segments[nixIdx].filter(
  2397                                                           objects=AnalogSignalProxy)
  2398                                                       if len(tempAsigList) > 0:
  2399                                                           tempAsig = tempAsigList[0]
  2400                                                           checkCompatible[nixIdx] = True
  2401                                                           forceType[nixIdx] = tempAsig.dtype
  2402                                                           forceShape[nixIdx] = tempAsig.shape[0]  # ? docs say shape[1], but that's confusing
  2403                                                           forceFS[nixIdx] = tempAsig.sampling_rate
  2404                                                   reader.file.close()
  2405                                               #  if newBlock was loaded from a nix file, strip the old nix_names away:
  2406                                               #  todo: replace with function from this module
  2407                                               if purgeNixNames:
  2408                                                   newBlock = purgeNixAnn(newBlock)
  2409                                               #
  2410                                               writer = NixIO(filename=trialBasePath + '.nix')
  2411                                               nixblock = writer.nix_file.blocks[nixBlockIdx]
  2412                                               nixblockName = nixblock.name
  2413                                               if 'nix_name' in newBlock.annotations.keys():
  2414                                                   try:
  2415                                                       assert newBlock.annotations['nix_name'] == nixblockName
  2416                                                   except Exception:
  2417                                                       newBlock.annotations['nix_name'] = nixblockName
  2418                                               else:
  2419                                                   newBlock.annotate(nix_name=nixblockName)
  2420                                               #
  2421                                               for idx, segIdx in enumerate(neoSegIdx):
  2422                                                   nixIdx = nixSegIdx[idx]
  2423                                                   newSeg = newBlock.segments[segIdx]
  2424                                                   nixgroup = nixblock.groups[nixIdx]
  2425                                                   nixSegName = nixgroup.name
  2426                                                   if 'nix_name' in newSeg.annotations.keys():
  2427                                                       try:
  2428                                                           assert newSeg.annotations['nix_name'] == nixSegName
  2429                                                       except Exception:
  2430                                                           newSeg.annotations['nix_name'] = nixSegName
  2431                                                   else:
  2432                                                       newSeg.annotate(nix_name=nixSegName)
  2433                                                   #
  2434                                                   if writeEvents:
  2435                                                       eventList = newSeg.events
  2436                                                       eventOrder = np.argsort([i.name for i in eventList])
  2437                                                       for event in [eventList[i] for i in eventOrder]:
  2438                                                           event = writer._write_event(event, nixblock, nixgroup)
  2439                                                   #
  2440                                                   if writeAsigs:
  2441                                                       asigList = newSeg.filter(objects=AnalogSignal)
  2442                                                       asigOrder = np.argsort([i.name for i in asigList])
  2443                                                       for asig in [asigList[i] for i in asigOrder]:
  2444                                                           if checkCompatible[nixIdx]:
  2445                                                               assert asig.dtype == forceType[nixIdx]
  2446                                                               assert asig.sampling_rate == forceFS[nixIdx]
  2447                                                               #  print('asig.shape[0] = {}'.format(asig.shape[0]))
  2448                                                               #  print('forceShape[nixIdx] = {}'.format(forceShape[nixIdx]))
  2449                                                               assert asig.shape[0] == forceShape[nixIdx]
  2450                                                           asig = writer._write_analogsignal(asig, nixblock, nixgroup)
  2451                                                       #  for isig in newSeg.filter(objects=IrregularlySampledSignal):
  2452                                                       #      isig = writer._write_irregularlysampledsignal(
  2453                                                       #          isig, nixblock, nixgroup)
  2454                                                   #
  2455                                                   if writeSpikes:
  2456                                                       stList = newSeg.filter(objects=SpikeTrain)
  2457                                                       stOrder = np.argsort([i.name for i in stList])
  2458                                                       for st in [stList[i] for i in stOrder]:
  2459                                                           st = writer._write_spiketrain(st, nixblock, nixgroup)
  2460                                               #
  2461                                               for chanIdx in newBlock.filter(objects=ChannelIndex):
  2462                                                   chanIdx = writer._write_channelindex(chanIdx, nixblock)
  2463                                                   #  auto descends into units inside of _write_channelindex
  2464                                               writer._create_source_links(newBlock, nixblock)
  2465                                               writer.close()
  2466                                               print('Done adding block to Nix.')
  2467                                               return newBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadStProxy at line 2469

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2469                                           @profile
  2470                                           def loadStProxy(stProxy):
  2471                                               try:
  2472                                                   st = stProxy.load(
  2473                                                       magnitude_mode='rescaled',
  2474                                                       load_waveforms=True)
  2475                                               except Exception:
  2476                                                   st = stProxy.load(
  2477                                                       magnitude_mode='rescaled',
  2478                                                       load_waveforms=False)
  2479                                                   st.waveforms = np.asarray([]).reshape((0, 0, 0))*pq.mV
  2480                                               return st

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: preproc at line 2482

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2482                                           @profile
  2483                                           def preproc(
  2484                                                   fileName='Trial001',
  2485                                                   rawFolderPath='./',
  2486                                                   outputFolderPath='./', mapDF=None,
  2487                                                   # swapMaps=None,
  2488                                                   electrodeArrayName='utah',
  2489                                                   fillOverflow=True, removeJumps=True,
  2490                                                   removeMeanAcross=False,
  2491                                                   linearDetrend=False,
  2492                                                   interpolateOutliers=False, calcOutliers=False,
  2493                                                   outlierMaskFilterOpts=None,
  2494                                                   outlierThreshold=1,
  2495                                                   calcArtifactTrace=False,
  2496                                                   motorEncoderMask=None,
  2497                                                   calcAverageLFP=False,
  2498                                                   eventInfo=None,
  2499                                                   spikeSourceType='', spikePath=None,
  2500                                                   chunkSize=1800, equalChunks=True, chunkList=None, chunkOffset=0,
  2501                                                   writeMode='rw',
  2502                                                   signal_group_mode='split-all', trialInfo=None,
  2503                                                   asigNameList=None, ainpNameList=None, nameSuffix='',
  2504                                                   saveFromAsigNameList=True,
  2505                                                   calcRigEvents=True, normalizeByImpedance=False,
  2506                                                   LFPFilterOpts=None, encoderCountPerDegree=180e2,
  2507                                                   outlierRemovalDebugFlag=False, impedanceFilePath=None
  2508                                                   ):
  2509                                               #  base file name
  2510                                               rawBasePath = os.path.join(rawFolderPath, fileName)
  2511                                               outputFilePath = os.path.join(
  2512                                                   outputFolderPath,
  2513                                                   fileName + nameSuffix + '.nix')
  2514                                               if os.path.exists(outputFilePath):
  2515                                                   os.remove(outputFilePath)
  2516                                               #  instantiate reader, get metadata
  2517                                               print('Loading\n{}\n'.format(rawBasePath))
  2518                                               reader = BlackrockIO(
  2519                                                   filename=rawBasePath, nsx_to_load=5)
  2520                                               reader.parse_header()
  2521                                               # metadata = reader.header
  2522                                               #  absolute section index
  2523                                               dummyBlock = readBlockFixNames(
  2524                                                   reader,
  2525                                                   block_index=0, lazy=True,
  2526                                                   signal_group_mode=signal_group_mode,
  2527                                                   mapDF=mapDF, reduceChannelIndexes=True,
  2528                                                   # swapMaps=swapMaps
  2529                                                   )
  2530                                               segLen = dummyBlock.segments[0].analogsignals[0].shape[0] / (
  2531                                                   dummyBlock.segments[0].analogsignals[0].sampling_rate)
  2532                                               nChunks = math.ceil(segLen / chunkSize)
  2533                                               #
  2534                                               if equalChunks:
  2535                                                   actualChunkSize = (segLen / nChunks).magnitude
  2536                                               else:
  2537                                                   actualChunkSize = chunkSize
  2538                                               if chunkList is None:
  2539                                                   chunkList = range(nChunks)
  2540                                               chunkingMetadata = {}
  2541                                               for chunkIdx in chunkList:
  2542                                                   print('preproc on chunk {}'.format(chunkIdx))
  2543                                                   #  instantiate spike reader if requested
  2544                                                   if spikeSourceType == 'tdc':
  2545                                                       if spikePath is None:
  2546                                                           spikePath = os.path.join(
  2547                                                               outputFolderPath, 'tdc_' + fileName,
  2548                                                               'tdc_' + fileName + '.nix')
  2549                                                       print('loading {}'.format(spikePath))
  2550                                                       spikeReader = nixio_fr.NixIO(filename=spikePath)
  2551                                                   else:
  2552                                                       spikeReader = None
  2553                                                   #  absolute section index
  2554                                                   block = readBlockFixNames(
  2555                                                       reader,
  2556                                                       block_index=0, lazy=True,
  2557                                                       signal_group_mode=signal_group_mode,
  2558                                                       mapDF=mapDF, reduceChannelIndexes=True,
  2559                                                       # swapMaps=swapMaps
  2560                                                       )
  2561                                                   if spikeReader is not None:
  2562                                                       spikeBlock = readBlockFixNames(
  2563                                                           spikeReader, block_index=0, lazy=True,
  2564                                                           signal_group_mode=signal_group_mode,
  2565                                                           mapDF=mapDF, reduceChannelIndexes=True,
  2566                                                           # swapMaps=swapMaps
  2567                                                           )
  2568                                                       spikeBlock = purgeNixAnn(spikeBlock)
  2569                                                   else:
  2570                                                       spikeBlock = None
  2571                                                   #
  2572                                                   #  instantiate writer
  2573                                                   if (nChunks == 1) or (len(chunkList) == 1):
  2574                                                       partNameSuffix = ""
  2575                                                       thisChunkOutFilePath = outputFilePath
  2576                                                   else:
  2577                                                       partNameSuffix = '_pt{:0>3}'.format(chunkIdx)
  2578                                                       thisChunkOutFilePath = (
  2579                                                           outputFilePath
  2580                                                           .replace('.nix', partNameSuffix + '.nix'))
  2581                                                   #
  2582                                                   if os.path.exists(thisChunkOutFilePath):
  2583                                                       os.remove(thisChunkOutFilePath)
  2584                                                   writer = NixIO(
  2585                                                       filename=thisChunkOutFilePath, mode=writeMode)
  2586                                                   chunkTStart = chunkIdx * actualChunkSize + chunkOffset
  2587                                                   chunkTStop = (chunkIdx + 1) * actualChunkSize + chunkOffset
  2588                                                   chunkingMetadata[chunkIdx] = {
  2589                                                       'filename': thisChunkOutFilePath,
  2590                                                       'partNameSuffix': partNameSuffix,
  2591                                                       'chunkTStart': chunkTStart,
  2592                                                       'chunkTStop': chunkTStop}
  2593                                                   block.annotate(chunkTStart=chunkTStart)
  2594                                                   block.annotate(chunkTStop=chunkTStop)
  2595                                                   block.annotate(
  2596                                                       recDatetimeStr=(
  2597                                                           block
  2598                                                           .rec_datetime
  2599                                                           .replace(tzinfo=timezone.utc)
  2600                                                           .isoformat())
  2601                                                       )
  2602                                                   #
  2603                                                   preprocBlockToNix(
  2604                                                       block, writer,
  2605                                                       chunkTStart=chunkTStart,
  2606                                                       chunkTStop=chunkTStop,
  2607                                                       fillOverflow=fillOverflow,
  2608                                                       removeJumps=removeJumps,
  2609                                                       interpolateOutliers=interpolateOutliers,
  2610                                                       calcOutliers=calcOutliers,
  2611                                                       outlierThreshold=outlierThreshold,
  2612                                                       outlierMaskFilterOpts=outlierMaskFilterOpts,
  2613                                                       calcArtifactTrace=calcArtifactTrace,
  2614                                                       linearDetrend=linearDetrend,
  2615                                                       motorEncoderMask=motorEncoderMask,
  2616                                                       electrodeArrayName=electrodeArrayName,
  2617                                                       calcAverageLFP=calcAverageLFP,
  2618                                                       eventInfo=eventInfo,
  2619                                                       asigNameList=asigNameList, ainpNameList=ainpNameList,
  2620                                                       saveFromAsigNameList=saveFromAsigNameList,
  2621                                                       spikeSourceType=spikeSourceType,
  2622                                                       spikeBlock=spikeBlock,
  2623                                                       calcRigEvents=calcRigEvents,
  2624                                                       normalizeByImpedance=normalizeByImpedance,
  2625                                                       removeMeanAcross=removeMeanAcross,
  2626                                                       LFPFilterOpts=LFPFilterOpts,
  2627                                                       encoderCountPerDegree=encoderCountPerDegree,
  2628                                                       outlierRemovalDebugFlag=outlierRemovalDebugFlag,
  2629                                                       impedanceFilePath=impedanceFilePath,
  2630                                                       )
  2631                                                   #### diagnostics
  2632                                                   diagnosticFolder = os.path.join(
  2633                                                       outputFolderPath,
  2634                                                       'preprocDiagnostics',
  2635                                                       # fileName + nameSuffix + partNameSuffix
  2636                                                       )
  2637                                                   if not os.path.exists(diagnosticFolder):
  2638                                                       os.mkdir(diagnosticFolder)
  2639                                                   asigDiagnostics = {}
  2640                                                   outlierDiagnostics = {}
  2641                                                   diagnosticText = ''
  2642                                                   for asig in block.filter(objects=AnalogSignal):
  2643                                                       annNames = ['mean_removal_r2', 'mean_removal_group']
  2644                                                       for annName in annNames:
  2645                                                           if annName in asig.annotations:
  2646                                                               if asig.name not in asigDiagnostics:
  2647                                                                   asigDiagnostics[asig.name] = {}
  2648                                                               asigDiagnostics[asig.name].update({
  2649                                                                   annName: asig.annotations[annName]})
  2650                                                       annNames = [
  2651                                                           'outlierProportion', 'nDim',
  2652                                                           'noveltyThreshold', 'outlierThreshold'
  2653                                                           ]
  2654                                                       for annName in annNames:
  2655                                                           if annName in asig.annotations:
  2656                                                               if asig.name not in outlierDiagnostics:
  2657                                                                   outlierDiagnostics[asig.name] = {}
  2658                                                               outlierDiagnostics[asig.name].update({
  2659                                                                   annName: '{}'.format(asig.annotations[annName])
  2660                                                               })
  2661                                                   if removeMeanAcross:
  2662                                                       asigDiagnosticsDF = pd.DataFrame(asigDiagnostics).T
  2663                                                       asigDiagnosticsDF.sort_values(by='mean_removal_r2', inplace=True)
  2664                                                       diagnosticText += '<h2>LFP Diagnostics</h2>\n'
  2665                                                       diagnosticText += asigDiagnosticsDF.to_html()
  2666                                                       fig, ax = plt.subplots()
  2667                                                       sns.distplot(asigDiagnosticsDF['mean_removal_r2'], ax=ax)
  2668                                                       ax.set_ylabel('Count of analog signals')
  2669                                                       ax.set_xlabel('R^2 of regressing mean against signal')
  2670                                                       fig.savefig(os.path.join(
  2671                                                               diagnosticFolder,
  2672                                                               fileName + nameSuffix + partNameSuffix + '_meanRemovalR2.png'
  2673                                                           ))
  2674                                                   if interpolateOutliers:
  2675                                                       outlierDiagnosticsDF = pd.DataFrame(outlierDiagnostics).T
  2676                                                       diagnosticText += '<h2>Outlier Diagnostics</h2>\n'
  2677                                                       diagnosticText += outlierDiagnosticsDF.to_html()
  2678                                                   diagnosticTextPath = os.path.join(
  2679                                                       diagnosticFolder,
  2680                                                       fileName + nameSuffix + partNameSuffix + '_asigDiagnostics.html'
  2681                                                       )
  2682                                                   with open(diagnosticTextPath, 'w') as _f:
  2683                                                       _f.write(diagnosticText)
  2684                                                   writer.close()
  2685                                               chunkingInfoPath = os.path.join(
  2686                                                   outputFolderPath,
  2687                                                   fileName + nameSuffix +
  2688                                                   '_chunkingInfo.json'
  2689                                                   )
  2690                                               if os.path.exists(chunkingInfoPath):
  2691                                                   os.remove(chunkingInfoPath)
  2692                                               with open(chunkingInfoPath, 'w') as f:
  2693                                                   json.dump(chunkingMetadata, f)
  2694                                               return

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: preprocBlockToNix at line 2696

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2696                                           @profile
  2697                                           def preprocBlockToNix(
  2698                                                   block, writer,
  2699                                                   chunkTStart=None,
  2700                                                   chunkTStop=None,
  2701                                                   eventInfo=None,
  2702                                                   fillOverflow=False, calcAverageLFP=False,
  2703                                                   interpolateOutliers=False, calcOutliers=False,
  2704                                                   calcArtifactTrace=False,
  2705                                                   outlierMaskFilterOpts=None,
  2706                                                   useMeanToCenter=False,   # mean center? median center?
  2707                                                   linearDetrend=False,
  2708                                                   zScoreEachTrace=False,
  2709                                                   outlierThreshold=1,
  2710                                                   motorEncoderMask=None,
  2711                                                   electrodeArrayName='utah',
  2712                                                   removeJumps=False, trackMemory=True,
  2713                                                   asigNameList=None, ainpNameList=None,
  2714                                                   saveFromAsigNameList=True,
  2715                                                   spikeSourceType='', spikeBlock=None,
  2716                                                   calcRigEvents=True,
  2717                                                   normalizeByImpedance=True,
  2718                                                   impedanceFilePath=None,
  2719                                                   removeMeanAcross=False,
  2720                                                   LFPFilterOpts=None, encoderCountPerDegree=180e2,
  2721                                                   outlierRemovalDebugFlag=False,
  2722                                                   ):
  2723                                               #  prune out nev spike placeholders
  2724                                               #  (will get added back on a chunk by chunk basis,
  2725                                               #  if not pruning units)
  2726                                               if spikeSourceType == 'nev':
  2727                                                   pruneOutUnits = False
  2728                                               else:
  2729                                                   pruneOutUnits = True
  2730                                               #
  2731                                               for chanIdx in block.channel_indexes:
  2732                                                   if chanIdx.units:
  2733                                                       for unit in chanIdx.units:
  2734                                                           if unit.spiketrains:
  2735                                                               unit.spiketrains = []
  2736                                                       if pruneOutUnits:
  2737                                                           chanIdx.units = []
  2738                                               #
  2739                                               if spikeBlock is not None:
  2740                                                   for chanIdx in spikeBlock.channel_indexes:
  2741                                                       if chanIdx.units:
  2742                                                           for unit in chanIdx.units:
  2743                                                               if unit.spiketrains:
  2744                                                                   unit.spiketrains = []
  2745                                               #  precalculate new segment
  2746                                               seg = block.segments[0]
  2747                                               #  remove chanIndexes assigned to units; makes more sense to
  2748                                               #  only use chanIdx for asigs and spikes on that asig
  2749                                               #  block.channel_indexes = (
  2750                                               #      [chanIdx for chanIdx in block.channel_indexes if (
  2751                                               #          chanIdx.analogsignals)])
  2752                                               if calcAverageLFP:
  2753                                                   lastIndex = len(block.channel_indexes)
  2754                                                   lastID = block.channel_indexes[-1].channel_ids[0] + 1
  2755                                                   if asigNameList is None:
  2756                                                       asigNameList = [
  2757                                                           [
  2758                                                               childBaseName(a.name, 'seg')
  2759                                                               for a in seg.analogsignals
  2760                                                               if not (('ainp' in a.name) or ('analog' in a.name))]
  2761                                                           ]
  2762                                                   nMeanChans = len(asigNameList)
  2763                                                   #
  2764                                                   meanChIdxList = []
  2765                                                   for meanChIdx in range(nMeanChans):
  2766                                                       tempChIdx = ChannelIndex(
  2767                                                           index=[lastIndex + meanChIdx],
  2768                                                           channel_names=['{}_rawAverage_{}'.format(electrodeArrayName, meanChIdx)],
  2769                                                           channel_ids=[lastID + meanChIdx],
  2770                                                           name='{}_rawAverage_{}'.format(electrodeArrayName, meanChIdx),
  2771                                                           file_origin=block.channel_indexes[-1].file_origin
  2772                                                           )
  2773                                                       tempChIdx.merge_annotations(block.channel_indexes[-1])
  2774                                                       block.channel_indexes.append(tempChIdx)
  2775                                                       meanChIdxList.append(tempChIdx)
  2776                                                       lastIndex += 1
  2777                                                       lastID += 1
  2778                                                   lastIndex = len(block.channel_indexes)
  2779                                                   lastID = block.channel_indexes[-1].channel_ids[0] + 1
  2780                                                   # if calcArtifactTrace:
  2781                                                   if True:
  2782                                                       artChIdxList = []
  2783                                                       for artChIdx in range(nMeanChans):
  2784                                                           tempChIdx = ChannelIndex(
  2785                                                               index=[lastIndex + artChIdx],
  2786                                                               channel_names=['{}_artifact_{}'.format(electrodeArrayName, artChIdx)],
  2787                                                               channel_ids=[lastID + artChIdx],
  2788                                                               name='{}_artifact_{}'.format(electrodeArrayName, artChIdx),
  2789                                                               file_origin=block.channel_indexes[-1].file_origin
  2790                                                               )
  2791                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2792                                                           block.channel_indexes.append(tempChIdx)
  2793                                                           artChIdxList.append(tempChIdx)
  2794                                                           lastIndex += 1
  2795                                                           lastID += 1
  2796                                                   # if calcOutliers:
  2797                                                   if True:
  2798                                                       devChIdxList = []
  2799                                                       for devChIdx in range(nMeanChans):
  2800                                                           tempChIdx = ChannelIndex(
  2801                                                               index=[lastIndex + devChIdx],
  2802                                                               channel_names=['{}_deviation_{}'.format(electrodeArrayName, devChIdx)],
  2803                                                               channel_ids=[lastID + devChIdx],
  2804                                                               name='{}_deviation_{}'.format(electrodeArrayName, devChIdx),
  2805                                                               file_origin=block.channel_indexes[-1].file_origin
  2806                                                               )
  2807                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2808                                                           block.channel_indexes.append(tempChIdx)
  2809                                                           devChIdxList.append(tempChIdx)
  2810                                                           lastIndex += 1
  2811                                                           lastID += 1
  2812                                                       smDevChIdxList = []
  2813                                                       for devChIdx in range(nMeanChans):
  2814                                                           tempChIdx = ChannelIndex(
  2815                                                               index=[lastIndex + devChIdx],
  2816                                                               channel_names=['{}_smoothed_deviation_{}'.format(electrodeArrayName, devChIdx)],
  2817                                                               channel_ids=[lastID + devChIdx],
  2818                                                               name='{}_smoothed_deviation_{}'.format(electrodeArrayName, devChIdx),
  2819                                                               file_origin=block.channel_indexes[-1].file_origin
  2820                                                               )
  2821                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2822                                                           block.channel_indexes.append(tempChIdx)
  2823                                                           smDevChIdxList.append(tempChIdx)
  2824                                                           lastIndex += 1
  2825                                                           lastID += 1
  2826                                                       outMaskChIdxList = []
  2827                                                       for outMaskChIdx in range(nMeanChans):
  2828                                                           tempChIdx = ChannelIndex(
  2829                                                               index=[lastIndex + outMaskChIdx],
  2830                                                               channel_names=['{}_outlierMask_{}'.format(
  2831                                                                   electrodeArrayName, outMaskChIdx)],
  2832                                                               channel_ids=[lastID + outMaskChIdx],
  2833                                                               name='{}_outlierMask_{}'.format(
  2834                                                                   electrodeArrayName, outMaskChIdx),
  2835                                                               file_origin=block.channel_indexes[-1].file_origin
  2836                                                               )
  2837                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2838                                                           block.channel_indexes.append(tempChIdx)
  2839                                                           outMaskChIdxList.append(tempChIdx)
  2840                                                           lastIndex += 1
  2841                                                           lastID += 1
  2842                                               #  delete asig and irsig proxies from channel index list
  2843                                               for metaIdx, chanIdx in enumerate(block.channel_indexes):
  2844                                                   if chanIdx.analogsignals:
  2845                                                       chanIdx.analogsignals = []
  2846                                                   if chanIdx.irregularlysampledsignals:
  2847                                                       chanIdx.irregularlysampledsignals = []
  2848                                               newSeg = Segment(
  2849                                                       index=0, name=seg.name,
  2850                                                       description=seg.description,
  2851                                                       file_origin=seg.file_origin,
  2852                                                       file_datetime=seg.file_datetime,
  2853                                                       rec_datetime=seg.rec_datetime,
  2854                                                       **seg.annotations
  2855                                                   )
  2856                                               block.segments = [newSeg]
  2857                                               block, nixblock = writer.write_block_meta(block)
  2858                                               # descend into Segments
  2859                                               if impedanceFilePath is not None:
  2860                                                   try:
  2861                                                       impedances = prb_meta.getLatestImpedance(
  2862                                                           block=block, impedanceFilePath=impedanceFilePath)
  2863                                                       averageImpedance = impedances['impedance'].median()
  2864                                                   except Exception:
  2865                                                       traceback.print_exc()
  2866                                               # for segIdx, seg in enumerate(oldSegList):
  2867                                               if spikeBlock is not None:
  2868                                                   spikeSeg = spikeBlock.segments[0]
  2869                                               else:
  2870                                                   spikeSeg = seg
  2871                                               #
  2872                                               if trackMemory:
  2873                                                   print('memory usage: {:.1f} MB'.format(
  2874                                                       prf.memory_usage_psutil()))
  2875                                               newSeg, nixgroup = writer._write_segment_meta(newSeg, nixblock)
  2876                                               #  trim down list of analog signals if necessary
  2877                                               asigNameListSeg = []
  2878                                               if (removeMeanAcross or calcAverageLFP):
  2879                                                   meanGroups = {}
  2880                                               for subListIdx, subList in enumerate(asigNameList):
  2881                                                   subListSeg = [
  2882                                                       'seg{}_{}'.format(0, a)
  2883                                                       for a in subList]
  2884                                                   asigNameListSeg += subListSeg
  2885                                                   if (removeMeanAcross or calcAverageLFP):
  2886                                                       meanGroups[subListIdx] = subListSeg
  2887                                               aSigList = []
  2888                                               # [asig.name for asig in seg.analogsignals]
  2889                                               for a in seg.analogsignals:
  2890                                                   # if np.any([n in a.name for n in asigNameListSeg]):
  2891                                                   if a.name in asigNameListSeg:
  2892                                                       aSigList.append(a)
  2893                                               if ainpNameList is not None:
  2894                                                   ainpNameListSeg = [
  2895                                                       'seg{}_{}'.format(0, a)
  2896                                                       for a in ainpNameList]
  2897                                                   ainpList = []
  2898                                                   for a in seg.analogsignals:
  2899                                                       if np.any([n == a.name for n in ainpNameListSeg]):
  2900                                                           ainpList.append(a)
  2901                                               else:
  2902                                                   ainpList = [
  2903                                                       a
  2904                                                       for a in seg.analogsignals
  2905                                                       if (('ainp' in a.name) or ('analog' in a.name))]
  2906                                                   ainpNameListSeg = [a.name for a in aSigList]
  2907                                               nAsigs = len(aSigList)
  2908                                               if LFPFilterOpts is not None:
  2909                                                   def filterFun(sig, filterCoeffs=None):
  2910                                                       # sig[:] = signal.sosfiltfilt(
  2911                                                       sig[:] = signal.sosfilt(
  2912                                                           filterCoeffs, sig.magnitude.flatten())[:, np.newaxis] * sig.units
  2913                                                       return sig
  2914                                                   filterCoeffs = hf.makeFilterCoeffsSOS(
  2915                                                       LFPFilterOpts, float(seg.analogsignals[0].sampling_rate))
  2916                                                   if False:
  2917                                                       fig, ax1, ax2 = hf.plotFilterResponse(
  2918                                                           filterCoeffs,
  2919                                                           float(seg.analogsignals[0].sampling_rate))
  2920                                                       fig2, ax3, ax4 = hf.plotFilterImpulseResponse(
  2921                                                           LFPFilterOpts,
  2922                                                           float(seg.analogsignals[0].sampling_rate))
  2923                                                       plt.show()
  2924                                               # first pass through asigs, if removing mean across channels
  2925                                               if (removeMeanAcross or calcAverageLFP):
  2926                                                   for aSigIdx, aSigProxy in enumerate(seg.analogsignals):
  2927                                                       if aSigIdx == 0:
  2928                                                           # check bounds
  2929                                                           tStart = max(chunkTStart * pq.s, aSigProxy.t_start)
  2930                                                           tStop = min(chunkTStop * pq.s, aSigProxy.t_stop)
  2931                                                       loadThisOne = (aSigProxy in aSigList)
  2932                                                       if loadThisOne:
  2933                                                           if trackMemory:
  2934                                                               print(
  2935                                                                   'Extracting asig for mean, memory usage: {:.1f} MB'.format(
  2936                                                                       prf.memory_usage_psutil()))
  2937                                                           chanIdx = aSigProxy.channel_index
  2938                                                           asig = aSigProxy.load(
  2939                                                               time_slice=(tStart, tStop),
  2940                                                               magnitude_mode='rescaled')
  2941                                                           if 'tempLFPStore' not in locals():
  2942                                                               tempLFPStore = pd.DataFrame(
  2943                                                                   np.zeros(
  2944                                                                       (asig.shape[0], nAsigs),
  2945                                                                       dtype=np.float32),
  2946                                                                   columns=asigNameListSeg)
  2947                                                           if 'dummyAsig' not in locals():
  2948                                                               dummyAsig = asig.copy()
  2949                                                           #  perform requested preproc operations
  2950                                                           #  if LFPFilterOpts is not None:
  2951                                                           #      asig[:] = filterFun(
  2952                                                           #          asig, filterCoeffs=filterCoeffs)
  2953                                                           if normalizeByImpedance:
  2954                                                               elNmMatchMsk = impedances['elec'] == chanIdx.name
  2955                                                               '''
  2956                                                               asig.magnitude[:] = (
  2957                                                                   (asig.magnitude - np.median(asig.magnitude)) /
  2958                                                                   np.min(
  2959                                                                       impedances.loc[elNmMatchMsk, 'impedance']
  2960                                                                       ))
  2961                                                               '''
  2962                                                               asig.magnitude[:] = (
  2963                                                                   (asig.magnitude) * averageImpedance /
  2964                                                                   np.min(
  2965                                                                       impedances.loc[elNmMatchMsk, 'impedance']
  2966                                                                       ))
  2967                                                           # if fillOverflow:
  2968                                                           #     # fill in overflow:
  2969                                                           #     '''
  2970                                                           #     timeSection['data'], overflowMask = hf.fillInOverflow(
  2971                                                           #         timeSection['data'], fillMethod = 'average')
  2972                                                           #     badData.update({'overflow': overflowMask})
  2973                                                           #     '''
  2974                                                           #     pass
  2975                                                           # if removeJumps:
  2976                                                           #     # find unusual jumps in derivative or amplitude
  2977                                                           #     '''
  2978                                                           #     timeSection['data'], newBadData = hf.fillInJumps(timeSection['data'],
  2979                                                           #     timeSection['samp_per_s'], smoothing_ms = 0.5, nStdDiff = 50,
  2980                                                           #     nStdAmp = 100)
  2981                                                           #     badData.update(newBadData)
  2982                                                           #     '''
  2983                                                           #     pass
  2984                                                           tempLFPStore.loc[:, aSigProxy.name] = asig.magnitude.flatten()
  2985                                                           del asig
  2986                                                           gc.collect()
  2987                                                   # end of first pass
  2988                                                   if (removeMeanAcross or calcAverageLFP):
  2989                                                       centerLFP = np.zeros(
  2990                                                           (tempLFPStore.shape[0], len(asigNameList)),
  2991                                                           dtype=np.float32)
  2992                                                       spreadLFP = np.zeros(
  2993                                                           (tempLFPStore.shape[0], len(asigNameList)),
  2994                                                           dtype=np.float32)
  2995                                                       # if calcOutliers:
  2996                                                       if True:
  2997                                                           if outlierMaskFilterOpts is not None:
  2998                                                               filterCoeffsOutlierMask = hf.makeFilterCoeffsSOS(
  2999                                                                   outlierMaskFilterOpts, float(dummyAsig.sampling_rate))
  3000                                                           lfpDeviation = np.zeros(
  3001                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3002                                                               dtype=np.float32)
  3003                                                           smoothedDeviation = np.zeros(
  3004                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3005                                                               dtype=np.float32)
  3006                                                           outlierMask = np.zeros(
  3007                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3008                                                               dtype=np.bool)
  3009                                                           outlierMetadata = {}
  3010                                                       # if calcArtifactTrace:
  3011                                                       if True:
  3012                                                           artifactSignal = np.zeros(
  3013                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3014                                                               dtype=np.float32)
  3015                                                       ###############
  3016                                                       # tempLFPStore.iloc[:, 0] = np.nan  # for debugging axes
  3017                                                       #############
  3018                                                       plotDevFilterDebug = False
  3019                                                       if plotDevFilterDebug:
  3020                                                           try:
  3021                                                               devFiltDebugMask = (dummyAsig.times > 90 * pq.s) & (dummyAsig.times < 92 * pq.s)
  3022                                                           except Exception:
  3023                                                               pdb.set_trace()
  3024                                                           plotColIdx = 1
  3025                                                           ddfFig, ddfAx = plt.subplots(len(asigNameList), 1)
  3026                                                           ddfFig2, ddfAx2 = plt.subplots()
  3027                                                           ddfFig3, ddfAx3 = plt.subplots(
  3028                                                               1, len(asigNameList),
  3029                                                               sharey=True)
  3030                                                           if len(asigNameList) == 1:
  3031                                                               ddfAx = np.asarray([ddfAx])
  3032                                                               ddfAx3 = np.asarray([ddfAx3])
  3033                                                       for subListIdx, subList in enumerate(asigNameList):
  3034                                                           columnsForThisGroup = meanGroups[subListIdx]
  3035                                                           if trackMemory:
  3036                                                               print(
  3037                                                                   'asig group {}: calculating mean, memory usage: {:.1f} MB'.format(
  3038                                                                       subListIdx, prf.memory_usage_psutil()))
  3039                                                               print('this group contains\n{}'.format(columnsForThisGroup))
  3040                                                           if plotDevFilterDebug:
  3041                                                               ddfAx3[subListIdx].plot(
  3042                                                                   dummyAsig.times[devFiltDebugMask],
  3043                                                                   tempLFPStore.loc[:, columnsForThisGroup].iloc[devFiltDebugMask, plotColIdx],
  3044                                                                   label='original ch'
  3045                                                                   )
  3046                                                           if fillOverflow:
  3047                                                               print('Filling overflow...')
  3048                                                               # fill in overflow:
  3049                                                               tempLFPStore.loc[:, columnsForThisGroup], pltHandles = hf.fillInOverflow2(
  3050                                                                   tempLFPStore.loc[:, columnsForThisGroup].to_numpy(),
  3051                                                                   overFlowFillType='average',
  3052                                                                   overFlowThreshold=8000,
  3053                                                                   debuggingPlots=plotDevFilterDebug
  3054                                                                   )
  3055                                                               if plotDevFilterDebug:
  3056                                                                   pltHandles['ax'].set_title('ch grp {}'.format(subListIdx))
  3057                                                                   ddfAx3[subListIdx].plot(
  3058                                                                       dummyAsig.times[devFiltDebugMask],
  3059                                                                       tempLFPStore.loc[:, columnsForThisGroup].iloc[devFiltDebugMask, plotColIdx],
  3060                                                                       label='filled ch'
  3061                                                                       )
  3062                                                           # zscore of each trace
  3063                                                           if zScoreEachTrace:
  3064                                                               print('About to calculate zscore of each trace (along columns) for prelim outlier detection')
  3065                                                               columnZScore = pd.DataFrame(
  3066                                                                   stats.zscore(
  3067                                                                       tempLFPStore.loc[:, columnsForThisGroup],
  3068                                                                       axis=1),
  3069                                                                   index=tempLFPStore.index,
  3070                                                                   columns=columnsForThisGroup
  3071                                                                   )
  3072                                                               excludeFromMeanMask = columnZScore.abs() > 6
  3073                                                               if useMeanToCenter:
  3074                                                                   centerLFP[:, subListIdx] = (
  3075                                                                       tempLFPStore
  3076                                                                       .loc[:, columnsForThisGroup]
  3077                                                                       .mask(excludeFromMeanMask)
  3078                                                                       .mean(axis=1).to_numpy()
  3079                                                                       )
  3080                                                               else:
  3081                                                                   centerLFP[:, subListIdx] = (
  3082                                                                       tempLFPStore
  3083                                                                       .loc[:, columnsForThisGroup]
  3084                                                                       .mask(excludeFromMeanMask)
  3085                                                                       .median(axis=1).to_numpy()
  3086                                                                       )
  3087                                                           else:
  3088                                                               if useMeanToCenter:
  3089                                                                   centerLFP[:, subListIdx] = (
  3090                                                                       tempLFPStore
  3091                                                                       .loc[:, columnsForThisGroup]
  3092                                                                       .mean(axis=1).to_numpy()
  3093                                                                       )
  3094                                                               else:
  3095                                                                   centerLFP[:, subListIdx] = (
  3096                                                                       tempLFPStore
  3097                                                                       .loc[:, columnsForThisGroup]
  3098                                                                       .median(axis=1).to_numpy()
  3099                                                                       )
  3100                                                           if calcArtifactTrace:
  3101                                                               if LFPFilterOpts is not None:
  3102                                                                   print('applying LFPFilterOpts to cached asigs for artifact ID')
  3103                                                                   # tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfilt(
  3104                                                                   tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfiltfilt(
  3105                                                                       filterCoeffs, tempLFPStore.loc[:, columnsForThisGroup],
  3106                                                                       axis=0)
  3107                                                                   if useMeanToCenter:
  3108                                                                       tempCenter = (
  3109                                                                           tempLFPStore
  3110                                                                           .loc[:, columnsForThisGroup]
  3111                                                                           .mean(axis=1).diff().fillna(0)
  3112                                                                           )
  3113                                                                   else:
  3114                                                                       tempCenter = (
  3115                                                                           tempLFPStore
  3116                                                                           .loc[:, columnsForThisGroup]
  3117                                                                           .median(axis=1).diff().fillna(0)
  3118                                                                           )
  3119                                                               artifactSignal[:, subListIdx] = np.abs(stats.zscore(tempCenter.to_numpy()))
  3120                                                           if calcOutliers:
  3121                                                               if plotDevFilterDebug:
  3122                                                                   ddfAx[subListIdx].plot(
  3123                                                                       dummyAsig.times[devFiltDebugMask],
  3124                                                                       centerLFP[devFiltDebugMask, subListIdx],
  3125                                                                       label='mean of ch group'
  3126                                                                       )
  3127                                                               # filter the traces, if needed
  3128                                                               if LFPFilterOpts is not None:
  3129                                                                   print('applying LFPFilterOpts to cached asigs before outlier detection')
  3130                                                                   # tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfiltfilt(
  3131                                                                   tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfilt(
  3132                                                                       filterCoeffs, tempLFPStore.loc[:, columnsForThisGroup],
  3133                                                                       axis=0)
  3134                                                                   if plotDevFilterDebug:
  3135                                                                       ddfAx3[subListIdx].plot(
  3136                                                                           dummyAsig.times[devFiltDebugMask],
  3137                                                                           tempLFPStore.loc[:, columnsForThisGroup].iloc[devFiltDebugMask, plotColIdx],
  3138                                                                           label='filtered ch'
  3139                                                                           )
  3140                                                               ##################################
  3141                                                               print('Whitening cached traces before outlier detection')
  3142                                                               whitenByPCA = True
  3143                                                               if whitenByPCA:
  3144                                                                   projector = PCA(
  3145                                                                       n_components=None, whiten=True)
  3146                                                                   pcs = projector.fit_transform(
  3147                                                                       tempLFPStore.loc[:, columnsForThisGroup])
  3148                                                                   explVarMask = (
  3149                                                                       np.cumsum(projector.explained_variance_ratio_) < 1 - 1e-2)
  3150                                                                   explVarMask[0] = True  # (keep at least 1)
  3151                                                                   pcs = pcs[:, explVarMask]
  3152                                                                   nDim = pcs.shape[1]
  3153                                                                   lfpDeviation[:, subListIdx] = (pcs ** 2).sum(axis=1)
  3154                                                               else:  # whiten by mahalanobis distance
  3155                                                                   est = EmpiricalCovariance()
  3156                                                                   est.fit(tempLFPStore.loc[:, columnsForThisGroup].to_numpy())
  3157                                                                   lfpDeviation[:, subListIdx] = est.mahalanobis(
  3158                                                                       tempLFPStore.loc[:, columnsForThisGroup].to_numpy())
  3159                                                                   nDim = tempLFPStore.loc[:, columnsForThisGroup].shape[1]
  3160                                                               #
  3161                                                               transformedDeviation = stats.norm.isf(stats.chi2.sf(lfpDeviation[:, subListIdx], nDim))
  3162                                                               infMask = np.isinf(transformedDeviation)
  3163                                                               if infMask.any():
  3164                                                                   transformedDeviation[infMask] = transformedDeviation[~infMask].max()
  3165                                                               debugProbaTrans = False
  3166                                                               if debugProbaTrans:
  3167                                                                   fig, ax = plt.subplots()
  3168                                                                   tAx = ax.twinx()
  3169                                                                   plotMask = (dummyAsig.times >= 60 * pq.s) & (dummyAsig.times < 95 * pq.s)
  3170                                                                   ax.plot(dummyAsig.times[plotMask], transformedDeviation[plotMask], c='b', label='transformed deviation')
  3171                                                                   tAx.plot(dummyAsig.times[plotMask], lfpDeviation[plotMask, subListIdx], c='r', label='original deviation')
  3172                                                                   ax.legend(loc='upper left')
  3173                                                                   tAx.legend(loc='upper right')
  3174                                                                   plt.show()
  3175                                                               lfpDeviation[:, subListIdx] = transformedDeviation
  3176                                                               noveltyThreshold = stats.norm.interval(outlierThreshold)[1]
  3177                                                               # chi2Bounds = stats.chi2.interval(outlierThreshold, nDim)
  3178                                                               # lfpDeviation[:, subListIdx] = lfpDeviation[:, subListIdx] / chi2Bounds[1]
  3179                                                               # print('nDim = {}, chi2Lim = {}'.format(nDim, chi2Bounds))
  3180                                                               # noveltyThreshold = 1
  3181                                                               #
  3182                                                               outlierMetadata[subListIdx] = {
  3183                                                                   'nDim': nDim,
  3184                                                                   'noveltyThreshold': noveltyThreshold,
  3185                                                                   'outlierThreshold': outlierThreshold
  3186                                                                   }
  3187                                                               # smoothedDeviation = signal.sosfilt(
  3188                                                               print('Smoothing deviation')
  3189                                                               tempSmDev = signal.sosfiltfilt(
  3190                                                                   filterCoeffsOutlierMask, lfpDeviation[:, subListIdx])
  3191                                                               smoothedDeviation[:, subListIdx] = tempSmDev
  3192                                                               if plotDevFilterDebug:
  3193                                                                   ddfAx[subListIdx].plot(
  3194                                                                       dummyAsig.times[devFiltDebugMask],
  3195                                                                       lfpDeviation[devFiltDebugMask, subListIdx],
  3196                                                                       label='original deviation (ch grp {})'.format(subListIdx))
  3197                                                                   ddfAx[subListIdx].plot(
  3198                                                                       dummyAsig.times[devFiltDebugMask],
  3199                                                                       smoothedDeviation[devFiltDebugMask, subListIdx],
  3200                                                                       label='filtered deviation (ch grp {})'.format(subListIdx))
  3201                                                               ##
  3202                                                               print('Calculating outlier mask')
  3203                                                               outlierMask[:, subListIdx] = (
  3204                                                                   smoothedDeviation[:, subListIdx] > noveltyThreshold)
  3205                                                               if plotDevFilterDebug:
  3206                                                                   ddfAx[subListIdx].axhline(noveltyThreshold, c='r')
  3207                                                       if plotDevFilterDebug and calcOutliers:
  3208                                                           for subListIdx, subList in enumerate(asigNameList):
  3209                                                               ddfAx[subListIdx].legend(loc='upper right')
  3210                                                               ddfAx[subListIdx].set_title('Deviation')
  3211                                                               ddfAx3[subListIdx].legend(loc='upper right')
  3212                                                               ddfAx3[subListIdx].set_title('Example channel')
  3213                                                               ddfAx2.plot(
  3214                                                                   dummyAsig.times[devFiltDebugMask],
  3215                                                                   smoothedDeviation[devFiltDebugMask, subListIdx],
  3216                                                                   label='ch grp {}'.format(subListIdx))
  3217                                                               ddfAx2.set_title('Smoothed Deviation')
  3218                                                           ddfAx2.legend(loc='upper right')
  3219                                                           plt.show()
  3220                                                       #############
  3221                                                       del tempLFPStore
  3222                                                       gc.collect()
  3223                                               if (removeMeanAcross or calcAverageLFP):
  3224                                                   for mIdx, meanChIdx in enumerate(meanChIdxList):
  3225                                                       meanAsig = AnalogSignal(
  3226                                                           centerLFP[:, mIdx],
  3227                                                           units=dummyAsig.units,
  3228                                                           sampling_rate=dummyAsig.sampling_rate,
  3229                                                           # name='seg{}_{}'.format(idx, meanChIdx.name)
  3230                                                           name='seg{}_{}'.format(0, meanChIdx.name),
  3231                                                           t_start=tStart
  3232                                                       )
  3233                                                       # assign ownership to containers
  3234                                                       meanChIdx.analogsignals.append(meanAsig)
  3235                                                       newSeg.analogsignals.append(meanAsig)
  3236                                                       # assign parent to children
  3237                                                       meanChIdx.create_relationship()
  3238                                                       newSeg.create_relationship()
  3239                                                       # write out to file
  3240                                                       if LFPFilterOpts is not None:
  3241                                                           meanAsig[:] = filterFun(
  3242                                                               meanAsig, filterCoeffs=filterCoeffs)
  3243                                                       meanAsig = writer._write_analogsignal(
  3244                                                           meanAsig, nixblock, nixgroup)
  3245                                                   # if calcArtifactTrace:
  3246                                                   if True:
  3247                                                       for mIdx, artChIdx in enumerate(artChIdxList):
  3248                                                           artAsig = AnalogSignal(
  3249                                                               artifactSignal[:, mIdx],
  3250                                                               units=dummyAsig.units,
  3251                                                               sampling_rate=dummyAsig.sampling_rate,
  3252                                                               # name='seg{}_{}'.format(idx, devChIdx.name)
  3253                                                               name='seg{}_{}'.format(0, artChIdx.name),
  3254                                                               t_start=tStart
  3255                                                               )
  3256                                                           # assign ownership to containers
  3257                                                           artChIdx.analogsignals.append(artAsig)
  3258                                                           newSeg.analogsignals.append(artAsig)
  3259                                                           # assign parent to children
  3260                                                           artChIdx.create_relationship()
  3261                                                           newSeg.create_relationship()
  3262                                                           # write out to file
  3263                                                           artAsig = writer._write_analogsignal(
  3264                                                               artAsig, nixblock, nixgroup)
  3265                                                           #########################################################
  3266                                                   # if calcOutliers:
  3267                                                   if True:
  3268                                                       for mIdx, devChIdx in enumerate(devChIdxList):
  3269                                                           devAsig = AnalogSignal(
  3270                                                               lfpDeviation[:, mIdx],
  3271                                                               units=dummyAsig.units,
  3272                                                               sampling_rate=dummyAsig.sampling_rate,
  3273                                                               # name='seg{}_{}'.format(idx, devChIdx.name)
  3274                                                               name='seg{}_{}'.format(0, devChIdx.name),
  3275                                                               t_start=tStart
  3276                                                               )
  3277                                                           # assign ownership to containers
  3278                                                           devChIdx.analogsignals.append(devAsig)
  3279                                                           newSeg.analogsignals.append(devAsig)
  3280                                                           # assign parent to children
  3281                                                           devChIdx.create_relationship()
  3282                                                           newSeg.create_relationship()
  3283                                                           # write out to file
  3284                                                           devAsig = writer._write_analogsignal(
  3285                                                               devAsig, nixblock, nixgroup)
  3286                                                           #########################################################
  3287                                                       for mIdx, smDevChIdx in enumerate(smDevChIdxList):
  3288                                                           smDevAsig = AnalogSignal(
  3289                                                               smoothedDeviation[:, mIdx],
  3290                                                               units=dummyAsig.units,
  3291                                                               sampling_rate=dummyAsig.sampling_rate,
  3292                                                               # name='seg{}_{}'.format(idx, devChIdx.name)
  3293                                                               name='seg{}_{}'.format(0, smDevChIdx.name),
  3294                                                               t_start=tStart
  3295                                                               )
  3296                                                           # assign ownership to containers
  3297                                                           smDevChIdx.analogsignals.append(smDevAsig)
  3298                                                           newSeg.analogsignals.append(smDevAsig)
  3299                                                           # assign parent to children
  3300                                                           smDevChIdx.create_relationship()
  3301                                                           newSeg.create_relationship()
  3302                                                           # write out to file
  3303                                                           smDevAsig = writer._write_analogsignal(
  3304                                                               smDevAsig, nixblock, nixgroup)
  3305                                                           #########################################################
  3306                                                       for mIdx, outMaskChIdx in enumerate(outMaskChIdxList):
  3307                                                           outMaskAsig = AnalogSignal(
  3308                                                               outlierMask[:, mIdx],
  3309                                                               units=dummyAsig.units,
  3310                                                               sampling_rate=dummyAsig.sampling_rate,
  3311                                                               # name='seg{}_{}'.format(idx, outMaskChIdx.name)
  3312                                                               name='seg{}_{}'.format(0, outMaskChIdx.name),
  3313                                                               t_start=tStart, dtype=np.float32
  3314                                                               )
  3315                                                           outMaskAsig.annotations['outlierProportion'] = np.mean(outlierMask[:, mIdx])
  3316                                                           if calcOutliers:
  3317                                                               outMaskAsig.annotations.update(outlierMetadata[mIdx])
  3318                                                           # assign ownership to containers
  3319                                                           outMaskChIdx.analogsignals.append(outMaskAsig)
  3320                                                           newSeg.analogsignals.append(outMaskAsig)
  3321                                                           # assign parent to children
  3322                                                           outMaskChIdx.create_relationship()
  3323                                                           newSeg.create_relationship()
  3324                                                           # write out to file
  3325                                                           outMaskAsig = writer._write_analogsignal(
  3326                                                               outMaskAsig, nixblock, nixgroup)
  3327                                                   #
  3328                                                   w0 = 60
  3329                                                   bandQ = 20
  3330                                                   bw = w0/bandQ
  3331                                                   noiseSos = signal.iirfilter(
  3332                                                       N=8, Wn=[w0 - bw/2, w0 + bw/2],
  3333                                                       btype='band', ftype='butter',
  3334                                                       analog=False, fs=float(dummyAsig.sampling_rate),
  3335                                                       output='sos')
  3336                                                   # signal.hilbert does not have an option to zero pad
  3337                                                   nextLen = fftpack.helper.next_fast_len(dummyAsig.shape[0])
  3338                                                   deficit = int(nextLen - dummyAsig.shape[0])
  3339                                                   lDef = int(np.floor(deficit / 2))
  3340                                                   rDef = int(np.ceil(deficit / 2)) + 1
  3341                                                   temp = np.pad(
  3342                                                       dummyAsig.magnitude.flatten(),
  3343                                                       (lDef, rDef), mode='constant')
  3344                                                   # lineNoise = signal.sosfiltfilt(
  3345                                                   lineNoise = signal.sosfilt(
  3346                                                       noiseSos, temp, axis=0)
  3347                                                   lineNoiseH = signal.hilbert(lineNoise)
  3348                                                   lineNoise = lineNoise[lDef:-rDef]
  3349                                                   lineNoiseH = lineNoiseH[lDef:-rDef]
  3350                                                   lineNoisePhase = np.angle(lineNoiseH)
  3351                                                   lineNoisePhaseDF = pd.DataFrame(
  3352                                                       lineNoisePhase,
  3353                                                       index=dummyAsig.times,
  3354                                                       columns=['phase']
  3355                                                       )
  3356                                                   plotHilbert = False
  3357                                                   if plotHilbert:
  3358                                                       lineNoiseFreq = (
  3359                                                           np.diff(np.unwrap(lineNoisePhase)) /
  3360                                                           (2.0*np.pi) * float(dummyAsig.sampling_rate))
  3361                                                       lineNoiseEnvelope = np.abs(lineNoiseH)
  3362                                                       i1 = 300000; i2 = 330000
  3363                                                       fig, ax = plt.subplots(2, 1, sharex=True)
  3364                                                       ax[0].plot(dummyAsig.times[devFiltDebugMask], dummyAsig.magnitude[devFiltDebugMask, :])
  3365                                                       ax[0].plot(dummyAsig.times[devFiltDebugMask], lineNoise[devFiltDebugMask])
  3366                                                       ax[0].plot(dummyAsig.times[devFiltDebugMask], lineNoiseEnvelope[devFiltDebugMask])
  3367                                                       axFr = ax[1].twinx()
  3368                                                       ax[1].plot(
  3369                                                           dummyAsig.times[devFiltDebugMask], lineNoisePhase[devFiltDebugMask],
  3370                                                           c='r', label='phase')
  3371                                                       ax[1].legend()
  3372                                                       axFr.plot(
  3373                                                           dummyAsig.times[devFiltDebugMask], lineNoiseFreq[devFiltDebugMask],
  3374                                                           label='freq')
  3375                                                       axFr.set_ylim([59, 61])
  3376                                                       axFr.legend()
  3377                                                       plt.show()
  3378                                               # second pass through asigs, to save
  3379                                               for aSigIdx, aSigProxy in enumerate(seg.analogsignals):
  3380                                                   if aSigIdx == 0:
  3381                                                       # check bounds
  3382                                                       tStart = max(chunkTStart * pq.s, aSigProxy.t_start)
  3383                                                       tStop = min(chunkTStop * pq.s, aSigProxy.t_stop)
  3384                                                   loadThisOne = (
  3385                                                       (saveFromAsigNameList and (aSigProxy in aSigList)) or
  3386                                                       (aSigProxy in ainpList)
  3387                                                       )
  3388                                                   if loadThisOne:
  3389                                                       if trackMemory:
  3390                                                           print('writing asig {} ({}) memory usage: {:.1f} MB'.format(
  3391                                                               aSigIdx, aSigProxy.name, prf.memory_usage_psutil()))
  3392                                                       chanIdx = aSigProxy.channel_index
  3393                                                       asig = aSigProxy.load(
  3394                                                           time_slice=(tStart, tStop),
  3395                                                           magnitude_mode='rescaled')
  3396                                                       #  link AnalogSignal and ID providing channel_index
  3397                                                       asig.channel_index = chanIdx
  3398                                                       #  perform requested preproc operations
  3399                                                       if 'impedances' in locals():
  3400                                                           elNmMatchMsk = impedances['elec'] == chanIdx.name
  3401                                                           if elNmMatchMsk.any():
  3402                                                               originalImpedance = np.min(
  3403                                                                   impedances.loc[elNmMatchMsk, 'impedance']
  3404                                                                   )
  3405                                                               asig.annotations['originalImpedance'] = originalImpedance
  3406                                                               if normalizeByImpedance and (aSigProxy not in ainpList):
  3407                                                                   '''
  3408                                                                   asig.magnitude[:] = (
  3409                                                                       (asig.magnitude - np.median(asig.magnitude)) /
  3410                                                                       np.min(
  3411                                                                           impedances.loc[elNmMatchMsk, 'impedance']
  3412                                                                           )
  3413                                                                       )
  3414                                                                   '''
  3415                                                                   print('Normalizing {} by {} kOhms'.format(asig.name, originalImpedance))
  3416                                                                   asig.magnitude[:] = (
  3417                                                                       (asig.magnitude * averageImpedance) / originalImpedance
  3418                                                                       )
  3419                                                       if fillOverflow:
  3420                                                           # fill in overflow:
  3421                                                           asig.magnitude[:], _ = hf.fillInOverflow2(
  3422                                                               asig.magnitude[:],
  3423                                                               overFlowFillType='average',
  3424                                                               overFlowThreshold=8000,
  3425                                                               debuggingPlots=False
  3426                                                               )
  3427                                                       if removeJumps:
  3428                                                           # find unusual jumps in derivative or amplitude
  3429                                                           '''
  3430                                                           timeSection['data'], newBadData = hf.fillInJumps(timeSection['data'],
  3431                                                           timeSection['samp_per_s'], smoothing_ms = 0.5, nStdDiff = 50,
  3432                                                           nStdAmp = 100)
  3433                                                           badData.update(newBadData)
  3434                                                           '''
  3435                                                           pass
  3436                                                       if calcAverageLFP and (aSigProxy not in ainpList):
  3437                                                           for k, cols in meanGroups.items():
  3438                                                               if asig.name in cols:
  3439                                                                   whichColumnToSubtract = k
  3440                                                           noiseModel = np.polyfit(
  3441                                                               centerLFP[:, whichColumnToSubtract],
  3442                                                               asig.magnitude.flatten(), 1, full=True)
  3443                                                           rSq = 1 - noiseModel[1][0] / np.sum(asig.magnitude.flatten() ** 2)
  3444                                                           asig.annotations['mean_removal_r2'] = rSq
  3445                                                           asig.annotations['mean_removal_group'] = whichColumnToSubtract
  3446                                                           if linearDetrend:
  3447                                                               noiseTerm = np.polyval(
  3448                                                                   noiseModel[0],
  3449                                                                   centerLFP[:, whichColumnToSubtract])
  3450                                                           else:
  3451                                                               noiseTerm = centerLFP[:, whichColumnToSubtract]
  3452                                                           ###
  3453                                                           plotMeanSubtraction = False
  3454                                                           if plotMeanSubtraction:
  3455                                                               i1 = 300000; i2 = 330000
  3456                                                               fig, ax = plt.subplots(1, 1)
  3457                                                               ax.plot(asig.times[devFiltDebugMask], asig.magnitude[devFiltDebugMask, :], label='channel')
  3458                                                               ax.plot(asig.times[devFiltDebugMask], centerLFP[devFiltDebugMask, whichColumnToSubtract], label='mean')
  3459                                                               ax.plot(asig.times[devFiltDebugMask], noiseTerm[devFiltDebugMask], label='adjusted mean')
  3460                                                               ax.legend()
  3461                                                               plt.show()
  3462                                                           ###
  3463                                                           if removeMeanAcross:
  3464                                                               asig.magnitude[:] = np.atleast_2d(
  3465                                                                   asig.magnitude.flatten() - noiseTerm).transpose()
  3466                                                               # asig.magnitude[:] = (
  3467                                                               #     asig.magnitude - np.median(asig.magnitude))
  3468                                                       if (LFPFilterOpts is not None) and (aSigProxy not in ainpList):
  3469                                                           asig.magnitude[:] = filterFun(asig, filterCoeffs=filterCoeffs)
  3470                                                       if (interpolateOutliers) and (aSigProxy not in ainpList) and (not outlierRemovalDebugFlag):
  3471                                                           for k, cols in meanGroups.items():
  3472                                                               if asig.name in cols:
  3473                                                                   whichColumnToSubtract = k
  3474                                                           tempSer = pd.Series(asig.magnitude.flatten())
  3475                                                           tempSer.loc[outlierMask[:, whichColumnToSubtract]] = np.nan
  3476                                                           tempSer = (
  3477                                                               tempSer
  3478                                                               .interpolate(method='linear', limit_area='inside')
  3479                                                               .fillna(method='ffill')
  3480                                                               .fillna(method='bfill')
  3481                                                               )
  3482                                                           asig.magnitude[:, 0] = tempSer.to_numpy()
  3483                                                       # pdb.set_trace()
  3484                                                       if (aSigProxy in aSigList) or (aSigProxy in ainpList):
  3485                                                           # assign ownership to containers
  3486                                                           chanIdx.analogsignals.append(asig)
  3487                                                           newSeg.analogsignals.append(asig)
  3488                                                           # assign parent to children
  3489                                                           chanIdx.create_relationship()
  3490                                                           newSeg.create_relationship()
  3491                                                           # write out to file
  3492                                                           asig = writer._write_analogsignal(
  3493                                                               asig, nixblock, nixgroup)
  3494                                                       del asig
  3495                                                       gc.collect()
  3496                                               for irSigIdx, irSigProxy in enumerate(
  3497                                                       seg.irregularlysampledsignals):
  3498                                                   chanIdx = irSigProxy.channel_index
  3499                                                   #
  3500                                                   isig = irSigProxy.load(
  3501                                                       time_slice=(tStart, tStop),
  3502                                                       magnitude_mode='rescaled')
  3503                                                   #  link irregularlysampledSignal
  3504                                                   #  and ID providing channel_index
  3505                                                   isig.channel_index = chanIdx
  3506                                                   # assign ownership to containers
  3507                                                   chanIdx.irregularlysampledsignals.append(isig)
  3508                                                   newSeg.irregularlysampledsignals.append(isig)
  3509                                                   # assign parent to children
  3510                                                   chanIdx.create_relationship()
  3511                                                   newSeg.create_relationship()
  3512                                                   # write out to file
  3513                                                   isig = writer._write_irregularlysampledsignal(
  3514                                                       isig, nixblock, nixgroup)
  3515                                                   del isig
  3516                                                   gc.collect()
  3517                                               #
  3518                                               if len(spikeSourceType):
  3519                                                   for stIdx, stProxy in enumerate(spikeSeg.spiketrains):
  3520                                                       if trackMemory:
  3521                                                           print('writing spiketrains mem usage: {}'.format(
  3522                                                               prf.memory_usage_psutil()))
  3523                                                       unit = stProxy.unit
  3524                                                       st = loadStProxy(stProxy)
  3525                                                       #  have to manually slice tStop and tStart because
  3526                                                       #  array annotations are not saved natively in the nix file
  3527                                                       #  (we're getting them as plain annotations)
  3528                                                       timeMask = np.asarray(
  3529                                                           (st.times >= tStart) & (st.times < tStop),
  3530                                                           dtype=np.bool)
  3531                                                       try:
  3532                                                           if 'arrayAnnNames' in st.annotations:
  3533                                                               for key in st.annotations['arrayAnnNames']:
  3534                                                                   st.annotations[key] = np.asarray(
  3535                                                                       st.annotations[key])[timeMask]
  3536                                                           st = st[timeMask]
  3537                                                           st.t_start = tStart
  3538                                                           st.t_stop = tStop
  3539                                                       except Exception:
  3540                                                           traceback.print_exc()
  3541                                                       #  tdc may or may not have the same channel ids, but
  3542                                                       #  it will have consistent channel names
  3543                                                       nameParser = re.search(
  3544                                                           r'([a-zA-Z0-9]*)#(\d*)', unit.name)
  3545                                                       chanLabel = nameParser.group(1)
  3546                                                       unitId = nameParser.group(2)
  3547                                                       #
  3548                                                       chIdxName = unit.name.replace('_stim', '').split('#')[0]
  3549                                                       chanIdx = block.filter(objects=ChannelIndex, name=chIdxName)[0]
  3550                                                       # [i.name for i in block.filter(objects=ChannelIndex)]
  3551                                                       # [i.name for i in spikeBlock.filter(objects=Unit)]
  3552                                                       #  print(unit.name)
  3553                                                       if not (unit in chanIdx.units):
  3554                                                           # first time at this unit, add to its chanIdx
  3555                                                           unit.channel_index = chanIdx
  3556                                                           chanIdx.units.append(unit)
  3557                                                       #  except Exception:
  3558                                                       #      traceback.print_exc()
  3559                                                       st.name = 'seg{}_{}'.format(0, unit.name)
  3560                                                       # st.name = 'seg{}_{}'.format(idx, unit.name)
  3561                                                       #  link SpikeTrain and ID providing unit
  3562                                                       if calcAverageLFP:
  3563                                                           if 'arrayAnnNames' in st.annotations:
  3564                                                               st.annotations['arrayAnnNames'] = list(st.annotations['arrayAnnNames'])
  3565                                                           else:
  3566                                                               st.annotations['arrayAnnNames'] = []
  3567                                                           st.annotations['arrayAnnNames'].append('phase60hz')
  3568                                                           phase60hz = hf.interpolateDF(
  3569                                                               lineNoisePhaseDF,
  3570                                                               newX=st.times, columns=['phase']).to_numpy().flatten()
  3571                                                           st.annotations.update({'phase60hz': phase60hz})
  3572                                                           plotPhaseDist = False
  3573                                                           if plotPhaseDist:
  3574                                                               sns.distplot(phase60hz)
  3575                                                               plt.show()
  3576                                                       st.unit = unit
  3577                                                       # assign ownership to containers
  3578                                                       unit.spiketrains.append(st)
  3579                                                       newSeg.spiketrains.append(st)
  3580                                                       # assign parent to children
  3581                                                       unit.create_relationship()
  3582                                                       newSeg.create_relationship()
  3583                                                       # write out to file
  3584                                                       st = writer._write_spiketrain(st, nixblock, nixgroup)
  3585                                                       del st
  3586                                               #  process proprio trial related events
  3587                                               if calcRigEvents:
  3588                                                   print('Processing rig events...')
  3589                                                   analogData = []
  3590                                                   for key, value in eventInfo['inputIDs'].items():
  3591                                                       searchName = 'seg{}_'.format(0) + value
  3592                                                       ainpAsig = seg.filter(
  3593                                                           objects=AnalogSignalProxy,
  3594                                                           name=searchName)[0]
  3595                                                       ainpData = ainpAsig.load(
  3596                                                           time_slice=(tStart, tStop),
  3597                                                           magnitude_mode='rescaled')
  3598                                                       analogData.append(
  3599                                                           pd.DataFrame(ainpData.magnitude, columns=[key]))
  3600                                                       del ainpData
  3601                                                       gc.collect()
  3602                                                   motorData = pd.concat(analogData, axis=1)
  3603                                                   del analogData
  3604                                                   gc.collect()
  3605                                                   if motorEncoderMask is not None:
  3606                                                       ainpData = ainpAsig.load(
  3607                                                           time_slice=(tStart, tStop),
  3608                                                           magnitude_mode='rescaled')
  3609                                                       ainpTime = ainpData.times.magnitude
  3610                                                       meTimeMask = np.zeros_like(ainpTime, dtype=np.bool)
  3611                                                       for meTimeBounds in motorEncoderMask:
  3612                                                           meTimeMask = (
  3613                                                               meTimeMask |
  3614                                                               (
  3615                                                                   (ainpTime > meTimeBounds[0]) &
  3616                                                                   (ainpTime < meTimeBounds[1])
  3617                                                                   )
  3618                                                               )
  3619                                                       columnsToOverride = ['A-', 'A+', 'B-', 'B+', 'Z-', 'Z+']
  3620                                                       for colName in columnsToOverride:
  3621                                                           motorData.loc[~meTimeMask, colName] = motorData.loc[:, colName].quantile(q=0.05)
  3622                                                       del ainpData, ainpTime
  3623                                                       gc.collect()
  3624                                                   motorData = mea.processMotorData(
  3625                                                       motorData, ainpAsig.sampling_rate.magnitude,
  3626                                                       encoderCountPerDegree=encoderCountPerDegree
  3627                                                       )
  3628                                                   keepCols = [
  3629                                                       'position', 'velocity', 'velocityCat',
  3630                                                       'rightBut_int', 'leftBut_int',
  3631                                                       'rightLED_int', 'leftLED_int', 'simiTrigs_int']
  3632                                                   for colName in keepCols:
  3633                                                       if trackMemory:
  3634                                                           print('writing motorData memory usage: {:.1f} MB'.format(
  3635                                                               prf.memory_usage_psutil()))
  3636                                                       chanIdx = ChannelIndex(
  3637                                                           name=colName,
  3638                                                           index=np.asarray([0]),
  3639                                                           channel_names=np.asarray([0]))
  3640                                                       block.channel_indexes.append(chanIdx)
  3641                                                       motorAsig = AnalogSignal(
  3642                                                           motorData[colName].to_numpy() * pq.mV,
  3643                                                           name=colName,
  3644                                                           sampling_rate=ainpAsig.sampling_rate,
  3645                                                           dtype=np.float32)
  3646                                                       motorAsig.t_start = ainpAsig.t_start
  3647                                                       motorAsig.channel_index = chanIdx
  3648                                                       # assign ownership to containers
  3649                                                       chanIdx.analogsignals.append(motorAsig)
  3650                                                       newSeg.analogsignals.append(motorAsig)
  3651                                                       chanIdx.create_relationship()
  3652                                                       newSeg.create_relationship()
  3653                                                       # write out to file
  3654                                                       motorAsig = writer._write_analogsignal(
  3655                                                           motorAsig, nixblock, nixgroup)
  3656                                                       del motorAsig
  3657                                                       gc.collect()
  3658                                                   _, trialEvents = mea.getTrials(
  3659                                                       motorData, ainpAsig.sampling_rate.magnitude,
  3660                                                       float(tStart.magnitude), trialType=None)
  3661                                                   trialEvents.fillna(0)
  3662                                                   trialEvents.rename(
  3663                                                       columns={
  3664                                                           'Label': 'rig_property',
  3665                                                           'Details': 'rig_value'},
  3666                                                       inplace=True)
  3667                                                   del motorData
  3668                                                   gc.collect()
  3669                                                   eventList = eventDataFrameToEvents(
  3670                                                       trialEvents,
  3671                                                       idxT='Time',
  3672                                                       annCol=['rig_property', 'rig_value'])
  3673                                                   for event in eventList:
  3674                                                       if trackMemory:
  3675                                                           print(
  3676                                                               'writing motor events memory usage: {:.1f} MB'
  3677                                                               .format(prf.memory_usage_psutil()))
  3678                                                       event.segment = newSeg
  3679                                                       newSeg.events.append(event)
  3680                                                       newSeg.create_relationship()
  3681                                                       # write out to file
  3682                                                       event = writer._write_event(event, nixblock, nixgroup)
  3683                                                       del event
  3684                                                       gc.collect()
  3685                                                   del trialEvents, eventList
  3686                                               #
  3687                                               for eventProxy in seg.events:
  3688                                                   event = eventProxy.load(
  3689                                                       time_slice=(tStart, tStop))
  3690                                                   event.t_start = tStart
  3691                                                   event.t_stop = tStop
  3692                                                   event.segment = newSeg
  3693                                                   newSeg.events.append(event)
  3694                                                   newSeg.create_relationship()
  3695                                                   # write out to file
  3696                                                   event = writer._write_event(event, nixblock, nixgroup)
  3697                                                   del event
  3698                                                   gc.collect()
  3699                                               #
  3700                                               for epochProxy in seg.epochs:
  3701                                                   epoch = epochProxy.load(
  3702                                                       time_slice=(tStart, tStop))
  3703                                                   epoch.t_start = tStart
  3704                                                   epoch.t_stop = tStop
  3705                                                   epoch.segment = newSeg
  3706                                                   newSeg.events.append(epoch)
  3707                                                   newSeg.create_relationship()
  3708                                                   # write out to file
  3709                                                   epoch = writer._write_epoch(epoch, nixblock, nixgroup)
  3710                                                   del epoch
  3711                                                   gc.collect()
  3712                                               #
  3713                                               chanIdxDiscardNames = []
  3714                                               # descend into ChannelIndexes
  3715                                               for chanIdx in block.channel_indexes:
  3716                                                   if chanIdx.analogsignals or chanIdx.units:
  3717                                                       chanIdx = writer._write_channelindex(chanIdx, nixblock)
  3718                                                   else:
  3719                                                       chanIdxDiscardNames.append(chanIdx.name)
  3720                                               block.channel_indexes = [
  3721                                                   i
  3722                                                   for i in block.channel_indexes
  3723                                                   if i.name not in chanIdxDiscardNames
  3724                                                   ]
  3725                                               writer._create_source_links(block, nixblock)
  3726                                               return

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: purgeNixAnn at line 3728

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3728                                           @profile
  3729                                           def purgeNixAnn(
  3730                                                   block, annNames=['nix_name', 'neo_name']):
  3731                                               for annName in annNames:
  3732                                                   block.annotations.pop(annName, None)
  3733                                               for child in block.children_recur:
  3734                                                   if child.annotations:
  3735                                                       child.annotations = {
  3736                                                           k: v
  3737                                                           for k, v in child.annotations.items()
  3738                                                           if k not in annNames}
  3739                                               for child in block.data_children_recur:
  3740                                                   if child.annotations:
  3741                                                       child.annotations = {
  3742                                                           k: v
  3743                                                           for k, v in child.annotations.items()
  3744                                                           if k not in annNames}
  3745                                               return block

Total time: 0.51557 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadContainerArrayAnn at line 3747

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3747                                           @profile
  3748                                           def loadContainerArrayAnn(
  3749                                                   container=None, trainList=None
  3750                                                   ):
  3751         1         13.0     13.0      0.0      assert (container is not None) or (trainList is not None)
  3752                                               #
  3753         1         11.0     11.0      0.0      spikesAndEvents = []
  3754         1         10.0     10.0      0.0      returnObj = []
  3755         1          8.0      8.0      0.0      if container is not None:
  3756                                                   #  need the line below! (RD: don't remember why, consider removing)
  3757         1      14376.0  14376.0      0.3          container.create_relationship()
  3758                                                   #
  3759         1         11.0     11.0      0.0          spikesAndEvents += (
  3760         1      15590.0  15590.0      0.3              container.filter(objects=SpikeTrain) +
  3761         1      14074.0  14074.0      0.3              container.filter(objects=Event)
  3762                                                       )
  3763         1          8.0      8.0      0.0          returnObj.append(container)
  3764         1          5.0      5.0      0.0      if trainList is not None:
  3765                                                   spikesAndEvents += trainList
  3766                                                   returnObj.append(trainList)
  3767                                               #
  3768         1          6.0      6.0      0.0      if len(returnObj) == 1:
  3769         1          6.0      6.0      0.0          returnObj = returnObj[0]
  3770                                               else:
  3771                                                   returnObj = tuple(returnObj)
  3772                                               #
  3773        40        281.0      7.0      0.0      for st in spikesAndEvents:
  3774        39    5111284.0 131058.6     99.1          st = loadObjArrayAnn(st)
  3775         1         12.0     12.0      0.0      return returnObj

Total time: 0.508454 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadObjArrayAnn at line 3777

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3777                                           @profile
  3778                                           def loadObjArrayAnn(st):
  3779        39        726.0     18.6      0.0      if 'arrayAnnNames' in st.annotations.keys():
  3780        39        450.0     11.5      0.0          if isinstance(st.annotations['arrayAnnNames'], str):
  3781                                                       st.annotations['arrayAnnNames'] = [st.annotations['arrayAnnNames']]
  3782        39        295.0      7.6      0.0          elif isinstance(st.annotations['arrayAnnNames'], tuple):
  3783                                                       st.annotations['arrayAnnNames'] = [i for i in st.annotations['arrayAnnNames']]
  3784                                                   #
  3785       507      45915.0     90.6      0.9          for key in st.annotations['arrayAnnNames']:
  3786                                                       #  fromRaw, the ann come back as tuple, need to recast
  3787       468       2797.0      6.0      0.1              try:
  3788       468     229619.0    490.6      4.5                  if len(st.times) == 1:
  3789                                                               st.annotations[key] = np.atleast_1d(st.annotations[key]).flatten()
  3790       468       4415.0      9.4      0.1                  st.array_annotations.update(
  3791       468    2016146.0   4308.0     39.7                      {key: np.asarray(st.annotations[key])})
  3792       468    2782796.0   5946.1     54.7                  st.annotations[key] = np.asarray(st.annotations[key])
  3793                                                       except Exception:
  3794                                                           print('Error with {}'.format(st.name))
  3795                                                           traceback.print_exc()
  3796                                                           pdb.set_trace()
  3797        39        398.0     10.2      0.0      if hasattr(st, 'waveforms'):
  3798        39        305.0      7.8      0.0          if st.waveforms is None:
  3799                                                       st.waveforms = np.asarray([]).reshape((0, 0, 0)) * pq.mV
  3800        39        442.0     11.3      0.0          elif not len(st.waveforms):
  3801                                                       st.waveforms = np.asarray([]).reshape((0, 0, 0)) * pq.mV
  3802        39        236.0      6.1      0.0      return st

Total time: 7.70821 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadWithArrayAnn at line 3804

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3804                                           @profile
  3805                                           def loadWithArrayAnn(
  3806                                                   dataPath, fromRaw=False,
  3807                                                   mapDF=None, reduceChannelIndexes=False):
  3808         1          5.0      5.0      0.0      if fromRaw:
  3809                                                   reader = nixio_fr.NixIO(filename=dataPath)
  3810                                                   block = readBlockFixNames(
  3811                                                       reader, lazy=False,
  3812                                                       mapDF=mapDF,
  3813                                                       reduceChannelIndexes=reduceChannelIndexes)
  3814                                               else:
  3815         1     170234.0 170234.0      0.2          reader = NixIO(filename=dataPath)
  3816         1   71019161.0 71019161.0     92.1          block = reader.read_block()
  3817                                                   # [un.name for un in block.filter(objects=Unit)]
  3818                                                   # [len(un.spiketrains) for un in block.filter(objects=Unit)]
  3819                                               
  3820         1    5156564.0 5156564.0      6.7      block = loadContainerArrayAnn(container=block)
  3821                                               
  3822         1         17.0     17.0      0.0      if fromRaw:
  3823                                                   reader.file.close()
  3824                                               else:
  3825         1     736059.0 736059.0      1.0          reader.close()
  3826         1         33.0     33.0      0.0      return block

Total time: 7.70832 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: blockFromPath at line 3828

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3828                                           @profile
  3829                                           def blockFromPath(
  3830                                                   dataPath, lazy=False, mapDF=None,
  3831                                                   reduceChannelIndexes=False, loadList=None,
  3832                                                   purgeNixNames=False, chunkingInfoPath=None):
  3833         1         35.0     35.0      0.0      chunkingMetadata = None
  3834         1         11.0     11.0      0.0      if chunkingInfoPath is not None:
  3835                                                   if os.path.exists(chunkingInfoPath):
  3836                                                       with open(chunkingInfoPath, 'r') as f:
  3837                                                           chunkingMetadata = json.load(f)
  3838         1          8.0      8.0      0.0      if chunkingMetadata is None:
  3839                                                   chunkingMetadata = {
  3840         1          6.0      6.0      0.0              '0': {
  3841         1          7.0      7.0      0.0                  'filename': dataPath,
  3842         1          7.0      7.0      0.0                  'partNameSuffix': '',
  3843         1          7.0      7.0      0.0                  'chunkTStart': 0,
  3844         1         10.0     10.0      0.0                  'chunkTStop': 'NaN'
  3845                                                       }}
  3846         2         61.0     30.5      0.0      for idx, (chunkIdxStr, chunkMeta) in enumerate(chunkingMetadata.items()):   
  3847         1          8.0      8.0      0.0          thisDataPath = chunkMeta['filename']
  3848         1        418.0    418.0      0.0          assert os.path.exists(thisDataPath)
  3849         1          8.0      8.0      0.0          if idx == 0:
  3850         1          7.0      7.0      0.0              if lazy:
  3851                                                           dataReader = nixio_fr.NixIO(
  3852                                                               filename=thisDataPath)
  3853                                                           dataBlock = readBlockFixNames(
  3854                                                               dataReader, lazy=lazy, mapDF=mapDF,
  3855                                                               reduceChannelIndexes=reduceChannelIndexes,
  3856                                                               purgeNixNames=purgeNixNames, loadList=loadList)
  3857                                                       else:
  3858         1          7.0      7.0      0.0                  dataReader = None
  3859         1   77082586.0 77082586.0    100.0                  dataBlock = loadWithArrayAnn(thisDataPath)
  3860                                                   else:
  3861                                                       if lazy:
  3862                                                           dataReader2 = nixio_fr.NixIO(
  3863                                                               filename=thisDataPath)
  3864                                                           dataBlock2 = readBlockFixNames(
  3865                                                               dataReader2, lazy=lazy, mapDF=mapDF,
  3866                                                               reduceChannelIndexes=reduceChannelIndexes, loadList=loadList)
  3867                                                       else:
  3868                                                           dataReader2 = None
  3869                                                           dataBlock2 = loadWithArrayAnn(thisDataPath)
  3870                                                       maxSegIdx = len(dataBlock.segments)
  3871                                                       typesNeedRenaming = [
  3872                                                           SpikeTrainProxy, AnalogSignalProxy, EventProxy,
  3873                                                           SpikeTrain, AnalogSignal, Event]
  3874                                                       for segIdx, seg in enumerate(dataBlock2.segments):
  3875                                                           if seg.name is None:
  3876                                                               seg.name = 'seg{}_'.format(maxSegIdx + segIdx)
  3877                                                           else:
  3878                                                               if 'seg{}_'.format(maxSegIdx + segIdx) not in seg.name:
  3879                                                                   seg.name = (
  3880                                                                       'seg{}_{}'
  3881                                                                       .format(
  3882                                                                           maxSegIdx + segIdx,
  3883                                                                           childBaseName(seg.name, 'seg')))
  3884                                                           for objType in typesNeedRenaming:
  3885                                                               for child in seg.filter(objects=objType):
  3886                                                                   if 'seg{}_'.format(maxSegIdx + segIdx) not in child.name:
  3887                                                                       child.name = (
  3888                                                                           'seg{}_{}'
  3889                                                                           .format(
  3890                                                                               maxSegIdx + segIdx, childBaseName(child.name, 'seg')))
  3891                                                       dataBlock.merge(dataBlock2)
  3892         1         26.0     26.0      0.0      return dataReader, dataBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: calcBinarizedArray at line 3894

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3894                                           @profile
  3895                                           def calcBinarizedArray(
  3896                                                   dataBlock, samplingRate,
  3897                                                   binnedSpikePath=None,
  3898                                                   saveToFile=True, matchT=None):
  3899                                               #
  3900                                               spikeMatBlock = Block(name=dataBlock.name + '_binarized')
  3901                                               spikeMatBlock.merge_annotations(dataBlock)
  3902                                               #
  3903                                               allSpikeTrains = [
  3904                                                   i for i in dataBlock.filter(objects=SpikeTrain)]
  3905                                               #
  3906                                               for st in allSpikeTrains:
  3907                                                   chanList = spikeMatBlock.filter(
  3908                                                       objects=ChannelIndex, name=st.unit.name)
  3909                                                   if not len(chanList):
  3910                                                       chanIdx = ChannelIndex(name=st.unit.name, index=np.asarray([0]))
  3911                                                       #  print(chanIdx.name)
  3912                                                       spikeMatBlock.channel_indexes.append(chanIdx)
  3913                                                       thisUnit = Unit(name=st.unit.name)
  3914                                                       chanIdx.units.append(thisUnit)
  3915                                                       thisUnit.channel_index = chanIdx
  3916                                               #
  3917                                               for segIdx, seg in enumerate(dataBlock.segments):
  3918                                                   newSeg = Segment(name='seg{}_{}'.format(segIdx, spikeMatBlock.name))
  3919                                                   newSeg.merge_annotations(seg)
  3920                                                   spikeMatBlock.segments.append(newSeg)
  3921                                                   #  tStart = dataBlock.segments[0].t_start
  3922                                                   #  tStop = dataBlock.segments[0].t_stop
  3923                                                   tStart = seg.t_start
  3924                                                   tStop = seg.t_stop
  3925                                                   # make dummy binary spike train, in case ths chan didn't fire
  3926                                                   segSpikeTrains = [
  3927                                                       i for i in seg.filter(objects=SpikeTrain) if '#' in i.name]
  3928                                                   dummyBin = binarize(
  3929                                                       segSpikeTrains[0],
  3930                                                       sampling_rate=samplingRate,
  3931                                                       t_start=tStart,
  3932                                                       t_stop=tStop + samplingRate ** -1) * 0
  3933                                                   for chanIdx in spikeMatBlock.channel_indexes:
  3934                                                       #  print(chanIdx.name)
  3935                                                       stList = seg.filter(
  3936                                                           objects=SpikeTrain,
  3937                                                           name='seg{}_{}'.format(segIdx, chanIdx.name)
  3938                                                           )
  3939                                                       if len(stList):
  3940                                                           st = stList[0]
  3941                                                           print('binarizing {}'.format(st.name))
  3942                                                           stBin = binarize(
  3943                                                               st,
  3944                                                               sampling_rate=samplingRate,
  3945                                                               t_start=tStart,
  3946                                                               t_stop=tStop + samplingRate ** -1)
  3947                                                           spikeMatBlock.segments[segIdx].spiketrains.append(st)
  3948                                                           #  to do: link st to spikematblock's chidx and units
  3949                                                           assert len(chanIdx.filter(objects=Unit)) == 1
  3950                                                           thisUnit = chanIdx.filter(objects=Unit)[0]
  3951                                                           thisUnit.spiketrains.append(st)
  3952                                                           st.unit = thisUnit
  3953                                                           st.segment = spikeMatBlock.segments[segIdx]
  3954                                                       else:
  3955                                                           print('{} has no spikes'.format(st.name))
  3956                                                           stBin = dummyBin
  3957                                                       skipStAnnNames = [
  3958                                                           'nix_name', 'neo_name', 'arrayAnnNames']
  3959                                                       if 'arrayAnnNames' in st.annotations:
  3960                                                           skipStAnnNames += list(st.annotations['arrayAnnNames'])
  3961                                                       asigAnn = {
  3962                                                           k: v
  3963                                                           for k, v in st.annotations.items()
  3964                                                           if k not in skipStAnnNames
  3965                                                           }
  3966                                                       asig = AnalogSignal(
  3967                                                           stBin * samplingRate,
  3968                                                           name='seg{}_{}_raster'.format(segIdx, st.unit.name),
  3969                                                           sampling_rate=samplingRate,
  3970                                                           dtype=np.int,
  3971                                                           **asigAnn)
  3972                                                       if matchT is not None:
  3973                                                           asig = asig[:matchT.shape[0], :]
  3974                                                       asig.t_start = tStart
  3975                                                       asig.annotate(binWidth=1 / samplingRate.magnitude)
  3976                                                       chanIdx.analogsignals.append(asig)
  3977                                                       asig.channel_index = chanIdx
  3978                                                       spikeMatBlock.segments[segIdx].analogsignals.append(asig)
  3979                                               #
  3980                                               for chanIdx in spikeMatBlock.channel_indexes:
  3981                                                   chanIdx.name = chanIdx.name + '_raster'
  3982                                               #
  3983                                               spikeMatBlock.create_relationship()
  3984                                               spikeMatBlock = purgeNixAnn(spikeMatBlock)
  3985                                               if saveToFile:
  3986                                                   if os.path.exists(binnedSpikePath):
  3987                                                       os.remove(binnedSpikePath)
  3988                                                   writer = NixIO(filename=binnedSpikePath)
  3989                                                   writer.write_block(spikeMatBlock, use_obj_names=True)
  3990                                                   writer.close()
  3991                                               return spikeMatBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: calcFR at line 3993

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3993                                           @profile
  3994                                           def calcFR(
  3995                                                   binnedPath, dataPath,
  3996                                                   suffix='fr', aggregateFun=None,
  3997                                                   chanNames=None, rasterOpts=None, verbose=False
  3998                                                   ):
  3999                                               print('Loading rasters...')
  4000                                               masterSpikeMats, _ = loadSpikeMats(
  4001                                                   binnedPath, rasterOpts,
  4002                                                   aggregateFun=aggregateFun,
  4003                                                   chans=chanNames,
  4004                                                   loadAll=True, checkReferences=False)
  4005                                               print('Loading data file...')
  4006                                               dataReader = nixio_fr.NixIO(
  4007                                                   filename=dataPath)
  4008                                               dataBlock = dataReader.read_block(
  4009                                                   block_index=0, lazy=True,
  4010                                                   signal_group_mode='split-all')
  4011                                               masterBlock = Block()
  4012                                               masterBlock.name = dataBlock.annotations['neo_name']
  4013                                               #
  4014                                               for segIdx, segSpikeMat in masterSpikeMats.items():
  4015                                                   print('Calculating FR for segment {}'.format(segIdx))
  4016                                                   spikeMatDF = segSpikeMat.reset_index().rename(
  4017                                                       columns={'bin': 't'})
  4018                                           
  4019                                                   dataSeg = dataBlock.segments[segIdx]
  4020                                                   dummyAsig = dataSeg.filter(
  4021                                                       objects=AnalogSignalProxy)[0].load(channel_indexes=[0])
  4022                                                   samplingRate = dummyAsig.sampling_rate
  4023                                                   newT = dummyAsig.times.magnitude
  4024                                                   spikeMatDF['t'] = spikeMatDF['t'] + newT[0]
  4025                                           
  4026                                                   segSpikeMatInterp = hf.interpolateDF(
  4027                                                       spikeMatDF, pd.Series(newT),
  4028                                                       kind='linear', fill_value=(0, 0),
  4029                                                       x='t')
  4030                                                   spikeMatBlockInterp = dataFrameToAnalogSignals(
  4031                                                       segSpikeMatInterp,
  4032                                                       idxT='t', useColNames=True,
  4033                                                       dataCol=segSpikeMatInterp.drop(columns='t').columns,
  4034                                                       samplingRate=samplingRate)
  4035                                                   spikeMatBlockInterp.name = dataBlock.annotations['neo_name']
  4036                                                   spikeMatBlockInterp.annotate(
  4037                                                       nix_name=dataBlock.annotations['neo_name'])
  4038                                                   spikeMatBlockInterp.segments[0].name = dataSeg.annotations['neo_name']
  4039                                                   spikeMatBlockInterp.segments[0].annotate(
  4040                                                       nix_name=dataSeg.annotations['neo_name'])
  4041                                                   asigList = spikeMatBlockInterp.filter(objects=AnalogSignal)
  4042                                                   for asig in asigList:
  4043                                                       asig.annotate(binWidth=rasterOpts['binWidth'])
  4044                                                       if '_raster' in asig.name:
  4045                                                           asig.name = asig.name.replace('_raster', '_' + suffix)
  4046                                                       asig.name = 'seg{}_{}'.format(segIdx, childBaseName(asig.name, 'seg'))
  4047                                                       asig.annotate(nix_name=asig.name)
  4048                                                   chanIdxList = spikeMatBlockInterp.filter(objects=ChannelIndex)
  4049                                                   for chanIdx in chanIdxList:
  4050                                                       if '_raster' in chanIdx.name:
  4051                                                           chanIdx.name = chanIdx.name.replace('_raster', '_' + suffix)
  4052                                                       chanIdx.annotate(nix_name=chanIdx.name)
  4053                                           
  4054                                                   # masterBlock.merge(spikeMatBlockInterp)
  4055                                                   frBlockPath = dataPath.replace('_analyze.nix', '_fr.nix')
  4056                                                   writer = NixIO(filename=frBlockPath)
  4057                                                   writer.write_block(spikeMatBlockInterp, use_obj_names=True)
  4058                                                   writer.close()
  4059                                               #
  4060                                               dataReader.file.close()
  4061                                               return masterBlock

Timer unit: 1e-07 s

Total time: 864.172 s
File: C\../../analysis-code/exportForDeepSpine.py
Function: exportForDeepSpineWrapper at line 48

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    48                                           @profile
    49                                           def exportForDeepSpineWrapper():
    50         1      12738.0  12738.0      0.0      sns.set()
    51         1       1775.0   1775.0      0.0      sns.set_color_codes("dark")
    52         1       1252.0   1252.0      0.0      sns.set_context("talk")
    53         1       3255.0   3255.0      0.0      sns.set_style("whitegrid")
    54                                               #
    55         1         28.0     28.0      0.0      analysisSubFolder = os.path.join(
    56         1        200.0    200.0      0.0          scratchFolder, arguments['analysisName']
    57                                                   )
    58         1         19.0     19.0      0.0      alignSubFolder = os.path.join(
    59         1        136.0    136.0      0.0          analysisSubFolder, arguments['alignFolderName']
    60                                                   )
    61         1        132.0    132.0      0.0      calcSubFolder = os.path.join(alignSubFolder, 'dataframes')
    62         1        746.0    746.0      0.0      if not os.path.exists(calcSubFolder):
    63                                                   os.makedirs(calcSubFolder, exist_ok=True)
    64                                           
    65         1         23.0     23.0      0.0      if arguments['processAll']:
    66         1         19.0     19.0      0.0          prefix = assembledName
    67                                               else:
    68                                                   prefix = ns5FileName
    69         1         23.0     23.0      0.0      alignedAsigsKWargs['dataQuery'] = ash.processAlignQueryArgs(
    70         1         84.0     84.0      0.0          namedQueries, **arguments)
    71                                               alignedAsigsKWargs['unitNames'], alignedAsigsKWargs['unitQuery'] = (
    72         1         21.0     21.0      0.0          ash.processUnitQueryArgs(
    73         1         87.0     87.0      0.0              namedQueries, analysisSubFolder, **arguments))
    74         1         21.0     21.0      0.0      outlierTrialNames = ash.processOutlierTrials(
    75         1     136644.0 136644.0      0.0          calcSubFolder, prefix, **arguments)
    76                                           
    77         1         34.0     34.0      0.0      if arguments['window'] == 'XS':
    78         1         19.0     19.0      0.0          cropWindow = (-100e-3, 400e-3)
    79                                               elif arguments['window'] == 'XSPre':
    80                                                   cropWindow = (-600e-3, -100e-3)
    81                                           
    82         1         22.0     22.0      0.0      alignedAsigsKWargs.update(dict(
    83         1         19.0     19.0      0.0          duplicateControlsByProgram=False,
    84         1         19.0     19.0      0.0          makeControlProgram=False,
    85         1         19.0     19.0      0.0          metaDataToCategories=False,
    86         1         19.0     19.0      0.0          removeFuzzyName=False,
    87         1         18.0     18.0      0.0          decimate=1,
    88         1         18.0     18.0      0.0          windowSize=cropWindow,
    89         1         45.0     45.0      0.0          transposeToColumns='feature', concatOn='columns',))
    90                                               #
    91         1         27.0     27.0      0.0      triggeredPath = os.path.join(
    92         1         19.0     19.0      0.0          alignSubFolder,
    93         1         19.0     19.0      0.0          prefix + '_{}_{}.nix'.format(
    94         1        207.0    207.0      0.0              arguments['inputBlockName'], arguments['window']))
    95         1         20.0     20.0      0.0      outputPath = os.path.join(
    96         1         19.0     19.0      0.0          alignSubFolder,
    97         1         19.0     19.0      0.0          prefix + '_{}_{}_export.h5'.format(
    98         1        145.0    145.0      0.0              arguments['inputBlockName'], arguments['window']))
    99         1        571.0    571.0      0.0      print('loading {}'.format(triggeredPath))
   100                                           
   101         1         33.0     33.0      0.0      dataReader, dataBlock = ns5.blockFromPath(
   102         1   86142206.0 86142206.0      1.0          triggeredPath, lazy=arguments['lazy'])
   103         1         33.0     33.0      0.0      asigWide = ns5.alignedAsigsToDF(
   104         1  213136758.0 213136758.0      2.5          dataBlock, **alignedAsigsKWargs)
   105                                               # asigWide is a dataframe
   106         1    6160404.0 6160404.0      0.1      metaData = asigWide.index.to_frame()
   107         1    2025718.0 2025718.0      0.0      elecNames = metaData['electrode'].unique()
   108                                           
   109                                               # elecRegex = r'([\-]?[\S\s]*\d)([\+]?[\S\s]*\d)'
   110                                               # elecRegex = r'((?:\-|\+)(?:(?:rostral|caudal)\S_\S\S\S)*)*'
   111                                           
   112         1         31.0     31.0      0.0      elecRegex = r'((?:\-|\+)(?:(?:rostral|caudal)\S_\S\S\S)*)'
   113         1         20.0     20.0      0.0      chanRegex = r'((?:rostral|caudal)\S_\S\S\S)'
   114         1         19.0     19.0      0.0      elecChanNames = []
   115         1         20.0     20.0      0.0      stimConfigLookup = {}
   116        14        334.0     23.9      0.0      for comboName in elecNames:
   117        13      10165.0    781.9      0.0          matches = re.findall(elecRegex, comboName)
   118        13        256.0     19.7      0.0          if matches:
   119        13       3044.0    234.2      0.0              print(comboName)
   120        13        291.0     22.4      0.0              thisLookup = {'cathodes': [], 'anodes': []}
   121        26        547.0     21.0      0.0              for matchGroup in matches:
   122        13       2357.0    181.3      0.0                  print('\t' + matchGroup)
   123        13        262.0     20.2      0.0                  if len(matchGroup):
   124        13       6881.0    529.3      0.0                      theseChanNames = re.findall(chanRegex, matchGroup)
   125        13        252.0     19.4      0.0                      if theseChanNames:
   126        26        505.0     19.4      0.0                          for chanName in theseChanNames:
   127        13        276.0     21.2      0.0                              if chanName not in elecChanNames:
   128        13        266.0     20.5      0.0                                  elecChanNames.append(chanName)
   129        13        279.0     21.5      0.0                          if '-' in matchGroup:
   130        26        509.0     19.6      0.0                              for chanName in theseChanNames:
   131        13        261.0     20.1      0.0                                  if chanName not in thisLookup['cathodes']:
   132        13        264.0     20.3      0.0                                      thisLookup['cathodes'].append(chanName)
   133        13        276.0     21.2      0.0                          if '+' in matchGroup:
   134                                                                       for chanName in theseChanNames:
   135                                                                           if chanName not in thisLookup['anodes']:
   136                                                                               thisLookup['anodes'].append(chanName)
   137        13        308.0     23.7      0.0              stimConfigLookup[comboName] = thisLookup
   138                                           
   139         1         37.0     37.0      0.0      eesColumns = pd.MultiIndex.from_tuples(
   140         1         90.0     90.0      0.0          [(eCN, 'amplitude') for eCN in sorted(elecChanNames)],
   141         1      17803.0  17803.0      0.0          names=['object', 'property']
   142                                                   )
   143                                               #
   144         1    1480587.0 1480587.0      0.0      trialIndex = pd.Index(np.unique(metaData['bin']))
   145         1         69.0     69.0      0.0      trialColumns = pd.MultiIndex.from_tuples(
   146                                                   [
   147         1         18.0     18.0      0.0              ('hip_flexion_r', 'angle'), ('knee_angle_r', 'angle'),
   148         1         26.0     26.0      0.0              ('hip_flexion_l', 'angle'), ('knee_angle_l', 'angle'),
   149         1      19075.0  19075.0      0.0          ], names=['object', 'property'])
   150                                               #
   151                                               # manualPeriod = 0.01
   152                                               # manualStimTimes = np.arange(0, 0.3 + manualPeriod, manualPeriod)
   153                                               # manualEESWaveform = trialIndex.isin(manualStimTimes)
   154                                               # print(metaData.reset_index(drop=True))
   155                                               # print(metaData['electrode'])
   156         1         31.0     31.0      0.0      nullKinematics = pd.DataFrame(
   157         1       2445.0   2445.0      0.0          0, index=trialIndex, columns=trialColumns)
   158         1         23.0     23.0      0.0      kinKey = '/sling/kinematics'
   159         1      82357.0  82357.0      0.0      with pd.HDFStore(outputPath) as store:
   160         1     279629.0 279629.0      0.0          nullKinematics.to_hdf(store, kinKey)
   161         1         33.0     33.0      0.0      eesIdx = 0
   162                                           
   163       716   15218244.0  21254.5      0.2      for stimName, stimGroup in asigWide.groupby(['electrode', 'RateInHz', 'nominalCurrent']):
   164       715   17376245.0  24302.4      0.2          if stimGroup.groupby(['segment', 't']).ngroups < 5:
   165                                                       continue
   166       715     453305.0    634.0      0.0          print(stimName)
   167      7865  132867432.0  16893.5      1.5          for trialIdx, (trialName, trialGroup) in enumerate(stimGroup.groupby(['segment', 't'])):
   168      7150     451960.0     63.2      0.0              stimKey = '/sling/sheep/spindle_0/biophysical/ees_{:0>3}/stim'.format(eesIdx)
   169      7150     396013.0     55.4      0.0              eesPeriod = stimName[1] ** -1
   170      7150     671304.0     93.9      0.0              stimTimes = np.arange(0, 0.3, eesPeriod)
   171      7150    2595517.0    363.0      0.0              EESWaveform = np.zeros_like(trialIndex)
   172                                                       # TODO replace this with the hf.findClosestTimes implementation
   173      7150     236425.0     33.1      0.0              if not arguments['noStim']:
   174    124410    3508500.0     28.2      0.0                  for stimTime in stimTimes:
   175    117260  374359293.0   3192.6      4.3                      closestIndexTime = np.argmin(np.abs((trialIndex - stimTime)))
   176    117260    3995605.0     34.1      0.0                      EESWaveform[closestIndexTime] = 1
   177      7150     189892.0     26.6      0.0              eesIdx += 1
   178      7150   19766796.0   2764.6      0.2              theseResults = pd.DataFrame(0, index=trialIndex, columns=eesColumns)
   179     14300     553600.0     38.7      0.0              for cathodeName in stimConfigLookup[stimName[0]]['cathodes']:
   180      7150  101094559.0  14139.1      1.2                  theseResults.loc[:, (cathodeName, 'amplitude')] = EESWaveform * stimName[2] / len(stimConfigLookup[stimName[0]]['cathodes'])
   181      7150     242410.0     33.9      0.0              for anodeName in stimConfigLookup[stimName[0]]['anodes']:
   182                                                           theseResults.loc[:, (anodeName, 'amplitude')] = EESWaveform * stimName[2] * (-1) / len(stimConfigLookup[stimName[0]]['anodes'])
   183    100100    6694972.0     66.9      0.1              for cName, lag in trialGroup.columns:
   184     92950    2624787.0     28.2      0.0                  if 'EmgEnv' in cName:
   185     92950    3433137.0     36.9      0.0                      mName = cName.split('EmgEnv')[0]
   186     92950 3419932034.0  36793.2     39.6                      theseResults.loc[:, (mName, 'emg_env')] = trialGroup[cName].to_numpy()
   187                                                           elif 'Emg' in cName:
   188                                                               mName = cName.split('Emg')[0]
   189                                                               theseResults.loc[:, (mName, 'emg')] = trialGroup[cName].to_numpy()
   190                                                           elif ('caudal' in cName) or ('rostral' in cName):
   191                                                               lfpName = cName[:-4]
   192                                                               theseResults.loc[:, (lfpName, 'lfp')] = trialGroup[cName].to_numpy()
   193                                                           elif ('Acc' in cName):
   194                                                               nameParts = cName.split('Acc')
   195                                                               mName = nameParts[0]
   196                                                               theseResults.loc[:, (mName, 'acc_{}'.format(nameParts[1][0].lower()))] = trialGroup[cName].to_numpy()
   197      7150  856021283.0 119723.3      9.9              with pd.HDFStore(outputPath) as store:
   198      7150 3224944838.0 451041.2     37.3                  theseResults.to_hdf(store, stimKey)
   199                                                           thisMetadata = {
   200      7150     300145.0     42.0      0.0                      'globalIdx': eesIdx, 'combinationIdx': trialIdx,
   201      7150     208536.0     29.2      0.0                      'electrode': stimName[0], 'RateInHz': stimName[1],
   202      7150     239558.0     33.5      0.0                      'amplitude': stimName[2]}
   203      7150     247678.0     34.6      0.0                  if arguments['maskOutlierBlocks']:
   204      7150   17625327.0   2465.1      0.2                      thisMetadata['outlierTrial'] = outlierTrialNames.loc[trialName]
   205      7150  125938054.0  17613.7      1.5                  store.get_storer(stimKey).attrs.metadata = thisMetadata
   206                                           
   207         1         41.0     41.0      0.0      if arguments['lazy']:
   208                                                   dataReader.file.close()
   209         1         19.0     19.0      0.0      return

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: analogSignalsToDataFrame at line 43

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    43                                           @profile
    44                                           def analogSignalsToDataFrame(
    45                                                   analogsignals, idxT='t', useChanNames=False):
    46                                               asigList = []
    47                                               for asig in analogsignals:
    48                                                   if asig.shape[1] == 1:
    49                                                       if useChanNames:
    50                                                           colNames = [str(asig.channel_index.name)]
    51                                                       else:
    52                                                           colNames = [str(asig.name)]
    53                                                   else:
    54                                                       colNames = [
    55                                                           asig.name +
    56                                                           '_{}'.format(i) for i in
    57                                                           asig.channel_index.channel_ids
    58                                                           ]
    59                                                   asigList.append(
    60                                                       pd.DataFrame(
    61                                                           asig.magnitude, columns=colNames,
    62                                                           index=range(asig.shape[0])))
    63                                               asigList.append(
    64                                                   pd.DataFrame(
    65                                                       asig.times.magnitude, columns=[idxT],
    66                                                       index=range(asig.shape[0])))
    67                                               return pd.concat(asigList, axis=1)

Total time: 0.0059138 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: listChanNames at line 69

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    69                                           @profile
    70                                           def listChanNames(
    71                                                   dataBlock, chanQuery,
    72                                                   objType=AnalogSignalProxy, condition=None):
    73                                               allChanList = [
    74         1         12.0     12.0      0.0          i.name
    75         1       8623.0   8623.0     14.6          for i in dataBlock.filter(objects=objType)]
    76         1          9.0      9.0      0.0      if condition == 'hasAsigs':
    77                                                   allChanList = [
    78                                                       i
    79                                                       for i in allChanList
    80                                                       if len(dataBlock.filter(objects=objType, name=i)[0].analogsignals)
    81                                                   ]
    82         1         16.0     16.0      0.0      chansToTrigger = pd.DataFrame(
    83         1        795.0    795.0      1.3          np.unique(allChanList),
    84         1       8491.0   8491.0     14.4          columns=['chanName'])
    85         1         11.0     11.0      0.0      if chanQuery is not None:
    86         1         20.0     20.0      0.0          chansToTrigger = chansToTrigger.query(
    87         1      41150.0  41150.0     69.6              chanQuery, engine='python')['chanName'].to_list()
    88                                               else:
    89                                                   chansToTrigger = chansToTrigger['chanName'].to_list()
    90         1         11.0     11.0      0.0      return chansToTrigger

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: spikeDictToSpikeTrains at line 92

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    92                                           @profile
    93                                           def spikeDictToSpikeTrains(
    94                                                   spikes, block=None, seg=None,
    95                                                   probeName='insTD', t_stop=None,
    96                                                   waveformUnits=pq.uV,
    97                                                   sampling_rate=3e4 * pq.Hz):
    98                                           
    99                                               if block is None:
   100                                                   assert seg is None
   101                                                   block = Block()
   102                                                   seg = Segment(name=probeName + ' segment')
   103                                                   block.segments.append(seg)
   104                                           
   105                                               if t_stop is None:
   106                                                   t_stop = hf.getLastSpikeTime(spikes) + 1
   107                                           
   108                                               for idx, chanName in enumerate(spikes['ChannelID']):
   109                                                   #  unique units on this channel
   110                                                   unitsOnThisChan = pd.unique(spikes['Classification'][idx])
   111                                                   nixChanName = probeName + '{}'.format(chanName)
   112                                                   chanIdx = ChannelIndex(
   113                                                       name=nixChanName,
   114                                                       index=np.asarray([idx]),
   115                                                       channel_names=np.asarray([nixChanName]))
   116                                                   block.channel_indexes.append(chanIdx)
   117                                                   
   118                                                   for unitIdx, unitName in enumerate(unitsOnThisChan):
   119                                                       unitMask = spikes['Classification'][idx] == unitName
   120                                                       # this unit's spike timestamps
   121                                                       theseTimes = spikes['TimeStamps'][idx][unitMask]
   122                                                       # this unit's waveforms
   123                                                       if len(spikes['Waveforms'][idx].shape) == 3:
   124                                                           theseWaveforms = spikes['Waveforms'][idx][unitMask, :, :]
   125                                                           theseWaveforms = np.swapaxes(theseWaveforms, 1, 2)
   126                                                       elif len(spikes['Waveforms'][idx].shape) == 2:
   127                                                           theseWaveforms = (
   128                                                               spikes['Waveforms'][idx][unitMask, np.newaxis, :])
   129                                                       else:
   130                                                           raise(Exception('spikes[Waveforms] has bad shape'))
   131                                           
   132                                                       unitName = '{}#{}'.format(nixChanName, unitIdx)
   133                                                       unit = Unit(name=unitName)
   134                                                       unit.channel_index = chanIdx
   135                                                       chanIdx.units.append(unit)
   136                                           
   137                                                       train = SpikeTrain(
   138                                                           times=theseTimes, t_stop=t_stop, units='sec',
   139                                                           name=unitName, sampling_rate=sampling_rate,
   140                                                           waveforms=theseWaveforms*waveformUnits,
   141                                                           left_sweep=0, dtype=np.float32)
   142                                                       unit.spiketrains.append(train)
   143                                                       seg.spiketrains.append(train)
   144                                           
   145                                                       unit.create_relationship()
   146                                                   chanIdx.create_relationship()
   147                                               seg.create_relationship()
   148                                               block.create_relationship()
   149                                               return block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: spikeTrainsToSpikeDict at line 151

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   151                                           @profile
   152                                           def spikeTrainsToSpikeDict(
   153                                                   spiketrains):
   154                                               nCh = len(spiketrains)
   155                                               spikes = {
   156                                                   'ChannelID': [i for i in range(nCh)],
   157                                                   'Classification': [np.asarray([]) for i in range(nCh)],
   158                                                   'NEUEVWAV_HeaderIndices': [None for i in range(nCh)],
   159                                                   'TimeStamps': [np.asarray([]) for i in range(nCh)],
   160                                                   'Units': 'uV',
   161                                                   'Waveforms': [np.asarray([]) for i in range(nCh)],
   162                                                   'basic_headers': {'TimeStampResolution': 3e4},
   163                                                   'extended_headers': []
   164                                                   }
   165                                               for idx, st in enumerate(spiketrains):
   166                                                   spikes['ChannelID'][idx] = st.name
   167                                                   if len(spikes['TimeStamps'][idx]):
   168                                                       spikes['TimeStamps'][idx] = np.stack((
   169                                                           spikes['TimeStamps'][idx],
   170                                                           st.times.magnitude), axis=-1)
   171                                                   else:
   172                                                       spikes['TimeStamps'][idx] = st.times.magnitude
   173                                                   
   174                                                   theseWaveforms = np.swapaxes(
   175                                                       st.waveforms, 1, 2)
   176                                                   theseWaveforms = np.atleast_2d(np.squeeze(
   177                                                       theseWaveforms))
   178                                                       
   179                                                   if len(spikes['Waveforms'][idx]):
   180                                                       spikes['Waveforms'][idx] = np.stack((
   181                                                           spikes['Waveforms'][idx],
   182                                                           theseWaveforms.magnitude), axis=-1)
   183                                                   else:
   184                                                       spikes['Waveforms'][idx] = theseWaveforms.magnitude
   185                                                   
   186                                                   classVals = st.times.magnitude ** 0 * idx
   187                                                   if len(spikes['Classification'][idx]):
   188                                                       spikes['Classification'][idx] = np.stack((
   189                                                           spikes['Classification'][idx],
   190                                                           classVals), axis=-1)
   191                                                   else:
   192                                                       spikes['Classification'][idx] = classVals
   193                                               return spikes

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: channelIndexesToSpikeDict at line 195

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   195                                           @profile
   196                                           def channelIndexesToSpikeDict(
   197                                                   channel_indexes):
   198                                               nCh = len(channel_indexes)
   199                                               spikes = {
   200                                                   'ChannelID': [i for i in range(nCh)],
   201                                                   'Classification': [np.asarray([]) for i in range(nCh)],
   202                                                   'NEUEVWAV_HeaderIndices': [None for i in range(nCh)],
   203                                                   'TimeStamps': [np.asarray([]) for i in range(nCh)],
   204                                                   'Units': 'uV',
   205                                                   'Waveforms': [np.asarray([]) for i in range(nCh)],
   206                                                   'basic_headers': {'TimeStampResolution': 3e4},
   207                                                   'extended_headers': []
   208                                                   }
   209                                               #  allocate fields for annotations
   210                                               for dummyCh in channel_indexes:
   211                                                   if len(dummyCh.units):
   212                                                       dummyUnit = dummyCh.units[0]
   213                                                       if len(dummyUnit.spiketrains):
   214                                                           if len(dummyUnit.spiketrains[0].times):
   215                                                               break
   216                                               dummySt = [
   217                                                   st
   218                                                   for st in dummyUnit.spiketrains
   219                                                   if len(st.times)][0]
   220                                               #  allocate fields for array annotations (per spike)
   221                                               if dummySt.array_annotations:
   222                                                   for key in dummySt.array_annotations.keys():
   223                                                       spikes.update({key: [np.asarray([]) for i in range(nCh)]})
   224                                                   
   225                                               maxUnitIdx = 0
   226                                               for idx, chIdx in enumerate(channel_indexes):
   227                                                   spikes['ChannelID'][idx] = chIdx.name
   228                                                   for unitIdx, thisUnit in enumerate(chIdx.units):
   229                                                       for stIdx, st in enumerate(thisUnit.spiketrains):
   230                                                           if not len(st.times):
   231                                                               continue
   232                                                           #  print(
   233                                                           #      'unit {} has {} spiketrains'.format(
   234                                                           #          thisUnit.name,
   235                                                           #          len(thisUnit.spiketrains)))
   236                                                           if len(spikes['TimeStamps'][idx]):
   237                                                               spikes['TimeStamps'][idx] = np.concatenate((
   238                                                                   spikes['TimeStamps'][idx],
   239                                                                   st.times.magnitude), axis=0)
   240                                                           else:
   241                                                               spikes['TimeStamps'][idx] = st.times.magnitude
   242                                                           #  reshape waveforms to comply with BRM convention
   243                                                           theseWaveforms = np.swapaxes(
   244                                                               st.waveforms, 1, 2)
   245                                                           theseWaveforms = np.atleast_2d(np.squeeze(
   246                                                               theseWaveforms))
   247                                                           #  append waveforms
   248                                                           if len(spikes['Waveforms'][idx]):
   249                                                               try:
   250                                                                   spikes['Waveforms'][idx] = np.concatenate((
   251                                                                       spikes['Waveforms'][idx],
   252                                                                       theseWaveforms.magnitude), axis=0)
   253                                                               except Exception:
   254                                                                   traceback.print_exc()
   255                                                           else:
   256                                                               spikes['Waveforms'][idx] = theseWaveforms.magnitude
   257                                                           #  give each unit a global index
   258                                                           classVals = st.times.magnitude ** 0 * maxUnitIdx
   259                                                           st.array_annotations.update({'Classification': classVals})
   260                                                           #  expand array_annotations into spikes dict
   261                                                           for key, value in st.array_annotations.items():
   262                                                               if len(spikes[key][idx]):
   263                                                                   spikes[key][idx] = np.concatenate((
   264                                                                       spikes[key][idx],
   265                                                                       value), axis=0)
   266                                                               else:
   267                                                                   spikes[key][idx] = value
   268                                                           for key, value in st.annotations.items():
   269                                                               if key not in spikes['basic_headers']:
   270                                                                   spikes['basic_headers'].update({key: {}})
   271                                                               try:
   272                                                                   spikes['basic_headers'][key].update({maxUnitIdx: value})
   273                                                               except Exception:
   274                                                                   pass
   275                                                           maxUnitIdx += 1
   276                                               return spikes

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: unitSpikeTrainArrayAnnToDF at line 278

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   278                                           @profile
   279                                           def unitSpikeTrainArrayAnnToDF(
   280                                                   spikeTrainContainer):
   281                                               #  list contains different segments
   282                                               if isinstance(spikeTrainContainer, ChannelIndex):
   283                                                   assert len(spikeTrainContainer.units) == 0
   284                                                   spiketrains = spikeTrainContainer.units[0].spiketrains
   285                                               elif isinstance(spikeTrainContainer, Unit):
   286                                                   spiketrains = spikeTrainContainer.spiketrains
   287                                               elif isinstance(spikeTrainContainer, list):
   288                                                   spiketrains = spikeTrainContainer
   289                                               fullAnnotationsDict = {}
   290                                               for segIdx, st in enumerate(spiketrains):
   291                                                   theseAnnDF = pd.DataFrame(st.array_annotations)
   292                                                   theseAnnDF['t'] = st.times.magnitude
   293                                                   fullAnnotationsDict.update({segIdx: theseAnnDF})
   294                                               annotationsDF = pd.concat(
   295                                                   fullAnnotationsDict, names=['segment', 'index'], sort=True)
   296                                               return annotationsDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getSpikeDFMetadata at line 298

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   298                                           @profile
   299                                           def getSpikeDFMetadata(spikeDF, metaDataCols):
   300                                               spikeDF.reset_index(inplace=True)
   301                                               metaDataCols = np.atleast_1d(metaDataCols)
   302                                               spikeDF.index.name = 'metaDataIdx'
   303                                               metaDataDF = spikeDF.loc[:, metaDataCols].copy()
   304                                               newSpikeDF = spikeDF.drop(columns=metaDataCols).reset_index()
   305                                               return newSpikeDF, metaDataDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: transposeSpikeDF at line 307

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   307                                           @profile
   308                                           def transposeSpikeDF(
   309                                                   spikeDF, transposeToColumns,
   310                                                   fastTranspose=False):
   311                                               newColumnNames = np.atleast_1d(transposeToColumns).tolist()
   312                                               originalColumnNames = np.atleast_1d(spikeDF.columns.names)
   313                                               metaDataCols = np.setdiff1d(spikeDF.index.names, newColumnNames).tolist()
   314                                               if fastTranspose:
   315                                                   #  fast but memory inefficient
   316                                                   return spikeDF.stack().unstack(transposeToColumns)
   317                                               else:
   318                                                   raise(Warning('Caution! transposeSpikeDF might not be working, needs testing RD 06252019'))
   319                                                   #  stash annotations, transpose, recover annotations
   320                                                   newSpikeDF, metaDataDF = getSpikeDFMetadata(spikeDF, metaDataCols)
   321                                                   del spikeDF
   322                                                   gc.collect()
   323                                                   #
   324                                                   newSpikeDF = newSpikeDF.stack().unstack(newColumnNames)
   325                                                   newSpikeDF.reset_index(inplace=True)
   326                                                   #  set the index
   327                                                   newIdxLabels = np.concatenate(
   328                                                       [originalColumnNames, metaDataCols]).tolist()
   329                                                   newSpikeDF.loc[:, metaDataCols] = (
   330                                                       metaDataDF
   331                                                       .loc[newSpikeDF['metaDataIdx'].to_list(), metaDataCols]
   332                                                       .to_numpy())
   333                                                   newSpikeDF = (
   334                                                       newSpikeDF
   335                                                       .drop(columns=['metaDataIdx'])
   336                                                       .set_index(newIdxLabels))
   337                                                   return newSpikeDF

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: concatenateBlocks at line 339

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   339                                           @profile
   340                                           def concatenateBlocks(
   341                                                   asigBlocks, spikeBlocks, eventBlocks, chunkingMetadata,
   342                                                   samplingRate, chanQuery, lazy, trackMemory, verbose
   343                                                   ):
   344                                               # Scan ahead through all files and ensure that
   345                                               # spikeTrains and units are present across all assembled files
   346                                               channelIndexCache = {}
   347                                               unitCache = {}
   348                                               asigCache = []
   349                                               asigAnnCache = {}
   350                                               spiketrainCache = {}
   351                                               eventCache = {}
   352                                               # get list of channels and units
   353                                               for idx, (chunkIdxStr, chunkMeta) in enumerate(chunkingMetadata.items()):
   354                                                   gc.collect()
   355                                                   chunkIdx = int(chunkIdxStr)
   356                                                   asigBlock = asigBlocks[chunkIdx]
   357                                                   asigSeg = asigBlock.segments[0]
   358                                                   spikeBlock = spikeBlocks[chunkIdx]
   359                                                   eventBlock = eventBlocks[chunkIdx]
   360                                                   eventSeg = eventBlock.segments[0]
   361                                                   for chIdx in asigBlock.filter(objects=ChannelIndex):
   362                                                       chAlreadyThere = (chIdx.name in channelIndexCache.keys())
   363                                                       if not chAlreadyThere:
   364                                                           newChIdx = copy(chIdx)
   365                                                           newChIdx.analogsignals = []
   366                                                           newChIdx.units = []
   367                                                           channelIndexCache[chIdx.name] = newChIdx
   368                                                   for unit in (spikeBlock.filter(objects=Unit)):
   369                                                       if lazy:
   370                                                           theseSpiketrains = []
   371                                                           for stP in unit.spiketrains:
   372                                                               st = loadStProxy(stP)
   373                                                               if len(st.times) > 0:
   374                                                                   theseSpiketrains.append(st)
   375                                                       else:
   376                                                           theseSpiketrains = [
   377                                                               st
   378                                                               for st in unit.spiketrains
   379                                                               if len(st.times)
   380                                                               ]
   381                                                       for st in theseSpiketrains:
   382                                                           st = loadObjArrayAnn(st)
   383                                                           if len(st.times):
   384                                                               st.magnitude[:] = st.times.magnitude + spikeBlock.annotations['chunkTStart']
   385                                                               st.t_start = min(0 * pq.s, st.times[0] * 0.999)
   386                                                               st.t_stop = max(
   387                                                                   st.t_stop + spikeBlock.annotations['chunkTStart'] * pq.s,
   388                                                                   st.times[-1] * 1.001)
   389                                                           else:
   390                                                               st.t_start += spikeBlock.annotations['chunkTStart'] * pq.s
   391                                                               st.t_stop += spikeBlock.annotations['chunkTStart'] * pq.s
   392                                                       uAlreadyThere = (unit.name in unitCache.keys())
   393                                                       if not uAlreadyThere:
   394                                                           newUnit = copy(unit)
   395                                                           newUnit.spiketrains = []
   396                                                           newUnit.annotations['parentChanName'] = unit.channel_index.name
   397                                                           unitCache[unit.name] = newUnit
   398                                                           spiketrainCache[unit.name] = theseSpiketrains
   399                                                       else:
   400                                                           spiketrainCache[unit.name] = spiketrainCache[unit.name] + theseSpiketrains
   401                                                   #
   402                                                   if lazy:
   403                                                       evList = [
   404                                                           evP.load()
   405                                                           for evP in eventSeg.events]
   406                                                   else:
   407                                                       evList = eventSeg.events
   408                                                   for event in evList:
   409                                                       event.magnitude[:] = event.magnitude + eventBlock.annotations['chunkTStart']
   410                                                       if event.name in eventCache.keys():
   411                                                           eventCache[event.name].append(event)
   412                                                       else:
   413                                                           eventCache[event.name] = [event]
   414                                                   # take the requested analog signal channels
   415                                                   if lazy:
   416                                                       tdChanNames = listChanNames(
   417                                                           asigBlock, chanQuery, objType=AnalogSignalProxy)
   418                                                       #############
   419                                                       # tdChanNames = ['seg0_utah1', 'seg0_utah10']
   420                                                       ##############
   421                                                       asigList = []
   422                                                       for asigP in asigSeg.analogsignals:
   423                                                           if asigP.name in tdChanNames:
   424                                                               asig = asigP.load()
   425                                                               asig.channel_index = asigP.channel_index
   426                                                               asigList.append(asig)
   427                                                               if trackMemory:
   428                                                                   print('loading {} from proxy object. memory usage: {:.1f} MB'.format(
   429                                                                       asigP.name, prf.memory_usage_psutil()))
   430                                                   else:
   431                                                       tdChanNames = listChanNames(
   432                                                           asigBlock, chanQuery, objType=AnalogSignal)
   433                                                       asigList = [
   434                                                           asig
   435                                                           for asig in asigSeg.analogsignals
   436                                                           if asig.name in tdChanNames
   437                                                           ]
   438                                                   for asig in asigList:
   439                                                       if asig.size > 0:
   440                                                           dummyAsig = asig
   441                                                   if idx == 0:
   442                                                       outputBlock = Block(
   443                                                           name=asigBlock.name,
   444                                                           file_origin=asigBlock.file_origin,
   445                                                           file_datetime=asigBlock.file_datetime,
   446                                                           rec_datetime=asigBlock.rec_datetime,
   447                                                           **asigBlock.annotations
   448                                                       )
   449                                                       newSeg = Segment(
   450                                                           index=0, name=asigSeg.name,
   451                                                           description=asigSeg.description,
   452                                                           file_origin=asigSeg.file_origin,
   453                                                           file_datetime=asigSeg.file_datetime,
   454                                                           rec_datetime=asigSeg.rec_datetime,
   455                                                           **asigSeg.annotations
   456                                                       )
   457                                                       outputBlock.segments = [newSeg]
   458                                                       for asig in asigList:
   459                                                           asigAnnCache[asig.name] = asig.annotations
   460                                                           asigAnnCache[asig.name]['parentChanName'] = asig.channel_index.name
   461                                                       asigUnits = dummyAsig.units
   462                                                   tdDF = analogSignalsToDataFrame(asigList)
   463                                                   del asigList  # asigs saved to dataframe, no longer needed
   464                                                   tdDF.loc[:, 't'] += asigBlock.annotations['chunkTStart']
   465                                                   tdDF.set_index('t', inplace=True)
   466                                                   if samplingRate != dummyAsig.sampling_rate:
   467                                                       lowPassOpts = {
   468                                                           'low': {
   469                                                               'Wn': float(samplingRate / 2),
   470                                                               'N': 4,
   471                                                               'btype': 'low',
   472                                                               'ftype': 'bessel'
   473                                                           }
   474                                                       }
   475                                                       newT = pd.Series(
   476                                                           np.arange(
   477                                                               dummyAsig.t_start + asigBlock.annotations['chunkTStart'] * pq.s,
   478                                                               dummyAsig.t_stop + asigBlock.annotations['chunkTStart'] * pq.s,
   479                                                               1/samplingRate))
   480                                                       if samplingRate < dummyAsig.sampling_rate:
   481                                                           filterCoeffs = hf.makeFilterCoeffsSOS(
   482                                                               lowPassOpts, float(dummyAsig.sampling_rate))
   483                                                           if trackMemory:
   484                                                               print('Filtering analog data before downsampling. memory usage: {:.1f} MB'.format(
   485                                                                   prf.memory_usage_psutil()))
   486                                                           '''
   487                                                           ### check that axis=0 is the correct option
   488                                                           dummyDF = tdDF.iloc[:, :4].copy()
   489                                                           filteredAsigs0 = signal.sosfiltfilt( filterCoeffs, dummyDF.to_numpy(), axis=0)
   490                                                           filteredAsigs1 = signal.sosfiltfilt( filterCoeffs, dummyDF.to_numpy(), axis=1)
   491                                                           ###
   492                                                           '''
   493                                                           filteredAsigs = signal.sosfiltfilt(
   494                                                               filterCoeffs, tdDF.to_numpy(),
   495                                                               axis=0)
   496                                                           tdDF = pd.DataFrame(
   497                                                               filteredAsigs,
   498                                                               index=tdDF.index,
   499                                                               columns=tdDF.columns)
   500                                                           if trackMemory:
   501                                                               print('Just finished analog data filtering before downsampling. memory usage: {:.1f} MB'.format(
   502                                                                   prf.memory_usage_psutil()))
   503                                                       tdInterp = hf.interpolateDF(
   504                                                           tdDF, newT,
   505                                                           kind='linear', fill_value='extrapolate',
   506                                                           verbose=verbose)
   507                                                       # free up memory used by full resolution asigs
   508                                                       del tdDF
   509                                                   else:
   510                                                       tdInterp = tdDF
   511                                                   #
   512                                                   asigCache.append(tdInterp)
   513                                                   #
   514                                                   print('Finished chunk {}'.format(chunkIdxStr))
   515                                               allTdDF = pd.concat(asigCache)
   516                                               # TODO: check for nans, if, for example a signal is partially missing
   517                                               allTdDF.fillna(method='bfill', inplace=True)
   518                                               allTdDF.fillna(method='ffill', inplace=True)
   519                                               for asigName in allTdDF.columns:
   520                                                   newAsig = AnalogSignal(
   521                                                       allTdDF[asigName].to_numpy() * asigUnits,
   522                                                       name=asigName,
   523                                                       sampling_rate=samplingRate,
   524                                                       dtype=np.float32,
   525                                                       **asigAnnCache[asigName])
   526                                                   chIdxName = asigAnnCache[asigName]['parentChanName']
   527                                                   chIdx = channelIndexCache[chIdxName]
   528                                                   # cross-assign ownership to containers
   529                                                   chIdx.analogsignals.append(newAsig)
   530                                                   newSeg.analogsignals.append(newAsig)
   531                                                   newAsig.channel_index = chIdx
   532                                                   newAsig.segment = newSeg
   533                                               #
   534                                               for uName, unit in unitCache.items():
   535                                                   # concatenate spike times, waveforms, etc.
   536                                                   if len(spiketrainCache[unit.name]):
   537                                                       consolidatedTimes = np.concatenate([
   538                                                               st.times.magnitude
   539                                                               for st in spiketrainCache[unit.name]
   540                                                           ])
   541                                                       # TODO:   decide whether to include this step
   542                                                       #         which snaps the spike times to the nearest
   543                                                       #         *sampled* data point
   544                                                       #
   545                                                       # consolidatedTimes, timesIndex = hf.closestSeries(
   546                                                       #     takeFrom=pd.Series(consolidatedTimes),
   547                                                       #     compareTo=pd.Series(allTdDF.index))
   548                                                       #
   549                                                       # find an example spiketrain with array_annotations
   550                                                       for st in spiketrainCache[unit.name]:
   551                                                           if len(st.times):
   552                                                               dummySt = st
   553                                                               break
   554                                                       consolidatedAnn = {
   555                                                           key: np.array([])
   556                                                           for key, value in dummySt.array_annotations.items()
   557                                                           }
   558                                                       for key, value in consolidatedAnn.items():
   559                                                           consolidatedAnn[key] = np.concatenate([
   560                                                               st.annotations[key]
   561                                                               for st in spiketrainCache[unit.name]
   562                                                           ])
   563                                                       consolidatedWaveforms = np.concatenate([
   564                                                           st.waveforms
   565                                                           for st in spiketrainCache[unit.name]
   566                                                           ])
   567                                                       spikeTStop = max([
   568                                                           st.t_stop
   569                                                           for st in spiketrainCache[unit.name]
   570                                                           ])
   571                                                       spikeTStart = max([
   572                                                           st.t_start
   573                                                           for st in spiketrainCache[unit.name]
   574                                                           ])
   575                                                       spikeAnnotations = {
   576                                                           key: value
   577                                                           for key, value in dummySt.annotations.items()
   578                                                           if key not in dummySt.annotations['arrayAnnNames']
   579                                                       }
   580                                                       newSt = SpikeTrain(
   581                                                           name=dummySt.name,
   582                                                           times=consolidatedTimes, units='sec', t_stop=spikeTStop,
   583                                                           waveforms=consolidatedWaveforms * dummySt.waveforms.units,
   584                                                           left_sweep=dummySt.left_sweep,
   585                                                           sampling_rate=dummySt.sampling_rate,
   586                                                           t_start=spikeTStart, **spikeAnnotations,
   587                                                           array_annotations=consolidatedAnn)
   588                                                       # cross-assign ownership to containers
   589                                                       unit.spiketrains.append(newSt)
   590                                                       newSt.unit = unit
   591                                                       newSeg.spiketrains.append(newSt)
   592                                                       newSt.segment = newSeg
   593                                                       # link chIdxes and Units
   594                                                       if unit.annotations['parentChanName'] in channelIndexCache:
   595                                                           chIdx = channelIndexCache[unit.annotations['parentChanName']]
   596                                                           if unit not in chIdx.units:
   597                                                               chIdx.units.append(unit)
   598                                                               unit.channel_index = chIdx
   599                                                       else:
   600                                                           newChIdx = ChannelIndex(
   601                                                               name=unit.annotations['parentChanName'], index=0)
   602                                                           channelIndexCache[unit.annotations['parentChanName']] = newChIdx
   603                                                           if unit not in newChIdx.units:
   604                                                               newChIdx.units.append(unit)
   605                                                               unit.channel_index = newChIdx
   606                                               #
   607                                               for evName, eventList in eventCache.items():
   608                                                   consolidatedTimes = np.concatenate([
   609                                                       ev.times.magnitude
   610                                                       for ev in eventList
   611                                                       ])
   612                                                   consolidatedLabels = np.concatenate([
   613                                                       ev.labels
   614                                                       for ev in eventList
   615                                                       ])
   616                                                   newEvent = Event(
   617                                                       name=evName,
   618                                                       times=consolidatedTimes * pq.s,
   619                                                       labels=consolidatedLabels
   620                                                       )
   621                                                   # if len(newEvent):
   622                                                   newEvent.segment = newSeg
   623                                                   newSeg.events.append(newEvent)
   624                                               for chIdxName, chIdx in channelIndexCache.items():
   625                                                   if len(chIdx.analogsignals) or len(chIdx.units):
   626                                                       outputBlock.channel_indexes.append(chIdx)
   627                                                       chIdx.block = outputBlock
   628                                               #
   629                                               outputBlock = purgeNixAnn(outputBlock)
   630                                               createRelationship = False
   631                                               if createRelationship:
   632                                                   outputBlock.create_relationship()
   633                                               return outputBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: concatenateEventsContainer at line 660

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   660                                           @profile
   661                                           def concatenateEventsContainer(eventContainer, linkParents=True):
   662                                               if isinstance(eventContainer, dict):
   663                                                   listOfEvents = list(eventContainer.values())
   664                                               else:
   665                                                   listOfEvents = eventContainer
   666                                               nonEmptyEvents = [ev for ev in listOfEvents if len(ev.times)]
   667                                               if not len(nonEmptyEvents) > 0:
   668                                                   return listOfEvents[0]
   669                                               masterEvent = listOfEvents[0]
   670                                               for evIdx, ev in enumerate(listOfEvents[1:]):
   671                                                   try:
   672                                                       masterEvent = masterEvent.merge(ev)
   673                                                   except Exception:
   674                                                       traceback.print_exc()
   675                                                       pdb.set_trace()
   676                                               if masterEvent.array_annotations is not None:
   677                                                   arrayAnnNames = list(masterEvent.array_annotations.keys())
   678                                                   masterEvent.annotations.update(masterEvent.array_annotations)
   679                                                   masterEvent.annotations['arrayAnnNames'] = arrayAnnNames
   680                                               if linkParents:
   681                                                   masterEvent.segment = listOfEvents[0].segment
   682                                                   if isinstance(masterEvent, SpikeTrain):
   683                                                       masterEvent.unit = listOfEvents[0].unit
   684                                               return masterEvent

Total time: 9.73725 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: unitSpikeTrainWaveformsToDF at line 743

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   743                                           @profile
   744                                           def unitSpikeTrainWaveformsToDF(
   745                                                   spikeTrainContainer,
   746                                                   dataQuery=None,
   747                                                   transposeToColumns='bin', fastTranspose=True,
   748                                                   lags=None, decimate=1, rollingWindow=None,
   749                                                   getMetaData=True, verbose=False,
   750                                                   whichSegments=None, windowSize=None, procFun=None):
   751                                               #  list contains different segments from *one* unit
   752        13        595.0     45.8      0.0      if isinstance(spikeTrainContainer, ChannelIndex):
   753                                                   assert len(spikeTrainContainer.units) == 0
   754                                                   spiketrains = spikeTrainContainer.units[0].spiketrains
   755        13        412.0     31.7      0.0      elif isinstance(spikeTrainContainer, Unit):
   756        13        439.0     33.8      0.0          spiketrains = spikeTrainContainer.spiketrains
   757                                               else:
   758                                                   raise(Exception('not a valid container'))
   759                                               # TODO check if really need to assert uniqueness?
   760        13        376.0     28.9      0.0      uniqueSpiketrains = []
   761        91       2622.0     28.8      0.0      for st in spiketrains:
   762        78      19461.0    249.5      0.0          if not np.any([st is i for i in uniqueSpiketrains]):
   763        39       1221.0     31.3      0.0              uniqueSpiketrains.append(st)
   764                                               #  subsampling options
   765        13        460.0     35.4      0.0      decimate = int(decimate)
   766        13        397.0     30.5      0.0      if whichSegments is not None:
   767                                                   uniqueSpiketrains = [
   768                                                       uniqueSpiketrains[i]
   769                                                       for i in whichSegments
   770                                                   ]
   771                                               #
   772        13        344.0     26.5      0.0      waveformsList = []
   773                                               #
   774        52       2546.0     49.0      0.0      for segIdx, stIn in enumerate(uniqueSpiketrains):
   775        39       1399.0     35.9      0.0          if verbose:
   776                                                       print('extracting spiketrain from {}'.format(stIn.segment))
   777                                                   #  make sure is not a proxyObj
   778        39       2045.0     52.4      0.0          if isinstance(stIn, SpikeTrainProxy):
   779                                                       st = loadStProxy(stIn)
   780                                                       if (getMetaData) or (dataQuery is not None):
   781                                                           # if there's a query, get metadata temporarily to resolve it
   782                                                           st = loadObjArrayAnn(st)
   783                                                   else:
   784        39       1515.0     38.8      0.0              st = stIn
   785                                                   #  extract bins spaced by decimate argument
   786        39      81799.0   2097.4      0.1          if not st.times.any():
   787                                                       continue
   788        39       1729.0     44.3      0.0          if verbose:
   789                                                       print('extracting wf from {}'.format(stIn.segment))
   790        39       1732.0     44.4      0.0          wf = np.asarray(
   791        39      17968.0    460.7      0.0              np.squeeze(st.waveforms),
   792        39       6062.0    155.4      0.0              dtype='float32')
   793        39       1756.0     45.0      0.0          if wf.ndim == 3:
   794                                                       print('Waveforms from more than one channel!')
   795                                                       if wf.shape[1] > 0:
   796                                                           wf = wf[:, 0, :]
   797        39     198314.0   5085.0      0.2          wfDF = pd.DataFrame(wf)
   798        39       1992.0     51.1      0.0          samplingRate = st.sampling_rate
   799                                                   bins = (
   800        39      38305.0    982.2      0.0              np.asarray(wfDF.columns) / samplingRate -
   801        39     103182.0   2645.7      0.1              st.left_sweep)
   802        39     139480.0   3576.4      0.1          wfDF.columns = np.around(bins.magnitude, decimals=6)
   803        39       1866.0     47.8      0.0          if windowSize is not None:
   804                                                       winMask = (
   805        39      85894.0   2202.4      0.1                  (wfDF.columns >= windowSize[0]) &
   806        39      57464.0   1473.4      0.1                  (wfDF.columns <= windowSize[1]))
   807        39    2936133.0  75285.5      3.0              wfDF = wfDF.loc[:, winMask]
   808        39       1775.0     45.5      0.0          if procFun is not None:
   809                                                       wfDF = procFun(wfDF, st)
   810        39       1495.0     38.3      0.0          idxLabels = ['segment', 'originalIndex', 't']
   811        39   25610296.0 656674.3     26.3          wfDF.loc[:, 't'] = np.asarray(st.times.magnitude)
   812        39       1437.0     36.8      0.0          if (getMetaData) or (dataQuery is not None):
   813                                                       # if there's a query, get metadata temporarily to resolve it
   814        39        947.0     24.3      0.0              annDict = {}
   815       507      12133.0     23.9      0.0              for k, values in st.array_annotations.items():
   816       468      16304.0     34.8      0.0                  if isinstance(getMetaData, Iterable):
   817                                                               # if selecting metadata fields, check that
   818                                                               # the key is in the provided list
   819                                                               if k not in getMetaData:
   820                                                                   continue
   821       468      14270.0     30.5      0.0                  if isinstance(values[0], str):
   822       117       7524.0     64.3      0.0                      v = np.asarray(values, dtype='str')
   823                                                           else:
   824       351      10395.0     29.6      0.0                      v = np.asarray(values)
   825       468      12713.0     27.2      0.0                  annDict.update({k: v})
   826                                                       skipAnnNames = (
   827        39       1521.0     39.0      0.0                  st.annotations['arrayAnnNames'] +
   828                                                           [
   829        39        824.0     21.1      0.0                      'arrayAnnNames', 'arrayAnnDTypes',
   830        39        809.0     20.7      0.0                      'nix_name', 'neo_name', 'id',
   831        39       1126.0     28.9      0.0                      'cell_label', 'cluster_label', 'max_on_channel', 'binWidth']
   832                                                           )
   833        39    1677869.0  43022.3      1.7              annDF = pd.DataFrame(annDict)
   834       702      19231.0     27.4      0.0              for k, value in st.annotations.items():
   835       663      23369.0     35.2      0.0                  if isinstance(getMetaData, Iterable):
   836                                                               # if selecting metadata fields, check that
   837                                                               # the key is in the provided list
   838                                                               if k not in getMetaData:
   839                                                                   continue
   840       663      17340.0     26.2      0.0                  if k not in skipAnnNames:
   841                                                               annDF.loc[:, k] = value
   842                                                       #
   843        39       1303.0     33.4      0.0              if isinstance(getMetaData, Iterable):
   844                                                           doNotFillList = idxLabels + ['feature', 'bin']
   845                                                           fieldsNeedFiller = [
   846                                                               mdn
   847                                                               for mdn in getMetaData
   848                                                               if (mdn not in doNotFillList) and (mdn not in annDF.columns)]
   849                                                           for mdName in fieldsNeedFiller:
   850                                                               annDF.loc[:, mdName] = 'NA'
   851        39      17937.0    459.9      0.0              annColumns = annDF.columns.to_list()
   852        39        974.0     25.0      0.0              if getMetaData:
   853       507      12008.0     23.7      0.0                  for annNm in annColumns:
   854       468      11731.0     25.1      0.0                      if annNm not in idxLabels:
   855       468      11528.0     24.6      0.0                          idxLabels.append(annNm)
   856                                                           # idxLabels += annColumns
   857        39    5823792.0 149328.0      6.0              spikeDF = annDF.join(wfDF)
   858                                                   else:
   859                                                       spikeDF = wfDF
   860                                                       del wfDF, st
   861        39    1477393.0  37881.9      1.5          spikeDF.loc[:, 'segment'] = segIdx
   862        39    1015368.0  26035.1      1.0          spikeDF.loc[:, 'originalIndex'] = spikeDF.index
   863        39       1388.0     35.6      0.0          spikeDF.columns.name = 'bin'
   864                                                   #
   865        39       2604.0     66.8      0.0          if dataQuery is not None:
   866        39   17761552.0 455424.4     18.2              spikeDF.query(dataQuery, inplace=True)
   867        39       2591.0     66.4      0.0              if not getMetaData:
   868                                                           spikeDF.drop(columns=annColumns, inplace=True)
   869        39       2007.0     51.5      0.0          waveformsList.append(spikeDF)
   870                                               #
   871        13    3330583.0 256198.7      3.4      zeroLagWaveformsDF = pd.concat(waveformsList, axis='index')
   872        13        472.0     36.3      0.0      if verbose:
   873                                                   prf.print_memory_usage('before transposing waveforms')
   874                                               # TODO implement lags and rolling window addition here
   875        13     538273.0  41405.6      0.6      metaDF = zeroLagWaveformsDF.loc[:, idxLabels].copy()
   876        13     798334.0  61410.3      0.8      zeroLagWaveformsDF.drop(columns=idxLabels, inplace=True)
   877        13        435.0     33.5      0.0      if lags is None:
   878        13        269.0     20.7      0.0          lags = [0]
   879        13        349.0     26.8      0.0      laggedWaveformsDict = {
   880        13        601.0     46.2      0.0          (spikeTrainContainer.name, k): None for k in lags}
   881        26        526.0     20.2      0.0      for lag in lags:
   882        13        303.0     23.3      0.0          if isinstance(lag, int):
   883        13        254.0     19.5      0.0              shiftedWaveform = zeroLagWaveformsDF.shift(
   884        13     473564.0  36428.0      0.5                  lag, axis='columns')
   885        13        407.0     31.3      0.0              if rollingWindow is not None:
   886                                                           halfRollingWin = int(np.ceil(rollingWindow/2))
   887                                                           seekIdx = slice(
   888                                                               halfRollingWin, -halfRollingWin+1, decimate)
   889                                                           # seekIdx = slice(None, None, decimate)
   890                                                           #shiftedWaveform = (
   891                                                           #    shiftedWaveform
   892                                                           #    .rolling(
   893                                                           #        window=rollingWindow, win_type='gaussian',
   894                                                           #        axis='columns', center=True)
   895                                                           #    .mean(std=halfRollingWin))
   896                                                           shiftedWaveform = (
   897                                                               shiftedWaveform
   898                                                               .rolling(
   899                                                                   window=rollingWindow, 
   900                                                                   axis='columns', center=True)
   901                                                               .mean())
   902                                                       else:
   903        13        230.0     17.7      0.0                  halfRollingWin = 0
   904        13        358.0     27.5      0.0                  seekIdx = slice(None, None, decimate)
   905                                                           if False:
   906                                                               oldShiftedWaveform = zeroLagWaveformsDF.shift(
   907                                                                   lag, axis='columns')
   908                                                               plt.plot(oldShiftedWaveform.iloc[0, :])
   909                                                               plt.plot(shiftedWaveform.iloc[0, :])
   910                                                               plt.show()
   911                                                       laggedWaveformsDict[
   912                                                           (spikeTrainContainer.name, lag)] = (
   913        13     513336.0  39487.4      0.5                      shiftedWaveform.iloc[:, seekIdx].copy())
   914        13        554.0     42.6      0.0          if isinstance(lag, tuple):
   915                                                       halfRollingWin = int(np.ceil(lag[1]/2))
   916                                                       seekIdx = slice(
   917                                                           halfRollingWin, -halfRollingWin+1, decimate)
   918                                                       # seekIdx = slice(None, None, decimate)
   919                                                       shiftedWaveform = (
   920                                                           zeroLagWaveformsDF
   921                                                           .shift(lag[0], axis='columns')
   922                                                           .rolling(
   923                                                               window=lag[1], win_type='gaussian',
   924                                                               axis='columns', center=True)
   925                                                           .mean(std=halfRollingWin))
   926                                                       laggedWaveformsDict[
   927                                                           (spikeTrainContainer.name, lag)] = (
   928                                                               shiftedWaveform.iloc[:, seekIdx].copy())
   929                                               #
   930        13        270.0     20.8      0.0      if transposeToColumns == 'feature':
   931                                                   # stack the bin, name the feature column
   932                                                   # 
   933        26        980.0     37.7      0.0          for idx, (key, value) in enumerate(laggedWaveformsDict.items()):
   934        13        265.0     20.4      0.0              if idx == 0:
   935        13        408.0     31.4      0.0                  stackedIndexDF = pd.concat(
   936        13    1012293.0  77868.7      1.0                      [metaDF, value], axis='columns')
   937        13    3275594.0 251968.8      3.4                  stackedIndexDF.set_index(idxLabels, inplace=True)
   938                                                           # don't drop nans for now - might need to keep track of them
   939                                                           # if we need to equalize to another array later
   940        13   23813104.0 1831777.2     24.5                  newIndex = stackedIndexDF.stack(dropna=False).index
   941        13        594.0     45.7      0.0                  idxLabels.append('bin')
   942        13    5498669.0 422974.5      5.6              laggedWaveformsDict[key] = value.stack(dropna=False).to_frame(name=key).reset_index(drop=True)
   943        13        462.0     35.5      0.0          waveformsDF = pd.concat(
   944        13        397.0     30.5      0.0              laggedWaveformsDict.values(),
   945        13     817147.0  62857.5      0.8              axis='columns')
   946        13       8436.0    648.9      0.0          waveformsDF.columns.names = ['feature', 'lag']
   947        13       2933.0    225.6      0.0          waveformsDF.index = newIndex
   948        13        423.0     32.5      0.0          waveformsDF.columns.name = 'feature'
   949                                               elif transposeToColumns == 'bin':
   950                                                   # add the feature column
   951                                                   waveformsDF = pd.concat(
   952                                                       laggedWaveformsDict,
   953                                                       names=['feature', 'lag', 'originalDummy']).reset_index()
   954                                                   waveformsDF = pd.concat(
   955                                                       [
   956                                                           metaDF.reset_index(drop=True),
   957                                                           waveformsDF.drop(columns='originalDummy')],
   958                                                       axis='columns')
   959                                                   idxLabels += ['feature', 'lag']
   960                                                   waveformsDF.columns.name = 'bin'
   961                                                   waveformsDF.set_index(idxLabels, inplace=True)
   962                                               #
   963        13        373.0     28.7      0.0      if transposeToColumns != waveformsDF.columns.name:
   964                                                   waveformsDF = transposeSpikeDF(
   965                                                       waveformsDF, transposeToColumns,
   966                                                       fastTranspose=fastTranspose)
   967        13        273.0     21.0      0.0      return waveformsDF

Total time: 21.2203 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: concatenateUnitSpikeTrainWaveformsDF at line 969

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   969                                           @profile
   970                                           def concatenateUnitSpikeTrainWaveformsDF(
   971                                                   units, dataQuery=None,
   972                                                   transposeToColumns='bin', concatOn='index',
   973                                                   fastTranspose=True, getMetaData=True, verbose=False,
   974                                                   addLags=None, decimate=1, rollingWindow=None,
   975                                                   metaDataToCategories=False, windowSize=None,
   976                                                   whichSegments=None, procFun=None):
   977         1          8.0      8.0      0.0      allUnits = []
   978        14        326.0     23.3      0.0      for thisUnit in units:
   979        13        777.0     59.8      0.0          hasAnySpikes = []
   980        91       2629.0     28.9      0.0          for stIn in thisUnit.spiketrains:
   981        78       2227.0     28.6      0.0              if isinstance(stIn, SpikeTrainProxy):
   982                                                           st = stIn.load(
   983                                                               magnitude_mode='rescaled',
   984                                                               load_waveforms=False)
   985                                                       else:
   986        78       1808.0     23.2      0.0                  st = stIn
   987        78     104224.0   1336.2      0.0              hasAnySpikes.append(st.times.any())
   988        13       5456.0    419.7      0.0          if np.any(hasAnySpikes):
   989        13        407.0     31.3      0.0              allUnits.append(thisUnit)
   990         1         25.0     25.0      0.0      waveformsList = []
   991        14        304.0     21.7      0.0      for idx, thisUnit in enumerate(allUnits):
   992        13        176.0     13.5      0.0          if verbose:
   993                                                       print('concatenating unitDF {}'.format(thisUnit.name))
   994        13        170.0     13.1      0.0          lags = None
   995        13        179.0     13.8      0.0          if addLags is not None:
   996                                                       if thisUnit.name in addLags:
   997                                                           lags = addLags[thisUnit.name]
   998        13        252.0     19.4      0.0          unitWaveforms = unitSpikeTrainWaveformsToDF(
   999        13        201.0     15.5      0.0              thisUnit, dataQuery=dataQuery,
  1000        13        181.0     13.9      0.0              transposeToColumns=transposeToColumns,
  1001        13        205.0     15.8      0.0              fastTranspose=fastTranspose, getMetaData=getMetaData,
  1002        13        169.0     13.0      0.0              lags=lags, decimate=decimate, rollingWindow=rollingWindow,
  1003        13        171.0     13.2      0.0              verbose=verbose, windowSize=windowSize,
  1004        13   98993650.0 7614896.2     46.7              whichSegments=whichSegments, procFun=procFun)
  1005        13        434.0     33.4      0.0          if idx == 0:
  1006         1        189.0    189.0      0.0              idxLabels = unitWaveforms.index.names
  1007        13        252.0     19.4      0.0          if (concatOn == 'columns') and (idx > 0):
  1008                                                       # other than first time, we already have the metadata
  1009        12     881861.0  73488.4      0.4              unitWaveforms.reset_index(drop=True, inplace=True)
  1010                                                   else:
  1011                                                       # first time, or if concatenating indices,
  1012                                                       # keep the the metadata
  1013         1    6809192.0 6809192.0      3.2              unitWaveforms.reset_index(inplace=True)
  1014         1         53.0     53.0      0.0              if metaDataToCategories:
  1015                                                           # convert metadata to categoricals to free memory
  1016                                                           #
  1017                                                           unitWaveforms[idxLabels] = (
  1018                                                               unitWaveforms[idxLabels]
  1019                                                               .astype('category')
  1020                                                               )
  1021        13        407.0     31.3      0.0          waveformsList.append(unitWaveforms)
  1022        13        174.0     13.4      0.0          del unitWaveforms
  1023        13        160.0     12.3      0.0          if verbose:
  1024                                                       print('memory usage: {:.1f} MB'.format(prf.memory_usage_psutil()))
  1025         1          8.0      8.0      0.0      if verbose:
  1026                                                   print(
  1027                                                       'about to join all, memory usage: {:.1f} MB'
  1028                                                       .format(prf.memory_usage_psutil()))
  1029                                               #  if concatenating indexes, reset the index of the result
  1030                                               #  ignoreIndex = (concatOn == 'index')
  1031         1         22.0     22.0      0.0      allWaveforms = pd.concat(
  1032         1    5909613.0 5909613.0      2.8          waveformsList, axis=concatOn,
  1033                                                   # ignore_index=ignoreIndex
  1034                                                   )
  1035         1     873039.0 873039.0      0.4      del waveformsList
  1036         1         24.0     24.0      0.0      if verbose:
  1037                                                   print(
  1038                                                       'finished concatenating, memory usage: {:.1f} MB'
  1039                                                       .format(prf.memory_usage_psutil()))
  1040         1         15.0     15.0      0.0      try:
  1041         1   32928384.0 32928384.0     15.5          allWaveforms.set_index(idxLabels, inplace=True)
  1042         1         48.0     48.0      0.0          allWaveforms.sort_index(
  1043         1         13.0     13.0      0.0              level=['segment', 'originalIndex', 't'],
  1044         1   64928392.0 64928392.0     30.6              axis='index', inplace=True, kind='mergesort')
  1045         1         38.0     38.0      0.0          allWaveforms.sort_index(
  1046         1     756902.0 756902.0      0.4              axis='columns', inplace=True, kind='mergesort')
  1047                                               except Exception:
  1048                                                   pdb.set_trace()
  1049         1         17.0     17.0      0.0      return allWaveforms

Total time: 21.3135 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: alignedAsigsToDF at line 1051

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1051                                           @profile
  1052                                           def alignedAsigsToDF(
  1053                                                   dataBlock, unitNames=None,
  1054                                                   unitQuery=None, dataQuery=None,
  1055                                                   collapseSizes=False, verbose=False,
  1056                                                   duplicateControlsByProgram=False,
  1057                                                   amplitudeColumn='amplitude',
  1058                                                   programColumn='program',
  1059                                                   electrodeColumn='electrode',
  1060                                                   transposeToColumns='bin', concatOn='index', fastTranspose=True,
  1061                                                   addLags=None, decimate=1, rollingWindow=None,
  1062                                                   whichSegments=None, windowSize=None,
  1063                                                   getMetaData=True, metaDataToCategories=True,
  1064                                                   outlierTrials=None, invertOutlierMask=False,
  1065                                                   makeControlProgram=False, removeFuzzyName=False, procFun=None):
  1066                                               #  channels to trigger
  1067         1         18.0     18.0      0.0      if unitNames is None:
  1068         1      59391.0  59391.0      0.0          unitNames = listChanNames(dataBlock, unitQuery, objType=Unit)
  1069         1         15.0     15.0      0.0      allUnits = []
  1070        14        228.0     16.3      0.0      for uName in unitNames:
  1071        13     113090.0   8699.2      0.1          allUnits += dataBlock.filter(objects=Unit, name=uName)
  1072         1         17.0     17.0      0.0      allWaveforms = concatenateUnitSpikeTrainWaveformsDF(
  1073         1         16.0     16.0      0.0          allUnits, dataQuery=dataQuery,
  1074         1         14.0     14.0      0.0          transposeToColumns=transposeToColumns, concatOn=concatOn,
  1075         1         16.0     16.0      0.0          fastTranspose=fastTranspose,
  1076         1         14.0     14.0      0.0          addLags=addLags, decimate=decimate, rollingWindow=rollingWindow,
  1077         1         14.0     14.0      0.0          verbose=verbose, whichSegments=whichSegments,
  1078         1         15.0     15.0      0.0          windowSize=windowSize, procFun=procFun,
  1079         1  212219501.0 212219501.0     99.6          getMetaData=getMetaData, metaDataToCategories=metaDataToCategories)
  1080                                               #
  1081         1         34.0     34.0      0.0      manipulateIndex = np.any(
  1082                                                   [
  1083         1         16.0     16.0      0.0              collapseSizes, duplicateControlsByProgram,
  1084         1        433.0    433.0      0.0              makeControlProgram, removeFuzzyName
  1085                                                       ])
  1086         1         17.0     17.0      0.0      if outlierTrials is not None:
  1087                                                   def rejectionLookup(entry):
  1088                                                       key = []
  1089                                                       for subKey in outlierTrials.index.names:
  1090                                                           keyIdx = allWaveforms.index.names.index(subKey)
  1091                                                           key.append(entry[keyIdx])
  1092                                                       # print(key)
  1093                                                       # outlierTrials.iloc[1, :]
  1094                                                       # allWaveforms.iloc[1, :]
  1095                                                       return outlierTrials[tuple(key)]
  1096                                                   #
  1097                                                   outlierMask = np.asarray(
  1098                                                       allWaveforms.index.map(rejectionLookup),
  1099                                                       dtype=np.bool)
  1100                                                   if invertOutlierMask:
  1101                                                       outlierMask = ~outlierMask
  1102                                                   allWaveforms = allWaveforms.loc[~outlierMask, :]
  1103         1         20.0     20.0      0.0      if manipulateIndex and getMetaData:
  1104                                                   idxLabels = allWaveforms.index.names
  1105                                                   allWaveforms.reset_index(inplace=True)
  1106                                                   # 
  1107                                                   if collapseSizes:
  1108                                                       try:
  1109                                                           allWaveforms.loc[allWaveforms['pedalSizeCat'] == 'XL', 'pedalSizeCat'] = 'L'
  1110                                                           allWaveforms.loc[allWaveforms['pedalSizeCat'] == 'XS', 'pedalSizeCat'] = 'S'
  1111                                                       except Exception:
  1112                                                           traceback.print_exc()
  1113                                                   if makeControlProgram:
  1114                                                       try:
  1115                                                           allWaveforms.loc[allWaveforms[amplitudeColumn] == 0, programColumn] = 999
  1116                                                           allWaveforms.loc[allWaveforms[amplitudeColumn] == 0, electrodeColumn] = 'control'
  1117                                                       except Exception:
  1118                                                           traceback.print_exc()
  1119                                                   if duplicateControlsByProgram:
  1120                                                       #
  1121                                                       noStimWaveforms = (
  1122                                                           allWaveforms
  1123                                                           .loc[allWaveforms[amplitudeColumn] == 0, :]
  1124                                                           )
  1125                                                       stimWaveforms = (
  1126                                                           allWaveforms
  1127                                                           .loc[allWaveforms[amplitudeColumn] != 0, :]
  1128                                                           .copy()
  1129                                                           )
  1130                                                       uniqProgs = stimWaveforms[programColumn].unique()
  1131                                                       progElecLookup = {}
  1132                                                       #pdb.set_trace()
  1133                                                       for progIdx in uniqProgs:
  1134                                                           theseStimDF = stimWaveforms.loc[
  1135                                                               stimWaveforms[programColumn] == progIdx,
  1136                                                               electrodeColumn]
  1137                                                           elecIdx = theseStimDF.iloc[0]
  1138                                                           progElecLookup.update({progIdx: elecIdx})
  1139                                                       #
  1140                                                       if makeControlProgram:
  1141                                                           uniqProgs = np.append(uniqProgs, 999)
  1142                                                           progElecLookup.update({999: 'control'})
  1143                                                       #
  1144                                                       for progIdx in uniqProgs:
  1145                                                           dummyWaveforms = noStimWaveforms.copy()
  1146                                                           dummyWaveforms.loc[:, programColumn] = progIdx
  1147                                                           dummyWaveforms.loc[:, electrodeColumn] = progElecLookup[progIdx]
  1148                                                           stimWaveforms = pd.concat([stimWaveforms, dummyWaveforms])
  1149                                                       stimWaveforms.reset_index(drop=True, inplace=True)
  1150                                                       allWaveforms = stimWaveforms
  1151                                                   #
  1152                                                   if removeFuzzyName:
  1153                                                       fuzzyNamesBase = [
  1154                                                           i.replace('Fuzzy', '')
  1155                                                           for i in idxLabels
  1156                                                           if 'Fuzzy' in i]
  1157                                                       colRenamer = {n + 'Fuzzy': n for n in fuzzyNamesBase}
  1158                                                       fuzzyNamesBasePresent = [
  1159                                                           i
  1160                                                           for i in fuzzyNamesBase
  1161                                                           if i in allWaveforms.columns]
  1162                                                       allWaveforms.drop(columns=fuzzyNamesBasePresent, inplace=True)
  1163                                                       allWaveforms.rename(columns=colRenamer, inplace=True)
  1164                                                       idxLabels = np.unique(
  1165                                                           [i.replace('Fuzzy', '') for i in idxLabels])
  1166                                                   #
  1167                                                   allWaveforms.set_index(
  1168                                                       list(idxLabels),
  1169                                                       inplace=True)
  1170                                                   if isinstance(allWaveforms.columns, pd.MultiIndex):
  1171                                                       allWaveforms.columns = allWaveforms.columns.remove_unused_levels()
  1172                                               #
  1173         1         17.0     17.0      0.0      if transposeToColumns == 'feature':
  1174         1       5624.0   5624.0      0.0          zipNames = zip(pd.unique(allWaveforms.columns.get_level_values('feature')).tolist(), unitNames)
  1175         1         19.0     19.0      0.0          try:
  1176         1        347.0    347.0      0.0              assert np.all([i == j for i, j in zipNames]), 'columns out of requested order!'
  1177                                                   except Exception:
  1178                                                       traceback.print_exc()
  1179                                                       allWaveforms.reindex(columns=unitNames)
  1180         1         37.0     37.0      0.0      if isinstance(allWaveforms.columns, pd.MultiIndex):
  1181         1       9395.0   9395.0      0.0          allWaveforms.columns = allWaveforms.columns.remove_unused_levels()
  1182         1         35.0     35.0      0.0      allWaveforms.sort_index(
  1183         1     726862.0 726862.0      0.3          axis='columns', inplace=True, kind='mergesort')
  1184         1         23.0     23.0      0.0      return allWaveforms

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getAsigsAlignedToEvents at line 1186

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1186                                           @profile
  1187                                           def getAsigsAlignedToEvents(
  1188                                                   eventBlock=None, signalBlock=None,
  1189                                                   chansToTrigger=None, chanQuery=None,
  1190                                                   eventName=None, windowSize=None,
  1191                                                   minNReps=None,
  1192                                                   appendToExisting=False,
  1193                                                   checkReferences=True, verbose=False,
  1194                                                   fileName=None, folderPath=None, chunkSize=None
  1195                                                   ):
  1196                                               #  get signals from same block as events?
  1197                                               if signalBlock is None:
  1198                                                   signalBlock = eventBlock
  1199                                               #  channels to trigger
  1200                                               if chansToTrigger is None:
  1201                                                   chansToTrigger = listChanNames(
  1202                                                       signalBlock, chanQuery, objType=ChannelIndex, condition='hasAsigs')
  1203                                               #  allocate block for spiketrains
  1204                                               masterBlock = Block()
  1205                                               try:
  1206                                                   masterBlock.name = signalBlock.annotations['neo_name']
  1207                                                   masterBlock.annotate(nix_name=signalBlock.annotations['neo_name'])
  1208                                               except Exception:
  1209                                                   masterBlock.name = signalBlock.name
  1210                                                   masterBlock.annotate(neo_name=signalBlock.name)
  1211                                                   masterBlock.annotate(nix_name=signalBlock.name)
  1212                                               #  make channels and units for triggered time series
  1213                                               for chanName in chansToTrigger:
  1214                                                   chanIdx = ChannelIndex(name=chanName + '#0', index=[0])
  1215                                                   chanIdx.annotate(nix_name=chanIdx.name)
  1216                                                   thisUnit = Unit(name=chanIdx.name)
  1217                                                   thisUnit.annotate(nix_name=chanIdx.name)
  1218                                                   chanIdx.units.append(thisUnit)
  1219                                                   thisUnit.channel_index = chanIdx
  1220                                                   masterBlock.channel_indexes.append(chanIdx)
  1221                                                   sigChanIdxList = signalBlock.filter(
  1222                                                       objects=ChannelIndex, name=chanName)
  1223                                                   if len(sigChanIdxList):
  1224                                                       sigChanIdx = sigChanIdxList[0]
  1225                                                       if sigChanIdx.coordinates is not None:
  1226                                                           coordUnits = sigChanIdx.coordinates[0][0].units
  1227                                                           chanIdx.coordinates = np.asarray(sigChanIdx.coordinates) * coordUnits
  1228                                                           thisUnit.annotations['parentChanXCoords'] = float(chanIdx.coordinates[:, 0].magnitude)
  1229                                                           thisUnit.annotations['parentChanYCoords'] = float(chanIdx.coordinates[:, 1].magnitude)
  1230                                                           thisUnit.annotations['parentChanCoordinateUnits'] = '{}'.format(coordUnits)
  1231                                               #
  1232                                               totalNSegs = 0
  1233                                               #  print([evSeg.events[3].name for evSeg in eventBlock.segments])
  1234                                               allAlignEventsList = []
  1235                                               for segIdx, eventSeg in enumerate(eventBlock.segments):
  1236                                                   thisEventName = 'seg{}_{}'.format(segIdx, eventName)
  1237                                                   try:
  1238                                                       assert len(eventSeg.filter(name=thisEventName)) == 1
  1239                                                   except Exception:
  1240                                                       traceback.print_exc()
  1241                                                   allEvIn = eventSeg.filter(name=thisEventName)[0]
  1242                                                   if isinstance(allEvIn, EventProxy):
  1243                                                       allAlignEvents = loadObjArrayAnn(allEvIn.load())
  1244                                                   elif isinstance(allEvIn, Event):
  1245                                                       allAlignEvents = allEvIn
  1246                                                   else:
  1247                                                       raise(Exception(
  1248                                                           '{} must be an Event or EventProxy!'
  1249                                                           .format(eventName)))
  1250                                                   allAlignEventsList.append(allAlignEvents)
  1251                                               allAlignEventsDF = unitSpikeTrainArrayAnnToDF(allAlignEventsList)
  1252                                               #
  1253                                               breakDownData = (
  1254                                                   allAlignEventsDF
  1255                                                   .groupby(minNReps['categories'])
  1256                                                   .agg('count')
  1257                                                   .iloc[:, 0]
  1258                                                   )
  1259                                               try:
  1260                                                   breakDownData[breakDownData > minNReps['n']].to_csv(
  1261                                                       os.path.join(
  1262                                                           folderPath, 'numRepetitionsEachCondition.csv'
  1263                                                       ), header=True
  1264                                                   )
  1265                                               except Exception:
  1266                                                   traceback.print_exc()
  1267                                               allAlignEventsDF.loc[:, 'keepMask'] = False
  1268                                               for name, group in allAlignEventsDF.groupby(minNReps['categories']):
  1269                                                   allAlignEventsDF.loc[group.index, 'keepMask'] = (
  1270                                                       breakDownData[name] > minNReps['n'])
  1271                                               for segIdx, group in allAlignEventsDF.groupby('segment'):
  1272                                                   allAlignEventsList[segIdx].array_annotations['keepMask'] = group['keepMask'].to_numpy()
  1273                                               #
  1274                                               for segIdx, eventSeg in enumerate(eventBlock.segments):
  1275                                                   if verbose:
  1276                                                       print(
  1277                                                           'getAsigsAlignedToEvents on segment {} of {}'
  1278                                                           .format(segIdx + 1, len(eventBlock.segments)))
  1279                                                   allAlignEvents = allAlignEventsList[segIdx]
  1280                                                   if chunkSize is None:
  1281                                                       alignEventGroups = [allAlignEvents]
  1282                                                   else:
  1283                                                       nChunks = max(
  1284                                                           int(np.floor(allAlignEvents.shape[0] / chunkSize)),
  1285                                                           1)
  1286                                                       alignEventGroups = []
  1287                                                       for i in range(nChunks):
  1288                                                           if not (i == (nChunks - 1)):
  1289                                                               # not last one
  1290                                                               alignEventGroups.append(
  1291                                                                   allAlignEvents[i * chunkSize: (i + 1) * chunkSize])
  1292                                                           else:
  1293                                                               alignEventGroups.append(
  1294                                                                   allAlignEvents[i * chunkSize:])
  1295                                                   signalSeg = signalBlock.segments[segIdx]
  1296                                                   for subSegIdx, alignEvents in enumerate(alignEventGroups):
  1297                                                       # seg to contain triggered time series
  1298                                                       if verbose:
  1299                                                           print(
  1300                                                               'getAsigsAlignedToEvents on subSegment {} of {}'
  1301                                                               .format(subSegIdx + 1, len(alignEventGroups)))
  1302                                                       if not alignEvents.shape[0] > 0:
  1303                                                           continue
  1304                                                       newSeg = Segment(name='seg{}_'.format(int(totalNSegs)))
  1305                                                       newSeg.annotate(nix_name=newSeg.name)
  1306                                                       masterBlock.segments.append(newSeg)
  1307                                                       for chanName in chansToTrigger:
  1308                                                           asigName = 'seg{}_{}'.format(segIdx, chanName)
  1309                                                           if verbose:
  1310                                                               print(
  1311                                                                   'getAsigsAlignedToEvents on channel {}'
  1312                                                                   .format(chanName))
  1313                                                           assert len(signalSeg.filter(name=asigName)) == 1
  1314                                                           asig = signalSeg.filter(name=asigName)[0]
  1315                                                           nominalWinLen = int(
  1316                                                               (windowSize[1] - windowSize[0]) *
  1317                                                               asig.sampling_rate - 1)
  1318                                                           validMask = (
  1319                                                               ((
  1320                                                                   alignEvents + windowSize[1] +
  1321                                                                   asig.sampling_rate ** (-1)) < asig.t_stop) &
  1322                                                               ((
  1323                                                                   alignEvents + windowSize[0] -
  1324                                                                   asig.sampling_rate ** (-1)) > asig.t_start)
  1325                                                               )
  1326                                                           thisKeepMask = alignEvents.array_annotations['keepMask']
  1327                                                           fullMask = (validMask & thisKeepMask)
  1328                                                           alignEvents = alignEvents[fullMask]
  1329                                                           # array_annotations get sliced with the event, but regular anns do not
  1330                                                           for annName in alignEvents.annotations['arrayAnnNames']:
  1331                                                               alignEvents.annotations[annName] = (
  1332                                                                   alignEvents.array_annotations[annName])
  1333                                                           if isinstance(asig, AnalogSignalProxy):
  1334                                                               if checkReferences:
  1335                                                                   da = (
  1336                                                                       asig
  1337                                                                       ._rawio
  1338                                                                       .da_list['blocks'][0]['segments'][segIdx]['data'])
  1339                                                                   print('segIdx {}, asig.name {}'.format(
  1340                                                                       segIdx, asig.name))
  1341                                                                   print('asig._global_channel_indexes = {}'.format(
  1342                                                                       asig._global_channel_indexes))
  1343                                                                   print('asig references {}'.format(
  1344                                                                       da[asig._global_channel_indexes[0]]))
  1345                                                                   try:
  1346                                                                       assert (
  1347                                                                           asig.name
  1348                                                                           in da[asig._global_channel_indexes[0]].name)
  1349                                                                   except Exception:
  1350                                                                       traceback.print_exc()
  1351                                                               rawWaveforms = [
  1352                                                                   asig.load(
  1353                                                                       time_slice=(t + windowSize[0], t + windowSize[1]))
  1354                                                                   for t in alignEvents]
  1355                                                               if any([rW.shape[0] < nominalWinLen for rW in rawWaveforms]):
  1356                                                                   rawWaveforms = [
  1357                                                                       asig.load(
  1358                                                                           time_slice=(t + windowSize[0], t + windowSize[1] + asig.sampling_period))
  1359                                                                       for t in alignEvents]
  1360                                                           elif isinstance(asig, AnalogSignal):
  1361                                                               rawWaveforms = []
  1362                                                               for t in alignEvents:
  1363                                                                   asigMask = (asig.times > t + windowSize[0]) & (asig.times < t + windowSize[1])
  1364                                                                   rawWaveforms.append(asig[asigMask[:, np.newaxis]])
  1365                                                           else:
  1366                                                               raise(Exception('{} must be an AnalogSignal or AnalogSignalProxy!'.format(asigName)))
  1367                                                           #
  1368                                                           samplingRate = asig.sampling_rate
  1369                                                           waveformUnits = rawWaveforms[0].units
  1370                                                           #  fix length if roundoff error
  1371                                                           #  minLen = min([rW.shape[0] for rW in rawWaveforms])
  1372                                                           rawWaveforms = [rW[:nominalWinLen] for rW in rawWaveforms]
  1373                                                           #
  1374                                                           spikeWaveforms = (
  1375                                                               np.hstack([rW.magnitude for rW in rawWaveforms])
  1376                                                               .transpose()[:, np.newaxis, :] * waveformUnits
  1377                                                               )
  1378                                                           #
  1379                                                           thisUnit = masterBlock.filter(
  1380                                                               objects=Unit, name=chanName + '#0')[0]
  1381                                                           skipEventAnnNames = (
  1382                                                               ['nix_name', 'neo_name']
  1383                                                               )
  1384                                                           stAnn = {
  1385                                                               k: v
  1386                                                               for k, v in alignEvents.annotations.items()
  1387                                                               if k not in skipEventAnnNames
  1388                                                               }
  1389                                                           skipAsigAnnNames = (
  1390                                                               ['channel_id', 'nix_name', 'neo_name']
  1391                                                               )
  1392                                                           stAnn.update({
  1393                                                               k: v
  1394                                                               for k, v in asig.annotations.items()
  1395                                                               if k not in skipAsigAnnNames
  1396                                                           })
  1397                                                           st = SpikeTrain(
  1398                                                               name='seg{}_{}'.format(int(totalNSegs), thisUnit.name),
  1399                                                               times=alignEvents.times,
  1400                                                               waveforms=spikeWaveforms,
  1401                                                               t_start=asig.t_start, t_stop=asig.t_stop,
  1402                                                               left_sweep=windowSize[0] * (-1),
  1403                                                               sampling_rate=samplingRate,
  1404                                                               **stAnn
  1405                                                               )
  1406                                                           st.annotate(nix_name=st.name)
  1407                                                           st.annotations['unitAnnotations'] = json.dumps(
  1408                                                               thisUnit.annotations.copy())
  1409                                                           thisUnit.spiketrains.append(st)
  1410                                                           newSeg.spiketrains.append(st)
  1411                                                           st.unit = thisUnit
  1412                                                       totalNSegs += 1
  1413                                               try:
  1414                                                   eventBlock.filter(
  1415                                                       objects=EventProxy)[0]._rawio.file.close()
  1416                                               except Exception:
  1417                                                   traceback.print_exc()
  1418                                               if signalBlock is not eventBlock:
  1419                                                   try:
  1420                                                       signalBlock.filter(
  1421                                                           objects=AnalogSignalProxy)[0]._rawio.file.close()
  1422                                                   except Exception:
  1423                                                       traceback.print_exc()
  1424                                               triggeredPath = os.path.join(
  1425                                                   folderPath, fileName + '.nix')
  1426                                               if not os.path.exists(triggeredPath):
  1427                                                   appendToExisting = False
  1428                                           
  1429                                               if appendToExisting:
  1430                                                   allSegs = list(range(len(masterBlock.segments)))
  1431                                                   addBlockToNIX(
  1432                                                       masterBlock, neoSegIdx=allSegs,
  1433                                                       writeSpikes=True,
  1434                                                       fileName=fileName,
  1435                                                       folderPath=folderPath,
  1436                                                       purgeNixNames=False,
  1437                                                       nixBlockIdx=0, nixSegIdx=allSegs)
  1438                                               else:
  1439                                                   if os.path.exists(triggeredPath):
  1440                                                       os.remove(triggeredPath)
  1441                                                   masterBlock = purgeNixAnn(masterBlock)
  1442                                                   writer = NixIO(filename=triggeredPath)
  1443                                                   writer.write_block(masterBlock, use_obj_names=True)
  1444                                                   writer.close()
  1445                                               return masterBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: alignedAsigDFtoSpikeTrain at line 1447

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1447                                           @profile
  1448                                           def alignedAsigDFtoSpikeTrain(
  1449                                                   allWaveforms, dataBlock=None, matchSamplingRate=True):
  1450                                               masterBlock = Block()
  1451                                               masterBlock.name = dataBlock.annotations['neo_name']
  1452                                               masterBlock.annotate(nix_name=dataBlock.annotations['neo_name'])
  1453                                               for segIdx, group in allWaveforms.groupby('segment'):
  1454                                                   print('Saving trajectoriess for segment {}'.format(segIdx))
  1455                                                   dataSeg = dataBlock.segments[segIdx]
  1456                                                   exSt = dataSeg.spiketrains[0]
  1457                                                   if isinstance(exSt, SpikeTrainProxy):
  1458                                                       print(
  1459                                                           'alignedAsigDFtoSpikeTrain basing seg {} on {}'
  1460                                                           .format(segIdx, exSt.name))
  1461                                                       stProxy = exSt
  1462                                                       exSt = loadStProxy(stProxy)
  1463                                                       exSt = loadObjArrayAnn(exSt)
  1464                                                   print('exSt.left_sweep is {}'.format(exSt.left_sweep))
  1465                                                   wfBins = ((np.arange(exSt.waveforms.shape[2]) / (exSt.sampling_rate)) - exSt.left_sweep).magnitude
  1466                                                   # seg to contain triggered time series
  1467                                                   newSeg = Segment(name=dataSeg.annotations['neo_name'])
  1468                                                   newSeg.annotate(nix_name=dataSeg.annotations['neo_name'])
  1469                                                   masterBlock.segments.append(newSeg)
  1470                                                   #
  1471                                                   if group.columns.name == 'bin':
  1472                                                       grouper = group.groupby('feature')
  1473                                                       colsAre = 'bin'
  1474                                                   elif group.columns.name == 'feature':
  1475                                                       grouper = group.iteritems()
  1476                                                       colsAre = 'feature'
  1477                                                   for featName, featGroup in grouper:
  1478                                                       print('Saving {}...'.format(featName))
  1479                                                       if featName[-2:] == '#0':
  1480                                                           cleanFeatName = featName
  1481                                                       else:
  1482                                                           cleanFeatName = featName + '#0'
  1483                                                       if segIdx == 0:
  1484                                                           #  allocate units
  1485                                                           chanIdx = ChannelIndex(
  1486                                                               name=cleanFeatName, index=[0])
  1487                                                           chanIdx.annotate(nix_name=chanIdx.name)
  1488                                                           thisUnit = Unit(name=chanIdx.name)
  1489                                                           thisUnit.annotate(nix_name=chanIdx.name)
  1490                                                           chanIdx.units.append(thisUnit)
  1491                                                           thisUnit.channel_index = chanIdx
  1492                                                           masterBlock.channel_indexes.append(chanIdx)
  1493                                                       else:
  1494                                                           thisUnit = masterBlock.filter(
  1495                                                               objects=Unit, name=cleanFeatName)[0]
  1496                                                       if colsAre == 'bin':
  1497                                                           spikeWaveformsDF = featGroup
  1498                                                       elif colsAre == 'feature':
  1499                                                           if isinstance(featGroup, pd.Series):
  1500                                                               featGroup = featGroup.to_frame(name=featName)
  1501                                                               featGroup.columns.name = 'feature'
  1502                                                           spikeWaveformsDF = transposeSpikeDF(
  1503                                                               featGroup,
  1504                                                               'bin', fastTranspose=True)
  1505                                                       if matchSamplingRate:
  1506                                                           if len(spikeWaveformsDF.columns) != len(wfBins):
  1507                                                               wfDF = spikeWaveformsDF.reset_index(drop=True).T
  1508                                                               wfDF = hf.interpolateDF(wfDF, wfBins)
  1509                                                               spikeWaveformsDF = wfDF.T.set_index(spikeWaveformsDF.index)
  1510                                                       spikeWaveforms = spikeWaveformsDF.to_numpy()[:, np.newaxis, :]
  1511                                                       arrAnnDF = spikeWaveformsDF.index.to_frame()
  1512                                                       spikeTimes = arrAnnDF['t']
  1513                                                       arrAnnDF.drop(columns='t', inplace=True)
  1514                                                       arrAnn = {}
  1515                                                       colsToKeep = arrAnnDF.columns.drop(['originalIndex', 'feature', 'segment', 'lag'])
  1516                                                       for cName in colsToKeep:
  1517                                                           values = arrAnnDF[cName].to_numpy()
  1518                                                           if isinstance(values[0], str):
  1519                                                               values = values.astype('U')
  1520                                                           arrAnn.update({str(cName): values.flatten()})
  1521                                                       arrayAnnNames = {
  1522                                                           'arrayAnnNames': list(arrAnn.keys())}
  1523                                                       st = SpikeTrain(
  1524                                                           name='seg{}_{}'.format(int(segIdx), thisUnit.name),
  1525                                                           times=spikeTimes.to_numpy() * exSt.units,
  1526                                                           waveforms=spikeWaveforms * pq.dimensionless,
  1527                                                           t_start=exSt.t_start, t_stop=exSt.t_stop,
  1528                                                           left_sweep=exSt.left_sweep,
  1529                                                           sampling_rate=exSt.sampling_rate,
  1530                                                           **arrAnn, **arrayAnnNames
  1531                                                           )
  1532                                                       st.annotate(nix_name=st.name)
  1533                                                       thisUnit.spiketrains.append(st)
  1534                                                       newSeg.spiketrains.append(st)
  1535                                                       st.unit = thisUnit
  1536                                               return masterBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: dataFrameToAnalogSignals at line 1538

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1538                                           @profile
  1539                                           def dataFrameToAnalogSignals(
  1540                                                   df,
  1541                                                   block=None, seg=None,
  1542                                                   idxT='NSPTime',
  1543                                                   probeName='insTD', samplingRate=500*pq.Hz,
  1544                                                   timeUnits=pq.s, measureUnits=pq.mV,
  1545                                                   dataCol=['channel_0', 'channel_1'],
  1546                                                   useColNames=False, forceColNames=None,
  1547                                                   namePrefix='', nameSuffix='', verbose=False):
  1548                                               if block is None:
  1549                                                   assert seg is None
  1550                                                   block = Block(name=probeName)
  1551                                                   seg = Segment(name='seg0_' + probeName)
  1552                                                   block.segments.append(seg)
  1553                                               if verbose:
  1554                                                   print('in dataFrameToAnalogSignals...')
  1555                                               for idx, colName in enumerate(dataCol):
  1556                                                   if verbose:
  1557                                                       print('    {}'.format(colName))
  1558                                                   if forceColNames is not None:
  1559                                                       chanName = forceColNames[idx]
  1560                                                   elif useColNames:
  1561                                                       chanName = namePrefix + colName + nameSuffix
  1562                                                   else:
  1563                                                       chanName = namePrefix + (probeName.lower() + '{}'.format(idx)) + nameSuffix
  1564                                                   #
  1565                                                   chanIdx = ChannelIndex(
  1566                                                       name=chanName,
  1567                                                       # index=None,
  1568                                                       index=np.asarray([idx]),
  1569                                                       # channel_names=np.asarray([chanName])
  1570                                                       )
  1571                                                   block.channel_indexes.append(chanIdx)
  1572                                                   asig = AnalogSignal(
  1573                                                       df[colName].to_numpy() * measureUnits,
  1574                                                       name='seg0_' + chanName,
  1575                                                       sampling_rate=samplingRate,
  1576                                                       dtype=np.float32,
  1577                                                       # **ann
  1578                                                       )
  1579                                                   if idxT is not None:
  1580                                                       asig.t_start = df[idxT].iloc[0] * timeUnits
  1581                                                   else:
  1582                                                       asig.t_start = df.index[0] * timeUnits
  1583                                                   asig.channel_index = chanIdx
  1584                                                   # assign ownership to containers
  1585                                                   chanIdx.analogsignals.append(asig)
  1586                                                   seg.analogsignals.append(asig)
  1587                                                   chanIdx.create_relationship()
  1588                                               # assign parent to children
  1589                                               block.create_relationship()
  1590                                               seg.create_relationship()
  1591                                               return block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: eventDataFrameToEvents at line 1593

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1593                                           @profile
  1594                                           def eventDataFrameToEvents(
  1595                                                   eventDF, idxT=None,
  1596                                                   annCol=None,
  1597                                                   eventName='', tUnits=pq.s,
  1598                                                   makeList=True
  1599                                                   ):
  1600                                               if makeList:
  1601                                                   eventList = []
  1602                                                   for colName in annCol:
  1603                                                       originalDType = type(eventDF[colName].to_numpy()[0]).__name__
  1604                                                       event = Event(
  1605                                                           name=eventName + colName,
  1606                                                           times=eventDF[idxT].to_numpy() * tUnits,
  1607                                                           labels=eventDF[colName].astype(originalDType).to_numpy()
  1608                                                           )
  1609                                                       event.annotate(originalDType=originalDType)
  1610                                                       eventList.append(event)
  1611                                                   return eventList
  1612                                               else:
  1613                                                   if annCol is None:
  1614                                                       annCol = eventDF.drop(columns=idxT).columns
  1615                                                   event = Event(
  1616                                                       name=eventName,
  1617                                                       times=eventDF[idxT].to_numpy() * tUnits,
  1618                                                       labels=np.asarray(eventDF.index)
  1619                                                       )
  1620                                                   event.annotations.update(
  1621                                                       {
  1622                                                           'arrayAnnNames': [],
  1623                                                           'arrayAnnDTypes': []
  1624                                                           })
  1625                                                   for colName in annCol:
  1626                                                       originalDType = type(eventDF[colName].to_numpy()[0]).__name__
  1627                                                       arrayAnn = eventDF[colName].astype(originalDType).to_numpy()
  1628                                                       event.array_annotations.update(
  1629                                                           {colName: arrayAnn})
  1630                                                       event.annotations['arrayAnnNames'].append(colName)
  1631                                                       event.annotations['arrayAnnDTypes'].append(originalDType)
  1632                                                       event.annotations.update(
  1633                                                           {colName: arrayAnn})
  1634                                                   return event

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: eventsToDataFrame at line 1636

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1636                                           @profile
  1637                                           def eventsToDataFrame(
  1638                                                   events, idxT='t', names=None
  1639                                                   ):
  1640                                               eventDict = {}
  1641                                               calculatedT = False
  1642                                               for event in events:
  1643                                                   if names is not None:
  1644                                                       if event.name not in names:
  1645                                                           continue
  1646                                                   if len(event.times):
  1647                                                       if not calculatedT:
  1648                                                           t = pd.Series(event.times.magnitude)
  1649                                                           calculatedT = True
  1650                                                       try:
  1651                                                           values = event.array_annotations['labels']
  1652                                                       except Exception:
  1653                                                           values = event.labels
  1654                                                       if isinstance(values[0], bytes):
  1655                                                           #  event came from hdf, need to recover dtype
  1656                                                           if 'originalDType' in event.annotations:
  1657                                                               dtypeStr = event.annotations['originalDType'].split(';')[-1]
  1658                                                               if 'np.' not in dtypeStr:
  1659                                                                   dtypeStr = 'np.' + dtypeStr
  1660                                                               originalDType = eval(dtypeStr)
  1661                                                               values = np.asarray(values, dtype=originalDType)
  1662                                                           else:
  1663                                                               values = np.asarray(values, dtype=np.str)
  1664                                                       #  print(values.dtype)
  1665                                                       eventDict.update({
  1666                                                           event.name: pd.Series(values)})
  1667                                               eventDict.update({idxT: t})
  1668                                               return pd.concat(eventDict, axis=1)

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadSpikeMats at line 1670

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1670                                           @profile
  1671                                           def loadSpikeMats(
  1672                                                   dataPath, rasterOpts,
  1673                                                   alignTimes=None, chans=None, loadAll=False,
  1674                                                   absoluteBins=False, transposeSpikeMat=False,
  1675                                                   checkReferences=False,
  1676                                                   aggregateFun=None):
  1677                                           
  1678                                               reader = nixio_fr.NixIO(filename=dataPath)
  1679                                               chanNames = reader.header['signal_channels']['name']
  1680                                               
  1681                                               if chans is not None:
  1682                                                   sigMask = np.isin(chanNames, chans)
  1683                                                   chanNames = chanNames[sigMask]
  1684                                                   
  1685                                               chanIdx = reader.channel_name_to_index(chanNames)
  1686                                               
  1687                                               if not loadAll:
  1688                                                   assert alignTimes is not None
  1689                                                   spikeMats = {i: None for i in alignTimes.index}
  1690                                                   validTrials = pd.Series(True, index=alignTimes.index)
  1691                                               else:
  1692                                                   spikeMats = {
  1693                                                       i: None for i in range(reader.segment_count(block_index=0))}
  1694                                                   validTrials = None
  1695                                               
  1696                                               for segIdx in range(reader.segment_count(block_index=0)):
  1697                                                   if checkReferences:
  1698                                                       for i, cIdx in enumerate(chanIdx):
  1699                                                           da = reader.da_list['blocks'][0]['segments'][segIdx]['data'][cIdx]
  1700                                                           print('name {}, da.name {}'.format(chanNames[i], da.name))
  1701                                                           try:
  1702                                                               assert chanNames[i] in da.name, 'reference problem!!'
  1703                                                           except Exception:
  1704                                                               traceback.print_exc()
  1705                                                   tStart = reader.get_signal_t_start(
  1706                                                       block_index=0, seg_index=segIdx)
  1707                                                   fs = reader.get_signal_sampling_rate(
  1708                                                       channel_indexes=chanIdx
  1709                                                       )
  1710                                                   sigSize = reader.get_signal_size(
  1711                                                       block_index=0, seg_index=segIdx
  1712                                                       )
  1713                                                   tStop = sigSize / fs + tStart
  1714                                                   #  convert to indices early to avoid floating point problems
  1715                                                   
  1716                                                   intervalIdx = int(round(rasterOpts['binInterval'] * fs))
  1717                                                   #  halfIntervalIdx = int(round(intervalIdx / 2))
  1718                                                   
  1719                                                   widthIdx = int(round(rasterOpts['binWidth'] * fs))
  1720                                                   halfWidthIdx = int(round(widthIdx / 2))
  1721                                                   
  1722                                                   if rasterOpts['smoothKernelWidth'] is not None:
  1723                                                       kernWidthIdx = int(round(rasterOpts['smoothKernelWidth'] * fs))
  1724                                                   
  1725                                                   theBins = None
  1726                                           
  1727                                                   if not loadAll:
  1728                                                       winStartIdx = int(round(rasterOpts['windowSize'][0] * fs))
  1729                                                       winStopIdx = int(round(rasterOpts['windowSize'][1] * fs))
  1730                                                       timeMask = (alignTimes > tStart) & (alignTimes < tStop)
  1731                                                       maskedTimes = alignTimes[timeMask]
  1732                                                   else:
  1733                                                       #  irrelevant, will load all
  1734                                                       maskedTimes = pd.Series(np.nan)
  1735                                           
  1736                                                   for idx, tOnset in maskedTimes.iteritems():
  1737                                                       if not loadAll:
  1738                                                           idxOnset = int(round((tOnset - tStart) * fs))
  1739                                                           #  can't not be ints
  1740                                                           iStart = idxOnset + winStartIdx - int(3 * halfWidthIdx)
  1741                                                           iStop = idxOnset + winStopIdx + int(3 * halfWidthIdx)
  1742                                                       else:
  1743                                                           winStartIdx = 0
  1744                                                           iStart = 0
  1745                                                           iStop = sigSize
  1746                                           
  1747                                                       if iStart < 0:
  1748                                                           #  near the first edge
  1749                                                           validTrials[idx] = False
  1750                                                       elif (sigSize < iStop):
  1751                                                           #  near the ending edge
  1752                                                           validTrials[idx] = False
  1753                                                       else:
  1754                                                           #  valid slices
  1755                                                           try:
  1756                                                               rawSpikeMat = pd.DataFrame(
  1757                                                                   reader.get_analogsignal_chunk(
  1758                                                                       block_index=0, seg_index=segIdx,
  1759                                                                       i_start=iStart, i_stop=iStop,
  1760                                                                       channel_names=chanNames))
  1761                                                           except Exception:
  1762                                                               traceback.print_exc()
  1763                                                               #
  1764                                                           if aggregateFun is None:
  1765                                                               procSpikeMat = rawSpikeMat.rolling(
  1766                                                                   window=3 * widthIdx, center=True,
  1767                                                                   win_type='gaussian'
  1768                                                                   ).mean(std=halfWidthIdx)
  1769                                                           else:
  1770                                                               procSpikeMat = rawSpikeMat.rolling(
  1771                                                                   window=widthIdx, center=True
  1772                                                                   ).apply(
  1773                                                                       aggregateFun,
  1774                                                                       raw=True,
  1775                                                                       kwargs={'fs': fs, 'nSamp': widthIdx})
  1776                                                           #
  1777                                                           if rasterOpts['smoothKernelWidth'] is not None:
  1778                                                               procSpikeMat = (
  1779                                                                   procSpikeMat
  1780                                                                   .rolling(
  1781                                                                       window=3 * kernWidthIdx, center=True,
  1782                                                                       win_type='gaussian')
  1783                                                                   .mean(std=kernWidthIdx/2)
  1784                                                                   .dropna().iloc[::intervalIdx, :]
  1785                                                               )
  1786                                                           else:
  1787                                                               procSpikeMat = (
  1788                                                                   procSpikeMat
  1789                                                                   .dropna().iloc[::intervalIdx, :]
  1790                                                               )
  1791                                           
  1792                                                           procSpikeMat.columns = chanNames
  1793                                                           procSpikeMat.columns.name = 'unit'
  1794                                                           if theBins is None:
  1795                                                               theBins = np.asarray(
  1796                                                                   procSpikeMat.index + winStartIdx) / fs
  1797                                                           if absoluteBins:
  1798                                                               procSpikeMat.index = theBins + idxOnset / fs
  1799                                                           else:
  1800                                                               procSpikeMat.index = theBins
  1801                                                           procSpikeMat.index.name = 'bin'
  1802                                                           if loadAll:
  1803                                                               smIdx = segIdx
  1804                                                           else:
  1805                                                               smIdx = idx
  1806                                                               
  1807                                                           spikeMats[smIdx] = procSpikeMat
  1808                                                           if transposeSpikeMat:
  1809                                                               spikeMats[smIdx] = spikeMats[smIdx].transpose()
  1810                                                       #  plt.imshow(rawSpikeMat.to_numpy(), aspect='equal'); plt.show()
  1811                                               return spikeMats, validTrials

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: synchronizeINStoNSP at line 1813

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1813                                           @profile
  1814                                           def synchronizeINStoNSP(
  1815                                                   tapTimestampsNSP=None, tapTimestampsINS=None,
  1816                                                   precalculatedFun=None,
  1817                                                   NSPTimeRanges=(None, None),
  1818                                                   td=None, accel=None, insBlock=None, trialSegment=None, degree=1,
  1819                                                   trimSpiketrains=False
  1820                                                   ):
  1821                                               print('Trial Segment {}'.format(trialSegment))
  1822                                               if precalculatedFun is None:
  1823                                                   assert ((tapTimestampsNSP is not None) & (tapTimestampsINS is not None))
  1824                                                   # sanity check that the intervals match
  1825                                                   insDiff = tapTimestampsINS.diff().dropna().values
  1826                                                   nspDiff = tapTimestampsNSP.diff().dropna().values
  1827                                                   print('On the INS, the diff() between taps was\n{}'.format(insDiff))
  1828                                                   print('On the NSP, the diff() between taps was\n{}'.format(nspDiff))
  1829                                                   print('This amounts to a msec difference of\n{}'.format(
  1830                                                       (insDiff - nspDiff) * 1e3))
  1831                                                   if (insDiff - nspDiff > 20e-3).any():
  1832                                                       raise(Exception('Tap trains too different!'))
  1833                                                   #
  1834                                                   if degree > 0:
  1835                                                       synchPolyCoeffsINStoNSP = np.polyfit(
  1836                                                           x=tapTimestampsINS.values, y=tapTimestampsNSP.values,
  1837                                                           deg=degree)
  1838                                                   else:
  1839                                                       timeOffset = tapTimestampsNSP.values - tapTimestampsINS.values
  1840                                                       synchPolyCoeffsINStoNSP = np.array([1, np.mean(timeOffset)])
  1841                                                   timeInterpFunINStoNSP = np.poly1d(synchPolyCoeffsINStoNSP)
  1842                                               else:
  1843                                                   timeInterpFunINStoNSP = precalculatedFun
  1844                                               if td is not None:
  1845                                                   td.loc[:, 'NSPTime'] = pd.Series(
  1846                                                       timeInterpFunINStoNSP(td['t']), index=td['t'].index)
  1847                                                   td.loc[:, 'NSPTime'] = timeInterpFunINStoNSP(td['t'].to_numpy())
  1848                                               if accel is not None:
  1849                                                   accel.loc[:, 'NSPTime'] = pd.Series(
  1850                                                       timeInterpFunINStoNSP(accel['t']), index=accel['t'].index)
  1851                                               if insBlock is not None:
  1852                                                   # allUnits = [st.unit for st in insBlock.segments[0].spiketrains]
  1853                                                   # [un.name for un in insBlock.filter(objects=Unit)]
  1854                                                   for unit in insBlock.filter(objects=Unit):
  1855                                                       tStart = NSPTimeRanges[0]
  1856                                                       tStop = NSPTimeRanges[1]
  1857                                                       uniqueSt = []
  1858                                                       for st in unit.spiketrains:
  1859                                                           if st not in uniqueSt:
  1860                                                               uniqueSt.append(st)
  1861                                                           else:
  1862                                                               continue
  1863                                                           print('Synchronizing {}'.format(st.name))
  1864                                                           if len(st.times):
  1865                                                               segMaskSt = np.array(
  1866                                                                   st.array_annotations['trialSegment'],
  1867                                                                   dtype=np.int) == trialSegment
  1868                                                               st.magnitude[segMaskSt] = (
  1869                                                                   timeInterpFunINStoNSP(st.times[segMaskSt].magnitude))
  1870                                                               if trimSpiketrains:
  1871                                                                   print('Trimming spiketrain')
  1872                                                                   #  kludgey fix for weirdness concerning t_start
  1873                                                                   st.t_start = min(tStart, st.times[0] * 0.999)
  1874                                                                   st.t_stop = min(tStop, st.times[-1] * 1.001)
  1875                                                                   validMask = st < st.t_stop
  1876                                                                   if ~validMask.all():
  1877                                                                       print('Deleted some spikes')
  1878                                                                       st = st[validMask]
  1879                                                                       # delete invalid spikes
  1880                                                                       if 'arrayAnnNames' in st.annotations.keys():
  1881                                                                           for key in st.annotations['arrayAnnNames']:
  1882                                                                               try:
  1883                                                                                   # st.annotations[key] = np.array(st.array_annotations[key])
  1884                                                                                   st.annotations[key] = np.delete(st.annotations[key], ~validMask)
  1885                                                                               except Exception:
  1886                                                                                   traceback.print_exc()
  1887                                                                                   pdb.set_trace()
  1888                                                           else:
  1889                                                               if trimSpiketrains:
  1890                                                                   st.t_start = tStart
  1891                                                                   st.t_stop = tStop
  1892                                                   #
  1893                                                   allEvents = [
  1894                                                       ev
  1895                                                       for ev in insBlock.filter(objects=Event)
  1896                                                       if ('ins' in ev.name) and ('concatenate' not in ev.name)]
  1897                                                   concatEvents = [
  1898                                                       ev
  1899                                                       for ev in insBlock.filter(objects=Event)
  1900                                                       if ('ins' in ev.name) and ('concatenate' in ev.name)]
  1901                                                   eventsDF = eventsToDataFrame(allEvents, idxT='t')
  1902                                                   newNames = {i: childBaseName(i, 'seg') for i in eventsDF.columns}
  1903                                                   eventsDF.rename(columns=newNames, inplace=True)
  1904                                                   segMask = hf.getStimSerialTrialSegMask(eventsDF, trialSegment)
  1905                                                   evTStart = eventsDF.loc[segMask, 't'].min() * pq.s
  1906                                                   evTStop = eventsDF.loc[segMask, 't'].max() * pq.s
  1907                                                   # print('allEvents[0].shape = {}'.format(allEvents[0].shape))
  1908                                                   # print('allEvents[0].magnitude[segMask][0] = {}'.format(allEvents[0].magnitude[segMask][0]))
  1909                                                   for event in (allEvents + concatEvents):
  1910                                                       if trimSpiketrains:
  1911                                                           thisSegMask = (event.times >= evTStart) & (event.times <= evTStop)
  1912                                                       else:
  1913                                                           thisSegMask = (event.times >= evTStart) & (event.times < evTStop)
  1914                                                       event.magnitude[thisSegMask] = (
  1915                                                           timeInterpFunINStoNSP(event.times[thisSegMask].magnitude))
  1916                                                   # print('allEvents[0].magnitude[segMask][0] = {}'.format(allEvents[0].magnitude[segMask][0]))
  1917                                                   # if len(concatEvents) > trialSegment:
  1918                                                   #     concatEvents[trialSegment].magnitude[:] = timeInterpFunINStoNSP(
  1919                                                   #         concatEvents[trialSegment].times[:].magnitude)
  1920                                               return td, accel, insBlock, timeInterpFunINStoNSP

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: findSegsIncluding at line 1922

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1922                                           @profile
  1923                                           def findSegsIncluding(
  1924                                                   block, timeSlice=None):
  1925                                               segBoundsList = []
  1926                                               for segIdx, seg in enumerate(block.segments):
  1927                                                   segBoundsList.append(pd.DataFrame({
  1928                                                       't_start': seg.t_start,
  1929                                                       't_stop': seg.t_stop
  1930                                                       }, index=[segIdx]))
  1931                                           
  1932                                               segBounds = pd.concat(segBoundsList)
  1933                                               if timeSlice[0] is not None:
  1934                                                   segMask = (segBounds['t_start'] * pq.s >= timeSlice[0]) & (
  1935                                                       segBounds['t_stop'] * pq.s <= timeSlice[1])
  1936                                                   requestedSegs = segBounds.loc[segMask, :]
  1937                                               else:
  1938                                                   timeSlice = (None, None)
  1939                                                   requestedSegs = segBounds
  1940                                               return segBounds, requestedSegs

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: findSegsIncluded at line 1942

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1942                                           @profile
  1943                                           def findSegsIncluded(
  1944                                                   block, timeSlice=None):
  1945                                               segBoundsList = []
  1946                                               for segIdx, seg in enumerate(block.segments):
  1947                                                   segBoundsList.append(pd.DataFrame({
  1948                                                       't_start': seg.t_start,
  1949                                                       't_stop': seg.t_stop
  1950                                                       }, index=[segIdx]))
  1951                                           
  1952                                               segBounds = pd.concat(segBoundsList)
  1953                                               if timeSlice[0] is not None:
  1954                                                   segMask = (segBounds['t_start'] * pq.s <= timeSlice[0]) | (
  1955                                                       segBounds['t_stop'] * pq.s >= timeSlice[1])
  1956                                                   requestedSegs = segBounds.loc[segMask, :]
  1957                                               else:
  1958                                                   timeSlice = (None, None)
  1959                                                   requestedSegs = segBounds
  1960                                               return segBounds, requestedSegs

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getElecLookupTable at line 1962

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1962                                           @profile
  1963                                           def getElecLookupTable(
  1964                                                   block, elecIds=None):
  1965                                               lookupTableList = []
  1966                                               for metaIdx, chanIdx in enumerate(block.channel_indexes):
  1967                                                   if chanIdx.analogsignals:
  1968                                                       #  print(chanIdx.name)
  1969                                                       lookupTableList.append(pd.DataFrame({
  1970                                                           'channelNames': np.asarray(chanIdx.channel_names, dtype=np.str),
  1971                                                           'index': chanIdx.index,
  1972                                                           'metaIndex': metaIdx * chanIdx.index**0,
  1973                                                           'localIndex': (
  1974                                                               list(range(chanIdx.analogsignals[0].shape[1])))
  1975                                                           }))
  1976                                               lookupTable = pd.concat(lookupTableList, ignore_index=True)
  1977                                           
  1978                                               if elecIds is None:
  1979                                                   requestedIndices = lookupTable
  1980                                               else:
  1981                                                   if isinstance(elecIds[0], str):
  1982                                                       idxMask = lookupTable['channelNames'].isin(elecIds)
  1983                                                       requestedIndices = lookupTable.loc[idxMask, :]
  1984                                               return lookupTable, requestedIndices

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: getNIXData at line 1986

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  1986                                           @profile
  1987                                           def getNIXData(
  1988                                                   fileName=None,
  1989                                                   folderPath=None,
  1990                                                   reader=None, blockIdx=0,
  1991                                                   elecIds=None, startTime_s=None,
  1992                                                   dataLength_s=None, downsample=1,
  1993                                                   signal_group_mode='group-by-same-units',
  1994                                                   closeReader=False):
  1995                                               #  Open file and extract headers
  1996                                               if reader is None:
  1997                                                   assert (fileName is not None) and (folderPath is not None)
  1998                                                   filePath = os.path.join(folderPath, fileName) + '.nix'
  1999                                                   reader = nixio_fr.NixIO(filename=filePath)
  2000                                           
  2001                                               block = reader.read_block(
  2002                                                   block_index=blockIdx, lazy=True,
  2003                                                   signal_group_mode=signal_group_mode)
  2004                                           
  2005                                               for segIdx, seg in enumerate(block.segments):
  2006                                                   seg.events = [i.load() for i in seg.events]
  2007                                                   seg.epochs = [i.load() for i in seg.epochs]
  2008                                           
  2009                                               # find elecIds
  2010                                               lookupTable, requestedIndices = getElecLookupTable(
  2011                                                   block, elecIds=elecIds)
  2012                                           
  2013                                               # find segments that contain the requested times
  2014                                               if dataLength_s is not None:
  2015                                                   assert startTime_s is not None
  2016                                                   timeSlice = (
  2017                                                       startTime_s * pq.s,
  2018                                                       (startTime_s + dataLength_s) * pq.s)
  2019                                               else:
  2020                                                   timeSlice = (None, None)
  2021                                               segBounds, requestedSegs = findSegsIncluding(block, timeSlice)
  2022                                               #
  2023                                               data = pd.DataFrame(columns=elecIds + ['t'])
  2024                                               for segIdx in requestedSegs.index:
  2025                                                   seg = block.segments[segIdx]
  2026                                                   if dataLength_s is not None:
  2027                                                       timeSlice = (
  2028                                                           max(timeSlice[0], seg.t_start),
  2029                                                           min(timeSlice[1], seg.t_stop)
  2030                                                           )
  2031                                                   else:
  2032                                                       timeSlice = (seg.t_start, seg.t_stop)
  2033                                                   segData = pd.DataFrame()
  2034                                                   for metaIdx in pd.unique(requestedIndices['metaIndex']):
  2035                                                       metaIdxMatch = requestedIndices['metaIndex'] == metaIdx
  2036                                                       theseRequestedIndices = requestedIndices.loc[
  2037                                                           metaIdxMatch, :]
  2038                                                       theseElecIds = theseRequestedIndices['channelNames']
  2039                                                       asig = seg.analogsignals[metaIdx]
  2040                                                       thisTimeSlice = (
  2041                                                           max(timeSlice[0], asig.t_start),
  2042                                                           min(timeSlice[1], asig.t_stop)
  2043                                                           )
  2044                                                       reqData = asig.load(
  2045                                                           time_slice=thisTimeSlice,
  2046                                                           channel_indexes=theseRequestedIndices['localIndex'].to_numpy())
  2047                                                       segData = pd.concat((
  2048                                                               segData,
  2049                                                               pd.DataFrame(
  2050                                                                   reqData.magnitude, columns=theseElecIds.to_numpy())),
  2051                                                           axis=1)
  2052                                                   segT = reqData.times
  2053                                                   segData['t'] = segT
  2054                                                   data = pd.concat(
  2055                                                       (data, segData),
  2056                                                       axis=0, ignore_index=True)
  2057                                               channelData = {
  2058                                                   'data': data,
  2059                                                   't': data['t']
  2060                                                   }
  2061                                               if closeReader:
  2062                                                   reader.file.close()
  2063                                                   block = None
  2064                                                   # closing the reader breaks its connection to the block
  2065                                               return channelData, block

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: childBaseName at line 2067

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2067                                           @profile
  2068                                           def childBaseName(
  2069                                                   childName, searchTerm):
  2070                                               if searchTerm in childName:
  2071                                                   baseName = '_'.join(childName.split('_')[1:])
  2072                                               else:
  2073                                                   baseName = childName
  2074                                               return baseName

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: readBlockFixNames at line 2076

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2076                                           @profile
  2077                                           def readBlockFixNames(
  2078                                                   rawioReader,
  2079                                                   block_index=0, signal_group_mode='split-all',
  2080                                                   lazy=True, mapDF=None, reduceChannelIndexes=False,
  2081                                                   loadList=None, purgeNixNames=False
  2082                                                   ):
  2083                                               headerSignalChan = pd.DataFrame(
  2084                                                   rawioReader.header['signal_channels']).set_index('id')
  2085                                               headerUnitChan = pd.DataFrame(
  2086                                                   rawioReader.header['unit_channels']).set_index('id')
  2087                                               dataBlock = rawioReader.read_block(
  2088                                                   block_index=block_index, lazy=lazy,
  2089                                                   signal_group_mode=signal_group_mode)
  2090                                               if dataBlock.name is None:
  2091                                                   if 'neo_name' in dataBlock.annotations:
  2092                                                       dataBlock.name = dataBlock.annotations['neo_name']
  2093                                               #  on first segment, rename the chan_indexes and units
  2094                                               seg0 = dataBlock.segments[0]
  2095                                               asigLikeList = (
  2096                                                   seg0.filter(objects=AnalogSignalProxy) +
  2097                                                   seg0.filter(objects=AnalogSignal))
  2098                                               if mapDF is not None:
  2099                                                   if headerSignalChan.size > 0:
  2100                                                       asigNameChanger = {}
  2101                                                       for nevID in mapDF['nevID']:
  2102                                                           if int(nevID) in headerSignalChan.index:
  2103                                                               labelFromMap = (
  2104                                                                   mapDF
  2105                                                                   .loc[mapDF['nevID'] == nevID, 'label']
  2106                                                                   .iloc[0])
  2107                                                               asigNameChanger[
  2108                                                                   headerSignalChan.loc[int(nevID), 'name']] = labelFromMap
  2109                                                   else:
  2110                                                       asigOrigNames = np.unique(
  2111                                                           [i.split('#')[0] for i in headerUnitChan['name']])
  2112                                                       asigNameChanger = {}
  2113                                                       for origName in asigOrigNames:
  2114                                                           # ripple specific
  2115                                                           formattedName = origName.replace('.', '_').replace(' raw', '')
  2116                                                           if mapDF['label'].str.contains(formattedName).any():
  2117                                                               asigNameChanger[origName] = formattedName
  2118                                               else:
  2119                                                   asigNameChanger = dict()
  2120                                               for asig in asigLikeList:
  2121                                                   asigBaseName = childBaseName(asig.name, 'seg')
  2122                                                   asig.name = (
  2123                                                       asigNameChanger[asigBaseName]
  2124                                                       if asigBaseName in asigNameChanger
  2125                                                       else asigBaseName)
  2126                                                   if mapDF is not None:
  2127                                                       if (mapDF['label'] == asig.name).any():
  2128                                                           asig.annotations['xCoords'] = float(mapDF.loc[mapDF['label'] == asig.name, 'xcoords'].iloc[0])
  2129                                                           asig.annotations['yCoords'] = float(mapDF.loc[mapDF['label'] == asig.name, 'ycoords'].iloc[0])
  2130                                                           asig.annotations['zCoords'] = float(mapDF.loc[mapDF['label'] == asig.name, 'zcoords'].iloc[0])
  2131                                                   if 'Channel group ' in asig.channel_index.name:
  2132                                                       newChanName = (
  2133                                                           asigNameChanger[asigBaseName]
  2134                                                           if asigBaseName in asigNameChanger
  2135                                                           else asigBaseName)
  2136                                                       asig.channel_index.name = newChanName
  2137                                                       if 'neo_name' in asig.channel_index.annotations:
  2138                                                           asig.channel_index.annotations['neo_name'] = newChanName
  2139                                                       if 'nix_name' in asig.channel_index.annotations:
  2140                                                           asig.channel_index.annotations['nix_name'] = newChanName
  2141                                                       if mapDF is not None:
  2142                                                           try:
  2143                                                               asig.channel_index.coordinates = np.asarray([
  2144                                                                   asig.annotations['xCoords'], asig.annotations['yCoords'], asig.annotations['zCoords']
  2145                                                               ])[np.newaxis, :] * pq.um
  2146                                                           except Exception:
  2147                                                               pass
  2148                                               spikeTrainLikeList = (
  2149                                                   seg0.filter(objects=SpikeTrainProxy) +
  2150                                                   seg0.filter(objects=SpikeTrain))
  2151                                               # add channels for channelIndex that has no asigs but has spikes
  2152                                               nExtraChans = 0
  2153                                               for stp in spikeTrainLikeList:
  2154                                                   stpBaseName = childBaseName(stp.name, 'seg')
  2155                                                   nameParser = re.search(r'ch(\d*)#(\d*)', stpBaseName)
  2156                                                   if nameParser is not None:
  2157                                                       # first time at this unit, rename it
  2158                                                       chanId = int(nameParser.group(1))
  2159                                                       unitId = int(nameParser.group(2))
  2160                                                       if chanId >= 5121:
  2161                                                           isRippleStimChan = True
  2162                                                           chanId = chanId - 5120
  2163                                                       else:
  2164                                                           isRippleStimChan = False
  2165                                                       ####################
  2166                                                       # asigBaseName = headerSignalChan.loc[chanId, 'name']
  2167                                                       # if mapDF is not None:
  2168                                                       #     if asigBaseName in asigNameChanger:
  2169                                                       #         chanIdLabel = (
  2170                                                       #             asigNameChanger[asigBaseName]
  2171                                                       #             if asigBaseName in asigNameChanger
  2172                                                       #             else asigBaseName)
  2173                                                       #     else:
  2174                                                       #         chanIdLabel = asigBaseName
  2175                                                       # else:
  2176                                                       #     chanIdLabel = asigBaseName
  2177                                                       ###################
  2178                                                       # if swapMaps is not None:
  2179                                                       #     nameCandidates = (swapMaps['to'].loc[swapMaps['to']['nevID'] == chanId, 'label']).to_list()
  2180                                                       # elif mapDF is not None:
  2181                                                       #     nameCandidates = (mapDF.loc[mapDF['nevID'] == chanId, 'label']).to_list()
  2182                                                       # else:
  2183                                                       #     nameCandidates = []
  2184                                                       ##############################
  2185                                                       if mapDF is not None:
  2186                                                           nameCandidates = (
  2187                                                               mapDF
  2188                                                               .loc[mapDF['nevID'] == chanId, 'label']
  2189                                                               .to_list())
  2190                                                       else:
  2191                                                           nameCandidates = []
  2192                                                       if len(nameCandidates) == 1:
  2193                                                           chanIdLabel = nameCandidates[0]
  2194                                                       elif chanId in headerSignalChan:
  2195                                                           chanIdLabel = headerSignalChan.loc[chanId, 'name']
  2196                                                       else:
  2197                                                           chanIdLabel = 'ch{}'.format(chanId)
  2198                                                       #
  2199                                                       if isRippleStimChan:
  2200                                                           stp.name = '{}_stim#{}'.format(chanIdLabel, unitId)
  2201                                                       else:
  2202                                                           stp.name = '{}#{}'.format(chanIdLabel, unitId)
  2203                                                       stp.unit.name = stp.name
  2204                                                   ########################################
  2205                                                   # sanitize ripple names ####
  2206                                                   stp.name = stp.name.replace('.', '_').replace(' raw', '')
  2207                                                   stp.unit.name = stp.unit.name.replace('.', '_').replace(' raw', '')
  2208                                                   ###########################################
  2209                                                   if 'ChannelIndex for ' in stp.unit.channel_index.name:
  2210                                                       newChanName = stp.name.replace('_stim#0', '')
  2211                                                       # remove unit #
  2212                                                       newChanName = re.sub(r'#\d', '', newChanName)
  2213                                                       stp.unit.channel_index.name = newChanName
  2214                                                       # units and analogsignals have different channel_indexes when loaded by nix
  2215                                                       # add them to each other's parent list
  2216                                                       allMatchingChIdx = dataBlock.filter(
  2217                                                           objects=ChannelIndex, name=newChanName)
  2218                                                       if (len(allMatchingChIdx) > 1) and reduceChannelIndexes:
  2219                                                           assert len(allMatchingChIdx) == 2
  2220                                                           targetChIdx = [
  2221                                                               ch
  2222                                                               for ch in allMatchingChIdx
  2223                                                               if ch is not stp.unit.channel_index][0]
  2224                                                           oldChIdx = stp.unit.channel_index
  2225                                                           targetChIdx.units.append(stp.unit)
  2226                                                           stp.unit.channel_index = targetChIdx
  2227                                                           oldChIdx.units.remove(stp.unit)
  2228                                                           if not (len(oldChIdx.units) or len(oldChIdx.analogsignals)):
  2229                                                               dataBlock.channel_indexes.remove(oldChIdx)
  2230                                                           del oldChIdx
  2231                                                           targetChIdx.create_relationship()
  2232                                                       elif reduceChannelIndexes:
  2233                                                           if newChanName not in headerSignalChan['name']:
  2234                                                               stp.unit.channel_index.index = np.asarray(
  2235                                                                   [headerSignalChan['name'].size + nExtraChans])
  2236                                                               stp.unit.channel_index.channel_ids = np.asarray(
  2237                                                                   [headerSignalChan['name'].size + nExtraChans])
  2238                                                               stp.unit.channel_index.channel_names = np.asarray(
  2239                                                                   [newChanName])
  2240                                                               nExtraChans += 1
  2241                                                           if 'neo_name' not in allMatchingChIdx[0].annotations:
  2242                                                               allMatchingChIdx[0].annotations['neo_name'] = allMatchingChIdx[0].name
  2243                                                           if 'nix_name' not in allMatchingChIdx[0].annotations:
  2244                                                               allMatchingChIdx[0].annotations['nix_name'] = allMatchingChIdx[0].name
  2245                                                   stp.unit.channel_index.name = stp.unit.channel_index.name.replace('.', '_').replace(' raw', '')
  2246                                               #  rename the children
  2247                                               typesNeedRenaming = [
  2248                                                   SpikeTrainProxy, AnalogSignalProxy, EventProxy,
  2249                                                   SpikeTrain, AnalogSignal, Event]
  2250                                               for segIdx, seg in enumerate(dataBlock.segments):
  2251                                                   if seg.name is None:
  2252                                                       seg.name = 'seg{}_'.format(segIdx)
  2253                                                   else:
  2254                                                       if 'seg{}_'.format(segIdx) not in seg.name:
  2255                                                           seg.name = (
  2256                                                               'seg{}_{}'
  2257                                                               .format(
  2258                                                                   segIdx,
  2259                                                                   childBaseName(seg.name, 'seg')))
  2260                                                   for objType in typesNeedRenaming:
  2261                                                       for child in seg.filter(objects=objType):
  2262                                                           if 'seg{}_'.format(segIdx) not in child.name:
  2263                                                               child.name = (
  2264                                                                   'seg{}_{}'
  2265                                                                   .format(
  2266                                                                       segIdx, childBaseName(child.name, 'seg')))
  2267                                                           #  todo: decide if below is needed
  2268                                                           #  elif 'seg' in child.name:
  2269                                                           #      childBaseName = '_'.join(child.name.split('_')[1:])
  2270                                                           #      child.name = 'seg{}_{}'.format(segIdx, childBaseName)
  2271                                               # [i.name for i in dataBlock.filter(objects=Unit)]
  2272                                               # [i.name for i in dataBlock.filter(objects=ChannelIndex)]
  2273                                               # [i.name for i in dataBlock.filter(objects=SpikeTrain)]
  2274                                               # [i.name for i in dataBlock.filter(objects=SpikeTrainProxy)]
  2275                                               if lazy:
  2276                                                   for stP in dataBlock.filter(objects=SpikeTrainProxy):
  2277                                                       if 'unitAnnotations' in stP.annotations:
  2278                                                           unAnnStr = stP.annotations['unitAnnotations']
  2279                                                           stP.unit.annotations.update(json.loads(unAnnStr))
  2280                                               if (loadList is not None) and lazy:
  2281                                                   if 'asigs' in loadList:
  2282                                                       loadAsigList(
  2283                                                           dataBlock, listOfAsigProxyNames=loadList['asigs'],
  2284                                                           replaceInParents=True)
  2285                                                   if 'events' in loadList:
  2286                                                       loadEventList(
  2287                                                           dataBlock,
  2288                                                           listOfEventNames=loadList['events'],
  2289                                                           replaceInParents=True)
  2290                                                   if 'spiketrains' in loadList:
  2291                                                       loadSpikeTrainList(
  2292                                                           dataBlock,
  2293                                                           listOfSpikeTrainNames=loadList['spiketrains'],
  2294                                                           replaceInParents=True)
  2295                                               if purgeNixNames:
  2296                                                   dataBlock = purgeNixAnn(dataBlock)
  2297                                               return dataBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadSpikeTrainList at line 2299

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2299                                           @profile
  2300                                           def loadSpikeTrainList(
  2301                                                   dataBlock, listOfSpikeTrainNames=None,
  2302                                                   replaceInParents=True):
  2303                                               listOfSpikeTrains = []
  2304                                               if listOfSpikeTrainNames is None:
  2305                                                   listOfSpikeTrainNames = [
  2306                                                       stp.name
  2307                                                       for stp in dataBlock.filter(objects=SpikeTrainProxy)]
  2308                                               for stP in dataBlock.filter(objects=SpikeTrainProxy):
  2309                                                   if stP.name in listOfSpikeTrainNames:
  2310                                                       st = loadObjArrayAnn(stP.load())
  2311                                                       listOfSpikeTrains.append(st)
  2312                                                       if replaceInParents:
  2313                                                           seg = stP.segment
  2314                                                           segStNames = [s.name for s in seg.spiketrains]
  2315                                                           idxInSeg = segStNames.index(stP.name)
  2316                                                           seg.spiketrains[idxInSeg] = st
  2317                                                           #
  2318                                                           unit = stP.unit
  2319                                                           unitStNames = [s.name for s in unit.spiketrains]
  2320                                                           st.unit = unit
  2321                                                           idxInUnit = unitStNames.index(stP.name)
  2322                                                           unit.spiketrains[idxInUnit] = st
  2323                                               return listOfSpikeTrains

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadEventList at line 2325

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2325                                           @profile
  2326                                           def loadEventList(
  2327                                                   dataBlock,
  2328                                                   listOfEventNames=None, replaceInParents=True):
  2329                                               listOfEvents = []
  2330                                               if listOfEventNames is None:
  2331                                                   listOfEventNames = [
  2332                                                       evp.name
  2333                                                       for evp in dataBlock.filter(objects=EventProxy)]
  2334                                               for evP in dataBlock.filter(objects=EventProxy):
  2335                                                   if evP.name in listOfEventNames:
  2336                                                       ev = loadObjArrayAnn(evP.load())
  2337                                                       listOfEvents.append(ev)
  2338                                                       if replaceInParents:
  2339                                                           seg = evP.segment
  2340                                                           segEvNames = [e.name for e in seg.events]
  2341                                                           idxInSeg = segEvNames.index(evP.name)
  2342                                                           seg.events[idxInSeg] = ev
  2343                                               return listOfEvents

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadAsigList at line 2345

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2345                                           @profile
  2346                                           def loadAsigList(
  2347                                                   dataBlock, listOfAsigProxyNames=None, replaceInParents=True):
  2348                                               listOfAsigs = []
  2349                                               if listOfAsigProxyNames is None:
  2350                                                   listOfAsigProxyNames = [
  2351                                                       asigp.name
  2352                                                       for asigp in dataBlock.filter(objects=AnalogSignalProxy)]
  2353                                               for asigP in dataBlock.filter(objects=AnalogSignalProxy):
  2354                                                   if asigP.name in listOfAsigProxyNames:
  2355                                                       asig = asigP.load()
  2356                                                       asig.annotations = asigP.annotations.copy()
  2357                                                       listOfAsigs.append(asig)
  2358                                                       #
  2359                                                       if replaceInParents:
  2360                                                           seg = asigP.segment
  2361                                                           segAsigNames = [ag.name for ag in seg.analogsignals]
  2362                                                           asig.segment = seg
  2363                                                           idxInSeg = segAsigNames.index(asigP.name)
  2364                                                           seg.analogsignals[idxInSeg] = asig
  2365                                                           #
  2366                                                           chIdx = asigP.channel_index
  2367                                                           chIdxAsigNames = [ag.name for ag in chIdx.analogsignals]
  2368                                                           asig.channel_index = chIdx
  2369                                                           idxInChIdx = chIdxAsigNames.index(asigP.name)
  2370                                                           chIdx.analogsignals[idxInChIdx] = asig
  2371                                               return listOfAsigs

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: addBlockToNIX at line 2373

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2373                                           @profile
  2374                                           def addBlockToNIX(
  2375                                                   newBlock, neoSegIdx=[0],
  2376                                                   writeAsigs=True, writeSpikes=True, writeEvents=True,
  2377                                                   asigNameList=None,
  2378                                                   purgeNixNames=False,
  2379                                                   fileName=None,
  2380                                                   folderPath=None,
  2381                                                   nixBlockIdx=0, nixSegIdx=[0],
  2382                                                   ):
  2383                                               #  base file name
  2384                                               trialBasePath = os.path.join(folderPath, fileName)
  2385                                               if writeAsigs:
  2386                                                   # peek at file to ensure compatibility
  2387                                                   reader = nixio_fr.NixIO(filename=trialBasePath + '.nix')
  2388                                                   tempBlock = reader.read_block(
  2389                                                       block_index=nixBlockIdx,
  2390                                                       lazy=True, signal_group_mode='split-all')
  2391                                                   checkCompatible = {i: False for i in nixSegIdx}
  2392                                                   forceShape = {i: None for i in nixSegIdx}
  2393                                                   forceType = {i: None for i in nixSegIdx}
  2394                                                   forceFS = {i: None for i in nixSegIdx}
  2395                                                   for nixIdx in nixSegIdx:
  2396                                                       tempAsigList = tempBlock.segments[nixIdx].filter(
  2397                                                           objects=AnalogSignalProxy)
  2398                                                       if len(tempAsigList) > 0:
  2399                                                           tempAsig = tempAsigList[0]
  2400                                                           checkCompatible[nixIdx] = True
  2401                                                           forceType[nixIdx] = tempAsig.dtype
  2402                                                           forceShape[nixIdx] = tempAsig.shape[0]  # ? docs say shape[1], but that's confusing
  2403                                                           forceFS[nixIdx] = tempAsig.sampling_rate
  2404                                                   reader.file.close()
  2405                                               #  if newBlock was loaded from a nix file, strip the old nix_names away:
  2406                                               #  todo: replace with function from this module
  2407                                               if purgeNixNames:
  2408                                                   newBlock = purgeNixAnn(newBlock)
  2409                                               #
  2410                                               writer = NixIO(filename=trialBasePath + '.nix')
  2411                                               nixblock = writer.nix_file.blocks[nixBlockIdx]
  2412                                               nixblockName = nixblock.name
  2413                                               if 'nix_name' in newBlock.annotations.keys():
  2414                                                   try:
  2415                                                       assert newBlock.annotations['nix_name'] == nixblockName
  2416                                                   except Exception:
  2417                                                       newBlock.annotations['nix_name'] = nixblockName
  2418                                               else:
  2419                                                   newBlock.annotate(nix_name=nixblockName)
  2420                                               #
  2421                                               for idx, segIdx in enumerate(neoSegIdx):
  2422                                                   nixIdx = nixSegIdx[idx]
  2423                                                   newSeg = newBlock.segments[segIdx]
  2424                                                   nixgroup = nixblock.groups[nixIdx]
  2425                                                   nixSegName = nixgroup.name
  2426                                                   if 'nix_name' in newSeg.annotations.keys():
  2427                                                       try:
  2428                                                           assert newSeg.annotations['nix_name'] == nixSegName
  2429                                                       except Exception:
  2430                                                           newSeg.annotations['nix_name'] = nixSegName
  2431                                                   else:
  2432                                                       newSeg.annotate(nix_name=nixSegName)
  2433                                                   #
  2434                                                   if writeEvents:
  2435                                                       eventList = newSeg.events
  2436                                                       eventOrder = np.argsort([i.name for i in eventList])
  2437                                                       for event in [eventList[i] for i in eventOrder]:
  2438                                                           event = writer._write_event(event, nixblock, nixgroup)
  2439                                                   #
  2440                                                   if writeAsigs:
  2441                                                       asigList = newSeg.filter(objects=AnalogSignal)
  2442                                                       asigOrder = np.argsort([i.name for i in asigList])
  2443                                                       for asig in [asigList[i] for i in asigOrder]:
  2444                                                           if checkCompatible[nixIdx]:
  2445                                                               assert asig.dtype == forceType[nixIdx]
  2446                                                               assert asig.sampling_rate == forceFS[nixIdx]
  2447                                                               #  print('asig.shape[0] = {}'.format(asig.shape[0]))
  2448                                                               #  print('forceShape[nixIdx] = {}'.format(forceShape[nixIdx]))
  2449                                                               assert asig.shape[0] == forceShape[nixIdx]
  2450                                                           asig = writer._write_analogsignal(asig, nixblock, nixgroup)
  2451                                                       #  for isig in newSeg.filter(objects=IrregularlySampledSignal):
  2452                                                       #      isig = writer._write_irregularlysampledsignal(
  2453                                                       #          isig, nixblock, nixgroup)
  2454                                                   #
  2455                                                   if writeSpikes:
  2456                                                       stList = newSeg.filter(objects=SpikeTrain)
  2457                                                       stOrder = np.argsort([i.name for i in stList])
  2458                                                       for st in [stList[i] for i in stOrder]:
  2459                                                           st = writer._write_spiketrain(st, nixblock, nixgroup)
  2460                                               #
  2461                                               for chanIdx in newBlock.filter(objects=ChannelIndex):
  2462                                                   chanIdx = writer._write_channelindex(chanIdx, nixblock)
  2463                                                   #  auto descends into units inside of _write_channelindex
  2464                                               writer._create_source_links(newBlock, nixblock)
  2465                                               writer.close()
  2466                                               print('Done adding block to Nix.')
  2467                                               return newBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadStProxy at line 2469

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2469                                           @profile
  2470                                           def loadStProxy(stProxy):
  2471                                               try:
  2472                                                   st = stProxy.load(
  2473                                                       magnitude_mode='rescaled',
  2474                                                       load_waveforms=True)
  2475                                               except Exception:
  2476                                                   st = stProxy.load(
  2477                                                       magnitude_mode='rescaled',
  2478                                                       load_waveforms=False)
  2479                                                   st.waveforms = np.asarray([]).reshape((0, 0, 0))*pq.mV
  2480                                               return st

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: preproc at line 2482

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2482                                           @profile
  2483                                           def preproc(
  2484                                                   fileName='Trial001',
  2485                                                   rawFolderPath='./',
  2486                                                   outputFolderPath='./', mapDF=None,
  2487                                                   # swapMaps=None,
  2488                                                   electrodeArrayName='utah',
  2489                                                   fillOverflow=True, removeJumps=True,
  2490                                                   removeMeanAcross=False,
  2491                                                   linearDetrend=False,
  2492                                                   interpolateOutliers=False, calcOutliers=False,
  2493                                                   outlierMaskFilterOpts=None,
  2494                                                   outlierThreshold=1,
  2495                                                   calcArtifactTrace=False,
  2496                                                   motorEncoderMask=None,
  2497                                                   calcAverageLFP=False,
  2498                                                   eventInfo=None,
  2499                                                   spikeSourceType='', spikePath=None,
  2500                                                   chunkSize=1800, equalChunks=True, chunkList=None, chunkOffset=0,
  2501                                                   writeMode='rw',
  2502                                                   signal_group_mode='split-all', trialInfo=None,
  2503                                                   asigNameList=None, ainpNameList=None, nameSuffix='',
  2504                                                   saveFromAsigNameList=True,
  2505                                                   calcRigEvents=True, normalizeByImpedance=False,
  2506                                                   LFPFilterOpts=None, encoderCountPerDegree=180e2,
  2507                                                   outlierRemovalDebugFlag=False, impedanceFilePath=None
  2508                                                   ):
  2509                                               #  base file name
  2510                                               rawBasePath = os.path.join(rawFolderPath, fileName)
  2511                                               outputFilePath = os.path.join(
  2512                                                   outputFolderPath,
  2513                                                   fileName + nameSuffix + '.nix')
  2514                                               if os.path.exists(outputFilePath):
  2515                                                   os.remove(outputFilePath)
  2516                                               #  instantiate reader, get metadata
  2517                                               print('Loading\n{}\n'.format(rawBasePath))
  2518                                               reader = BlackrockIO(
  2519                                                   filename=rawBasePath, nsx_to_load=5)
  2520                                               reader.parse_header()
  2521                                               # metadata = reader.header
  2522                                               #  absolute section index
  2523                                               dummyBlock = readBlockFixNames(
  2524                                                   reader,
  2525                                                   block_index=0, lazy=True,
  2526                                                   signal_group_mode=signal_group_mode,
  2527                                                   mapDF=mapDF, reduceChannelIndexes=True,
  2528                                                   # swapMaps=swapMaps
  2529                                                   )
  2530                                               segLen = dummyBlock.segments[0].analogsignals[0].shape[0] / (
  2531                                                   dummyBlock.segments[0].analogsignals[0].sampling_rate)
  2532                                               nChunks = math.ceil(segLen / chunkSize)
  2533                                               #
  2534                                               if equalChunks:
  2535                                                   actualChunkSize = (segLen / nChunks).magnitude
  2536                                               else:
  2537                                                   actualChunkSize = chunkSize
  2538                                               if chunkList is None:
  2539                                                   chunkList = range(nChunks)
  2540                                               chunkingMetadata = {}
  2541                                               for chunkIdx in chunkList:
  2542                                                   print('preproc on chunk {}'.format(chunkIdx))
  2543                                                   #  instantiate spike reader if requested
  2544                                                   if spikeSourceType == 'tdc':
  2545                                                       if spikePath is None:
  2546                                                           spikePath = os.path.join(
  2547                                                               outputFolderPath, 'tdc_' + fileName,
  2548                                                               'tdc_' + fileName + '.nix')
  2549                                                       print('loading {}'.format(spikePath))
  2550                                                       spikeReader = nixio_fr.NixIO(filename=spikePath)
  2551                                                   else:
  2552                                                       spikeReader = None
  2553                                                   #  absolute section index
  2554                                                   block = readBlockFixNames(
  2555                                                       reader,
  2556                                                       block_index=0, lazy=True,
  2557                                                       signal_group_mode=signal_group_mode,
  2558                                                       mapDF=mapDF, reduceChannelIndexes=True,
  2559                                                       # swapMaps=swapMaps
  2560                                                       )
  2561                                                   if spikeReader is not None:
  2562                                                       spikeBlock = readBlockFixNames(
  2563                                                           spikeReader, block_index=0, lazy=True,
  2564                                                           signal_group_mode=signal_group_mode,
  2565                                                           mapDF=mapDF, reduceChannelIndexes=True,
  2566                                                           # swapMaps=swapMaps
  2567                                                           )
  2568                                                       spikeBlock = purgeNixAnn(spikeBlock)
  2569                                                   else:
  2570                                                       spikeBlock = None
  2571                                                   #
  2572                                                   #  instantiate writer
  2573                                                   if (nChunks == 1) or (len(chunkList) == 1):
  2574                                                       partNameSuffix = ""
  2575                                                       thisChunkOutFilePath = outputFilePath
  2576                                                   else:
  2577                                                       partNameSuffix = '_pt{:0>3}'.format(chunkIdx)
  2578                                                       thisChunkOutFilePath = (
  2579                                                           outputFilePath
  2580                                                           .replace('.nix', partNameSuffix + '.nix'))
  2581                                                   #
  2582                                                   if os.path.exists(thisChunkOutFilePath):
  2583                                                       os.remove(thisChunkOutFilePath)
  2584                                                   writer = NixIO(
  2585                                                       filename=thisChunkOutFilePath, mode=writeMode)
  2586                                                   chunkTStart = chunkIdx * actualChunkSize + chunkOffset
  2587                                                   chunkTStop = (chunkIdx + 1) * actualChunkSize + chunkOffset
  2588                                                   chunkingMetadata[chunkIdx] = {
  2589                                                       'filename': thisChunkOutFilePath,
  2590                                                       'partNameSuffix': partNameSuffix,
  2591                                                       'chunkTStart': chunkTStart,
  2592                                                       'chunkTStop': chunkTStop}
  2593                                                   block.annotate(chunkTStart=chunkTStart)
  2594                                                   block.annotate(chunkTStop=chunkTStop)
  2595                                                   block.annotate(
  2596                                                       recDatetimeStr=(
  2597                                                           block
  2598                                                           .rec_datetime
  2599                                                           .replace(tzinfo=timezone.utc)
  2600                                                           .isoformat())
  2601                                                       )
  2602                                                   #
  2603                                                   preprocBlockToNix(
  2604                                                       block, writer,
  2605                                                       chunkTStart=chunkTStart,
  2606                                                       chunkTStop=chunkTStop,
  2607                                                       fillOverflow=fillOverflow,
  2608                                                       removeJumps=removeJumps,
  2609                                                       interpolateOutliers=interpolateOutliers,
  2610                                                       calcOutliers=calcOutliers,
  2611                                                       outlierThreshold=outlierThreshold,
  2612                                                       outlierMaskFilterOpts=outlierMaskFilterOpts,
  2613                                                       calcArtifactTrace=calcArtifactTrace,
  2614                                                       linearDetrend=linearDetrend,
  2615                                                       motorEncoderMask=motorEncoderMask,
  2616                                                       electrodeArrayName=electrodeArrayName,
  2617                                                       calcAverageLFP=calcAverageLFP,
  2618                                                       eventInfo=eventInfo,
  2619                                                       asigNameList=asigNameList, ainpNameList=ainpNameList,
  2620                                                       saveFromAsigNameList=saveFromAsigNameList,
  2621                                                       spikeSourceType=spikeSourceType,
  2622                                                       spikeBlock=spikeBlock,
  2623                                                       calcRigEvents=calcRigEvents,
  2624                                                       normalizeByImpedance=normalizeByImpedance,
  2625                                                       removeMeanAcross=removeMeanAcross,
  2626                                                       LFPFilterOpts=LFPFilterOpts,
  2627                                                       encoderCountPerDegree=encoderCountPerDegree,
  2628                                                       outlierRemovalDebugFlag=outlierRemovalDebugFlag,
  2629                                                       impedanceFilePath=impedanceFilePath,
  2630                                                       )
  2631                                                   #### diagnostics
  2632                                                   diagnosticFolder = os.path.join(
  2633                                                       outputFolderPath,
  2634                                                       'preprocDiagnostics',
  2635                                                       # fileName + nameSuffix + partNameSuffix
  2636                                                       )
  2637                                                   if not os.path.exists(diagnosticFolder):
  2638                                                       os.mkdir(diagnosticFolder)
  2639                                                   asigDiagnostics = {}
  2640                                                   outlierDiagnostics = {}
  2641                                                   diagnosticText = ''
  2642                                                   for asig in block.filter(objects=AnalogSignal):
  2643                                                       annNames = ['mean_removal_r2', 'mean_removal_group']
  2644                                                       for annName in annNames:
  2645                                                           if annName in asig.annotations:
  2646                                                               if asig.name not in asigDiagnostics:
  2647                                                                   asigDiagnostics[asig.name] = {}
  2648                                                               asigDiagnostics[asig.name].update({
  2649                                                                   annName: asig.annotations[annName]})
  2650                                                       annNames = [
  2651                                                           'outlierProportion', 'nDim',
  2652                                                           'noveltyThreshold', 'outlierThreshold'
  2653                                                           ]
  2654                                                       for annName in annNames:
  2655                                                           if annName in asig.annotations:
  2656                                                               if asig.name not in outlierDiagnostics:
  2657                                                                   outlierDiagnostics[asig.name] = {}
  2658                                                               outlierDiagnostics[asig.name].update({
  2659                                                                   annName: '{}'.format(asig.annotations[annName])
  2660                                                               })
  2661                                                   if removeMeanAcross:
  2662                                                       asigDiagnosticsDF = pd.DataFrame(asigDiagnostics).T
  2663                                                       asigDiagnosticsDF.sort_values(by='mean_removal_r2', inplace=True)
  2664                                                       diagnosticText += '<h2>LFP Diagnostics</h2>\n'
  2665                                                       diagnosticText += asigDiagnosticsDF.to_html()
  2666                                                       fig, ax = plt.subplots()
  2667                                                       sns.distplot(asigDiagnosticsDF['mean_removal_r2'], ax=ax)
  2668                                                       ax.set_ylabel('Count of analog signals')
  2669                                                       ax.set_xlabel('R^2 of regressing mean against signal')
  2670                                                       fig.savefig(os.path.join(
  2671                                                               diagnosticFolder,
  2672                                                               fileName + nameSuffix + partNameSuffix + '_meanRemovalR2.png'
  2673                                                           ))
  2674                                                   if interpolateOutliers:
  2675                                                       outlierDiagnosticsDF = pd.DataFrame(outlierDiagnostics).T
  2676                                                       diagnosticText += '<h2>Outlier Diagnostics</h2>\n'
  2677                                                       diagnosticText += outlierDiagnosticsDF.to_html()
  2678                                                   diagnosticTextPath = os.path.join(
  2679                                                       diagnosticFolder,
  2680                                                       fileName + nameSuffix + partNameSuffix + '_asigDiagnostics.html'
  2681                                                       )
  2682                                                   with open(diagnosticTextPath, 'w') as _f:
  2683                                                       _f.write(diagnosticText)
  2684                                                   writer.close()
  2685                                               chunkingInfoPath = os.path.join(
  2686                                                   outputFolderPath,
  2687                                                   fileName + nameSuffix +
  2688                                                   '_chunkingInfo.json'
  2689                                                   )
  2690                                               if os.path.exists(chunkingInfoPath):
  2691                                                   os.remove(chunkingInfoPath)
  2692                                               with open(chunkingInfoPath, 'w') as f:
  2693                                                   json.dump(chunkingMetadata, f)
  2694                                               return

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: preprocBlockToNix at line 2696

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  2696                                           @profile
  2697                                           def preprocBlockToNix(
  2698                                                   block, writer,
  2699                                                   chunkTStart=None,
  2700                                                   chunkTStop=None,
  2701                                                   eventInfo=None,
  2702                                                   fillOverflow=False, calcAverageLFP=False,
  2703                                                   interpolateOutliers=False, calcOutliers=False,
  2704                                                   calcArtifactTrace=False,
  2705                                                   outlierMaskFilterOpts=None,
  2706                                                   useMeanToCenter=False,   # mean center? median center?
  2707                                                   linearDetrend=False,
  2708                                                   zScoreEachTrace=False,
  2709                                                   outlierThreshold=1,
  2710                                                   motorEncoderMask=None,
  2711                                                   electrodeArrayName='utah',
  2712                                                   removeJumps=False, trackMemory=True,
  2713                                                   asigNameList=None, ainpNameList=None,
  2714                                                   saveFromAsigNameList=True,
  2715                                                   spikeSourceType='', spikeBlock=None,
  2716                                                   calcRigEvents=True,
  2717                                                   normalizeByImpedance=True,
  2718                                                   impedanceFilePath=None,
  2719                                                   removeMeanAcross=False,
  2720                                                   LFPFilterOpts=None, encoderCountPerDegree=180e2,
  2721                                                   outlierRemovalDebugFlag=False,
  2722                                                   ):
  2723                                               #  prune out nev spike placeholders
  2724                                               #  (will get added back on a chunk by chunk basis,
  2725                                               #  if not pruning units)
  2726                                               if spikeSourceType == 'nev':
  2727                                                   pruneOutUnits = False
  2728                                               else:
  2729                                                   pruneOutUnits = True
  2730                                               #
  2731                                               for chanIdx in block.channel_indexes:
  2732                                                   if chanIdx.units:
  2733                                                       for unit in chanIdx.units:
  2734                                                           if unit.spiketrains:
  2735                                                               unit.spiketrains = []
  2736                                                       if pruneOutUnits:
  2737                                                           chanIdx.units = []
  2738                                               #
  2739                                               if spikeBlock is not None:
  2740                                                   for chanIdx in spikeBlock.channel_indexes:
  2741                                                       if chanIdx.units:
  2742                                                           for unit in chanIdx.units:
  2743                                                               if unit.spiketrains:
  2744                                                                   unit.spiketrains = []
  2745                                               #  precalculate new segment
  2746                                               seg = block.segments[0]
  2747                                               #  remove chanIndexes assigned to units; makes more sense to
  2748                                               #  only use chanIdx for asigs and spikes on that asig
  2749                                               #  block.channel_indexes = (
  2750                                               #      [chanIdx for chanIdx in block.channel_indexes if (
  2751                                               #          chanIdx.analogsignals)])
  2752                                               if calcAverageLFP:
  2753                                                   lastIndex = len(block.channel_indexes)
  2754                                                   lastID = block.channel_indexes[-1].channel_ids[0] + 1
  2755                                                   if asigNameList is None:
  2756                                                       asigNameList = [
  2757                                                           [
  2758                                                               childBaseName(a.name, 'seg')
  2759                                                               for a in seg.analogsignals
  2760                                                               if not (('ainp' in a.name) or ('analog' in a.name))]
  2761                                                           ]
  2762                                                   nMeanChans = len(asigNameList)
  2763                                                   #
  2764                                                   meanChIdxList = []
  2765                                                   for meanChIdx in range(nMeanChans):
  2766                                                       tempChIdx = ChannelIndex(
  2767                                                           index=[lastIndex + meanChIdx],
  2768                                                           channel_names=['{}_rawAverage_{}'.format(electrodeArrayName, meanChIdx)],
  2769                                                           channel_ids=[lastID + meanChIdx],
  2770                                                           name='{}_rawAverage_{}'.format(electrodeArrayName, meanChIdx),
  2771                                                           file_origin=block.channel_indexes[-1].file_origin
  2772                                                           )
  2773                                                       tempChIdx.merge_annotations(block.channel_indexes[-1])
  2774                                                       block.channel_indexes.append(tempChIdx)
  2775                                                       meanChIdxList.append(tempChIdx)
  2776                                                       lastIndex += 1
  2777                                                       lastID += 1
  2778                                                   lastIndex = len(block.channel_indexes)
  2779                                                   lastID = block.channel_indexes[-1].channel_ids[0] + 1
  2780                                                   # if calcArtifactTrace:
  2781                                                   if True:
  2782                                                       artChIdxList = []
  2783                                                       for artChIdx in range(nMeanChans):
  2784                                                           tempChIdx = ChannelIndex(
  2785                                                               index=[lastIndex + artChIdx],
  2786                                                               channel_names=['{}_artifact_{}'.format(electrodeArrayName, artChIdx)],
  2787                                                               channel_ids=[lastID + artChIdx],
  2788                                                               name='{}_artifact_{}'.format(electrodeArrayName, artChIdx),
  2789                                                               file_origin=block.channel_indexes[-1].file_origin
  2790                                                               )
  2791                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2792                                                           block.channel_indexes.append(tempChIdx)
  2793                                                           artChIdxList.append(tempChIdx)
  2794                                                           lastIndex += 1
  2795                                                           lastID += 1
  2796                                                   # if calcOutliers:
  2797                                                   if True:
  2798                                                       devChIdxList = []
  2799                                                       for devChIdx in range(nMeanChans):
  2800                                                           tempChIdx = ChannelIndex(
  2801                                                               index=[lastIndex + devChIdx],
  2802                                                               channel_names=['{}_deviation_{}'.format(electrodeArrayName, devChIdx)],
  2803                                                               channel_ids=[lastID + devChIdx],
  2804                                                               name='{}_deviation_{}'.format(electrodeArrayName, devChIdx),
  2805                                                               file_origin=block.channel_indexes[-1].file_origin
  2806                                                               )
  2807                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2808                                                           block.channel_indexes.append(tempChIdx)
  2809                                                           devChIdxList.append(tempChIdx)
  2810                                                           lastIndex += 1
  2811                                                           lastID += 1
  2812                                                       smDevChIdxList = []
  2813                                                       for devChIdx in range(nMeanChans):
  2814                                                           tempChIdx = ChannelIndex(
  2815                                                               index=[lastIndex + devChIdx],
  2816                                                               channel_names=['{}_smoothed_deviation_{}'.format(electrodeArrayName, devChIdx)],
  2817                                                               channel_ids=[lastID + devChIdx],
  2818                                                               name='{}_smoothed_deviation_{}'.format(electrodeArrayName, devChIdx),
  2819                                                               file_origin=block.channel_indexes[-1].file_origin
  2820                                                               )
  2821                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2822                                                           block.channel_indexes.append(tempChIdx)
  2823                                                           smDevChIdxList.append(tempChIdx)
  2824                                                           lastIndex += 1
  2825                                                           lastID += 1
  2826                                                       outMaskChIdxList = []
  2827                                                       for outMaskChIdx in range(nMeanChans):
  2828                                                           tempChIdx = ChannelIndex(
  2829                                                               index=[lastIndex + outMaskChIdx],
  2830                                                               channel_names=['{}_outlierMask_{}'.format(
  2831                                                                   electrodeArrayName, outMaskChIdx)],
  2832                                                               channel_ids=[lastID + outMaskChIdx],
  2833                                                               name='{}_outlierMask_{}'.format(
  2834                                                                   electrodeArrayName, outMaskChIdx),
  2835                                                               file_origin=block.channel_indexes[-1].file_origin
  2836                                                               )
  2837                                                           tempChIdx.merge_annotations(block.channel_indexes[-1])
  2838                                                           block.channel_indexes.append(tempChIdx)
  2839                                                           outMaskChIdxList.append(tempChIdx)
  2840                                                           lastIndex += 1
  2841                                                           lastID += 1
  2842                                               #  delete asig and irsig proxies from channel index list
  2843                                               for metaIdx, chanIdx in enumerate(block.channel_indexes):
  2844                                                   if chanIdx.analogsignals:
  2845                                                       chanIdx.analogsignals = []
  2846                                                   if chanIdx.irregularlysampledsignals:
  2847                                                       chanIdx.irregularlysampledsignals = []
  2848                                               newSeg = Segment(
  2849                                                       index=0, name=seg.name,
  2850                                                       description=seg.description,
  2851                                                       file_origin=seg.file_origin,
  2852                                                       file_datetime=seg.file_datetime,
  2853                                                       rec_datetime=seg.rec_datetime,
  2854                                                       **seg.annotations
  2855                                                   )
  2856                                               block.segments = [newSeg]
  2857                                               block, nixblock = writer.write_block_meta(block)
  2858                                               # descend into Segments
  2859                                               if impedanceFilePath is not None:
  2860                                                   try:
  2861                                                       impedances = prb_meta.getLatestImpedance(
  2862                                                           block=block, impedanceFilePath=impedanceFilePath)
  2863                                                       averageImpedance = impedances['impedance'].median()
  2864                                                   except Exception:
  2865                                                       traceback.print_exc()
  2866                                               # for segIdx, seg in enumerate(oldSegList):
  2867                                               if spikeBlock is not None:
  2868                                                   spikeSeg = spikeBlock.segments[0]
  2869                                               else:
  2870                                                   spikeSeg = seg
  2871                                               #
  2872                                               if trackMemory:
  2873                                                   print('memory usage: {:.1f} MB'.format(
  2874                                                       prf.memory_usage_psutil()))
  2875                                               newSeg, nixgroup = writer._write_segment_meta(newSeg, nixblock)
  2876                                               #  trim down list of analog signals if necessary
  2877                                               asigNameListSeg = []
  2878                                               if (removeMeanAcross or calcAverageLFP):
  2879                                                   meanGroups = {}
  2880                                               for subListIdx, subList in enumerate(asigNameList):
  2881                                                   subListSeg = [
  2882                                                       'seg{}_{}'.format(0, a)
  2883                                                       for a in subList]
  2884                                                   asigNameListSeg += subListSeg
  2885                                                   if (removeMeanAcross or calcAverageLFP):
  2886                                                       meanGroups[subListIdx] = subListSeg
  2887                                               aSigList = []
  2888                                               # [asig.name for asig in seg.analogsignals]
  2889                                               for a in seg.analogsignals:
  2890                                                   # if np.any([n in a.name for n in asigNameListSeg]):
  2891                                                   if a.name in asigNameListSeg:
  2892                                                       aSigList.append(a)
  2893                                               if ainpNameList is not None:
  2894                                                   ainpNameListSeg = [
  2895                                                       'seg{}_{}'.format(0, a)
  2896                                                       for a in ainpNameList]
  2897                                                   ainpList = []
  2898                                                   for a in seg.analogsignals:
  2899                                                       if np.any([n == a.name for n in ainpNameListSeg]):
  2900                                                           ainpList.append(a)
  2901                                               else:
  2902                                                   ainpList = [
  2903                                                       a
  2904                                                       for a in seg.analogsignals
  2905                                                       if (('ainp' in a.name) or ('analog' in a.name))]
  2906                                                   ainpNameListSeg = [a.name for a in aSigList]
  2907                                               nAsigs = len(aSigList)
  2908                                               if LFPFilterOpts is not None:
  2909                                                   def filterFun(sig, filterCoeffs=None):
  2910                                                       # sig[:] = signal.sosfiltfilt(
  2911                                                       sig[:] = signal.sosfilt(
  2912                                                           filterCoeffs, sig.magnitude.flatten())[:, np.newaxis] * sig.units
  2913                                                       return sig
  2914                                                   filterCoeffs = hf.makeFilterCoeffsSOS(
  2915                                                       LFPFilterOpts, float(seg.analogsignals[0].sampling_rate))
  2916                                                   if False:
  2917                                                       fig, ax1, ax2 = hf.plotFilterResponse(
  2918                                                           filterCoeffs,
  2919                                                           float(seg.analogsignals[0].sampling_rate))
  2920                                                       fig2, ax3, ax4 = hf.plotFilterImpulseResponse(
  2921                                                           LFPFilterOpts,
  2922                                                           float(seg.analogsignals[0].sampling_rate))
  2923                                                       plt.show()
  2924                                               # first pass through asigs, if removing mean across channels
  2925                                               if (removeMeanAcross or calcAverageLFP):
  2926                                                   for aSigIdx, aSigProxy in enumerate(seg.analogsignals):
  2927                                                       if aSigIdx == 0:
  2928                                                           # check bounds
  2929                                                           tStart = max(chunkTStart * pq.s, aSigProxy.t_start)
  2930                                                           tStop = min(chunkTStop * pq.s, aSigProxy.t_stop)
  2931                                                       loadThisOne = (aSigProxy in aSigList)
  2932                                                       if loadThisOne:
  2933                                                           if trackMemory:
  2934                                                               print(
  2935                                                                   'Extracting asig for mean, memory usage: {:.1f} MB'.format(
  2936                                                                       prf.memory_usage_psutil()))
  2937                                                           chanIdx = aSigProxy.channel_index
  2938                                                           asig = aSigProxy.load(
  2939                                                               time_slice=(tStart, tStop),
  2940                                                               magnitude_mode='rescaled')
  2941                                                           if 'tempLFPStore' not in locals():
  2942                                                               tempLFPStore = pd.DataFrame(
  2943                                                                   np.zeros(
  2944                                                                       (asig.shape[0], nAsigs),
  2945                                                                       dtype=np.float32),
  2946                                                                   columns=asigNameListSeg)
  2947                                                           if 'dummyAsig' not in locals():
  2948                                                               dummyAsig = asig.copy()
  2949                                                           #  perform requested preproc operations
  2950                                                           #  if LFPFilterOpts is not None:
  2951                                                           #      asig[:] = filterFun(
  2952                                                           #          asig, filterCoeffs=filterCoeffs)
  2953                                                           if normalizeByImpedance:
  2954                                                               elNmMatchMsk = impedances['elec'] == chanIdx.name
  2955                                                               '''
  2956                                                               asig.magnitude[:] = (
  2957                                                                   (asig.magnitude - np.median(asig.magnitude)) /
  2958                                                                   np.min(
  2959                                                                       impedances.loc[elNmMatchMsk, 'impedance']
  2960                                                                       ))
  2961                                                               '''
  2962                                                               asig.magnitude[:] = (
  2963                                                                   (asig.magnitude) * averageImpedance /
  2964                                                                   np.min(
  2965                                                                       impedances.loc[elNmMatchMsk, 'impedance']
  2966                                                                       ))
  2967                                                           # if fillOverflow:
  2968                                                           #     # fill in overflow:
  2969                                                           #     '''
  2970                                                           #     timeSection['data'], overflowMask = hf.fillInOverflow(
  2971                                                           #         timeSection['data'], fillMethod = 'average')
  2972                                                           #     badData.update({'overflow': overflowMask})
  2973                                                           #     '''
  2974                                                           #     pass
  2975                                                           # if removeJumps:
  2976                                                           #     # find unusual jumps in derivative or amplitude
  2977                                                           #     '''
  2978                                                           #     timeSection['data'], newBadData = hf.fillInJumps(timeSection['data'],
  2979                                                           #     timeSection['samp_per_s'], smoothing_ms = 0.5, nStdDiff = 50,
  2980                                                           #     nStdAmp = 100)
  2981                                                           #     badData.update(newBadData)
  2982                                                           #     '''
  2983                                                           #     pass
  2984                                                           tempLFPStore.loc[:, aSigProxy.name] = asig.magnitude.flatten()
  2985                                                           del asig
  2986                                                           gc.collect()
  2987                                                   # end of first pass
  2988                                                   if (removeMeanAcross or calcAverageLFP):
  2989                                                       centerLFP = np.zeros(
  2990                                                           (tempLFPStore.shape[0], len(asigNameList)),
  2991                                                           dtype=np.float32)
  2992                                                       spreadLFP = np.zeros(
  2993                                                           (tempLFPStore.shape[0], len(asigNameList)),
  2994                                                           dtype=np.float32)
  2995                                                       # if calcOutliers:
  2996                                                       if True:
  2997                                                           if outlierMaskFilterOpts is not None:
  2998                                                               filterCoeffsOutlierMask = hf.makeFilterCoeffsSOS(
  2999                                                                   outlierMaskFilterOpts, float(dummyAsig.sampling_rate))
  3000                                                           lfpDeviation = np.zeros(
  3001                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3002                                                               dtype=np.float32)
  3003                                                           smoothedDeviation = np.zeros(
  3004                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3005                                                               dtype=np.float32)
  3006                                                           outlierMask = np.zeros(
  3007                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3008                                                               dtype=np.bool)
  3009                                                           outlierMetadata = {}
  3010                                                       # if calcArtifactTrace:
  3011                                                       if True:
  3012                                                           artifactSignal = np.zeros(
  3013                                                               (tempLFPStore.shape[0], len(asigNameList)),
  3014                                                               dtype=np.float32)
  3015                                                       ###############
  3016                                                       # tempLFPStore.iloc[:, 0] = np.nan  # for debugging axes
  3017                                                       #############
  3018                                                       plotDevFilterDebug = False
  3019                                                       if plotDevFilterDebug:
  3020                                                           try:
  3021                                                               devFiltDebugMask = (dummyAsig.times > 90 * pq.s) & (dummyAsig.times < 92 * pq.s)
  3022                                                           except Exception:
  3023                                                               pdb.set_trace()
  3024                                                           plotColIdx = 1
  3025                                                           ddfFig, ddfAx = plt.subplots(len(asigNameList), 1)
  3026                                                           ddfFig2, ddfAx2 = plt.subplots()
  3027                                                           ddfFig3, ddfAx3 = plt.subplots(
  3028                                                               1, len(asigNameList),
  3029                                                               sharey=True)
  3030                                                           if len(asigNameList) == 1:
  3031                                                               ddfAx = np.asarray([ddfAx])
  3032                                                               ddfAx3 = np.asarray([ddfAx3])
  3033                                                       for subListIdx, subList in enumerate(asigNameList):
  3034                                                           columnsForThisGroup = meanGroups[subListIdx]
  3035                                                           if trackMemory:
  3036                                                               print(
  3037                                                                   'asig group {}: calculating mean, memory usage: {:.1f} MB'.format(
  3038                                                                       subListIdx, prf.memory_usage_psutil()))
  3039                                                               print('this group contains\n{}'.format(columnsForThisGroup))
  3040                                                           if plotDevFilterDebug:
  3041                                                               ddfAx3[subListIdx].plot(
  3042                                                                   dummyAsig.times[devFiltDebugMask],
  3043                                                                   tempLFPStore.loc[:, columnsForThisGroup].iloc[devFiltDebugMask, plotColIdx],
  3044                                                                   label='original ch'
  3045                                                                   )
  3046                                                           if fillOverflow:
  3047                                                               print('Filling overflow...')
  3048                                                               # fill in overflow:
  3049                                                               tempLFPStore.loc[:, columnsForThisGroup], pltHandles = hf.fillInOverflow2(
  3050                                                                   tempLFPStore.loc[:, columnsForThisGroup].to_numpy(),
  3051                                                                   overFlowFillType='average',
  3052                                                                   overFlowThreshold=8000,
  3053                                                                   debuggingPlots=plotDevFilterDebug
  3054                                                                   )
  3055                                                               if plotDevFilterDebug:
  3056                                                                   pltHandles['ax'].set_title('ch grp {}'.format(subListIdx))
  3057                                                                   ddfAx3[subListIdx].plot(
  3058                                                                       dummyAsig.times[devFiltDebugMask],
  3059                                                                       tempLFPStore.loc[:, columnsForThisGroup].iloc[devFiltDebugMask, plotColIdx],
  3060                                                                       label='filled ch'
  3061                                                                       )
  3062                                                           # zscore of each trace
  3063                                                           if zScoreEachTrace:
  3064                                                               print('About to calculate zscore of each trace (along columns) for prelim outlier detection')
  3065                                                               columnZScore = pd.DataFrame(
  3066                                                                   stats.zscore(
  3067                                                                       tempLFPStore.loc[:, columnsForThisGroup],
  3068                                                                       axis=1),
  3069                                                                   index=tempLFPStore.index,
  3070                                                                   columns=columnsForThisGroup
  3071                                                                   )
  3072                                                               excludeFromMeanMask = columnZScore.abs() > 6
  3073                                                               if useMeanToCenter:
  3074                                                                   centerLFP[:, subListIdx] = (
  3075                                                                       tempLFPStore
  3076                                                                       .loc[:, columnsForThisGroup]
  3077                                                                       .mask(excludeFromMeanMask)
  3078                                                                       .mean(axis=1).to_numpy()
  3079                                                                       )
  3080                                                               else:
  3081                                                                   centerLFP[:, subListIdx] = (
  3082                                                                       tempLFPStore
  3083                                                                       .loc[:, columnsForThisGroup]
  3084                                                                       .mask(excludeFromMeanMask)
  3085                                                                       .median(axis=1).to_numpy()
  3086                                                                       )
  3087                                                           else:
  3088                                                               if useMeanToCenter:
  3089                                                                   centerLFP[:, subListIdx] = (
  3090                                                                       tempLFPStore
  3091                                                                       .loc[:, columnsForThisGroup]
  3092                                                                       .mean(axis=1).to_numpy()
  3093                                                                       )
  3094                                                               else:
  3095                                                                   centerLFP[:, subListIdx] = (
  3096                                                                       tempLFPStore
  3097                                                                       .loc[:, columnsForThisGroup]
  3098                                                                       .median(axis=1).to_numpy()
  3099                                                                       )
  3100                                                           if calcArtifactTrace:
  3101                                                               if LFPFilterOpts is not None:
  3102                                                                   print('applying LFPFilterOpts to cached asigs for artifact ID')
  3103                                                                   # tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfilt(
  3104                                                                   tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfiltfilt(
  3105                                                                       filterCoeffs, tempLFPStore.loc[:, columnsForThisGroup],
  3106                                                                       axis=0)
  3107                                                                   if useMeanToCenter:
  3108                                                                       tempCenter = (
  3109                                                                           tempLFPStore
  3110                                                                           .loc[:, columnsForThisGroup]
  3111                                                                           .mean(axis=1).diff().fillna(0)
  3112                                                                           )
  3113                                                                   else:
  3114                                                                       tempCenter = (
  3115                                                                           tempLFPStore
  3116                                                                           .loc[:, columnsForThisGroup]
  3117                                                                           .median(axis=1).diff().fillna(0)
  3118                                                                           )
  3119                                                               artifactSignal[:, subListIdx] = np.abs(stats.zscore(tempCenter.to_numpy()))
  3120                                                           if calcOutliers:
  3121                                                               if plotDevFilterDebug:
  3122                                                                   ddfAx[subListIdx].plot(
  3123                                                                       dummyAsig.times[devFiltDebugMask],
  3124                                                                       centerLFP[devFiltDebugMask, subListIdx],
  3125                                                                       label='mean of ch group'
  3126                                                                       )
  3127                                                               # filter the traces, if needed
  3128                                                               if LFPFilterOpts is not None:
  3129                                                                   print('applying LFPFilterOpts to cached asigs before outlier detection')
  3130                                                                   # tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfiltfilt(
  3131                                                                   tempLFPStore.loc[:, columnsForThisGroup] = signal.sosfilt(
  3132                                                                       filterCoeffs, tempLFPStore.loc[:, columnsForThisGroup],
  3133                                                                       axis=0)
  3134                                                                   if plotDevFilterDebug:
  3135                                                                       ddfAx3[subListIdx].plot(
  3136                                                                           dummyAsig.times[devFiltDebugMask],
  3137                                                                           tempLFPStore.loc[:, columnsForThisGroup].iloc[devFiltDebugMask, plotColIdx],
  3138                                                                           label='filtered ch'
  3139                                                                           )
  3140                                                               ##################################
  3141                                                               print('Whitening cached traces before outlier detection')
  3142                                                               whitenByPCA = True
  3143                                                               if whitenByPCA:
  3144                                                                   projector = PCA(
  3145                                                                       n_components=None, whiten=True)
  3146                                                                   pcs = projector.fit_transform(
  3147                                                                       tempLFPStore.loc[:, columnsForThisGroup])
  3148                                                                   explVarMask = (
  3149                                                                       np.cumsum(projector.explained_variance_ratio_) < 1 - 1e-2)
  3150                                                                   explVarMask[0] = True  # (keep at least 1)
  3151                                                                   pcs = pcs[:, explVarMask]
  3152                                                                   nDim = pcs.shape[1]
  3153                                                                   lfpDeviation[:, subListIdx] = (pcs ** 2).sum(axis=1)
  3154                                                               else:  # whiten by mahalanobis distance
  3155                                                                   est = EmpiricalCovariance()
  3156                                                                   est.fit(tempLFPStore.loc[:, columnsForThisGroup].to_numpy())
  3157                                                                   lfpDeviation[:, subListIdx] = est.mahalanobis(
  3158                                                                       tempLFPStore.loc[:, columnsForThisGroup].to_numpy())
  3159                                                                   nDim = tempLFPStore.loc[:, columnsForThisGroup].shape[1]
  3160                                                               #
  3161                                                               transformedDeviation = stats.norm.isf(stats.chi2.sf(lfpDeviation[:, subListIdx], nDim))
  3162                                                               infMask = np.isinf(transformedDeviation)
  3163                                                               if infMask.any():
  3164                                                                   transformedDeviation[infMask] = transformedDeviation[~infMask].max()
  3165                                                               debugProbaTrans = False
  3166                                                               if debugProbaTrans:
  3167                                                                   fig, ax = plt.subplots()
  3168                                                                   tAx = ax.twinx()
  3169                                                                   plotMask = (dummyAsig.times >= 60 * pq.s) & (dummyAsig.times < 95 * pq.s)
  3170                                                                   ax.plot(dummyAsig.times[plotMask], transformedDeviation[plotMask], c='b', label='transformed deviation')
  3171                                                                   tAx.plot(dummyAsig.times[plotMask], lfpDeviation[plotMask, subListIdx], c='r', label='original deviation')
  3172                                                                   ax.legend(loc='upper left')
  3173                                                                   tAx.legend(loc='upper right')
  3174                                                                   plt.show()
  3175                                                               lfpDeviation[:, subListIdx] = transformedDeviation
  3176                                                               noveltyThreshold = stats.norm.interval(outlierThreshold)[1]
  3177                                                               # chi2Bounds = stats.chi2.interval(outlierThreshold, nDim)
  3178                                                               # lfpDeviation[:, subListIdx] = lfpDeviation[:, subListIdx] / chi2Bounds[1]
  3179                                                               # print('nDim = {}, chi2Lim = {}'.format(nDim, chi2Bounds))
  3180                                                               # noveltyThreshold = 1
  3181                                                               #
  3182                                                               outlierMetadata[subListIdx] = {
  3183                                                                   'nDim': nDim,
  3184                                                                   'noveltyThreshold': noveltyThreshold,
  3185                                                                   'outlierThreshold': outlierThreshold
  3186                                                                   }
  3187                                                               # smoothedDeviation = signal.sosfilt(
  3188                                                               print('Smoothing deviation')
  3189                                                               tempSmDev = signal.sosfiltfilt(
  3190                                                                   filterCoeffsOutlierMask, lfpDeviation[:, subListIdx])
  3191                                                               smoothedDeviation[:, subListIdx] = tempSmDev
  3192                                                               if plotDevFilterDebug:
  3193                                                                   ddfAx[subListIdx].plot(
  3194                                                                       dummyAsig.times[devFiltDebugMask],
  3195                                                                       lfpDeviation[devFiltDebugMask, subListIdx],
  3196                                                                       label='original deviation (ch grp {})'.format(subListIdx))
  3197                                                                   ddfAx[subListIdx].plot(
  3198                                                                       dummyAsig.times[devFiltDebugMask],
  3199                                                                       smoothedDeviation[devFiltDebugMask, subListIdx],
  3200                                                                       label='filtered deviation (ch grp {})'.format(subListIdx))
  3201                                                               ##
  3202                                                               print('Calculating outlier mask')
  3203                                                               outlierMask[:, subListIdx] = (
  3204                                                                   smoothedDeviation[:, subListIdx] > noveltyThreshold)
  3205                                                               if plotDevFilterDebug:
  3206                                                                   ddfAx[subListIdx].axhline(noveltyThreshold, c='r')
  3207                                                       if plotDevFilterDebug and calcOutliers:
  3208                                                           for subListIdx, subList in enumerate(asigNameList):
  3209                                                               ddfAx[subListIdx].legend(loc='upper right')
  3210                                                               ddfAx[subListIdx].set_title('Deviation')
  3211                                                               ddfAx3[subListIdx].legend(loc='upper right')
  3212                                                               ddfAx3[subListIdx].set_title('Example channel')
  3213                                                               ddfAx2.plot(
  3214                                                                   dummyAsig.times[devFiltDebugMask],
  3215                                                                   smoothedDeviation[devFiltDebugMask, subListIdx],
  3216                                                                   label='ch grp {}'.format(subListIdx))
  3217                                                               ddfAx2.set_title('Smoothed Deviation')
  3218                                                           ddfAx2.legend(loc='upper right')
  3219                                                           plt.show()
  3220                                                       #############
  3221                                                       del tempLFPStore
  3222                                                       gc.collect()
  3223                                               if (removeMeanAcross or calcAverageLFP):
  3224                                                   for mIdx, meanChIdx in enumerate(meanChIdxList):
  3225                                                       meanAsig = AnalogSignal(
  3226                                                           centerLFP[:, mIdx],
  3227                                                           units=dummyAsig.units,
  3228                                                           sampling_rate=dummyAsig.sampling_rate,
  3229                                                           # name='seg{}_{}'.format(idx, meanChIdx.name)
  3230                                                           name='seg{}_{}'.format(0, meanChIdx.name),
  3231                                                           t_start=tStart
  3232                                                       )
  3233                                                       # assign ownership to containers
  3234                                                       meanChIdx.analogsignals.append(meanAsig)
  3235                                                       newSeg.analogsignals.append(meanAsig)
  3236                                                       # assign parent to children
  3237                                                       meanChIdx.create_relationship()
  3238                                                       newSeg.create_relationship()
  3239                                                       # write out to file
  3240                                                       if LFPFilterOpts is not None:
  3241                                                           meanAsig[:] = filterFun(
  3242                                                               meanAsig, filterCoeffs=filterCoeffs)
  3243                                                       meanAsig = writer._write_analogsignal(
  3244                                                           meanAsig, nixblock, nixgroup)
  3245                                                   # if calcArtifactTrace:
  3246                                                   if True:
  3247                                                       for mIdx, artChIdx in enumerate(artChIdxList):
  3248                                                           artAsig = AnalogSignal(
  3249                                                               artifactSignal[:, mIdx],
  3250                                                               units=dummyAsig.units,
  3251                                                               sampling_rate=dummyAsig.sampling_rate,
  3252                                                               # name='seg{}_{}'.format(idx, devChIdx.name)
  3253                                                               name='seg{}_{}'.format(0, artChIdx.name),
  3254                                                               t_start=tStart
  3255                                                               )
  3256                                                           # assign ownership to containers
  3257                                                           artChIdx.analogsignals.append(artAsig)
  3258                                                           newSeg.analogsignals.append(artAsig)
  3259                                                           # assign parent to children
  3260                                                           artChIdx.create_relationship()
  3261                                                           newSeg.create_relationship()
  3262                                                           # write out to file
  3263                                                           artAsig = writer._write_analogsignal(
  3264                                                               artAsig, nixblock, nixgroup)
  3265                                                           #########################################################
  3266                                                   # if calcOutliers:
  3267                                                   if True:
  3268                                                       for mIdx, devChIdx in enumerate(devChIdxList):
  3269                                                           devAsig = AnalogSignal(
  3270                                                               lfpDeviation[:, mIdx],
  3271                                                               units=dummyAsig.units,
  3272                                                               sampling_rate=dummyAsig.sampling_rate,
  3273                                                               # name='seg{}_{}'.format(idx, devChIdx.name)
  3274                                                               name='seg{}_{}'.format(0, devChIdx.name),
  3275                                                               t_start=tStart
  3276                                                               )
  3277                                                           # assign ownership to containers
  3278                                                           devChIdx.analogsignals.append(devAsig)
  3279                                                           newSeg.analogsignals.append(devAsig)
  3280                                                           # assign parent to children
  3281                                                           devChIdx.create_relationship()
  3282                                                           newSeg.create_relationship()
  3283                                                           # write out to file
  3284                                                           devAsig = writer._write_analogsignal(
  3285                                                               devAsig, nixblock, nixgroup)
  3286                                                           #########################################################
  3287                                                       for mIdx, smDevChIdx in enumerate(smDevChIdxList):
  3288                                                           smDevAsig = AnalogSignal(
  3289                                                               smoothedDeviation[:, mIdx],
  3290                                                               units=dummyAsig.units,
  3291                                                               sampling_rate=dummyAsig.sampling_rate,
  3292                                                               # name='seg{}_{}'.format(idx, devChIdx.name)
  3293                                                               name='seg{}_{}'.format(0, smDevChIdx.name),
  3294                                                               t_start=tStart
  3295                                                               )
  3296                                                           # assign ownership to containers
  3297                                                           smDevChIdx.analogsignals.append(smDevAsig)
  3298                                                           newSeg.analogsignals.append(smDevAsig)
  3299                                                           # assign parent to children
  3300                                                           smDevChIdx.create_relationship()
  3301                                                           newSeg.create_relationship()
  3302                                                           # write out to file
  3303                                                           smDevAsig = writer._write_analogsignal(
  3304                                                               smDevAsig, nixblock, nixgroup)
  3305                                                           #########################################################
  3306                                                       for mIdx, outMaskChIdx in enumerate(outMaskChIdxList):
  3307                                                           outMaskAsig = AnalogSignal(
  3308                                                               outlierMask[:, mIdx],
  3309                                                               units=dummyAsig.units,
  3310                                                               sampling_rate=dummyAsig.sampling_rate,
  3311                                                               # name='seg{}_{}'.format(idx, outMaskChIdx.name)
  3312                                                               name='seg{}_{}'.format(0, outMaskChIdx.name),
  3313                                                               t_start=tStart, dtype=np.float32
  3314                                                               )
  3315                                                           outMaskAsig.annotations['outlierProportion'] = np.mean(outlierMask[:, mIdx])
  3316                                                           if calcOutliers:
  3317                                                               outMaskAsig.annotations.update(outlierMetadata[mIdx])
  3318                                                           # assign ownership to containers
  3319                                                           outMaskChIdx.analogsignals.append(outMaskAsig)
  3320                                                           newSeg.analogsignals.append(outMaskAsig)
  3321                                                           # assign parent to children
  3322                                                           outMaskChIdx.create_relationship()
  3323                                                           newSeg.create_relationship()
  3324                                                           # write out to file
  3325                                                           outMaskAsig = writer._write_analogsignal(
  3326                                                               outMaskAsig, nixblock, nixgroup)
  3327                                                   #
  3328                                                   w0 = 60
  3329                                                   bandQ = 20
  3330                                                   bw = w0/bandQ
  3331                                                   noiseSos = signal.iirfilter(
  3332                                                       N=8, Wn=[w0 - bw/2, w0 + bw/2],
  3333                                                       btype='band', ftype='butter',
  3334                                                       analog=False, fs=float(dummyAsig.sampling_rate),
  3335                                                       output='sos')
  3336                                                   # signal.hilbert does not have an option to zero pad
  3337                                                   nextLen = fftpack.helper.next_fast_len(dummyAsig.shape[0])
  3338                                                   deficit = int(nextLen - dummyAsig.shape[0])
  3339                                                   lDef = int(np.floor(deficit / 2))
  3340                                                   rDef = int(np.ceil(deficit / 2)) + 1
  3341                                                   temp = np.pad(
  3342                                                       dummyAsig.magnitude.flatten(),
  3343                                                       (lDef, rDef), mode='constant')
  3344                                                   # lineNoise = signal.sosfiltfilt(
  3345                                                   lineNoise = signal.sosfilt(
  3346                                                       noiseSos, temp, axis=0)
  3347                                                   lineNoiseH = signal.hilbert(lineNoise)
  3348                                                   lineNoise = lineNoise[lDef:-rDef]
  3349                                                   lineNoiseH = lineNoiseH[lDef:-rDef]
  3350                                                   lineNoisePhase = np.angle(lineNoiseH)
  3351                                                   lineNoisePhaseDF = pd.DataFrame(
  3352                                                       lineNoisePhase,
  3353                                                       index=dummyAsig.times,
  3354                                                       columns=['phase']
  3355                                                       )
  3356                                                   plotHilbert = False
  3357                                                   if plotHilbert:
  3358                                                       lineNoiseFreq = (
  3359                                                           np.diff(np.unwrap(lineNoisePhase)) /
  3360                                                           (2.0*np.pi) * float(dummyAsig.sampling_rate))
  3361                                                       lineNoiseEnvelope = np.abs(lineNoiseH)
  3362                                                       i1 = 300000; i2 = 330000
  3363                                                       fig, ax = plt.subplots(2, 1, sharex=True)
  3364                                                       ax[0].plot(dummyAsig.times[devFiltDebugMask], dummyAsig.magnitude[devFiltDebugMask, :])
  3365                                                       ax[0].plot(dummyAsig.times[devFiltDebugMask], lineNoise[devFiltDebugMask])
  3366                                                       ax[0].plot(dummyAsig.times[devFiltDebugMask], lineNoiseEnvelope[devFiltDebugMask])
  3367                                                       axFr = ax[1].twinx()
  3368                                                       ax[1].plot(
  3369                                                           dummyAsig.times[devFiltDebugMask], lineNoisePhase[devFiltDebugMask],
  3370                                                           c='r', label='phase')
  3371                                                       ax[1].legend()
  3372                                                       axFr.plot(
  3373                                                           dummyAsig.times[devFiltDebugMask], lineNoiseFreq[devFiltDebugMask],
  3374                                                           label='freq')
  3375                                                       axFr.set_ylim([59, 61])
  3376                                                       axFr.legend()
  3377                                                       plt.show()
  3378                                               # second pass through asigs, to save
  3379                                               for aSigIdx, aSigProxy in enumerate(seg.analogsignals):
  3380                                                   if aSigIdx == 0:
  3381                                                       # check bounds
  3382                                                       tStart = max(chunkTStart * pq.s, aSigProxy.t_start)
  3383                                                       tStop = min(chunkTStop * pq.s, aSigProxy.t_stop)
  3384                                                   loadThisOne = (
  3385                                                       (saveFromAsigNameList and (aSigProxy in aSigList)) or
  3386                                                       (aSigProxy in ainpList)
  3387                                                       )
  3388                                                   if loadThisOne:
  3389                                                       if trackMemory:
  3390                                                           print('writing asig {} ({}) memory usage: {:.1f} MB'.format(
  3391                                                               aSigIdx, aSigProxy.name, prf.memory_usage_psutil()))
  3392                                                       chanIdx = aSigProxy.channel_index
  3393                                                       asig = aSigProxy.load(
  3394                                                           time_slice=(tStart, tStop),
  3395                                                           magnitude_mode='rescaled')
  3396                                                       #  link AnalogSignal and ID providing channel_index
  3397                                                       asig.channel_index = chanIdx
  3398                                                       #  perform requested preproc operations
  3399                                                       if 'impedances' in locals():
  3400                                                           elNmMatchMsk = impedances['elec'] == chanIdx.name
  3401                                                           if elNmMatchMsk.any():
  3402                                                               originalImpedance = np.min(
  3403                                                                   impedances.loc[elNmMatchMsk, 'impedance']
  3404                                                                   )
  3405                                                               asig.annotations['originalImpedance'] = originalImpedance
  3406                                                               if normalizeByImpedance and (aSigProxy not in ainpList):
  3407                                                                   '''
  3408                                                                   asig.magnitude[:] = (
  3409                                                                       (asig.magnitude - np.median(asig.magnitude)) /
  3410                                                                       np.min(
  3411                                                                           impedances.loc[elNmMatchMsk, 'impedance']
  3412                                                                           )
  3413                                                                       )
  3414                                                                   '''
  3415                                                                   print('Normalizing {} by {} kOhms'.format(asig.name, originalImpedance))
  3416                                                                   asig.magnitude[:] = (
  3417                                                                       (asig.magnitude * averageImpedance) / originalImpedance
  3418                                                                       )
  3419                                                       if fillOverflow:
  3420                                                           # fill in overflow:
  3421                                                           asig.magnitude[:], _ = hf.fillInOverflow2(
  3422                                                               asig.magnitude[:],
  3423                                                               overFlowFillType='average',
  3424                                                               overFlowThreshold=8000,
  3425                                                               debuggingPlots=False
  3426                                                               )
  3427                                                       if removeJumps:
  3428                                                           # find unusual jumps in derivative or amplitude
  3429                                                           '''
  3430                                                           timeSection['data'], newBadData = hf.fillInJumps(timeSection['data'],
  3431                                                           timeSection['samp_per_s'], smoothing_ms = 0.5, nStdDiff = 50,
  3432                                                           nStdAmp = 100)
  3433                                                           badData.update(newBadData)
  3434                                                           '''
  3435                                                           pass
  3436                                                       if calcAverageLFP and (aSigProxy not in ainpList):
  3437                                                           for k, cols in meanGroups.items():
  3438                                                               if asig.name in cols:
  3439                                                                   whichColumnToSubtract = k
  3440                                                           noiseModel = np.polyfit(
  3441                                                               centerLFP[:, whichColumnToSubtract],
  3442                                                               asig.magnitude.flatten(), 1, full=True)
  3443                                                           rSq = 1 - noiseModel[1][0] / np.sum(asig.magnitude.flatten() ** 2)
  3444                                                           asig.annotations['mean_removal_r2'] = rSq
  3445                                                           asig.annotations['mean_removal_group'] = whichColumnToSubtract
  3446                                                           if linearDetrend:
  3447                                                               noiseTerm = np.polyval(
  3448                                                                   noiseModel[0],
  3449                                                                   centerLFP[:, whichColumnToSubtract])
  3450                                                           else:
  3451                                                               noiseTerm = centerLFP[:, whichColumnToSubtract]
  3452                                                           ###
  3453                                                           plotMeanSubtraction = False
  3454                                                           if plotMeanSubtraction:
  3455                                                               i1 = 300000; i2 = 330000
  3456                                                               fig, ax = plt.subplots(1, 1)
  3457                                                               ax.plot(asig.times[devFiltDebugMask], asig.magnitude[devFiltDebugMask, :], label='channel')
  3458                                                               ax.plot(asig.times[devFiltDebugMask], centerLFP[devFiltDebugMask, whichColumnToSubtract], label='mean')
  3459                                                               ax.plot(asig.times[devFiltDebugMask], noiseTerm[devFiltDebugMask], label='adjusted mean')
  3460                                                               ax.legend()
  3461                                                               plt.show()
  3462                                                           ###
  3463                                                           if removeMeanAcross:
  3464                                                               asig.magnitude[:] = np.atleast_2d(
  3465                                                                   asig.magnitude.flatten() - noiseTerm).transpose()
  3466                                                               # asig.magnitude[:] = (
  3467                                                               #     asig.magnitude - np.median(asig.magnitude))
  3468                                                       if (LFPFilterOpts is not None) and (aSigProxy not in ainpList):
  3469                                                           asig.magnitude[:] = filterFun(asig, filterCoeffs=filterCoeffs)
  3470                                                       if (interpolateOutliers) and (aSigProxy not in ainpList) and (not outlierRemovalDebugFlag):
  3471                                                           for k, cols in meanGroups.items():
  3472                                                               if asig.name in cols:
  3473                                                                   whichColumnToSubtract = k
  3474                                                           tempSer = pd.Series(asig.magnitude.flatten())
  3475                                                           tempSer.loc[outlierMask[:, whichColumnToSubtract]] = np.nan
  3476                                                           tempSer = (
  3477                                                               tempSer
  3478                                                               .interpolate(method='linear', limit_area='inside')
  3479                                                               .fillna(method='ffill')
  3480                                                               .fillna(method='bfill')
  3481                                                               )
  3482                                                           asig.magnitude[:, 0] = tempSer.to_numpy()
  3483                                                       # pdb.set_trace()
  3484                                                       if (aSigProxy in aSigList) or (aSigProxy in ainpList):
  3485                                                           # assign ownership to containers
  3486                                                           chanIdx.analogsignals.append(asig)
  3487                                                           newSeg.analogsignals.append(asig)
  3488                                                           # assign parent to children
  3489                                                           chanIdx.create_relationship()
  3490                                                           newSeg.create_relationship()
  3491                                                           # write out to file
  3492                                                           asig = writer._write_analogsignal(
  3493                                                               asig, nixblock, nixgroup)
  3494                                                       del asig
  3495                                                       gc.collect()
  3496                                               for irSigIdx, irSigProxy in enumerate(
  3497                                                       seg.irregularlysampledsignals):
  3498                                                   chanIdx = irSigProxy.channel_index
  3499                                                   #
  3500                                                   isig = irSigProxy.load(
  3501                                                       time_slice=(tStart, tStop),
  3502                                                       magnitude_mode='rescaled')
  3503                                                   #  link irregularlysampledSignal
  3504                                                   #  and ID providing channel_index
  3505                                                   isig.channel_index = chanIdx
  3506                                                   # assign ownership to containers
  3507                                                   chanIdx.irregularlysampledsignals.append(isig)
  3508                                                   newSeg.irregularlysampledsignals.append(isig)
  3509                                                   # assign parent to children
  3510                                                   chanIdx.create_relationship()
  3511                                                   newSeg.create_relationship()
  3512                                                   # write out to file
  3513                                                   isig = writer._write_irregularlysampledsignal(
  3514                                                       isig, nixblock, nixgroup)
  3515                                                   del isig
  3516                                                   gc.collect()
  3517                                               #
  3518                                               if len(spikeSourceType):
  3519                                                   for stIdx, stProxy in enumerate(spikeSeg.spiketrains):
  3520                                                       if trackMemory:
  3521                                                           print('writing spiketrains mem usage: {}'.format(
  3522                                                               prf.memory_usage_psutil()))
  3523                                                       unit = stProxy.unit
  3524                                                       st = loadStProxy(stProxy)
  3525                                                       #  have to manually slice tStop and tStart because
  3526                                                       #  array annotations are not saved natively in the nix file
  3527                                                       #  (we're getting them as plain annotations)
  3528                                                       timeMask = np.asarray(
  3529                                                           (st.times >= tStart) & (st.times < tStop),
  3530                                                           dtype=np.bool)
  3531                                                       try:
  3532                                                           if 'arrayAnnNames' in st.annotations:
  3533                                                               for key in st.annotations['arrayAnnNames']:
  3534                                                                   st.annotations[key] = np.asarray(
  3535                                                                       st.annotations[key])[timeMask]
  3536                                                           st = st[timeMask]
  3537                                                           st.t_start = tStart
  3538                                                           st.t_stop = tStop
  3539                                                       except Exception:
  3540                                                           traceback.print_exc()
  3541                                                       #  tdc may or may not have the same channel ids, but
  3542                                                       #  it will have consistent channel names
  3543                                                       nameParser = re.search(
  3544                                                           r'([a-zA-Z0-9]*)#(\d*)', unit.name)
  3545                                                       chanLabel = nameParser.group(1)
  3546                                                       unitId = nameParser.group(2)
  3547                                                       #
  3548                                                       chIdxName = unit.name.replace('_stim', '').split('#')[0]
  3549                                                       chanIdx = block.filter(objects=ChannelIndex, name=chIdxName)[0]
  3550                                                       # [i.name for i in block.filter(objects=ChannelIndex)]
  3551                                                       # [i.name for i in spikeBlock.filter(objects=Unit)]
  3552                                                       #  print(unit.name)
  3553                                                       if not (unit in chanIdx.units):
  3554                                                           # first time at this unit, add to its chanIdx
  3555                                                           unit.channel_index = chanIdx
  3556                                                           chanIdx.units.append(unit)
  3557                                                       #  except Exception:
  3558                                                       #      traceback.print_exc()
  3559                                                       st.name = 'seg{}_{}'.format(0, unit.name)
  3560                                                       # st.name = 'seg{}_{}'.format(idx, unit.name)
  3561                                                       #  link SpikeTrain and ID providing unit
  3562                                                       if calcAverageLFP:
  3563                                                           if 'arrayAnnNames' in st.annotations:
  3564                                                               st.annotations['arrayAnnNames'] = list(st.annotations['arrayAnnNames'])
  3565                                                           else:
  3566                                                               st.annotations['arrayAnnNames'] = []
  3567                                                           st.annotations['arrayAnnNames'].append('phase60hz')
  3568                                                           phase60hz = hf.interpolateDF(
  3569                                                               lineNoisePhaseDF,
  3570                                                               newX=st.times, columns=['phase']).to_numpy().flatten()
  3571                                                           st.annotations.update({'phase60hz': phase60hz})
  3572                                                           plotPhaseDist = False
  3573                                                           if plotPhaseDist:
  3574                                                               sns.distplot(phase60hz)
  3575                                                               plt.show()
  3576                                                       st.unit = unit
  3577                                                       # assign ownership to containers
  3578                                                       unit.spiketrains.append(st)
  3579                                                       newSeg.spiketrains.append(st)
  3580                                                       # assign parent to children
  3581                                                       unit.create_relationship()
  3582                                                       newSeg.create_relationship()
  3583                                                       # write out to file
  3584                                                       st = writer._write_spiketrain(st, nixblock, nixgroup)
  3585                                                       del st
  3586                                               #  process proprio trial related events
  3587                                               if calcRigEvents:
  3588                                                   print('Processing rig events...')
  3589                                                   analogData = []
  3590                                                   for key, value in eventInfo['inputIDs'].items():
  3591                                                       searchName = 'seg{}_'.format(0) + value
  3592                                                       ainpAsig = seg.filter(
  3593                                                           objects=AnalogSignalProxy,
  3594                                                           name=searchName)[0]
  3595                                                       ainpData = ainpAsig.load(
  3596                                                           time_slice=(tStart, tStop),
  3597                                                           magnitude_mode='rescaled')
  3598                                                       analogData.append(
  3599                                                           pd.DataFrame(ainpData.magnitude, columns=[key]))
  3600                                                       del ainpData
  3601                                                       gc.collect()
  3602                                                   motorData = pd.concat(analogData, axis=1)
  3603                                                   del analogData
  3604                                                   gc.collect()
  3605                                                   if motorEncoderMask is not None:
  3606                                                       ainpData = ainpAsig.load(
  3607                                                           time_slice=(tStart, tStop),
  3608                                                           magnitude_mode='rescaled')
  3609                                                       ainpTime = ainpData.times.magnitude
  3610                                                       meTimeMask = np.zeros_like(ainpTime, dtype=np.bool)
  3611                                                       for meTimeBounds in motorEncoderMask:
  3612                                                           meTimeMask = (
  3613                                                               meTimeMask |
  3614                                                               (
  3615                                                                   (ainpTime > meTimeBounds[0]) &
  3616                                                                   (ainpTime < meTimeBounds[1])
  3617                                                                   )
  3618                                                               )
  3619                                                       columnsToOverride = ['A-', 'A+', 'B-', 'B+', 'Z-', 'Z+']
  3620                                                       for colName in columnsToOverride:
  3621                                                           motorData.loc[~meTimeMask, colName] = motorData.loc[:, colName].quantile(q=0.05)
  3622                                                       del ainpData, ainpTime
  3623                                                       gc.collect()
  3624                                                   motorData = mea.processMotorData(
  3625                                                       motorData, ainpAsig.sampling_rate.magnitude,
  3626                                                       encoderCountPerDegree=encoderCountPerDegree
  3627                                                       )
  3628                                                   keepCols = [
  3629                                                       'position', 'velocity', 'velocityCat',
  3630                                                       'rightBut_int', 'leftBut_int',
  3631                                                       'rightLED_int', 'leftLED_int', 'simiTrigs_int']
  3632                                                   for colName in keepCols:
  3633                                                       if trackMemory:
  3634                                                           print('writing motorData memory usage: {:.1f} MB'.format(
  3635                                                               prf.memory_usage_psutil()))
  3636                                                       chanIdx = ChannelIndex(
  3637                                                           name=colName,
  3638                                                           index=np.asarray([0]),
  3639                                                           channel_names=np.asarray([0]))
  3640                                                       block.channel_indexes.append(chanIdx)
  3641                                                       motorAsig = AnalogSignal(
  3642                                                           motorData[colName].to_numpy() * pq.mV,
  3643                                                           name=colName,
  3644                                                           sampling_rate=ainpAsig.sampling_rate,
  3645                                                           dtype=np.float32)
  3646                                                       motorAsig.t_start = ainpAsig.t_start
  3647                                                       motorAsig.channel_index = chanIdx
  3648                                                       # assign ownership to containers
  3649                                                       chanIdx.analogsignals.append(motorAsig)
  3650                                                       newSeg.analogsignals.append(motorAsig)
  3651                                                       chanIdx.create_relationship()
  3652                                                       newSeg.create_relationship()
  3653                                                       # write out to file
  3654                                                       motorAsig = writer._write_analogsignal(
  3655                                                           motorAsig, nixblock, nixgroup)
  3656                                                       del motorAsig
  3657                                                       gc.collect()
  3658                                                   _, trialEvents = mea.getTrials(
  3659                                                       motorData, ainpAsig.sampling_rate.magnitude,
  3660                                                       float(tStart.magnitude), trialType=None)
  3661                                                   trialEvents.fillna(0)
  3662                                                   trialEvents.rename(
  3663                                                       columns={
  3664                                                           'Label': 'rig_property',
  3665                                                           'Details': 'rig_value'},
  3666                                                       inplace=True)
  3667                                                   del motorData
  3668                                                   gc.collect()
  3669                                                   eventList = eventDataFrameToEvents(
  3670                                                       trialEvents,
  3671                                                       idxT='Time',
  3672                                                       annCol=['rig_property', 'rig_value'])
  3673                                                   for event in eventList:
  3674                                                       if trackMemory:
  3675                                                           print(
  3676                                                               'writing motor events memory usage: {:.1f} MB'
  3677                                                               .format(prf.memory_usage_psutil()))
  3678                                                       event.segment = newSeg
  3679                                                       newSeg.events.append(event)
  3680                                                       newSeg.create_relationship()
  3681                                                       # write out to file
  3682                                                       event = writer._write_event(event, nixblock, nixgroup)
  3683                                                       del event
  3684                                                       gc.collect()
  3685                                                   del trialEvents, eventList
  3686                                               #
  3687                                               for eventProxy in seg.events:
  3688                                                   event = eventProxy.load(
  3689                                                       time_slice=(tStart, tStop))
  3690                                                   event.t_start = tStart
  3691                                                   event.t_stop = tStop
  3692                                                   event.segment = newSeg
  3693                                                   newSeg.events.append(event)
  3694                                                   newSeg.create_relationship()
  3695                                                   # write out to file
  3696                                                   event = writer._write_event(event, nixblock, nixgroup)
  3697                                                   del event
  3698                                                   gc.collect()
  3699                                               #
  3700                                               for epochProxy in seg.epochs:
  3701                                                   epoch = epochProxy.load(
  3702                                                       time_slice=(tStart, tStop))
  3703                                                   epoch.t_start = tStart
  3704                                                   epoch.t_stop = tStop
  3705                                                   epoch.segment = newSeg
  3706                                                   newSeg.events.append(epoch)
  3707                                                   newSeg.create_relationship()
  3708                                                   # write out to file
  3709                                                   epoch = writer._write_epoch(epoch, nixblock, nixgroup)
  3710                                                   del epoch
  3711                                                   gc.collect()
  3712                                               #
  3713                                               chanIdxDiscardNames = []
  3714                                               # descend into ChannelIndexes
  3715                                               for chanIdx in block.channel_indexes:
  3716                                                   if chanIdx.analogsignals or chanIdx.units:
  3717                                                       chanIdx = writer._write_channelindex(chanIdx, nixblock)
  3718                                                   else:
  3719                                                       chanIdxDiscardNames.append(chanIdx.name)
  3720                                               block.channel_indexes = [
  3721                                                   i
  3722                                                   for i in block.channel_indexes
  3723                                                   if i.name not in chanIdxDiscardNames
  3724                                                   ]
  3725                                               writer._create_source_links(block, nixblock)
  3726                                               return

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: purgeNixAnn at line 3728

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3728                                           @profile
  3729                                           def purgeNixAnn(
  3730                                                   block, annNames=['nix_name', 'neo_name']):
  3731                                               for annName in annNames:
  3732                                                   block.annotations.pop(annName, None)
  3733                                               for child in block.children_recur:
  3734                                                   if child.annotations:
  3735                                                       child.annotations = {
  3736                                                           k: v
  3737                                                           for k, v in child.annotations.items()
  3738                                                           if k not in annNames}
  3739                                               for child in block.data_children_recur:
  3740                                                   if child.annotations:
  3741                                                       child.annotations = {
  3742                                                           k: v
  3743                                                           for k, v in child.annotations.items()
  3744                                                           if k not in annNames}
  3745                                               return block

Total time: 0.529503 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadContainerArrayAnn at line 3747

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3747                                           @profile
  3748                                           def loadContainerArrayAnn(
  3749                                                   container=None, trainList=None
  3750                                                   ):
  3751         1          6.0      6.0      0.0      assert (container is not None) or (trainList is not None)
  3752                                               #
  3753         1          5.0      5.0      0.0      spikesAndEvents = []
  3754         1          5.0      5.0      0.0      returnObj = []
  3755         1          5.0      5.0      0.0      if container is not None:
  3756                                                   #  need the line below! (RD: don't remember why, consider removing)
  3757         1       7492.0   7492.0      0.1          container.create_relationship()
  3758                                                   #
  3759         1          4.0      4.0      0.0          spikesAndEvents += (
  3760         1       8040.0   8040.0      0.2              container.filter(objects=SpikeTrain) +
  3761         1       7861.0   7861.0      0.1              container.filter(objects=Event)
  3762                                                       )
  3763         1          6.0      6.0      0.0          returnObj.append(container)
  3764         1          4.0      4.0      0.0      if trainList is not None:
  3765                                                   spikesAndEvents += trainList
  3766                                                   returnObj.append(trainList)
  3767                                               #
  3768         1          7.0      7.0      0.0      if len(returnObj) == 1:
  3769         1          5.0      5.0      0.0          returnObj = returnObj[0]
  3770                                               else:
  3771                                                   returnObj = tuple(returnObj)
  3772                                               #
  3773        40        317.0      7.9      0.0      for st in spikesAndEvents:
  3774        39    5271262.0 135160.6     99.6          st = loadObjArrayAnn(st)
  3775         1          8.0      8.0      0.0      return returnObj

Total time: 0.52414 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadObjArrayAnn at line 3777

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3777                                           @profile
  3778                                           def loadObjArrayAnn(st):
  3779        39        802.0     20.6      0.0      if 'arrayAnnNames' in st.annotations.keys():
  3780        39        508.0     13.0      0.0          if isinstance(st.annotations['arrayAnnNames'], str):
  3781                                                       st.annotations['arrayAnnNames'] = [st.annotations['arrayAnnNames']]
  3782        39        361.0      9.3      0.0          elif isinstance(st.annotations['arrayAnnNames'], tuple):
  3783                                                       st.annotations['arrayAnnNames'] = [i for i in st.annotations['arrayAnnNames']]
  3784                                                   #
  3785       507      44945.0     88.6      0.9          for key in st.annotations['arrayAnnNames']:
  3786                                                       #  fromRaw, the ann come back as tuple, need to recast
  3787       468       3083.0      6.6      0.1              try:
  3788       468     242724.0    518.6      4.6                  if len(st.times) == 1:
  3789                                                               st.annotations[key] = np.atleast_1d(st.annotations[key]).flatten()
  3790       468       4533.0      9.7      0.1                  st.array_annotations.update(
  3791       468    2090922.0   4467.8     39.9                      {key: np.asarray(st.annotations[key])})
  3792       468    2852001.0   6094.0     54.4                  st.annotations[key] = np.asarray(st.annotations[key])
  3793                                                       except Exception:
  3794                                                           print('Error with {}'.format(st.name))
  3795                                                           traceback.print_exc()
  3796                                                           pdb.set_trace()
  3797        39        451.0     11.6      0.0      if hasattr(st, 'waveforms'):
  3798        39        332.0      8.5      0.0          if st.waveforms is None:
  3799                                                       st.waveforms = np.asarray([]).reshape((0, 0, 0)) * pq.mV
  3800        39        473.0     12.1      0.0          elif not len(st.waveforms):
  3801                                                       st.waveforms = np.asarray([]).reshape((0, 0, 0)) * pq.mV
  3802        39        262.0      6.7      0.0      return st

Total time: 8.61409 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: loadWithArrayAnn at line 3804

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3804                                           @profile
  3805                                           def loadWithArrayAnn(
  3806                                                   dataPath, fromRaw=False,
  3807                                                   mapDF=None, reduceChannelIndexes=False):
  3808         1          4.0      4.0      0.0      if fromRaw:
  3809                                                   reader = nixio_fr.NixIO(filename=dataPath)
  3810                                                   block = readBlockFixNames(
  3811                                                       reader, lazy=False,
  3812                                                       mapDF=mapDF,
  3813                                                       reduceChannelIndexes=reduceChannelIndexes)
  3814                                               else:
  3815         1      78220.0  78220.0      0.1          reader = NixIO(filename=dataPath)
  3816         1   80104186.0 80104186.0     93.0          block = reader.read_block()
  3817                                                   # [un.name for un in block.filter(objects=Unit)]
  3818                                                   # [len(un.spiketrains) for un in block.filter(objects=Unit)]
  3819                                               
  3820         1    5296048.0 5296048.0      6.1      block = loadContainerArrayAnn(container=block)
  3821                                               
  3822         1          7.0      7.0      0.0      if fromRaw:
  3823                                                   reader.file.close()
  3824                                               else:
  3825         1     662447.0 662447.0      0.8          reader.close()
  3826         1         13.0     13.0      0.0      return block

Total time: 8.61419 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: blockFromPath at line 3828

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3828                                           @profile
  3829                                           def blockFromPath(
  3830                                                   dataPath, lazy=False, mapDF=None,
  3831                                                   reduceChannelIndexes=False, loadList=None,
  3832                                                   purgeNixNames=False, chunkingInfoPath=None):
  3833         1          9.0      9.0      0.0      chunkingMetadata = None
  3834         1          7.0      7.0      0.0      if chunkingInfoPath is not None:
  3835                                                   if os.path.exists(chunkingInfoPath):
  3836                                                       with open(chunkingInfoPath, 'r') as f:
  3837                                                           chunkingMetadata = json.load(f)
  3838         1          7.0      7.0      0.0      if chunkingMetadata is None:
  3839                                                   chunkingMetadata = {
  3840         1          6.0      6.0      0.0              '0': {
  3841         1          6.0      6.0      0.0                  'filename': dataPath,
  3842         1          6.0      6.0      0.0                  'partNameSuffix': '',
  3843         1          7.0      7.0      0.0                  'chunkTStart': 0,
  3844         1         10.0     10.0      0.0                  'chunkTStop': 'NaN'
  3845                                                       }}
  3846         2         40.0     20.0      0.0      for idx, (chunkIdxStr, chunkMeta) in enumerate(chunkingMetadata.items()):   
  3847         1          7.0      7.0      0.0          thisDataPath = chunkMeta['filename']
  3848         1        523.0    523.0      0.0          assert os.path.exists(thisDataPath)
  3849         1          9.0      9.0      0.0          if idx == 0:
  3850         1          7.0      7.0      0.0              if lazy:
  3851                                                           dataReader = nixio_fr.NixIO(
  3852                                                               filename=thisDataPath)
  3853                                                           dataBlock = readBlockFixNames(
  3854                                                               dataReader, lazy=lazy, mapDF=mapDF,
  3855                                                               reduceChannelIndexes=reduceChannelIndexes,
  3856                                                               purgeNixNames=purgeNixNames, loadList=loadList)
  3857                                                       else:
  3858         1          7.0      7.0      0.0                  dataReader = None
  3859         1   86141196.0 86141196.0    100.0                  dataBlock = loadWithArrayAnn(thisDataPath)
  3860                                                   else:
  3861                                                       if lazy:
  3862                                                           dataReader2 = nixio_fr.NixIO(
  3863                                                               filename=thisDataPath)
  3864                                                           dataBlock2 = readBlockFixNames(
  3865                                                               dataReader2, lazy=lazy, mapDF=mapDF,
  3866                                                               reduceChannelIndexes=reduceChannelIndexes, loadList=loadList)
  3867                                                       else:
  3868                                                           dataReader2 = None
  3869                                                           dataBlock2 = loadWithArrayAnn(thisDataPath)
  3870                                                       maxSegIdx = len(dataBlock.segments)
  3871                                                       typesNeedRenaming = [
  3872                                                           SpikeTrainProxy, AnalogSignalProxy, EventProxy,
  3873                                                           SpikeTrain, AnalogSignal, Event]
  3874                                                       for segIdx, seg in enumerate(dataBlock2.segments):
  3875                                                           if seg.name is None:
  3876                                                               seg.name = 'seg{}_'.format(maxSegIdx + segIdx)
  3877                                                           else:
  3878                                                               if 'seg{}_'.format(maxSegIdx + segIdx) not in seg.name:
  3879                                                                   seg.name = (
  3880                                                                       'seg{}_{}'
  3881                                                                       .format(
  3882                                                                           maxSegIdx + segIdx,
  3883                                                                           childBaseName(seg.name, 'seg')))
  3884                                                           for objType in typesNeedRenaming:
  3885                                                               for child in seg.filter(objects=objType):
  3886                                                                   if 'seg{}_'.format(maxSegIdx + segIdx) not in child.name:
  3887                                                                       child.name = (
  3888                                                                           'seg{}_{}'
  3889                                                                           .format(
  3890                                                                               maxSegIdx + segIdx, childBaseName(child.name, 'seg')))
  3891                                                       dataBlock.merge(dataBlock2)
  3892         1          8.0      8.0      0.0      return dataReader, dataBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: calcBinarizedArray at line 3894

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3894                                           @profile
  3895                                           def calcBinarizedArray(
  3896                                                   dataBlock, samplingRate,
  3897                                                   binnedSpikePath=None,
  3898                                                   saveToFile=True, matchT=None):
  3899                                               #
  3900                                               spikeMatBlock = Block(name=dataBlock.name + '_binarized')
  3901                                               spikeMatBlock.merge_annotations(dataBlock)
  3902                                               #
  3903                                               allSpikeTrains = [
  3904                                                   i for i in dataBlock.filter(objects=SpikeTrain)]
  3905                                               #
  3906                                               for st in allSpikeTrains:
  3907                                                   chanList = spikeMatBlock.filter(
  3908                                                       objects=ChannelIndex, name=st.unit.name)
  3909                                                   if not len(chanList):
  3910                                                       chanIdx = ChannelIndex(name=st.unit.name, index=np.asarray([0]))
  3911                                                       #  print(chanIdx.name)
  3912                                                       spikeMatBlock.channel_indexes.append(chanIdx)
  3913                                                       thisUnit = Unit(name=st.unit.name)
  3914                                                       chanIdx.units.append(thisUnit)
  3915                                                       thisUnit.channel_index = chanIdx
  3916                                               #
  3917                                               for segIdx, seg in enumerate(dataBlock.segments):
  3918                                                   newSeg = Segment(name='seg{}_{}'.format(segIdx, spikeMatBlock.name))
  3919                                                   newSeg.merge_annotations(seg)
  3920                                                   spikeMatBlock.segments.append(newSeg)
  3921                                                   #  tStart = dataBlock.segments[0].t_start
  3922                                                   #  tStop = dataBlock.segments[0].t_stop
  3923                                                   tStart = seg.t_start
  3924                                                   tStop = seg.t_stop
  3925                                                   # make dummy binary spike train, in case ths chan didn't fire
  3926                                                   segSpikeTrains = [
  3927                                                       i for i in seg.filter(objects=SpikeTrain) if '#' in i.name]
  3928                                                   dummyBin = binarize(
  3929                                                       segSpikeTrains[0],
  3930                                                       sampling_rate=samplingRate,
  3931                                                       t_start=tStart,
  3932                                                       t_stop=tStop + samplingRate ** -1) * 0
  3933                                                   for chanIdx in spikeMatBlock.channel_indexes:
  3934                                                       #  print(chanIdx.name)
  3935                                                       stList = seg.filter(
  3936                                                           objects=SpikeTrain,
  3937                                                           name='seg{}_{}'.format(segIdx, chanIdx.name)
  3938                                                           )
  3939                                                       if len(stList):
  3940                                                           st = stList[0]
  3941                                                           print('binarizing {}'.format(st.name))
  3942                                                           stBin = binarize(
  3943                                                               st,
  3944                                                               sampling_rate=samplingRate,
  3945                                                               t_start=tStart,
  3946                                                               t_stop=tStop + samplingRate ** -1)
  3947                                                           spikeMatBlock.segments[segIdx].spiketrains.append(st)
  3948                                                           #  to do: link st to spikematblock's chidx and units
  3949                                                           assert len(chanIdx.filter(objects=Unit)) == 1
  3950                                                           thisUnit = chanIdx.filter(objects=Unit)[0]
  3951                                                           thisUnit.spiketrains.append(st)
  3952                                                           st.unit = thisUnit
  3953                                                           st.segment = spikeMatBlock.segments[segIdx]
  3954                                                       else:
  3955                                                           print('{} has no spikes'.format(st.name))
  3956                                                           stBin = dummyBin
  3957                                                       skipStAnnNames = [
  3958                                                           'nix_name', 'neo_name', 'arrayAnnNames']
  3959                                                       if 'arrayAnnNames' in st.annotations:
  3960                                                           skipStAnnNames += list(st.annotations['arrayAnnNames'])
  3961                                                       asigAnn = {
  3962                                                           k: v
  3963                                                           for k, v in st.annotations.items()
  3964                                                           if k not in skipStAnnNames
  3965                                                           }
  3966                                                       asig = AnalogSignal(
  3967                                                           stBin * samplingRate,
  3968                                                           name='seg{}_{}_raster'.format(segIdx, st.unit.name),
  3969                                                           sampling_rate=samplingRate,
  3970                                                           dtype=np.int,
  3971                                                           **asigAnn)
  3972                                                       if matchT is not None:
  3973                                                           asig = asig[:matchT.shape[0], :]
  3974                                                       asig.t_start = tStart
  3975                                                       asig.annotate(binWidth=1 / samplingRate.magnitude)
  3976                                                       chanIdx.analogsignals.append(asig)
  3977                                                       asig.channel_index = chanIdx
  3978                                                       spikeMatBlock.segments[segIdx].analogsignals.append(asig)
  3979                                               #
  3980                                               for chanIdx in spikeMatBlock.channel_indexes:
  3981                                                   chanIdx.name = chanIdx.name + '_raster'
  3982                                               #
  3983                                               spikeMatBlock.create_relationship()
  3984                                               spikeMatBlock = purgeNixAnn(spikeMatBlock)
  3985                                               if saveToFile:
  3986                                                   if os.path.exists(binnedSpikePath):
  3987                                                       os.remove(binnedSpikePath)
  3988                                                   writer = NixIO(filename=binnedSpikePath)
  3989                                                   writer.write_block(spikeMatBlock, use_obj_names=True)
  3990                                                   writer.close()
  3991                                               return spikeMatBlock

Total time: 0 s
File: c:\users\peep sheep\nda2\data-analysis\dataAnalysis\preproc\ns5.py
Function: calcFR at line 3993

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
  3993                                           @profile
  3994                                           def calcFR(
  3995                                                   binnedPath, dataPath,
  3996                                                   suffix='fr', aggregateFun=None,
  3997                                                   chanNames=None, rasterOpts=None, verbose=False
  3998                                                   ):
  3999                                               print('Loading rasters...')
  4000                                               masterSpikeMats, _ = loadSpikeMats(
  4001                                                   binnedPath, rasterOpts,
  4002                                                   aggregateFun=aggregateFun,
  4003                                                   chans=chanNames,
  4004                                                   loadAll=True, checkReferences=False)
  4005                                               print('Loading data file...')
  4006                                               dataReader = nixio_fr.NixIO(
  4007                                                   filename=dataPath)
  4008                                               dataBlock = dataReader.read_block(
  4009                                                   block_index=0, lazy=True,
  4010                                                   signal_group_mode='split-all')
  4011                                               masterBlock = Block()
  4012                                               masterBlock.name = dataBlock.annotations['neo_name']
  4013                                               #
  4014                                               for segIdx, segSpikeMat in masterSpikeMats.items():
  4015                                                   print('Calculating FR for segment {}'.format(segIdx))
  4016                                                   spikeMatDF = segSpikeMat.reset_index().rename(
  4017                                                       columns={'bin': 't'})
  4018                                           
  4019                                                   dataSeg = dataBlock.segments[segIdx]
  4020                                                   dummyAsig = dataSeg.filter(
  4021                                                       objects=AnalogSignalProxy)[0].load(channel_indexes=[0])
  4022                                                   samplingRate = dummyAsig.sampling_rate
  4023                                                   newT = dummyAsig.times.magnitude
  4024                                                   spikeMatDF['t'] = spikeMatDF['t'] + newT[0]
  4025                                           
  4026                                                   segSpikeMatInterp = hf.interpolateDF(
  4027                                                       spikeMatDF, pd.Series(newT),
  4028                                                       kind='linear', fill_value=(0, 0),
  4029                                                       x='t')
  4030                                                   spikeMatBlockInterp = dataFrameToAnalogSignals(
  4031                                                       segSpikeMatInterp,
  4032                                                       idxT='t', useColNames=True,
  4033                                                       dataCol=segSpikeMatInterp.drop(columns='t').columns,
  4034                                                       samplingRate=samplingRate)
  4035                                                   spikeMatBlockInterp.name = dataBlock.annotations['neo_name']
  4036                                                   spikeMatBlockInterp.annotate(
  4037                                                       nix_name=dataBlock.annotations['neo_name'])
  4038                                                   spikeMatBlockInterp.segments[0].name = dataSeg.annotations['neo_name']
  4039                                                   spikeMatBlockInterp.segments[0].annotate(
  4040                                                       nix_name=dataSeg.annotations['neo_name'])
  4041                                                   asigList = spikeMatBlockInterp.filter(objects=AnalogSignal)
  4042                                                   for asig in asigList:
  4043                                                       asig.annotate(binWidth=rasterOpts['binWidth'])
  4044                                                       if '_raster' in asig.name:
  4045                                                           asig.name = asig.name.replace('_raster', '_' + suffix)
  4046                                                       asig.name = 'seg{}_{}'.format(segIdx, childBaseName(asig.name, 'seg'))
  4047                                                       asig.annotate(nix_name=asig.name)
  4048                                                   chanIdxList = spikeMatBlockInterp.filter(objects=ChannelIndex)
  4049                                                   for chanIdx in chanIdxList:
  4050                                                       if '_raster' in chanIdx.name:
  4051                                                           chanIdx.name = chanIdx.name.replace('_raster', '_' + suffix)
  4052                                                       chanIdx.annotate(nix_name=chanIdx.name)
  4053                                           
  4054                                                   # masterBlock.merge(spikeMatBlockInterp)
  4055                                                   frBlockPath = dataPath.replace('_analyze.nix', '_fr.nix')
  4056                                                   writer = NixIO(filename=frBlockPath)
  4057                                                   writer.write_block(spikeMatBlockInterp, use_obj_names=True)
  4058                                                   writer.close()
  4059                                               #
  4060                                               dataReader.file.close()
  4061                                               return masterBlock

